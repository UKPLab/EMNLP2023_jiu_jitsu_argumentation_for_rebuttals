reviews	canonical_rebuttals
"The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to ""directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems."" Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.' [SEP] 'rebuttal_mitigate-criticism"	This is, once again, motivated by making something work with modest amount of computation.
In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
This was a fun, albeit incremental paper.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
The empirical evaluation is quite weak- one sparsity setting, two baselines, one dataset' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
I do not think this work is ready for publication.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
The theoretical contribution is very limited.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The main issue of this paper is the fair comparisons with other works.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
Thus we may only apply the proposed model on a few tasks with exactly known F.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The paper misses the key baseline in Bayesian optimisation using tree structure [1] which can perform the prediction under the tree-structure dependencies.' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
One drawback is that it is highly specific to language models.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
- The premises of the analyses are not very convincing, limiting the significance of the paper.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
While better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.' [SEP] 'rebuttal_future	Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, the results are a bit misleading in their reporting of the std error.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
It is also not clear to me why these problems are important.' [SEP] 'rebuttal_reject-criticism	We are of course willing to further specify any details that the referee misses in the current paper. We would therefore like to kindly invite the referee to be specific about the details that he/she would like to be added to the manuscript.
* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)?' [SEP] 'rebuttal_structuring	Missing experiments to validate nature of bounds.
As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
1 The implementation steps of the proposed method (MoVE) are not clear.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
It is not clear how the compression ratio in table 1 is obtained.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
The clarity of this paper needs to be strengthened.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.' [SEP] 'rebuttal_answer	We based our title on that paper since it extends some of its results to nonlinear neural networks.
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
The difference with the other reference model (SVG) is less clear.' [SEP] 'rebuttal_done	We renamed all the models based on the original papers and their properties.
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- Lack of a strong explanation for the results or a solution to the problem' [SEP] 'rebuttal_done	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
Indeed, without referencing the original Pointer Network and (and especially the) Transformer papers, it would not be possible to understand this paper at all.' [SEP] 'rebuttal_done	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
6. Validation data / test sets: Throughout this work, it is unclear what / how validation is performed.' [SEP] 'rebuttal_mitigate-criticism	So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.
What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches?' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
Regarding the experiments on adversarial examples, I am not convinced of their relevance at all.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
The experiments were only done on simple image datasets.' [SEP] 'rebuttal_answer	In responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.
- Figures 1-4 are difficult to interpreted on a printed version.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).' [SEP] 'rebuttal_reject-criticism	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
However, this limits the novelty of the results relative to existing literature.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
Does the discriminator exclude the poisoning data according to certain rule?' [SEP] 'rebuttal_done	We haved add the above discussion to the latest version as Appendix A.9.
However, all experiments only show results in the MCAR setting, so the claim is not experimentally validated.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
"The paper also lacks experimental results, and the main conclusion from these results seems to be ""MNIST is not suitable for benchmarking of adversarial attacks"".' [SEP] 'rebuttal_social"	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
With lack of clear novel insights, or at least more systematic study on additional datasets of the 'winning' techniques and a sensitivity analysis, the paper does not give a valuable enough contribution to the field to merit publication.' [SEP] 'rebuttal_reject-criticism	On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models.
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
1) I wonder if the proposed method work for most GAN models, more experiments evaluated on more recent GAN-based  models should be added to verify the superiority claimed in this paper, e.g., TP-GAN [Huang et al., ICCV 2017], PIM [Zhao et al., CVPR 2018], DR-GAN [Tran et al., CVPR 2017], DA-GAN [Zhao et al., NIPS 2017], MH-Parser [Li et al., 2017], 3D-PIM [Zhao et al., IJCAI 2018], SimGAN [Shrivastava et al., CVPR 2016], AIM [Zhao et al., AAAI 2019].' [SEP] 'rebuttal_by-cr	Q1: We are evaluating the proposed metrics on more recent GAN-based models you suggested and will update the results once the results become available.
So at the end, I am a bit puzzled. I really like the idea, but I have the feeling that this technique should have been developed for more complicated setting. Or maybe it is actually not working on more difficult combinatorial problem (and this is hidden in the paper).' [SEP] 'rebuttal_by-cr	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
The experiments on SHREC17 show all three spherical methods under-performing other approaches.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.' [SEP] 'rebuttal_by-cr"	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
-3 For image-to-image translation experiments, no quantitative analysis whatsoever is offered so the reader can't really conclude anything about the effect of the proposed method in this domain.' [SEP] 'rebuttal_social	We appreciate such detailed and rigorous convergence analysis provided in [1] and [2].
This baseline was also missing in image reconstruction.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
*The experimental section is too limited.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
It is unclear, why one should use the proposed duality gap GAN.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
- The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
I believe that this paper is thus not in its final form and could be largely improved.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
"With this dataset, it's a bit of a stretch to say there was ""only a 1 point drop in BLEU score"". That's a significant drop, and in fact the DynamicConv paper goes to significant lengths to make a smaller 0.8 point improvement.' [SEP] 'rebuttal_done"	We will incorporate these results in the experiments section and post the updated manuscript shortly.
Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.' [SEP] 'rebuttal_structuring	3. Are the authors willing to release the code?
Cons:  unclear transfer learning model, insufficient experiments.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.' [SEP] 'rebuttal_answer	See ensuing discussion in p.18 following equation 36.
(3) A large body of graph neural network literature is omitted.' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
I believe it will not be great, but I think for completeness, you should add such a baseline.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
The figures are almost useless, because the captions contain very little information.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
Some details are missing, which is hardly reproduced by the other researchers.' [SEP] 'rebuttal_done	In addition to implementation details, the appendix has a rather detailed table of the architecture parameters.
This paper looks very hastily put together, especially pages 7 and 8.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
First, many details are missing, especially in the experiments, which makes the proposed method suspicious and non-convincing.' [SEP] 'rebuttal_answer	Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Although the idea seems to be interesting, the paper seems to be a bit incremental and is a simple application of existing GAN techniques.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
3. Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
The experimental results are not very convincing because many importance baselines are neglected.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
2. Although the experiments are detailed and interesting they support poor theoretical developments and use a very classical benchmark' [SEP] 'rebuttal_structuring	"3. ""Given the existing body of literature, I found the technical novelty of this paper rather weak"""
My overall concern is that the experiment result doesn’t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn’t really fit in the “transfer” learning scenario.' [SEP] 'rebuttal_answer	As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.
Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
They start from Equation (4) which is incorrectly denoted as the log-marginal distribution while it is the same conditional distribution introduced in Equation (3) with the extra summation for all the available data points.' [SEP] 'rebuttal_done	We added a new Section 4 in the revised version of the paper discussing these differences.
- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
"For example, ""The old system of private arbitration courts is off the table"" from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture.' [SEP] 'rebuttal_done"	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
This also becomes apparent in the experiments section, where rotational data augmentation is found to be necessary.' [SEP] 'rebuttal_done	In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.
However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
- Judging from Table 1, the proposed method does not seem to provide a large contribution.' [SEP] 'rebuttal_structuring	“Judging from Table 1, the proposed method does not seem to provide a large contribution.
Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.' [SEP] 'rebuttal_done	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
It is not even clear that the final compression of the baselines would not be better.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This was a fun, albeit incremental paper.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
I have doubts on applying the proposed method to higher dimensional inputs.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"""Table 4 shows that our first results are promising, even though they are not as good as the state of the art."" The state of the art on LibriSpeech is not Mohamed at al. 2019. See e.g. Irie et al. Interspeech 2019 for better result' [SEP] 'rebuttal_structuring"	-Q: Actionable consequences from paper:
2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem  or sample selection problem)' [SEP] 'rebuttal_answer	We think generating such justifications is a great next step and hope that our work will foster such interesting future research.
3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information.' [SEP] 'rebuttal_structuring"	1. Comments about the contributions and novelty
A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
The paper motivates the problem that we need to pick out an exploration sequence that optimizes learning progress, but then approximates it as simply measuring the return.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer's main comment that the experiments are not large scale.
Hence the theoretical sample complexities contributed are not comparable to those of MIME.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
In my opinion, to support the above claim, shouldn’t the authors provide a similar table as Table 1, directly comparing the certified radii of the natural images and adversarial images?' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
Since this work takes the approach of allowing stale weight updates, the author should also compare with existing distributed training approaches that use asynchronous updates, with or without model parallelism, for example, Dean et al., 2012.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- The writing looks very rushed, and should be improved.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
That said, I would like to see more analysis on the behavior of the proposed method under various interesting cases not tested yet.' [SEP] 'rebuttal_answer	Given the limited space provided, it would be difficult to fit a convergence analysis in our paper.
Optimizing compression rates should be done on the training set with a separate development set.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
Many of the results have been already presented in' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
However the theoretical contribution is poor. And the experiment uses a very classical benchmark providing simulated data.' [SEP] 'rebuttal_reject-criticism	We have added a sentence in the introduction emphasizing this crucial point.
The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN.' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
have no idea how confident the sampling based result is.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
The imagenet experiment lacks details.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
3. The graph neural networks used in the model are not described in the paper, only a reference to Paliwal et al (2019) is given.' [SEP] 'rebuttal_answer	As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.
- The shown inception scores are far from state-of-the-art.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Hence, I am not very sure whether the novelty of the paper is significant.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
- Lack of sufficient technical detail on models and dataset' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
Therefore, I don’t find interesting to report how DDGC improve upon “no baseline”, because known methods do even better.' [SEP] 'rebuttal_done	We also included the suggested related work (Balduzzi et al. 2018) in Section 5.
Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets.' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator.' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
* The biggest problem for me was the unconvincing results.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
The results of the paper do not give major insights into what are the preferred techniques for training GANs, and certainly not why and under what circumstances they'll work.' [SEP] 'rebuttal_done	We will update the labels in the ablation table to make this more clear.
However, this only affects the method of drawing the samples from a fixed known distribution and should have no more effect on the results than say a choice of a pseudo-random number generator.' [SEP] 'rebuttal_structuring	So, two responses are given below.
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I believe it will not be great, but I think for completeness, you should add such a baseline.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.' [SEP] 'rebuttal_summary	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
It is also not clear to me how domain translation is relevant to continual learning.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
3. This apart, I think that the experiment section is pretty hard to read, given all the metrics and methodology is in the Appendix.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
There were some experimental details that were poorly explained but in general the paper was readable.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
The current experimental settings are not matched with the practice environment.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
little improvements over the baselines or even significantly worse. More importantly,' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
If so it would be better to define the operator in a more general notation.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
1) The motivation is unclear and overall structure of the paper is confusing.' [SEP] 'rebuttal_done	We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
First of all, the setup for the AE and VAE is not specified.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
1) Provide stronger empirical results (these are not too convincing).' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Since this work takes the approach of allowing stale weight updates, the author should also compare with existing distributed training approaches that use asynchronous updates, with or without model parallelism, for example, Dean et al., 2012.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
First, In Theorem 2, which seems to be a main result of the paper, the authors were concerned with the condition when W_{ji} >0, but there is not conclusion if W_{ji} =0.' [SEP] 'rebuttal_done	To show that we can easily include these features, we have included in our appendix some results including non-structural features.
- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
"For example, I don't understand what does it mean in ""However, if training data is complete, ..... handle during missing data during test."" Another example would be the last few paragraphs on page 4; they are very unclear.' [SEP] 'rebuttal_answer"	Please see the last paragraph in page 5.
Ultimately this paper is interesting but falls well below the standards of exposition that I expect from a theory paper and doesn’t go very far at connecting the analysis back to the claimed motivation of investigating practical invariances. If the authors significantly improve the quality of the draft, I’ll be happy to revisit it and re-evaluate my score.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
There were some experimental details that were poorly explained but in general the paper was readable.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
2. The authors should provide ablation study and analysis of their CTAugment.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
The presented analysis seems to neglect the error term corresponding to the value function.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
We think that this is not enough, and more extensive experimental results would provide a better paper.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
It is also not clear to me why these problems are important.' [SEP] 'rebuttal_structuring	Re. somewhat limited contribution:
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
- It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.' [SEP] 'rebuttal_structuring	Q2. It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
"- There are better words than ""decent"" to describe the performance of DARTS, as it's very similar to the results in this work!' [SEP] 'rebuttal_answer"	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
Although I personally enjoyed reading the results that from control theory perspective are inline with GAN literature, the paper does not provide novel surprising results.' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
- The analysis in [1] handles the case of noisy updates, whereas the analysis given here only works for exact updates.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
Only some heuristic results are obtained for them without rigorous theory.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
The paper used very restricted Gaussian distributions for the formulation.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"The proposed method adapt a previously presented hierarchical clustering method in the ""standard space"" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model.' [SEP] 'rebuttal_structuring"	> I am reluctant to give a higher score due to its incremental contribution.
It is not clear how the compression ratio in table 1 is obtained.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
- Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch?' [SEP] 'rebuttal_done	We have already uploaded our source codes as well as the demonstration videos to the following sites.
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
* the idea of smoothing gradients is not new' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
It is also not clear to me why CIFAR datasets involve two domains and how these domains are relevant in each of the tasks.' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
The convergence analysis is on Z, not on parameters x and hyper-parameters theta.' [SEP] 'rebuttal_structuring	So, bounds here cannot be used to explain empirical observations in Section 5.
However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
2) The experimental results provided in this paper are weak.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
- Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016' [SEP] 'rebuttal_done	* In relation to the connection to IWAE, we have included a detailed discussion in Appendix E.1.
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Although some promising' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
Moreover, I don't think some of the presented experiments are necessary.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
The results are not strong. And, unfortunately, the model contribution currently is too modest.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
"The title claims way more than what is actually delivered in the paper, despite the fact that the authors have put in an ""On"" in the beginning of the title.' [SEP] 'rebuttal_done"	results in the updated paper.
Hence it is unclear how large the running time improvement is compared to a well-tuned baseline.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
So at the end, I am a bit puzzled. I really like the idea, but I have the feeling that this technique should have been developed for more complicated setting. Or maybe it is actually not working on more difficult combinatorial problem (and this is hidden in the paper).' [SEP] 'rebuttal_concede-criticism	We apologize if we painted an incorrect picture by calling it a “simple example” and a “staple introductory problem”.
- for Figure 6, there is not a clear conclusion.' [SEP] 'rebuttal_summary	There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
"And the analysis of the ""dynamic range"" of the algorithim is missing.' [SEP] 'rebuttal_future"	We leave more comprehensive studies on diversity to future work.
The paper has few really minor grammatical errors and typos. Please fix those before uploading the final draft.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
The results  are overall not very impressive.' [SEP] 'rebuttal_reject-criticism	[A]  We respectfully disagree.
Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
