reviews	canonical_rebuttals
The idea in this paper is novel but experiments do not seem to be enough.' [SEP] 'rebuttal_mitigate-criticism	This is, once again, motivated by making something work with modest amount of computation.
I also find weird the way that the authors arrive to their final objective in Equation (5).' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
- For semi-supervised classification, the paper did not report the best results in other baselines.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information.' [SEP] 'rebuttal_mitigate-criticism"	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The main drawback of the paper is that it seems to be more engineering-focused, and doesn’t provide much insight into semi-supervised learning.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Overall the paper, while interesting is unacceptably messy.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
There are some well-cited works that the authors may have missed. These are ultimately different approaches, but perhaps the authors can obtain some inspiration from these:' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
4. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.” However, they do not show any analysis to support this hypothesis.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
When I look at Figure 4abcd, it appears that the Convolution and Dilated Convolutions fit a clean signal faster (it is just not as clean.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- (W3) Baselines for transfer learning: I felt this was another notable oversight.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
However such problems are entirely missing in the results section.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
In terms of writing, the paper is a bit confusing in terms of motivations and notations.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It did also not match any numbers in Tab. 4 of the appendix.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"For example, I have trouble understanding the sentence ""So the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets."" in the introduction.' [SEP] 'rebuttal_mitigate-criticism"	So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.
"2.	Data size is too small, and the baselines' [SEP] 'rebuttal_future"	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
The results are not strong. And, unfortunately, the model contribution currently is too modest.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
Third, the datasets used in this paper are rather limited.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
- For the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbel-sinkhorn.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
=> Baselines: The comparison provided in the paper is weak.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
(1) The experimental results cannot show the usefulness of the proposed GCN.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
In conclusion, I suggest a reject of this paper due to the lacks of comprehensive study and evaluation.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The paper misses the key baseline in Bayesian optimisation using tree structure [1] which can perform the prediction under the tree-structure dependencies.' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
• Not all of the arrows in Figure 1 are pointing to the right lines.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
The paper is also littered with typos and vague statements (many enumerated below under *small issues*).' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
It would be nice if the authors pointed to a git repository with their code an experiments.' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
Table 8 rises some concerns.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
How does the proposed method perform in more complicated tasks such as' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Again, this follows from known results.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
(2) The function of the discriminator is not very clear, especially for the classification error test.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
* The biggest problem for me was the unconvincing results.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
First, In Theorem 2, which seems to be a main result of the paper, the authors were concerned with the condition when W_{ji} >0, but there is not conclusion if W_{ji} =0.' [SEP] 'rebuttal_structuring	So, two responses are given below.
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
* The paper states multiple times that VAEAC [Ivanov et al., 2019] cannot handle partially missing data, but I don’t think this is true, since their missing features imputation experiment uses the setup of 50% truly missing features.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
Lemma 3 is too trivial.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
As the approach uses NF is derived specifically for unstable dynamics, it is not clear to me how adding it would affect the training if the dynamics *is stable*. In this context, I consider that for example, the work by Balduzzi et al. 2018 may be relevant as it describes that the dynamics of games (the Jacobian) has two components, one of which describes the oscillating behavior (Hamiltonian game); whereas most games are a mix of oscillating and non-oscillating dynamics.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In general, the space that was used to explain the Transformer baselines---which are essentially straightforward ways to adapt transformers to this task---could have been used to give more detail on the dataset.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
In the experiment there is no details on how you set the hyperparameters of CW and EAD.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
One thing that puts me off is that, in Table 2, AnyBurl (the single one baseline authors considered other than the original NeuralLP) yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.' [SEP] 'rebuttal_by-cr	Thanks for pointing this out! We will make the presentation of the results consistent by highlighting the respective number.
As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
What's the relation between the size of a model and that of a data set? By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
The paper has an interesting potential but seems a bit limited in its present form.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
"Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being ""meta domain adaptation"".' [SEP] 'rebuttal_done"	Fig. 3 and other evaluations have been updated for the new test set.
There is a key concern about the feasibility of the numerical analysis for the first part.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
There has been a large body of work which shows that weaker attacks like membership attacks can be equally damaging, ii) Privacy is a worst-case guarantee.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
- It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It is also not clear how this theoretical result can shed insight on the empirical study of neural networks.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Is Harmonic Convolution applicable to complex STFT coefficients as well?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Overall, while I find the proposed approach simple -- the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
It would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
"1.	Lack of technical novelty.' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Although I personally enjoyed reading the results that from control theory perspective are inline with GAN literature, the paper does not provide novel surprising results.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
Also, please place the related work earlier on in the paper.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
Indeed, the manuscript introduces sample complexity results to justify the benefits of the out-of-sample procedure (th 1), but it seems to me that these give an incomplete picture.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
