reviews	canonical_rebuttals
The idea in this paper is novel but experiments do not seem to be enough.' [SEP] 'rebuttal_mitigate-criticism	This is, once again, motivated by making something work with modest amount of computation.
The paper motivates the problem that we need to pick out an exploration sequence that optimizes learning progress, but then approximates it as simply measuring the return.' [SEP] 'rebuttal_mitigate-criticism	This is, once again, motivated by making something work with modest amount of computation.
I also find weird the way that the authors arrive to their final objective in Equation (5).' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
The proposed method PowerSGD is an extension of the method in Yuan et al. (extended to handle stochastic gradient and momentum).' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
My main concerns are about the evaluation and comparison of standard neural models.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
- For semi-supervised classification, the paper did not report the best results in other baselines.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
There should be a better discussion of related work on the topic.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information.' [SEP] 'rebuttal_mitigate-criticism"	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The main drawback of the paper is that it seems to be more engineering-focused, and doesn’t provide much insight into semi-supervised learning.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
- limited amount of new insight, which is limiting as new and better understanding of GANs and practical guidelines are arguably the main contribution of a work of this type' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Overall the paper, while interesting is unacceptably messy.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
The notations are overall confusing and not explained well.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
There are some well-cited works that the authors may have missed. These are ultimately different approaches, but perhaps the authors can obtain some inspiration from these:' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
5, Although there is a notable difference, one may properly mention previous work Yamamoto et al. (2019), which uses GAN as an auxiliary loss within ClariNet and obtains high-fidelity speech ( https://r9y9.github.io/demos/projects/interspeech2019/ ).' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
On the other hand, it is hard to say what the _main_ contribution is, which in turn makes it difficult to evaluate whether the experimental evaluation is sufficient:' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
4. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.” However, they do not show any analysis to support this hypothesis.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
A proper baseline should have been compared.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
6, the experimental design of Sec. 4.2 is also a bit unfair.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
(2) The method is not well motivated.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
However it's not clear how is this range used in practice ? Do you sample uniformly $\alpha$ in this range to train the linear interpolation ?' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
When I look at Figure 4abcd, it appears that the Convolution and Dilated Convolutions fit a clean signal faster (it is just not as clean.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
- In Table 1, for ImageNet, Shadow Attach does not always generate adversarial examples that have on average larger certified radii than the natural parallel, at least for sigma=0.5 and 1.0. Could the authors explain the reason?' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
For example, one question is how often a single partial tree has multiple possible completions in the data.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- (W3) Baselines for transfer learning: I felt this was another notable oversight.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
However such problems are entirely missing in the results section.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
In terms of writing, the paper is a bit confusing in terms of motivations and notations.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It did also not match any numbers in Tab. 4 of the appendix.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- Architecture choice unclear: Why are $\sigma$ and $\omega$ separate networks.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"For example, I have trouble understanding the sentence ""So the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets."" in the introduction.' [SEP] 'rebuttal_mitigate-criticism"	So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.
* The issue of possible false positives due to sample dependence in fMRI data has again been ignored in the rebuttal.' [SEP] 'rebuttal_mitigate-criticism	So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.
"2.	Data size is too small, and the baselines' [SEP] 'rebuttal_future"	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
First, the labeled data portion is fixed and is relatively high' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Second, SST itself is only comparable with or even worse than the state-of-art methods.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- In Section 3.1 : “Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.” For me, this is not particularly clear.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
The results are not strong. And, unfortunately, the model contribution currently is too modest.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
Many of the results have been already presented in' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
Third, the datasets used in this paper are rather limited.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
- For the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbel-sinkhorn.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
The message of synthetic experiments would be stronger if more of them were available and if the comparison between LOE, TSTE, and OENN was made on more of them.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
I think all claims about running time should be corroborated by controlled experiments.' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
=> Baselines: The comparison provided in the paper is weak.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
4. Comparison with past works.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
However image captioning datasets are not mentioned.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
(1) The experimental results cannot show the usefulness of the proposed GCN.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
One observation from the submission is that the token set may need to very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive (I noticed that the BERT model is trained on 128 GPUs)' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
- The performance gain is not substantial in experiments.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
In conclusion, I suggest a reject of this paper due to the lacks of comprehensive study and evaluation.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
The paper has few really minor grammatical errors and typos. Please fix those before uploading the final draft.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
"8. ""Beyond fixed schedules, automatically adjusting the training of G and D remains untacked"" -- this is not 100% true.' [SEP] 'rebuttal_structuring"	Remark 1. Expression and detail
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
something that is either deterministic, or a probabilistic result with a small' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
p2-3, Section 3.1 - I found the equations impossible to read. What' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The paper misses the key baseline in Bayesian optimisation using tree structure [1] which can perform the prediction under the tree-structure dependencies.' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
"5. The term ""PowerSGD"" seems to have been used by other papers.' [SEP] 'rebuttal_done"	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
• Not all of the arrows in Figure 1 are pointing to the right lines.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
4. Fig. 3 (right): It is not clear' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
The paper is also littered with typos and vague statements (many enumerated below under *small issues*).' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
- some parts of the paper are quite unclear' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
It would be nice if the authors pointed to a git repository with their code an experiments.' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
- The hyperparameter selection regime (and the experiments used to find them) is not described' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
If the two NNs used in DNN and DNN(resized) are different, I believe it’s still possible to apply the algorithm in DNN(resized) to the NN used in DNN, and get a full trace in the figure as noise and projection changes, which would lead to more fair comparison.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The egocentric velocity field is not described (section 5)' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The paper is overall well written and intuitive but limited in evaluation and novelty (see e.g. [1,2] ) with only limited modifications (sharing low-level controller) for the multi agent case.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
Thus, it might be helpful to test the result on another dataset (e.g. WikiText).' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
Table 8 rises some concerns.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
(b) a significant clarification of Figure 4.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
But there are no comparisons between the proposed training method and previous related works.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
How does the proposed method perform in more complicated tasks such as' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Results on more scenes will make the performance more convincing.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Is the number right?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Again, this follows from known results.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
It looks like all the results are given on the test set. Did you not do any tuning on the validation data?' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
(2) The function of the discriminator is not very clear, especially for the classification error test.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
If so it would be better to define the operator in a more general notation.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
The proposed sampling distributions assumes independence between the random variables over which the authors optimize — I find it surprising that this leads to good empirical results' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
"- In figure 3 (c) ""number |T of input"" should be  ""number |T| of input""' [SEP] 'rebuttal_done"	Thanks for this; we have updated the draft to make the presentation clearer.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
However, my concern is about the experiments.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
* The biggest problem for me was the unconvincing results.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
First, In Theorem 2, which seems to be a main result of the paper, the authors were concerned with the condition when W_{ji} >0, but there is not conclusion if W_{ji} =0.' [SEP] 'rebuttal_structuring	So, two responses are given below.
2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper.' [SEP] 'rebuttal_structuring	So, two responses are given below.
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- There is a typo in equation 6' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
* The paper states multiple times that VAEAC [Ivanov et al., 2019] cannot handle partially missing data, but I don’t think this is true, since their missing features imputation experiment uses the setup of 50% truly missing features.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
Lemma 3 is too trivial.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
As the approach uses NF is derived specifically for unstable dynamics, it is not clear to me how adding it would affect the training if the dynamics *is stable*. In this context, I consider that for example, the work by Balduzzi et al. 2018 may be relevant as it describes that the dynamics of games (the Jacobian) has two components, one of which describes the oscillating behavior (Hamiltonian game); whereas most games are a mix of oscillating and non-oscillating dynamics.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
"- In the appendix, the statement ""Sarkar (2011) show that a similar statement as in Theorem 2 holds for a very general class of trees"" is confusing to me. The ""general class"", as far as I know, is actually *all* trees, weighted or unweighted.' [SEP] 'rebuttal_structuring"	Q4. Updated abstract and performance evaluation.
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In general, the space that was used to explain the Transformer baselines---which are essentially straightforward ways to adapt transformers to this task---could have been used to give more detail on the dataset.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
In the experiment there is no details on how you set the hyperparameters of CW and EAD.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
"2) It would be great if the paper can clearly define the experiments: ""waypoint"", ""oncoming"", ""mall"", and ""bottleneck"".' [SEP] 'rebuttal_answer"	We added new experiments showing the agent can learn to manipulate two balls.
In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
section 6.3, the authors show an experiments in this case, but only on a dense' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
One thing that puts me off is that, in Table 2, AnyBurl (the single one baseline authors considered other than the original NeuralLP) yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.' [SEP] 'rebuttal_by-cr	Thanks for pointing this out! We will make the presentation of the results consistent by highlighting the respective number.
ii) In table 2, I don’t really see any promising results compared to baselines. There are' [SEP] 'rebuttal_by-cr	Thanks for pointing this out! We will make the presentation of the results consistent by highlighting the respective number.
As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
The model is not well motivated and the optimization algorithm is also not well described.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
little improvements over the baselines or even significantly worse. More importantly,' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
What's the relation between the size of a model and that of a data set? By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
More precisely, for the digit datasets, the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
e. What are the two modalities in Table 2? The author should explain.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
- No comparison has been made between their approach and other previous approaches.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
The paper has an interesting potential but seems a bit limited in its present form.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
The paper has few really minor grammatical errors and typos. Please fix those before uploading the final draft.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
"Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being ""meta domain adaptation"".' [SEP] 'rebuttal_done"	Fig. 3 and other evaluations have been updated for the new test set.
First, I consider the tabular features as multi-feature data and less to be the multimodal data.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
There is a key concern about the feasibility of the numerical analysis for the first part.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
To evaluate the impact of the proposed changes in this paper, one would have to perform extended evaluations and ablations for the submission.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
which is much smaller than the number of time series usually involved say in gene regulatory network data' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
There has been a large body of work which shows that weaker attacks like membership attacks can be equally damaging, ii) Privacy is a worst-case guarantee.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
- It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
The convergence analysis is on Z, not on parameters x and hyper-parameters theta.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It is also not clear how this theoretical result can shed insight on the empirical study of neural networks.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Is Harmonic Convolution applicable to complex STFT coefficients as well?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Overall, while I find the proposed approach simple -- the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
And finally, the discussion of this figure makes claims about the behaviour of the model that seems to be too strong to be based on a single image experiment.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
It would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
"1.	Lack of technical novelty.' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Although I personally enjoyed reading the results that from control theory perspective are inline with GAN literature, the paper does not provide novel surprising results.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
Hence, I kindly do not think the outcome is truly a research result.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_done"	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
Also, please place the related work earlier on in the paper.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
Indeed, the manuscript introduces sample complexity results to justify the benefits of the out-of-sample procedure (th 1), but it seems to me that these give an incomplete picture.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
