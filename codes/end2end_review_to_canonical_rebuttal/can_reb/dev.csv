reviews	canonical_rebuttals
My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
That is, authors assumed that they have a degradation function F and all the inference process is just based on this known function.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
Thus we may only apply the proposed model on a few tasks with exactly known F.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
If the authors don't discuss a motivation then how will a reader know how to apply the tool?' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
- My biggest concern is that the technical contributions of the paper are not clear at all.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
This very much limits the utility of the method.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
Hence, the effectiveness and advantage of the proposed methods are not clear.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
I don’t think it is of any practical use to show that a new algorithm (such at DDGD) provide some defence compared to no defence.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
(2) The method is not well motivated.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
The model is not well motivated and the optimization algorithm is also not well described.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
I like the area of research the authors are looking into and I think it's an important application. However, the paper doesn't answer key questions about both the application and the models:' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
One drawback is that it is highly specific to language models.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
In addition, there is not much theoretical justification for it, it seems like a one-off trick.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
The main drawback of the paper is that it seems to be more engineering-focused, and doesn’t provide much insight into semi-supervised learning.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
The privacy definition employed in this work is problematic.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
There are also concerns about the motivations behind parts of the technique.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
Although the paper proposes an interesting framework I would argue that it is a “green apple” in the sense that authors need to motivate the approach better and expand the contribution beyond solving a particular instance.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
- limited amount of new insight, which is limiting as new and better understanding of GANs and practical guidelines are arguably the main contribution of a work of this type' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
* I found it difficult to follow the theoretical motivation for performing the work.' [SEP] 'rebuttal_reject-criticism	All of these insights are supported by a fair and unbiased rigorous experimental process.
The convergence analysis is on Z, not on parameters x and hyper-parameters theta.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
- The analysis in [1] handles the case of noisy updates, whereas the analysis given here only works for exact updates.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
The presented analysis seems to neglect the error term corresponding to the value function.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
Moreover, as observed by the authors this analysis currently rely on strong assumptions that might make it rather unrealistic.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
There is a key concern about the feasibility of the numerical analysis for the first part.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
Moreover, if I understand correctly the WGAN analysis does not take into account that G and D are non-linear, and it is unclear if these can be done.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
- It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
To evaluate the impact of the proposed changes in this paper, one would have to perform extended evaluations and ablations for the submission.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
"2.	What is the reason for using rule-based agents in all the experiments? It would have been more useful if all the analysis are done with RL agents rather than rule-based agents.' [SEP] 'rebuttal_done"	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
This has not been considered in the analysis.' [SEP] 'rebuttal_done	Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The equation (1) should hold for any \theta’, not \theta.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The equation (1) should contain \rho, not p.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
For example, it is unclear to me why some larger models are not amenable to truncation.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
From the supplementary, it seems Epsilon means the environment?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"Fourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The maths is not clear, in particular the gradient derivation in equation (8).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
But again, it's not super clear how the paper estimates this derivative.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
1. Are e_{i,t} and lambda_{i,t} vectors?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In the algorithm, the authors need to define the HT function in (3) and (4).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"I take issue with the usage of the phrase ""skill discovery"".' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It is in the end plugged into a continual learning algorithm which also performs domain transformation.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The purpose of the public set is explained only in section 5.2.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, I don't understand the use of $\alpha$ here.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
* Equations (1, 2): z and \phi are not consistently boldfaced' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I believe that the presentation of the proposed method can be significantly improved.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The method description was a bit confusing and unclear to me.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- the sentence under eq. (2)' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"4. I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
While the general description of the model is clear, details are lacking.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Similarly, you did not indicate what the deterministic version of your model is.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
-Some technical details  are missing.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Because of the above many discussions about discrete vs. continuous variables are missleading.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Many of the parameters here are also unclear and not properly defined/introduced.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
What is the relationship between $\theta$ and $\tilde\theta$ exactly?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The 3rd line of lemma 1: epsilon1 -> epsilon_1' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Page 14, Eq(14), \lambda should be s' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"Some statements don't make sense, however, eg. ""HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This makes the contribution of this paper in terms of the method' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- What is dt in Algorithm 1 description?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In later sections they use theta and theta’ for encoder/decoder resp.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Why mention it here, if it's not being defined.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Minor, 1/2 is missing in the last line of Eq 19.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Also perhaps better to use the curly sign for vector inequality.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
“From a high-level perspective both of these approaches” --> missing comma after “perspective”' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In equations 1 and 2, should a, b be written in capital? Since they represent random variables.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- There is a typo in equation 6' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The paper seems to lack clarity on certain design/ architecture/ model decisions.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Also, I had to go through a large chunk of the paper before coming across the exact setup.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I did not completely follow the arguments towards directed graph deconvolution operators.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I also struggled a little to understand what is the difference between forward interpolate and filtering.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
1) There is no clear rationale on why we need a new model based on Transformers for this task.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I am not convinced that the measure theoretic perspective is always' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
4.4, law of total variation -- define' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, what was not clear to me is how this reduces the non-stationarity of MARL.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- What is the choice of beta in the beta-VAE training objective?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
1. What is M in Algorithm 1 ?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Not sure why Eqns. 2 and 9 need any parentheses' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Generalizing this to the multi-channel input as the next step could make the proof more accessible.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The egocentric velocity field is not described (section 5)' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, the  transfer learning model is unclear.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I have doubts on applying the proposed method to higher dimensional inputs.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
ReLU network with 2 hidden layers, and it is unknown if it works in general.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
positive - it unlikely to be true that an undefended network is predominantly' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
adversarial examples (or counter-examples for property verification) with L_inf' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Your F and \tilde{f} are introduced as infinite series.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Isn't this just restating the point made in the first sentence?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
3, The notations in Eq. (1) and (2) are messy.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It is unclear how well the proposed method works in general.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In the start of Section 3, it is not clear why having the projection be sparse is desired.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
eqn (8): use something else to denote the function 'U'.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This clipping will also introduce bias, this is not discussed, and will probably lower variance.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It would be better if the authors were a little more careful in their use of terminology here.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
*The strategy proposed to introduce weak-supervision is too ad-hoc.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- U^m in Eq 1 is undefined and un-discussed.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The term p(w) disappears on the left hand side of Eq 2.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
p2-3, Section 3.1 - I found the equations impossible to read. What' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It is unclear how the model actually operates and uses attention during execution.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"-	Attention should be given to the notation in formulas (3) and (4).' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The projection function there is no longer accepts a 3D point parametrized by 3 variables.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Instead only depth is provided.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In addition, the subindex ‘1’ of the point ‘q’ is not explained.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Making this algorithm not very practical.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
So the closure axiom of a group is violated.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Did you try to have a single network?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
How does the transformer based method comparing to others?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Lemma 3 is too trivial.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
o feedforward rather than recurrent network;' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
Is that also true in this domain?' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_social"	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_social	We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_by-cr"	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_by-cr"	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Making this algorithm not very practical.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_by-cr"	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
So the closure axiom of a group is violated.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Did you try to have a single network?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_by-cr"	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
How does the transformer based method comparing to others?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_by-cr"	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_by-cr"	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_by-cr"	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_by-cr"	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_by-cr"	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_by-cr"	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Lemma 3 is too trivial.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
o feedforward rather than recurrent network;' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
Is that also true in this domain?' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_by-cr"	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_by-cr	We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_mitigate-criticism"	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_mitigate-criticism"	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Making this algorithm not very practical.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_mitigate-criticism"	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
So the closure axiom of a group is violated.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Did you try to have a single network?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_mitigate-criticism"	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
How does the transformer based method comparing to others?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_mitigate-criticism"	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_mitigate-criticism"	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_mitigate-criticism"	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_mitigate-criticism"	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_mitigate-criticism"	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_mitigate-criticism"	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Lemma 3 is too trivial.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
o feedforward rather than recurrent network;' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
Is that also true in this domain?' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_mitigate-criticism"	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_mitigate-criticism	We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.
5. lack of related work:  4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR 2019.' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
2. Missed citation: MnasNet [2] also incorporates the cost of architectures in their search process.' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
(3) A large body of graph neural network literature is omitted.' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
This is a thriving area that requires a careful literature review.' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
2) Related work, although extensive in terms of the number of references, do not help to place this work in the literature.' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
Listing related work is no the same as describing similarities and differences compared to previous methods.' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
The paper misses the key baseline in Bayesian optimisation using tree structure [1] which can perform the prediction under the tree-structure dependencies.' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
There are some well-cited works that the authors may have missed. These are ultimately different approaches, but perhaps the authors can obtain some inspiration from these:' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
5, Although there is a notable difference, one may properly mention previous work Yamamoto et al. (2019), which uses GAN as an auxiliary loss within ClariNet and obtains high-fidelity speech ( https://r9y9.github.io/demos/projects/interspeech2019/ ).' [SEP] 'rebuttal_concede-criticism	We missed this previous work.
"5. The term ""PowerSGD"" seems to have been used by other papers.' [SEP] 'rebuttal_concede-criticism"	We missed this previous work.
I do not see much insight into the problem.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
The main problem with this paper is that it is difficult to identify its main and novel contributions.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
At the conceptual level, the idea of jointly modeling local video events is not novel, and can date back to at least ten years ago in the paper “Learning realistic human actions from movies”, where the temporal pyramid matching was combined with the bag-of-visual-words framework to capture long-term temporal structure.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
The idea of having a separate class for out-distribution is a very interesting idea but unfortunately previously explored.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
The idea that introduces labels in VAE is not novel.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
At a high level, the idea of imposing a mixture of gaussian structure in the feature space of a deep neural network classifier is not new.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
The idea to extend the use of VAE-GANs to the video prediction setting is a pretty natural one and not especially novel.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
- The idea is a simple extension of existing work.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
Although the idea seems to be interesting, the paper seems to be a bit incremental and is a simple application of existing GAN techniques.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
ii) Although the idea of generating contrastive explanations is quite interesting, it is not that novel.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
Last comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
* the idea of smoothing gradients is not new' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
- the idea is trivial and simple. I don't think there's significant novelty here: all the components are existing and combining them seems very trivial to me.' [SEP] 'rebuttal_answer	After taking a close look, we make the following observations:
