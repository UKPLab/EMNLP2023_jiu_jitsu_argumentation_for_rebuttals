reviews	canonical_rebuttals
My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
That is, authors assumed that they have a degradation function F and all the inference process is just based on this known function.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
Thus we may only apply the proposed model on a few tasks with exactly known F.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
If the authors don't discuss a motivation then how will a reader know how to apply the tool?' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
- My biggest concern is that the technical contributions of the paper are not clear at all.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
This very much limits the utility of the method.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
Hence, the effectiveness and advantage of the proposed methods are not clear.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
I don’t think it is of any practical use to show that a new algorithm (such at DDGD) provide some defence compared to no defence.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
(2) The method is not well motivated.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
The model is not well motivated and the optimization algorithm is also not well described.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
I like the area of research the authors are looking into and I think it's an important application. However, the paper doesn't answer key questions about both the application and the models:' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
One drawback is that it is highly specific to language models.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
In addition, there is not much theoretical justification for it, it seems like a one-off trick.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
The main drawback of the paper is that it seems to be more engineering-focused, and doesn’t provide much insight into semi-supervised learning.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
The privacy definition employed in this work is problematic.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
There are also concerns about the motivations behind parts of the technique.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
Although the paper proposes an interesting framework I would argue that it is a “green apple” in the sense that authors need to motivate the approach better and expand the contribution beyond solving a particular instance.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
- limited amount of new insight, which is limiting as new and better understanding of GANs and practical guidelines are arguably the main contribution of a work of this type' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
* I found it difficult to follow the theoretical motivation for performing the work.' [SEP] 'rebuttal_structuring	[Q] Limited amount of new insight.
My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
That is, authors assumed that they have a degradation function F and all the inference process is just based on this known function.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
Thus we may only apply the proposed model on a few tasks with exactly known F.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
If the authors don't discuss a motivation then how will a reader know how to apply the tool?' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
- My biggest concern is that the technical contributions of the paper are not clear at all.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
This very much limits the utility of the method.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
Hence, the effectiveness and advantage of the proposed methods are not clear.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
I don’t think it is of any practical use to show that a new algorithm (such at DDGD) provide some defence compared to no defence.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
(2) The method is not well motivated.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
The model is not well motivated and the optimization algorithm is also not well described.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
I like the area of research the authors are looking into and I think it's an important application. However, the paper doesn't answer key questions about both the application and the models:' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
One drawback is that it is highly specific to language models.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
In addition, there is not much theoretical justification for it, it seems like a one-off trick.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
The main drawback of the paper is that it seems to be more engineering-focused, and doesn’t provide much insight into semi-supervised learning.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
The privacy definition employed in this work is problematic.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
There are also concerns about the motivations behind parts of the technique.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
Although the paper proposes an interesting framework I would argue that it is a “green apple” in the sense that authors need to motivate the approach better and expand the contribution beyond solving a particular instance.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
- limited amount of new insight, which is limiting as new and better understanding of GANs and practical guidelines are arguably the main contribution of a work of this type' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
* I found it difficult to follow the theoretical motivation for performing the work.' [SEP] 'rebuttal_by-cr	We will publish the code to compute conductance after the blind-review phase.
My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
That is, authors assumed that they have a degradation function F and all the inference process is just based on this known function.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
Thus we may only apply the proposed model on a few tasks with exactly known F.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
If the authors don't discuss a motivation then how will a reader know how to apply the tool?' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
- My biggest concern is that the technical contributions of the paper are not clear at all.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
This very much limits the utility of the method.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
Hence, the effectiveness and advantage of the proposed methods are not clear.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
I don’t think it is of any practical use to show that a new algorithm (such at DDGD) provide some defence compared to no defence.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
(2) The method is not well motivated.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
The model is not well motivated and the optimization algorithm is also not well described.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
I like the area of research the authors are looking into and I think it's an important application. However, the paper doesn't answer key questions about both the application and the models:' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
One drawback is that it is highly specific to language models.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
In addition, there is not much theoretical justification for it, it seems like a one-off trick.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
The main drawback of the paper is that it seems to be more engineering-focused, and doesn’t provide much insight into semi-supervised learning.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
The privacy definition employed in this work is problematic.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
There are also concerns about the motivations behind parts of the technique.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
Although the paper proposes an interesting framework I would argue that it is a “green apple” in the sense that authors need to motivate the approach better and expand the contribution beyond solving a particular instance.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
- limited amount of new insight, which is limiting as new and better understanding of GANs and practical guidelines are arguably the main contribution of a work of this type' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
* I found it difficult to follow the theoretical motivation for performing the work.' [SEP] 'rebuttal_done	In the new revision, we have added a discussion section to make a case for this.
The results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?' [SEP] 'rebuttal_done	We also included the suggested related work (Balduzzi et al. 2018) in Section 5.
"The performance of the proposed method is worse than the previous work but they claimed ""state-of-the-art"" results.' [SEP] 'rebuttal_done"	We also included the suggested related work (Balduzzi et al. 2018) in Section 5.
"- In the appendix, the statement ""Sarkar (2011) show that a similar statement as in Theorem 2 holds for a very general class of trees"" is confusing to me. The ""general class"", as far as I know, is actually *all* trees, weighted or unweighted.' [SEP] 'rebuttal_done"	We also included the suggested related work (Balduzzi et al. 2018) in Section 5.
I wonder the motivation of analyzing generalization of RNNs by the techniques established by Bartlett.' [SEP] 'rebuttal_done	We also included the suggested related work (Balduzzi et al. 2018) in Section 5.
You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.' [SEP] 'rebuttal_done	We also included the suggested related work (Balduzzi et al. 2018) in Section 5.
Therefore, I don’t find interesting to report how DDGC improve upon “no baseline”, because known methods do even better.' [SEP] 'rebuttal_done	We also included the suggested related work (Balduzzi et al. 2018) in Section 5.
While better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.' [SEP] 'rebuttal_done	We also included the suggested related work (Balduzzi et al. 2018) in Section 5.
As the approach uses NF is derived specifically for unstable dynamics, it is not clear to me how adding it would affect the training if the dynamics *is stable*. In this context, I consider that for example, the work by Balduzzi et al. 2018 may be relevant as it describes that the dynamics of games (the Jacobian) has two components, one of which describes the oscillating behavior (Hamiltonian game); whereas most games are a mix of oscillating and non-oscillating dynamics.' [SEP] 'rebuttal_done	We also included the suggested related work (Balduzzi et al. 2018) in Section 5.
The justification that the method of Khadka & Tumer (2018) cannot be extended to use CEM, since the RL policies do not comply with the covariance matrix is unclear to me.' [SEP] 'rebuttal_done	We also included the suggested related work (Balduzzi et al. 2018) in Section 5.
The results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?' [SEP] 'rebuttal_future	Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.
"The performance of the proposed method is worse than the previous work but they claimed ""state-of-the-art"" results.' [SEP] 'rebuttal_future"	Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.
"- In the appendix, the statement ""Sarkar (2011) show that a similar statement as in Theorem 2 holds for a very general class of trees"" is confusing to me. The ""general class"", as far as I know, is actually *all* trees, weighted or unweighted.' [SEP] 'rebuttal_future"	Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.
I wonder the motivation of analyzing generalization of RNNs by the techniques established by Bartlett.' [SEP] 'rebuttal_future	Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.
You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.' [SEP] 'rebuttal_future	Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.
Therefore, I don’t find interesting to report how DDGC improve upon “no baseline”, because known methods do even better.' [SEP] 'rebuttal_future	Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.
While better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.' [SEP] 'rebuttal_future	Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.
As the approach uses NF is derived specifically for unstable dynamics, it is not clear to me how adding it would affect the training if the dynamics *is stable*. In this context, I consider that for example, the work by Balduzzi et al. 2018 may be relevant as it describes that the dynamics of games (the Jacobian) has two components, one of which describes the oscillating behavior (Hamiltonian game); whereas most games are a mix of oscillating and non-oscillating dynamics.' [SEP] 'rebuttal_future	Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.
The justification that the method of Khadka & Tumer (2018) cannot be extended to use CEM, since the RL policies do not comply with the covariance matrix is unclear to me.' [SEP] 'rebuttal_future	Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.
The convergence analysis is on Z, not on parameters x and hyper-parameters theta.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
- The analysis in [1] handles the case of noisy updates, whereas the analysis given here only works for exact updates.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
The presented analysis seems to neglect the error term corresponding to the value function.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
Moreover, as observed by the authors this analysis currently rely on strong assumptions that might make it rather unrealistic.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
There is a key concern about the feasibility of the numerical analysis for the first part.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
Moreover, if I understand correctly the WGAN analysis does not take into account that G and D are non-linear, and it is unclear if these can be done.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
- It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
To evaluate the impact of the proposed changes in this paper, one would have to perform extended evaluations and ablations for the submission.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
"2.	What is the reason for using rule-based agents in all the experiments? It would have been more useful if all the analysis are done with RL agents rather than rule-based agents.' [SEP] 'rebuttal_answer"	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
This has not been considered in the analysis.' [SEP] 'rebuttal_answer	We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.
The convergence analysis is on Z, not on parameters x and hyper-parameters theta.' [SEP] 'rebuttal_structuring	So, bounds here cannot be used to explain empirical observations in Section 5.
- The analysis in [1] handles the case of noisy updates, whereas the analysis given here only works for exact updates.' [SEP] 'rebuttal_structuring	So, bounds here cannot be used to explain empirical observations in Section 5.
The presented analysis seems to neglect the error term corresponding to the value function.' [SEP] 'rebuttal_structuring	So, bounds here cannot be used to explain empirical observations in Section 5.
Moreover, as observed by the authors this analysis currently rely on strong assumptions that might make it rather unrealistic.' [SEP] 'rebuttal_structuring	So, bounds here cannot be used to explain empirical observations in Section 5.
There is a key concern about the feasibility of the numerical analysis for the first part.' [SEP] 'rebuttal_structuring	So, bounds here cannot be used to explain empirical observations in Section 5.
Moreover, if I understand correctly the WGAN analysis does not take into account that G and D are non-linear, and it is unclear if these can be done.' [SEP] 'rebuttal_structuring	So, bounds here cannot be used to explain empirical observations in Section 5.
- It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images.' [SEP] 'rebuttal_structuring	So, bounds here cannot be used to explain empirical observations in Section 5.
To evaluate the impact of the proposed changes in this paper, one would have to perform extended evaluations and ablations for the submission.' [SEP] 'rebuttal_structuring	So, bounds here cannot be used to explain empirical observations in Section 5.
"2.	What is the reason for using rule-based agents in all the experiments? It would have been more useful if all the analysis are done with RL agents rather than rule-based agents.' [SEP] 'rebuttal_structuring"	So, bounds here cannot be used to explain empirical observations in Section 5.
This has not been considered in the analysis.' [SEP] 'rebuttal_structuring	So, bounds here cannot be used to explain empirical observations in Section 5.
The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Yet the experiments considered in the paper are limited to very few time series.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The only set of experiments are comparisons on first 500 MNIST test images.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Moreover, I don't think some of the presented experiments are necessary.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.' [SEP] 'rebuttal_answer"	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.' [SEP] 'rebuttal_answer"	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
We think that this is not enough, and more extensive experimental results would provide a better paper.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
It would be nice to see a better case made for spherical convolutions within the experimental section.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
* The connections to deep learning seem arbitrary in some of the experiments.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Given the paper title, the reviewer would have expected more experiments in a multiple domain context.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
- The performance gain is not substantial in experiments.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
* The baselines in the experiments could be improved.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
3. The experimental study is weak.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
- Detailed experimental setups are missing.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
However, the experimental results are weak in justifying the paper's claims.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The main problems come from the experiments, which I would ask for more things.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.' [SEP] 'rebuttal_answer"	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Regarding the experimental evaluation of the model rather confusing.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Experimental results itself are fine but not complete.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The second weakness is experimental design.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
=> Environment: The experimental section of the paper can be further improved.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
6, the experimental design of Sec. 4.2 is also a bit unfair.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
So the experiments in this paper is also not convincing.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
However, my concern is about the experiments.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The paper’s primary drawback is the restrictive setting under which the experiments are performed.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Please run at least 10 experiments.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Additional experiments on at least ImageNet would have made the paper stronger.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
"2.	The experiments are rather insufficient.' [SEP] 'rebuttal_answer"	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
However, it seems the experiments do not seem to support this.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
In fact, the separate training seems to make this unlikely.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Thus the experiment comparison is not really fair.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
2 The experimental settings are not reasonable.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The current experimental settings are not matched with the practice environment.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The experiments of this paper lack comparisons to certified verification' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
It might be beneficial to include comparison to this approach in the experimental section.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
3) The experiments are completely preliminary and not reasonable:' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
These issues would maybe be excusable if not for the totally inadequate experimental validation.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The weight sharing was also needed further investigation and experimental data on sharing different parts.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The rest experiments' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
- The setup for the learning to permute experiment is not as general as it would imply in the main text.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
(4) For the error-specific attack task, it would be better to provide an ablation experiment.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
*The experimental section is too limited.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
3) The simulation is not convincing.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
"- at the start of section 3: what is an ""experiment""?' [SEP] 'rebuttal_answer"	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?' [SEP] 'rebuttal_answer"	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.' [SEP] 'rebuttal_answer"	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
this is important since all your experiments rely on that assumption.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.' [SEP] 'rebuttal_answer	These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.
The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Yet the experiments considered in the paper are limited to very few time series.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The only set of experiments are comparisons on first 500 MNIST test images.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Moreover, I don't think some of the presented experiments are necessary.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.' [SEP] 'rebuttal_structuring"	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.' [SEP] 'rebuttal_structuring"	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
We think that this is not enough, and more extensive experimental results would provide a better paper.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
It would be nice to see a better case made for spherical convolutions within the experimental section.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
* The connections to deep learning seem arbitrary in some of the experiments.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Given the paper title, the reviewer would have expected more experiments in a multiple domain context.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
- The performance gain is not substantial in experiments.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
* The baselines in the experiments could be improved.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
3. The experimental study is weak.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
- Detailed experimental setups are missing.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
However, the experimental results are weak in justifying the paper's claims.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The main problems come from the experiments, which I would ask for more things.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.' [SEP] 'rebuttal_structuring"	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Regarding the experimental evaluation of the model rather confusing.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Experimental results itself are fine but not complete.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The second weakness is experimental design.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
=> Environment: The experimental section of the paper can be further improved.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
6, the experimental design of Sec. 4.2 is also a bit unfair.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
So the experiments in this paper is also not convincing.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
However, my concern is about the experiments.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The paper’s primary drawback is the restrictive setting under which the experiments are performed.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Please run at least 10 experiments.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Additional experiments on at least ImageNet would have made the paper stronger.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
"2.	The experiments are rather insufficient.' [SEP] 'rebuttal_structuring"	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
However, it seems the experiments do not seem to support this.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
In fact, the separate training seems to make this unlikely.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Thus the experiment comparison is not really fair.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
2 The experimental settings are not reasonable.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The current experimental settings are not matched with the practice environment.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The experiments of this paper lack comparisons to certified verification' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
It might be beneficial to include comparison to this approach in the experimental section.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
3) The experiments are completely preliminary and not reasonable:' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
These issues would maybe be excusable if not for the totally inadequate experimental validation.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The weight sharing was also needed further investigation and experimental data on sharing different parts.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The rest experiments' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
- The setup for the learning to permute experiment is not as general as it would imply in the main text.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
(4) For the error-specific attack task, it would be better to provide an ablation experiment.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
*The experimental section is too limited.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
3) The simulation is not convincing.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
"- at the start of section 3: what is an ""experiment""?' [SEP] 'rebuttal_structuring"	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?' [SEP] 'rebuttal_structuring"	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.' [SEP] 'rebuttal_structuring"	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
this is important since all your experiments rely on that assumption.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.' [SEP] 'rebuttal_structuring	Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.
The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Yet the experiments considered in the paper are limited to very few time series.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The only set of experiments are comparisons on first 500 MNIST test images.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Moreover, I don't think some of the presented experiments are necessary.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.' [SEP] 'rebuttal_by-cr"	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.' [SEP] 'rebuttal_by-cr"	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
We think that this is not enough, and more extensive experimental results would provide a better paper.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
It would be nice to see a better case made for spherical convolutions within the experimental section.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
* The connections to deep learning seem arbitrary in some of the experiments.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Given the paper title, the reviewer would have expected more experiments in a multiple domain context.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
- The performance gain is not substantial in experiments.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
* The baselines in the experiments could be improved.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
3. The experimental study is weak.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
- Detailed experimental setups are missing.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
However, the experimental results are weak in justifying the paper's claims.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The main problems come from the experiments, which I would ask for more things.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.' [SEP] 'rebuttal_by-cr"	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Regarding the experimental evaluation of the model rather confusing.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Experimental results itself are fine but not complete.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The second weakness is experimental design.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
=> Environment: The experimental section of the paper can be further improved.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
6, the experimental design of Sec. 4.2 is also a bit unfair.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
So the experiments in this paper is also not convincing.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
However, my concern is about the experiments.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The paper’s primary drawback is the restrictive setting under which the experiments are performed.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Please run at least 10 experiments.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Additional experiments on at least ImageNet would have made the paper stronger.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
"2.	The experiments are rather insufficient.' [SEP] 'rebuttal_by-cr"	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
However, it seems the experiments do not seem to support this.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
In fact, the separate training seems to make this unlikely.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Thus the experiment comparison is not really fair.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
2 The experimental settings are not reasonable.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The current experimental settings are not matched with the practice environment.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The experiments of this paper lack comparisons to certified verification' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
It might be beneficial to include comparison to this approach in the experimental section.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
3) The experiments are completely preliminary and not reasonable:' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
These issues would maybe be excusable if not for the totally inadequate experimental validation.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The weight sharing was also needed further investigation and experimental data on sharing different parts.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The rest experiments' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
- The setup for the learning to permute experiment is not as general as it would imply in the main text.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
(4) For the error-specific attack task, it would be better to provide an ablation experiment.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
*The experimental section is too limited.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
3) The simulation is not convincing.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
"- at the start of section 3: what is an ""experiment""?' [SEP] 'rebuttal_by-cr"	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?' [SEP] 'rebuttal_by-cr"	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.' [SEP] 'rebuttal_by-cr"	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
this is important since all your experiments rely on that assumption.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.' [SEP] 'rebuttal_by-cr	Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.
The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Yet the experiments considered in the paper are limited to very few time series.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The only set of experiments are comparisons on first 500 MNIST test images.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Moreover, I don't think some of the presented experiments are necessary.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.' [SEP] 'rebuttal_reject-criticism"	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.' [SEP] 'rebuttal_reject-criticism"	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
We think that this is not enough, and more extensive experimental results would provide a better paper.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
It would be nice to see a better case made for spherical convolutions within the experimental section.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
* The connections to deep learning seem arbitrary in some of the experiments.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Given the paper title, the reviewer would have expected more experiments in a multiple domain context.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
- The performance gain is not substantial in experiments.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
* The baselines in the experiments could be improved.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
3. The experimental study is weak.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
- Detailed experimental setups are missing.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
However, the experimental results are weak in justifying the paper's claims.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The main problems come from the experiments, which I would ask for more things.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.' [SEP] 'rebuttal_reject-criticism"	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Regarding the experimental evaluation of the model rather confusing.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Experimental results itself are fine but not complete.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The second weakness is experimental design.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
=> Environment: The experimental section of the paper can be further improved.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
6, the experimental design of Sec. 4.2 is also a bit unfair.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
So the experiments in this paper is also not convincing.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
However, my concern is about the experiments.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The paper’s primary drawback is the restrictive setting under which the experiments are performed.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Please run at least 10 experiments.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Additional experiments on at least ImageNet would have made the paper stronger.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
"2.	The experiments are rather insufficient.' [SEP] 'rebuttal_reject-criticism"	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
However, it seems the experiments do not seem to support this.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
In fact, the separate training seems to make this unlikely.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Thus the experiment comparison is not really fair.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
2 The experimental settings are not reasonable.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The current experimental settings are not matched with the practice environment.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The experiments of this paper lack comparisons to certified verification' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
It might be beneficial to include comparison to this approach in the experimental section.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
3) The experiments are completely preliminary and not reasonable:' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
These issues would maybe be excusable if not for the totally inadequate experimental validation.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The weight sharing was also needed further investigation and experimental data on sharing different parts.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The rest experiments' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
- The setup for the learning to permute experiment is not as general as it would imply in the main text.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
(4) For the error-specific attack task, it would be better to provide an ablation experiment.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
*The experimental section is too limited.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
3) The simulation is not convincing.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
"- at the start of section 3: what is an ""experiment""?' [SEP] 'rebuttal_reject-criticism"	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?' [SEP] 'rebuttal_reject-criticism"	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.' [SEP] 'rebuttal_reject-criticism"	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
this is important since all your experiments rely on that assumption.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.' [SEP] 'rebuttal_reject-criticism	While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.
The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Yet the experiments considered in the paper are limited to very few time series.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
The only set of experiments are comparisons on first 500 MNIST test images.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Moreover, I don't think some of the presented experiments are necessary.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.' [SEP] 'rebuttal_done"	"Please refer to our updated ""Experiments"" section."
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.' [SEP] 'rebuttal_done"	"Please refer to our updated ""Experiments"" section."
We think that this is not enough, and more extensive experimental results would provide a better paper.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
It would be nice to see a better case made for spherical convolutions within the experimental section.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
* The connections to deep learning seem arbitrary in some of the experiments.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Given the paper title, the reviewer would have expected more experiments in a multiple domain context.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- The performance gain is not substantial in experiments.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
* The baselines in the experiments could be improved.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
3. The experimental study is weak.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- Detailed experimental setups are missing.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
However, the experimental results are weak in justifying the paper's claims.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
The main problems come from the experiments, which I would ask for more things.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.' [SEP] 'rebuttal_done"	"Please refer to our updated ""Experiments"" section."
Regarding the experimental evaluation of the model rather confusing.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Experimental results itself are fine but not complete.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
The second weakness is experimental design.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
=> Environment: The experimental section of the paper can be further improved.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
6, the experimental design of Sec. 4.2 is also a bit unfair.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
So the experiments in this paper is also not convincing.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
However, my concern is about the experiments.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
The paper’s primary drawback is the restrictive setting under which the experiments are performed.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Please run at least 10 experiments.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Additional experiments on at least ImageNet would have made the paper stronger.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
"2.	The experiments are rather insufficient.' [SEP] 'rebuttal_done"	"Please refer to our updated ""Experiments"" section."
However, it seems the experiments do not seem to support this.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
In fact, the separate training seems to make this unlikely.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Thus the experiment comparison is not really fair.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
2 The experimental settings are not reasonable.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
The current experimental settings are not matched with the practice environment.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
The experiments of this paper lack comparisons to certified verification' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
It might be beneficial to include comparison to this approach in the experimental section.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
3) The experiments are completely preliminary and not reasonable:' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
These issues would maybe be excusable if not for the totally inadequate experimental validation.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
The weight sharing was also needed further investigation and experimental data on sharing different parts.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
The rest experiments' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- The setup for the learning to permute experiment is not as general as it would imply in the main text.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
(4) For the error-specific attack task, it would be better to provide an ablation experiment.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
*The experimental section is too limited.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
3) The simulation is not convincing.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
"- at the start of section 3: what is an ""experiment""?' [SEP] 'rebuttal_done"	"Please refer to our updated ""Experiments"" section."
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?' [SEP] 'rebuttal_done"	"Please refer to our updated ""Experiments"" section."
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.' [SEP] 'rebuttal_done"	"Please refer to our updated ""Experiments"" section."
The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
this is important since all your experiments rely on that assumption.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.' [SEP] 'rebuttal_done	"Please refer to our updated ""Experiments"" section."
The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Yet the experiments considered in the paper are limited to very few time series.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
The only set of experiments are comparisons on first 500 MNIST test images.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Moreover, I don't think some of the presented experiments are necessary.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.' [SEP] 'rebuttal_concede-criticism"	2) For the experiment, we will train our experiments longer and modify our network.
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.' [SEP] 'rebuttal_concede-criticism"	2) For the experiment, we will train our experiments longer and modify our network.
We think that this is not enough, and more extensive experimental results would provide a better paper.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
It would be nice to see a better case made for spherical convolutions within the experimental section.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
* The connections to deep learning seem arbitrary in some of the experiments.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Given the paper title, the reviewer would have expected more experiments in a multiple domain context.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
- The performance gain is not substantial in experiments.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
* The baselines in the experiments could be improved.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
3. The experimental study is weak.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
- Detailed experimental setups are missing.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
However, the experimental results are weak in justifying the paper's claims.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
The main problems come from the experiments, which I would ask for more things.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.' [SEP] 'rebuttal_concede-criticism"	2) For the experiment, we will train our experiments longer and modify our network.
Regarding the experimental evaluation of the model rather confusing.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Experimental results itself are fine but not complete.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
The second weakness is experimental design.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
=> Environment: The experimental section of the paper can be further improved.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
6, the experimental design of Sec. 4.2 is also a bit unfair.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
So the experiments in this paper is also not convincing.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
However, my concern is about the experiments.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
The paper’s primary drawback is the restrictive setting under which the experiments are performed.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Please run at least 10 experiments.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Additional experiments on at least ImageNet would have made the paper stronger.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
"2.	The experiments are rather insufficient.' [SEP] 'rebuttal_concede-criticism"	2) For the experiment, we will train our experiments longer and modify our network.
However, it seems the experiments do not seem to support this.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
In fact, the separate training seems to make this unlikely.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Thus the experiment comparison is not really fair.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
2 The experimental settings are not reasonable.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
The current experimental settings are not matched with the practice environment.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
The experiments of this paper lack comparisons to certified verification' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
It might be beneficial to include comparison to this approach in the experimental section.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
3) The experiments are completely preliminary and not reasonable:' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
These issues would maybe be excusable if not for the totally inadequate experimental validation.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
The weight sharing was also needed further investigation and experimental data on sharing different parts.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
The rest experiments' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
- The setup for the learning to permute experiment is not as general as it would imply in the main text.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
(4) For the error-specific attack task, it would be better to provide an ablation experiment.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
*The experimental section is too limited.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
3) The simulation is not convincing.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
"- at the start of section 3: what is an ""experiment""?' [SEP] 'rebuttal_concede-criticism"	2) For the experiment, we will train our experiments longer and modify our network.
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?' [SEP] 'rebuttal_concede-criticism"	2) For the experiment, we will train our experiments longer and modify our network.
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.' [SEP] 'rebuttal_concede-criticism"	2) For the experiment, we will train our experiments longer and modify our network.
The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
this is important since all your experiments rely on that assumption.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.' [SEP] 'rebuttal_concede-criticism	2) For the experiment, we will train our experiments longer and modify our network.
The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Yet the experiments considered in the paper are limited to very few time series.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The only set of experiments are comparisons on first 500 MNIST test images.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Moreover, I don't think some of the presented experiments are necessary.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.' [SEP] 'rebuttal_followup"	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.' [SEP] 'rebuttal_followup"	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
We think that this is not enough, and more extensive experimental results would provide a better paper.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
It would be nice to see a better case made for spherical convolutions within the experimental section.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
* The connections to deep learning seem arbitrary in some of the experiments.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Given the paper title, the reviewer would have expected more experiments in a multiple domain context.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
- The performance gain is not substantial in experiments.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
* The baselines in the experiments could be improved.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
3. The experimental study is weak.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
- Detailed experimental setups are missing.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
However, the experimental results are weak in justifying the paper's claims.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The main problems come from the experiments, which I would ask for more things.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.' [SEP] 'rebuttal_followup"	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Regarding the experimental evaluation of the model rather confusing.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Experimental results itself are fine but not complete.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The second weakness is experimental design.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
=> Environment: The experimental section of the paper can be further improved.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
6, the experimental design of Sec. 4.2 is also a bit unfair.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
So the experiments in this paper is also not convincing.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
However, my concern is about the experiments.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The paper’s primary drawback is the restrictive setting under which the experiments are performed.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Please run at least 10 experiments.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Additional experiments on at least ImageNet would have made the paper stronger.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
"2.	The experiments are rather insufficient.' [SEP] 'rebuttal_followup"	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
However, it seems the experiments do not seem to support this.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
In fact, the separate training seems to make this unlikely.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Thus the experiment comparison is not really fair.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
2 The experimental settings are not reasonable.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The current experimental settings are not matched with the practice environment.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The experiments of this paper lack comparisons to certified verification' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
It might be beneficial to include comparison to this approach in the experimental section.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
3) The experiments are completely preliminary and not reasonable:' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
These issues would maybe be excusable if not for the totally inadequate experimental validation.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The weight sharing was also needed further investigation and experimental data on sharing different parts.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The rest experiments' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
- The setup for the learning to permute experiment is not as general as it would imply in the main text.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
(4) For the error-specific attack task, it would be better to provide an ablation experiment.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
*The experimental section is too limited.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
3) The simulation is not convincing.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
"- at the start of section 3: what is an ""experiment""?' [SEP] 'rebuttal_followup"	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?' [SEP] 'rebuttal_followup"	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.' [SEP] 'rebuttal_followup"	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
this is important since all your experiments rely on that assumption.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.' [SEP] 'rebuttal_followup	"- Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The equation (1) should hold for any \theta’, not \theta.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The equation (1) should contain \rho, not p.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
For example, it is unclear to me why some larger models are not amenable to truncation.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
From the supplementary, it seems Epsilon means the environment?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"Fourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The maths is not clear, in particular the gradient derivation in equation (8).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
But again, it's not super clear how the paper estimates this derivative.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
1. Are e_{i,t} and lambda_{i,t} vectors?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In the algorithm, the authors need to define the HT function in (3) and (4).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"I take issue with the usage of the phrase ""skill discovery"".' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It is in the end plugged into a continual learning algorithm which also performs domain transformation.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The purpose of the public set is explained only in section 5.2.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
However, I don't understand the use of $\alpha$ here.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
* Equations (1, 2): z and \phi are not consistently boldfaced' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I believe that the presentation of the proposed method can be significantly improved.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The method description was a bit confusing and unclear to me.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- the sentence under eq. (2)' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"4. I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
While the general description of the model is clear, details are lacking.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Similarly, you did not indicate what the deterministic version of your model is.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
-Some technical details  are missing.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Because of the above many discussions about discrete vs. continuous variables are missleading.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Many of the parameters here are also unclear and not properly defined/introduced.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
What is the relationship between $\theta$ and $\tilde\theta$ exactly?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The 3rd line of lemma 1: epsilon1 -> epsilon_1' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Page 14, Eq(14), \lambda should be s' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"Some statements don't make sense, however, eg. ""HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
This makes the contribution of this paper in terms of the method' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- What is dt in Algorithm 1 description?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In later sections they use theta and theta’ for encoder/decoder resp.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Why mention it here, if it's not being defined.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Minor, 1/2 is missing in the last line of Eq 19.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Also perhaps better to use the curly sign for vector inequality.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
“From a high-level perspective both of these approaches” --> missing comma after “perspective”' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In equations 1 and 2, should a, b be written in capital? Since they represent random variables.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- There is a typo in equation 6' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The paper seems to lack clarity on certain design/ architecture/ model decisions.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Also, I had to go through a large chunk of the paper before coming across the exact setup.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I did not completely follow the arguments towards directed graph deconvolution operators.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I also struggled a little to understand what is the difference between forward interpolate and filtering.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
1) There is no clear rationale on why we need a new model based on Transformers for this task.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I am not convinced that the measure theoretic perspective is always' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
4.4, law of total variation -- define' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
However, what was not clear to me is how this reduces the non-stationarity of MARL.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- What is the choice of beta in the beta-VAE training objective?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
1. What is M in Algorithm 1 ?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Not sure why Eqns. 2 and 9 need any parentheses' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Generalizing this to the multi-channel input as the next step could make the proof more accessible.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The egocentric velocity field is not described (section 5)' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
However, the  transfer learning model is unclear.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I have doubts on applying the proposed method to higher dimensional inputs.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
ReLU network with 2 hidden layers, and it is unknown if it works in general.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
positive - it unlikely to be true that an undefended network is predominantly' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
adversarial examples (or counter-examples for property verification) with L_inf' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Your F and \tilde{f} are introduced as infinite series.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Isn't this just restating the point made in the first sentence?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
3, The notations in Eq. (1) and (2) are messy.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It is unclear how well the proposed method works in general.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In the start of Section 3, it is not clear why having the projection be sparse is desired.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
eqn (8): use something else to denote the function 'U'.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
This clipping will also introduce bias, this is not discussed, and will probably lower variance.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It would be better if the authors were a little more careful in their use of terminology here.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
*The strategy proposed to introduce weak-supervision is too ad-hoc.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- U^m in Eq 1 is undefined and un-discussed.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The term p(w) disappears on the left hand side of Eq 2.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
p2-3, Section 3.1 - I found the equations impossible to read. What' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It is unclear how the model actually operates and uses attention during execution.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"-	Attention should be given to the notation in formulas (3) and (4).' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The projection function there is no longer accepts a 3D point parametrized by 3 variables.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Instead only depth is provided.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In addition, the subindex ‘1’ of the point ‘q’ is not explained.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The equation (1) should hold for any \theta’, not \theta.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The equation (1) should contain \rho, not p.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
For example, it is unclear to me why some larger models are not amenable to truncation.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
From the supplementary, it seems Epsilon means the environment?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"Fourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The maths is not clear, in particular the gradient derivation in equation (8).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
But again, it's not super clear how the paper estimates this derivative.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
1. Are e_{i,t} and lambda_{i,t} vectors?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
In the algorithm, the authors need to define the HT function in (3) and (4).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"I take issue with the usage of the phrase ""skill discovery"".' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
It is in the end plugged into a continual learning algorithm which also performs domain transformation.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The purpose of the public set is explained only in section 5.2.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
However, I don't understand the use of $\alpha$ here.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
* Equations (1, 2): z and \phi are not consistently boldfaced' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I believe that the presentation of the proposed method can be significantly improved.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The method description was a bit confusing and unclear to me.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- the sentence under eq. (2)' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"4. I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
While the general description of the model is clear, details are lacking.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Similarly, you did not indicate what the deterministic version of your model is.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
-Some technical details  are missing.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Because of the above many discussions about discrete vs. continuous variables are missleading.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Many of the parameters here are also unclear and not properly defined/introduced.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
What is the relationship between $\theta$ and $\tilde\theta$ exactly?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The 3rd line of lemma 1: epsilon1 -> epsilon_1' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- Page 14, Eq(14), \lambda should be s' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"Some statements don't make sense, however, eg. ""HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
This makes the contribution of this paper in terms of the method' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- What is dt in Algorithm 1 description?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
In later sections they use theta and theta’ for encoder/decoder resp.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Why mention it here, if it's not being defined.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Minor, 1/2 is missing in the last line of Eq 19.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Also perhaps better to use the curly sign for vector inequality.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
“From a high-level perspective both of these approaches” --> missing comma after “perspective”' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
In equations 1 and 2, should a, b be written in capital? Since they represent random variables.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- There is a typo in equation 6' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The paper seems to lack clarity on certain design/ architecture/ model decisions.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Also, I had to go through a large chunk of the paper before coming across the exact setup.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I did not completely follow the arguments towards directed graph deconvolution operators.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I also struggled a little to understand what is the difference between forward interpolate and filtering.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
1) There is no clear rationale on why we need a new model based on Transformers for this task.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I am not convinced that the measure theoretic perspective is always' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
4.4, law of total variation -- define' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
However, what was not clear to me is how this reduces the non-stationarity of MARL.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- What is the choice of beta in the beta-VAE training objective?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
1. What is M in Algorithm 1 ?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Not sure why Eqns. 2 and 9 need any parentheses' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Generalizing this to the multi-channel input as the next step could make the proof more accessible.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The egocentric velocity field is not described (section 5)' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
However, the  transfer learning model is unclear.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I have doubts on applying the proposed method to higher dimensional inputs.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
ReLU network with 2 hidden layers, and it is unknown if it works in general.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
positive - it unlikely to be true that an undefended network is predominantly' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
adversarial examples (or counter-examples for property verification) with L_inf' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- Your F and \tilde{f} are introduced as infinite series.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Isn't this just restating the point made in the first sentence?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
3, The notations in Eq. (1) and (2) are messy.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
It is unclear how well the proposed method works in general.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
In the start of Section 3, it is not clear why having the projection be sparse is desired.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
eqn (8): use something else to denote the function 'U'.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
This clipping will also introduce bias, this is not discussed, and will probably lower variance.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
It would be better if the authors were a little more careful in their use of terminology here.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
*The strategy proposed to introduce weak-supervision is too ad-hoc.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- U^m in Eq 1 is undefined and un-discussed.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The term p(w) disappears on the left hand side of Eq 2.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
p2-3, Section 3.1 - I found the equations impossible to read. What' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
It is unclear how the model actually operates and uses attention during execution.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"-	Attention should be given to the notation in formulas (3) and (4).' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
The projection function there is no longer accepts a 3D point parametrized by 3 variables.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
Instead only depth is provided.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
In addition, the subindex ‘1’ of the point ‘q’ is not explained.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.' [SEP] 'rebuttal_answer	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".' [SEP] 'rebuttal_answer"	The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The equation (1) should hold for any \theta’, not \theta.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The equation (1) should contain \rho, not p.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
For example, it is unclear to me why some larger models are not amenable to truncation.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
From the supplementary, it seems Epsilon means the environment?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"Fourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
The maths is not clear, in particular the gradient derivation in equation (8).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
But again, it's not super clear how the paper estimates this derivative.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
1. Are e_{i,t} and lambda_{i,t} vectors?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
In the algorithm, the authors need to define the HT function in (3) and (4).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"I take issue with the usage of the phrase ""skill discovery"".' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
It is in the end plugged into a continual learning algorithm which also performs domain transformation.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
The purpose of the public set is explained only in section 5.2.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
However, I don't understand the use of $\alpha$ here.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
* Equations (1, 2): z and \phi are not consistently boldfaced' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I believe that the presentation of the proposed method can be significantly improved.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
The method description was a bit confusing and unclear to me.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- the sentence under eq. (2)' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"4. I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
While the general description of the model is clear, details are lacking.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Similarly, you did not indicate what the deterministic version of your model is.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
-Some technical details  are missing.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Because of the above many discussions about discrete vs. continuous variables are missleading.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Many of the parameters here are also unclear and not properly defined/introduced.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
What is the relationship between $\theta$ and $\tilde\theta$ exactly?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The 3rd line of lemma 1: epsilon1 -> epsilon_1' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- Page 14, Eq(14), \lambda should be s' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"Some statements don't make sense, however, eg. ""HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
This makes the contribution of this paper in terms of the method' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- What is dt in Algorithm 1 description?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
In later sections they use theta and theta’ for encoder/decoder resp.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Why mention it here, if it's not being defined.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
Minor, 1/2 is missing in the last line of Eq 19.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Also perhaps better to use the curly sign for vector inequality.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
“From a high-level perspective both of these approaches” --> missing comma after “perspective”' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
In equations 1 and 2, should a, b be written in capital? Since they represent random variables.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- There is a typo in equation 6' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
The paper seems to lack clarity on certain design/ architecture/ model decisions.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Also, I had to go through a large chunk of the paper before coming across the exact setup.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
I did not completely follow the arguments towards directed graph deconvolution operators.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I also struggled a little to understand what is the difference between forward interpolate and filtering.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
1) There is no clear rationale on why we need a new model based on Transformers for this task.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I am not convinced that the measure theoretic perspective is always' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
4.4, law of total variation -- define' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
However, what was not clear to me is how this reduces the non-stationarity of MARL.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- What is the choice of beta in the beta-VAE training objective?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
1. What is M in Algorithm 1 ?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Not sure why Eqns. 2 and 9 need any parentheses' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Generalizing this to the multi-channel input as the next step could make the proof more accessible.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The egocentric velocity field is not described (section 5)' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
However, the  transfer learning model is unclear.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I have doubts on applying the proposed method to higher dimensional inputs.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
ReLU network with 2 hidden layers, and it is unknown if it works in general.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
positive - it unlikely to be true that an undefended network is predominantly' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
adversarial examples (or counter-examples for property verification) with L_inf' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
- Your F and \tilde{f} are introduced as infinite series.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Isn't this just restating the point made in the first sentence?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
3, The notations in Eq. (1) and (2) are messy.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
It is unclear how well the proposed method works in general.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
In the start of Section 3, it is not clear why having the projection be sparse is desired.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
eqn (8): use something else to denote the function 'U'.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
This clipping will also introduce bias, this is not discussed, and will probably lower variance.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
It would be better if the authors were a little more careful in their use of terminology here.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
*The strategy proposed to introduce weak-supervision is too ad-hoc.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
- U^m in Eq 1 is undefined and un-discussed.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The term p(w) disappears on the left hand side of Eq 2.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
p2-3, Section 3.1 - I found the equations impossible to read. What' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
It is unclear how the model actually operates and uses attention during execution.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
"-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
"-	Attention should be given to the notation in formulas (3) and (4).' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
The projection function there is no longer accepts a 3D point parametrized by 3 variables.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
Instead only depth is provided.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
In addition, the subindex ‘1’ of the point ‘q’ is not explained.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.' [SEP] 'rebuttal_structuring	"""Some technical details are missing."
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".' [SEP] 'rebuttal_structuring"	"""Some technical details are missing."
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The equation (1) should hold for any \theta’, not \theta.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The equation (1) should contain \rho, not p.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
For example, it is unclear to me why some larger models are not amenable to truncation.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
From the supplementary, it seems Epsilon means the environment?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"Fourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The maths is not clear, in particular the gradient derivation in equation (8).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
But again, it's not super clear how the paper estimates this derivative.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
1. Are e_{i,t} and lambda_{i,t} vectors?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
In the algorithm, the authors need to define the HT function in (3) and (4).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"I take issue with the usage of the phrase ""skill discovery"".' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
It is in the end plugged into a continual learning algorithm which also performs domain transformation.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The purpose of the public set is explained only in section 5.2.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
However, I don't understand the use of $\alpha$ here.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
* Equations (1, 2): z and \phi are not consistently boldfaced' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I believe that the presentation of the proposed method can be significantly improved.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The method description was a bit confusing and unclear to me.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- the sentence under eq. (2)' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"4. I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
While the general description of the model is clear, details are lacking.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Similarly, you did not indicate what the deterministic version of your model is.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
-Some technical details  are missing.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Because of the above many discussions about discrete vs. continuous variables are missleading.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Many of the parameters here are also unclear and not properly defined/introduced.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
What is the relationship between $\theta$ and $\tilde\theta$ exactly?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The 3rd line of lemma 1: epsilon1 -> epsilon_1' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- Page 14, Eq(14), \lambda should be s' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"Some statements don't make sense, however, eg. ""HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
This makes the contribution of this paper in terms of the method' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- What is dt in Algorithm 1 description?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
In later sections they use theta and theta’ for encoder/decoder resp.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Why mention it here, if it's not being defined.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Minor, 1/2 is missing in the last line of Eq 19.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Also perhaps better to use the curly sign for vector inequality.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
“From a high-level perspective both of these approaches” --> missing comma after “perspective”' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
In equations 1 and 2, should a, b be written in capital? Since they represent random variables.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- There is a typo in equation 6' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The paper seems to lack clarity on certain design/ architecture/ model decisions.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Also, I had to go through a large chunk of the paper before coming across the exact setup.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I did not completely follow the arguments towards directed graph deconvolution operators.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I also struggled a little to understand what is the difference between forward interpolate and filtering.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
1) There is no clear rationale on why we need a new model based on Transformers for this task.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I am not convinced that the measure theoretic perspective is always' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
4.4, law of total variation -- define' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
However, what was not clear to me is how this reduces the non-stationarity of MARL.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- What is the choice of beta in the beta-VAE training objective?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
1. What is M in Algorithm 1 ?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Not sure why Eqns. 2 and 9 need any parentheses' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Generalizing this to the multi-channel input as the next step could make the proof more accessible.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The egocentric velocity field is not described (section 5)' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
However, the  transfer learning model is unclear.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I have doubts on applying the proposed method to higher dimensional inputs.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
ReLU network with 2 hidden layers, and it is unknown if it works in general.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
positive - it unlikely to be true that an undefended network is predominantly' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
adversarial examples (or counter-examples for property verification) with L_inf' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- Your F and \tilde{f} are introduced as infinite series.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Isn't this just restating the point made in the first sentence?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
3, The notations in Eq. (1) and (2) are messy.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
It is unclear how well the proposed method works in general.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
In the start of Section 3, it is not clear why having the projection be sparse is desired.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
eqn (8): use something else to denote the function 'U'.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
This clipping will also introduce bias, this is not discussed, and will probably lower variance.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
It would be better if the authors were a little more careful in their use of terminology here.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
*The strategy proposed to introduce weak-supervision is too ad-hoc.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- U^m in Eq 1 is undefined and un-discussed.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The term p(w) disappears on the left hand side of Eq 2.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
p2-3, Section 3.1 - I found the equations impossible to read. What' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
It is unclear how the model actually operates and uses attention during execution.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"-	Attention should be given to the notation in formulas (3) and (4).' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
The projection function there is no longer accepts a 3D point parametrized by 3 variables.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
Instead only depth is provided.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
In addition, the subindex ‘1’ of the point ‘q’ is not explained.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.' [SEP] 'rebuttal_by-cr	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".' [SEP] 'rebuttal_by-cr"	In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The equation (1) should hold for any \theta’, not \theta.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The equation (1) should contain \rho, not p.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
For example, it is unclear to me why some larger models are not amenable to truncation.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
From the supplementary, it seems Epsilon means the environment?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"Fourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The maths is not clear, in particular the gradient derivation in equation (8).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
But again, it's not super clear how the paper estimates this derivative.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
1. Are e_{i,t} and lambda_{i,t} vectors?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
In the algorithm, the authors need to define the HT function in (3) and (4).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"I take issue with the usage of the phrase ""skill discovery"".' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
It is in the end plugged into a continual learning algorithm which also performs domain transformation.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The purpose of the public set is explained only in section 5.2.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
However, I don't understand the use of $\alpha$ here.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
* Equations (1, 2): z and \phi are not consistently boldfaced' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I believe that the presentation of the proposed method can be significantly improved.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The method description was a bit confusing and unclear to me.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- the sentence under eq. (2)' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"4. I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
While the general description of the model is clear, details are lacking.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Similarly, you did not indicate what the deterministic version of your model is.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
-Some technical details  are missing.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Because of the above many discussions about discrete vs. continuous variables are missleading.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Many of the parameters here are also unclear and not properly defined/introduced.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
What is the relationship between $\theta$ and $\tilde\theta$ exactly?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The 3rd line of lemma 1: epsilon1 -> epsilon_1' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- Page 14, Eq(14), \lambda should be s' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"Some statements don't make sense, however, eg. ""HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
This makes the contribution of this paper in terms of the method' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- What is dt in Algorithm 1 description?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
In later sections they use theta and theta’ for encoder/decoder resp.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Why mention it here, if it's not being defined.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Minor, 1/2 is missing in the last line of Eq 19.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Also perhaps better to use the curly sign for vector inequality.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
“From a high-level perspective both of these approaches” --> missing comma after “perspective”' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
In equations 1 and 2, should a, b be written in capital? Since they represent random variables.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- There is a typo in equation 6' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The paper seems to lack clarity on certain design/ architecture/ model decisions.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Also, I had to go through a large chunk of the paper before coming across the exact setup.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I did not completely follow the arguments towards directed graph deconvolution operators.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I also struggled a little to understand what is the difference between forward interpolate and filtering.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
1) There is no clear rationale on why we need a new model based on Transformers for this task.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I am not convinced that the measure theoretic perspective is always' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
4.4, law of total variation -- define' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
However, what was not clear to me is how this reduces the non-stationarity of MARL.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- What is the choice of beta in the beta-VAE training objective?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
1. What is M in Algorithm 1 ?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Not sure why Eqns. 2 and 9 need any parentheses' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Generalizing this to the multi-channel input as the next step could make the proof more accessible.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The egocentric velocity field is not described (section 5)' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
However, the  transfer learning model is unclear.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I have doubts on applying the proposed method to higher dimensional inputs.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
ReLU network with 2 hidden layers, and it is unknown if it works in general.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
positive - it unlikely to be true that an undefended network is predominantly' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
adversarial examples (or counter-examples for property verification) with L_inf' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- Your F and \tilde{f} are introduced as infinite series.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Isn't this just restating the point made in the first sentence?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
3, The notations in Eq. (1) and (2) are messy.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
It is unclear how well the proposed method works in general.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
In the start of Section 3, it is not clear why having the projection be sparse is desired.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
eqn (8): use something else to denote the function 'U'.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
This clipping will also introduce bias, this is not discussed, and will probably lower variance.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
It would be better if the authors were a little more careful in their use of terminology here.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
*The strategy proposed to introduce weak-supervision is too ad-hoc.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- U^m in Eq 1 is undefined and un-discussed.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The term p(w) disappears on the left hand side of Eq 2.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
p2-3, Section 3.1 - I found the equations impossible to read. What' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
It is unclear how the model actually operates and uses attention during execution.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"-	Attention should be given to the notation in formulas (3) and (4).' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
The projection function there is no longer accepts a 3D point parametrized by 3 variables.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
Instead only depth is provided.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
In addition, the subindex ‘1’ of the point ‘q’ is not explained.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.' [SEP] 'rebuttal_reject-criticism	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".' [SEP] 'rebuttal_reject-criticism"	As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The equation (1) should hold for any \theta’, not \theta.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The equation (1) should contain \rho, not p.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
For example, it is unclear to me why some larger models are not amenable to truncation.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
From the supplementary, it seems Epsilon means the environment?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"Fourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The maths is not clear, in particular the gradient derivation in equation (8).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
But again, it's not super clear how the paper estimates this derivative.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
1. Are e_{i,t} and lambda_{i,t} vectors?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
In the algorithm, the authors need to define the HT function in (3) and (4).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"I take issue with the usage of the phrase ""skill discovery"".' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
It is in the end plugged into a continual learning algorithm which also performs domain transformation.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The purpose of the public set is explained only in section 5.2.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
However, I don't understand the use of $\alpha$ here.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
* Equations (1, 2): z and \phi are not consistently boldfaced' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I believe that the presentation of the proposed method can be significantly improved.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The method description was a bit confusing and unclear to me.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- the sentence under eq. (2)' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"4. I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
While the general description of the model is clear, details are lacking.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Similarly, you did not indicate what the deterministic version of your model is.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
-Some technical details  are missing.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Because of the above many discussions about discrete vs. continuous variables are missleading.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Many of the parameters here are also unclear and not properly defined/introduced.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
What is the relationship between $\theta$ and $\tilde\theta$ exactly?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The 3rd line of lemma 1: epsilon1 -> epsilon_1' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- Page 14, Eq(14), \lambda should be s' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"Some statements don't make sense, however, eg. ""HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
This makes the contribution of this paper in terms of the method' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- What is dt in Algorithm 1 description?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
In later sections they use theta and theta’ for encoder/decoder resp.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Why mention it here, if it's not being defined.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Minor, 1/2 is missing in the last line of Eq 19.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Also perhaps better to use the curly sign for vector inequality.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
“From a high-level perspective both of these approaches” --> missing comma after “perspective”' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
In equations 1 and 2, should a, b be written in capital? Since they represent random variables.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- There is a typo in equation 6' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The paper seems to lack clarity on certain design/ architecture/ model decisions.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Also, I had to go through a large chunk of the paper before coming across the exact setup.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I did not completely follow the arguments towards directed graph deconvolution operators.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I also struggled a little to understand what is the difference between forward interpolate and filtering.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
1) There is no clear rationale on why we need a new model based on Transformers for this task.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I am not convinced that the measure theoretic perspective is always' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
4.4, law of total variation -- define' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
However, what was not clear to me is how this reduces the non-stationarity of MARL.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- What is the choice of beta in the beta-VAE training objective?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
1. What is M in Algorithm 1 ?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Not sure why Eqns. 2 and 9 need any parentheses' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Generalizing this to the multi-channel input as the next step could make the proof more accessible.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The egocentric velocity field is not described (section 5)' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
However, the  transfer learning model is unclear.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I have doubts on applying the proposed method to higher dimensional inputs.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
ReLU network with 2 hidden layers, and it is unknown if it works in general.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
positive - it unlikely to be true that an undefended network is predominantly' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
adversarial examples (or counter-examples for property verification) with L_inf' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- Your F and \tilde{f} are introduced as infinite series.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Isn't this just restating the point made in the first sentence?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
3, The notations in Eq. (1) and (2) are messy.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
It is unclear how well the proposed method works in general.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
In the start of Section 3, it is not clear why having the projection be sparse is desired.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
eqn (8): use something else to denote the function 'U'.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
This clipping will also introduce bias, this is not discussed, and will probably lower variance.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
It would be better if the authors were a little more careful in their use of terminology here.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
*The strategy proposed to introduce weak-supervision is too ad-hoc.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- U^m in Eq 1 is undefined and un-discussed.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The term p(w) disappears on the left hand side of Eq 2.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
p2-3, Section 3.1 - I found the equations impossible to read. What' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
It is unclear how the model actually operates and uses attention during execution.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"-	Attention should be given to the notation in formulas (3) and (4).' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
The projection function there is no longer accepts a 3D point parametrized by 3 variables.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
Instead only depth is provided.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
In addition, the subindex ‘1’ of the point ‘q’ is not explained.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.' [SEP] 'rebuttal_future	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".' [SEP] 'rebuttal_future"	While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The equation (1) should hold for any \theta’, not \theta.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The equation (1) should contain \rho, not p.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
For example, it is unclear to me why some larger models are not amenable to truncation.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
From the supplementary, it seems Epsilon means the environment?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"Fourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
The maths is not clear, in particular the gradient derivation in equation (8).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
But again, it's not super clear how the paper estimates this derivative.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
1. Are e_{i,t} and lambda_{i,t} vectors?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
In the algorithm, the authors need to define the HT function in (3) and (4).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"I take issue with the usage of the phrase ""skill discovery"".' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
It is in the end plugged into a continual learning algorithm which also performs domain transformation.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
The purpose of the public set is explained only in section 5.2.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
However, I don't understand the use of $\alpha$ here.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
* Equations (1, 2): z and \phi are not consistently boldfaced' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I believe that the presentation of the proposed method can be significantly improved.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
The method description was a bit confusing and unclear to me.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- the sentence under eq. (2)' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"4. I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
While the general description of the model is clear, details are lacking.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Similarly, you did not indicate what the deterministic version of your model is.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
-Some technical details  are missing.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Because of the above many discussions about discrete vs. continuous variables are missleading.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Many of the parameters here are also unclear and not properly defined/introduced.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
What is the relationship between $\theta$ and $\tilde\theta$ exactly?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The 3rd line of lemma 1: epsilon1 -> epsilon_1' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- Page 14, Eq(14), \lambda should be s' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"Some statements don't make sense, however, eg. ""HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
This makes the contribution of this paper in terms of the method' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- What is dt in Algorithm 1 description?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
In later sections they use theta and theta’ for encoder/decoder resp.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Why mention it here, if it's not being defined.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
Minor, 1/2 is missing in the last line of Eq 19.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Also perhaps better to use the curly sign for vector inequality.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
“From a high-level perspective both of these approaches” --> missing comma after “perspective”' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
In equations 1 and 2, should a, b be written in capital? Since they represent random variables.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- There is a typo in equation 6' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
The paper seems to lack clarity on certain design/ architecture/ model decisions.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Also, I had to go through a large chunk of the paper before coming across the exact setup.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
I did not completely follow the arguments towards directed graph deconvolution operators.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I also struggled a little to understand what is the difference between forward interpolate and filtering.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
1) There is no clear rationale on why we need a new model based on Transformers for this task.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I am not convinced that the measure theoretic perspective is always' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
4.4, law of total variation -- define' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
However, what was not clear to me is how this reduces the non-stationarity of MARL.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- What is the choice of beta in the beta-VAE training objective?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
1. What is M in Algorithm 1 ?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Not sure why Eqns. 2 and 9 need any parentheses' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Generalizing this to the multi-channel input as the next step could make the proof more accessible.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The egocentric velocity field is not described (section 5)' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
However, the  transfer learning model is unclear.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I have doubts on applying the proposed method to higher dimensional inputs.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
ReLU network with 2 hidden layers, and it is unknown if it works in general.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
positive - it unlikely to be true that an undefended network is predominantly' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
adversarial examples (or counter-examples for property verification) with L_inf' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
- Your F and \tilde{f} are introduced as infinite series.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Isn't this just restating the point made in the first sentence?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
3, The notations in Eq. (1) and (2) are messy.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
It is unclear how well the proposed method works in general.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
In the start of Section 3, it is not clear why having the projection be sparse is desired.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
eqn (8): use something else to denote the function 'U'.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
This clipping will also introduce bias, this is not discussed, and will probably lower variance.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
It would be better if the authors were a little more careful in their use of terminology here.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
*The strategy proposed to introduce weak-supervision is too ad-hoc.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
- U^m in Eq 1 is undefined and un-discussed.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The term p(w) disappears on the left hand side of Eq 2.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
p2-3, Section 3.1 - I found the equations impossible to read. What' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
It is unclear how the model actually operates and uses attention during execution.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
"-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
"-	Attention should be given to the notation in formulas (3) and (4).' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
The projection function there is no longer accepts a 3D point parametrized by 3 variables.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
Instead only depth is provided.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
In addition, the subindex ‘1’ of the point ‘q’ is not explained.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.' [SEP] 'rebuttal_done	5. We have added further empirical evidence to show that in the revision.
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".' [SEP] 'rebuttal_done"	5. We have added further empirical evidence to show that in the revision.
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The equation (1) should hold for any \theta’, not \theta.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The equation (1) should contain \rho, not p.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
For example, it is unclear to me why some larger models are not amenable to truncation.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
From the supplementary, it seems Epsilon means the environment?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"Fourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The maths is not clear, in particular the gradient derivation in equation (8).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
But again, it's not super clear how the paper estimates this derivative.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
1. Are e_{i,t} and lambda_{i,t} vectors?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
In the algorithm, the authors need to define the HT function in (3) and (4).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"I take issue with the usage of the phrase ""skill discovery"".' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
It is in the end plugged into a continual learning algorithm which also performs domain transformation.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The purpose of the public set is explained only in section 5.2.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
However, I don't understand the use of $\alpha$ here.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
* Equations (1, 2): z and \phi are not consistently boldfaced' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I believe that the presentation of the proposed method can be significantly improved.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The method description was a bit confusing and unclear to me.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- the sentence under eq. (2)' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"4. I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
While the general description of the model is clear, details are lacking.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Similarly, you did not indicate what the deterministic version of your model is.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
-Some technical details  are missing.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Because of the above many discussions about discrete vs. continuous variables are missleading.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Many of the parameters here are also unclear and not properly defined/introduced.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
What is the relationship between $\theta$ and $\tilde\theta$ exactly?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The 3rd line of lemma 1: epsilon1 -> epsilon_1' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- Page 14, Eq(14), \lambda should be s' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"Some statements don't make sense, however, eg. ""HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
This makes the contribution of this paper in terms of the method' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- What is dt in Algorithm 1 description?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
In later sections they use theta and theta’ for encoder/decoder resp.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Why mention it here, if it's not being defined.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Minor, 1/2 is missing in the last line of Eq 19.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Also perhaps better to use the curly sign for vector inequality.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
“From a high-level perspective both of these approaches” --> missing comma after “perspective”' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
In equations 1 and 2, should a, b be written in capital? Since they represent random variables.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- There is a typo in equation 6' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The paper seems to lack clarity on certain design/ architecture/ model decisions.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Also, I had to go through a large chunk of the paper before coming across the exact setup.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I did not completely follow the arguments towards directed graph deconvolution operators.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I also struggled a little to understand what is the difference between forward interpolate and filtering.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
1) There is no clear rationale on why we need a new model based on Transformers for this task.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I am not convinced that the measure theoretic perspective is always' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
4.4, law of total variation -- define' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
However, what was not clear to me is how this reduces the non-stationarity of MARL.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- What is the choice of beta in the beta-VAE training objective?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
1. What is M in Algorithm 1 ?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Not sure why Eqns. 2 and 9 need any parentheses' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Generalizing this to the multi-channel input as the next step could make the proof more accessible.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The egocentric velocity field is not described (section 5)' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
However, the  transfer learning model is unclear.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I have doubts on applying the proposed method to higher dimensional inputs.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
ReLU network with 2 hidden layers, and it is unknown if it works in general.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
positive - it unlikely to be true that an undefended network is predominantly' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
adversarial examples (or counter-examples for property verification) with L_inf' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- Your F and \tilde{f} are introduced as infinite series.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Isn't this just restating the point made in the first sentence?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
3, The notations in Eq. (1) and (2) are messy.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
It is unclear how well the proposed method works in general.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
In the start of Section 3, it is not clear why having the projection be sparse is desired.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
eqn (8): use something else to denote the function 'U'.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
This clipping will also introduce bias, this is not discussed, and will probably lower variance.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
It would be better if the authors were a little more careful in their use of terminology here.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
*The strategy proposed to introduce weak-supervision is too ad-hoc.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- U^m in Eq 1 is undefined and un-discussed.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The term p(w) disappears on the left hand side of Eq 2.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
p2-3, Section 3.1 - I found the equations impossible to read. What' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
It is unclear how the model actually operates and uses attention during execution.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"-	Attention should be given to the notation in formulas (3) and (4).' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
The projection function there is no longer accepts a 3D point parametrized by 3 variables.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
Instead only depth is provided.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
In addition, the subindex ‘1’ of the point ‘q’ is not explained.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.' [SEP] 'rebuttal_refute-question	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".' [SEP] 'rebuttal_refute-question"	>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The equation (1) should hold for any \theta’, not \theta.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The equation (1) should contain \rho, not p.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
For example, it is unclear to me why some larger models are not amenable to truncation.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
From the supplementary, it seems Epsilon means the environment?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"Fourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The maths is not clear, in particular the gradient derivation in equation (8).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
But again, it's not super clear how the paper estimates this derivative.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
1. Are e_{i,t} and lambda_{i,t} vectors?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
In the algorithm, the authors need to define the HT function in (3) and (4).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"I take issue with the usage of the phrase ""skill discovery"".' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
It is in the end plugged into a continual learning algorithm which also performs domain transformation.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The purpose of the public set is explained only in section 5.2.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
However, I don't understand the use of $\alpha$ here.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
* Equations (1, 2): z and \phi are not consistently boldfaced' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I believe that the presentation of the proposed method can be significantly improved.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The method description was a bit confusing and unclear to me.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- the sentence under eq. (2)' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"4. I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
While the general description of the model is clear, details are lacking.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Similarly, you did not indicate what the deterministic version of your model is.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
-Some technical details  are missing.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Because of the above many discussions about discrete vs. continuous variables are missleading.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Many of the parameters here are also unclear and not properly defined/introduced.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
What is the relationship between $\theta$ and $\tilde\theta$ exactly?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The 3rd line of lemma 1: epsilon1 -> epsilon_1' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- Page 14, Eq(14), \lambda should be s' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"Some statements don't make sense, however, eg. ""HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
This makes the contribution of this paper in terms of the method' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- What is dt in Algorithm 1 description?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
In later sections they use theta and theta’ for encoder/decoder resp.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Why mention it here, if it's not being defined.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Minor, 1/2 is missing in the last line of Eq 19.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Also perhaps better to use the curly sign for vector inequality.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
“From a high-level perspective both of these approaches” --> missing comma after “perspective”' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
In equations 1 and 2, should a, b be written in capital? Since they represent random variables.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- There is a typo in equation 6' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The paper seems to lack clarity on certain design/ architecture/ model decisions.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Also, I had to go through a large chunk of the paper before coming across the exact setup.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I did not completely follow the arguments towards directed graph deconvolution operators.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I also struggled a little to understand what is the difference between forward interpolate and filtering.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
1) There is no clear rationale on why we need a new model based on Transformers for this task.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I am not convinced that the measure theoretic perspective is always' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
4.4, law of total variation -- define' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
However, what was not clear to me is how this reduces the non-stationarity of MARL.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- What is the choice of beta in the beta-VAE training objective?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
1. What is M in Algorithm 1 ?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Not sure why Eqns. 2 and 9 need any parentheses' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Generalizing this to the multi-channel input as the next step could make the proof more accessible.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The egocentric velocity field is not described (section 5)' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
However, the  transfer learning model is unclear.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I have doubts on applying the proposed method to higher dimensional inputs.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
ReLU network with 2 hidden layers, and it is unknown if it works in general.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
positive - it unlikely to be true that an undefended network is predominantly' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
adversarial examples (or counter-examples for property verification) with L_inf' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- Your F and \tilde{f} are introduced as infinite series.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Isn't this just restating the point made in the first sentence?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
3, The notations in Eq. (1) and (2) are messy.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
It is unclear how well the proposed method works in general.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
In the start of Section 3, it is not clear why having the projection be sparse is desired.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
eqn (8): use something else to denote the function 'U'.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
This clipping will also introduce bias, this is not discussed, and will probably lower variance.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
It would be better if the authors were a little more careful in their use of terminology here.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
*The strategy proposed to introduce weak-supervision is too ad-hoc.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- U^m in Eq 1 is undefined and un-discussed.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The term p(w) disappears on the left hand side of Eq 2.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
p2-3, Section 3.1 - I found the equations impossible to read. What' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
It is unclear how the model actually operates and uses attention during execution.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"-	Attention should be given to the notation in formulas (3) and (4).' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
The projection function there is no longer accepts a 3D point parametrized by 3 variables.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Instead only depth is provided.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
In addition, the subindex ‘1’ of the point ‘q’ is not explained.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.' [SEP] 'rebuttal_reject-request	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".' [SEP] 'rebuttal_reject-request"	Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.
Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
As I have mentioned above, actually a lot of methods have been developed to address general image restoration tasks.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Thus I believe authors must compare their method with these state-of-the-art approaches.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The model-based method achieves better validation error than the other baselines that use actual data.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
There are tons of algorithms for image inpainting, denoising, and super-resolution, but the proposed method was not compared with them.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"While the authors say ""attributing a deep network’s prediction to its input is well-studied"" they don't compare directly against these methods.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
* Comparison with other methods did not take into account a variety of hyperparameters.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Hence the theoretical sample complexities contributed are not comparable to those of MIME.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It would have been useful to compare the general models here with some specific math problem-focused ones as well.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The proposed approach should compared to other stronger methods such as graph convolution neural network/message passing neural networks/structure2vec.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
However, it’s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
For example, in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
My main concerns are about the evaluation and comparison of standard neural models.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
My main concern about this paper is why this algorithm has a better performance than CW attack?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
I would suggest comparing with CW attack under different sets of hyper-parameters.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
One of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
This is true throughout; as stated before it is not clear how many parameters and how much memory these methods need, which makes it impossible to compare.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Why is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Three of these methods, i.e. Lime, GradCam, PDA, are not designed for producing contrastive explanations, so I am not sure to what extend this comparison is appropriate.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
However, attack in Wasserstein distance and some other methods can also do so.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Especially if the lower-level component of the hierarchy is pre-trained in a non-MARL setup, it can just be seen as part of the environment from the point of view of the MARL training, offerring limited new insight into MARL.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Finally, the CNN architecture should be compared with modern SAT solvers which have been participating to SAT competitions.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
If the two NNs used in DNN and DNN(resized) are different, I believe it’s still possible to apply the algorithm in DNN(resized) to the NN used in DNN, and get a full trace in the figure as noise and projection changes, which would lead to more fair comparison.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Though Kaleidoscope include these as special cases, it is not clear whether when given the same resources (either memory or computational cost), Kaleidoscope would outperform them.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Have the authors considered to use categorical or binary variables?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Given that BERT is most robust against their manipulation, it would be good to see a more powerful recurrent model for comparison.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Alternatively why not try using a factorization technique to reduce the rank and then see how well the method does for different ranks?' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
1a. Comparison to other exploration methods.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Since the proposed method starts from Laplace transform, it would be helpful to further discuss the connection between other methods that regularises the eigenvalues of the Jacobian (such as spectral-normalisation), which work in the frequency domain from a different perspective.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
But the only negative aspect is the basis competitor algorithms are very simple in nature without any form of learning and that are very old.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Otherwise, it is hard to tell how significant these correlations are. Since the end goal is to determine transferability of tasks and not the methods, it does seem like there are simpler baselines that you could compare against.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- The paper mentions the MSVV algorithm twice but no reference or explanation is provided.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
As I have mentioned above, actually a lot of methods have been developed to address general image restoration tasks.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Thus I believe authors must compare their method with these state-of-the-art approaches.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
The model-based method achieves better validation error than the other baselines that use actual data.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
There are tons of algorithms for image inpainting, denoising, and super-resolution, but the proposed method was not compared with them.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
"While the authors say ""attributing a deep network’s prediction to its input is well-studied"" they don't compare directly against these methods.' [SEP] 'rebuttal_answer"	We have cited this work in our related works section, and mentioned its impact.
* Comparison with other methods did not take into account a variety of hyperparameters.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Hence the theoretical sample complexities contributed are not comparable to those of MIME.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
It would have been useful to compare the general models here with some specific math problem-focused ones as well.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
The proposed approach should compared to other stronger methods such as graph convolution neural network/message passing neural networks/structure2vec.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
However, it’s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
For example, in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
My main concerns are about the evaluation and comparison of standard neural models.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
My main concern about this paper is why this algorithm has a better performance than CW attack?' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
I would suggest comparing with CW attack under different sets of hyper-parameters.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
One of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
This is true throughout; as stated before it is not clear how many parameters and how much memory these methods need, which makes it impossible to compare.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Why is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Three of these methods, i.e. Lime, GradCam, PDA, are not designed for producing contrastive explanations, so I am not sure to what extend this comparison is appropriate.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
However, attack in Wasserstein distance and some other methods can also do so.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Especially if the lower-level component of the hierarchy is pre-trained in a non-MARL setup, it can just be seen as part of the environment from the point of view of the MARL training, offerring limited new insight into MARL.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)?' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Finally, the CNN architecture should be compared with modern SAT solvers which have been participating to SAT competitions.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
If the two NNs used in DNN and DNN(resized) are different, I believe it’s still possible to apply the algorithm in DNN(resized) to the NN used in DNN, and get a full trace in the figure as noise and projection changes, which would lead to more fair comparison.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Though Kaleidoscope include these as special cases, it is not clear whether when given the same resources (either memory or computational cost), Kaleidoscope would outperform them.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Have the authors considered to use categorical or binary variables?' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Given that BERT is most robust against their manipulation, it would be good to see a more powerful recurrent model for comparison.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Alternatively why not try using a factorization technique to reduce the rank and then see how well the method does for different ranks?' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
1a. Comparison to other exploration methods.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
- Since the proposed method starts from Laplace transform, it would be helpful to further discuss the connection between other methods that regularises the eigenvalues of the Jacobian (such as spectral-normalisation), which work in the frequency domain from a different perspective.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
But the only negative aspect is the basis competitor algorithms are very simple in nature without any form of learning and that are very old.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Otherwise, it is hard to tell how significant these correlations are. Since the end goal is to determine transferability of tasks and not the methods, it does seem like there are simpler baselines that you could compare against.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
- The paper mentions the MSVV algorithm twice but no reference or explanation is provided.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model.' [SEP] 'rebuttal_answer	We have cited this work in our related works section, and mentioned its impact.
Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
As I have mentioned above, actually a lot of methods have been developed to address general image restoration tasks.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Thus I believe authors must compare their method with these state-of-the-art approaches.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
The model-based method achieves better validation error than the other baselines that use actual data.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
There are tons of algorithms for image inpainting, denoising, and super-resolution, but the proposed method was not compared with them.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
"While the authors say ""attributing a deep network’s prediction to its input is well-studied"" they don't compare directly against these methods.' [SEP] 'rebuttal_social"	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
* Comparison with other methods did not take into account a variety of hyperparameters.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Hence the theoretical sample complexities contributed are not comparable to those of MIME.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
It would have been useful to compare the general models here with some specific math problem-focused ones as well.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
The proposed approach should compared to other stronger methods such as graph convolution neural network/message passing neural networks/structure2vec.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
However, it’s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
For example, in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
My main concerns are about the evaluation and comparison of standard neural models.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
My main concern about this paper is why this algorithm has a better performance than CW attack?' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
I would suggest comparing with CW attack under different sets of hyper-parameters.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
One of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
This is true throughout; as stated before it is not clear how many parameters and how much memory these methods need, which makes it impossible to compare.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Why is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Three of these methods, i.e. Lime, GradCam, PDA, are not designed for producing contrastive explanations, so I am not sure to what extend this comparison is appropriate.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
However, attack in Wasserstein distance and some other methods can also do so.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Especially if the lower-level component of the hierarchy is pre-trained in a non-MARL setup, it can just be seen as part of the environment from the point of view of the MARL training, offerring limited new insight into MARL.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)?' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Finally, the CNN architecture should be compared with modern SAT solvers which have been participating to SAT competitions.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
If the two NNs used in DNN and DNN(resized) are different, I believe it’s still possible to apply the algorithm in DNN(resized) to the NN used in DNN, and get a full trace in the figure as noise and projection changes, which would lead to more fair comparison.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Though Kaleidoscope include these as special cases, it is not clear whether when given the same resources (either memory or computational cost), Kaleidoscope would outperform them.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Have the authors considered to use categorical or binary variables?' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Given that BERT is most robust against their manipulation, it would be good to see a more powerful recurrent model for comparison.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Alternatively why not try using a factorization technique to reduce the rank and then see how well the method does for different ranks?' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
1a. Comparison to other exploration methods.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
- Since the proposed method starts from Laplace transform, it would be helpful to further discuss the connection between other methods that regularises the eigenvalues of the Jacobian (such as spectral-normalisation), which work in the frequency domain from a different perspective.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
But the only negative aspect is the basis competitor algorithms are very simple in nature without any form of learning and that are very old.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Otherwise, it is hard to tell how significant these correlations are. Since the end goal is to determine transferability of tasks and not the methods, it does seem like there are simpler baselines that you could compare against.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
- The paper mentions the MSVV algorithm twice but no reference or explanation is provided.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model.' [SEP] 'rebuttal_social	We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.
Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
As I have mentioned above, actually a lot of methods have been developed to address general image restoration tasks.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Thus I believe authors must compare their method with these state-of-the-art approaches.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
The model-based method achieves better validation error than the other baselines that use actual data.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
There are tons of algorithms for image inpainting, denoising, and super-resolution, but the proposed method was not compared with them.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
"While the authors say ""attributing a deep network’s prediction to its input is well-studied"" they don't compare directly against these methods.' [SEP] 'rebuttal_by-cr"	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
* Comparison with other methods did not take into account a variety of hyperparameters.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Hence the theoretical sample complexities contributed are not comparable to those of MIME.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
It would have been useful to compare the general models here with some specific math problem-focused ones as well.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
The proposed approach should compared to other stronger methods such as graph convolution neural network/message passing neural networks/structure2vec.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
However, it’s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
For example, in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
My main concerns are about the evaluation and comparison of standard neural models.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
My main concern about this paper is why this algorithm has a better performance than CW attack?' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
I would suggest comparing with CW attack under different sets of hyper-parameters.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
One of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
This is true throughout; as stated before it is not clear how many parameters and how much memory these methods need, which makes it impossible to compare.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Why is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Three of these methods, i.e. Lime, GradCam, PDA, are not designed for producing contrastive explanations, so I am not sure to what extend this comparison is appropriate.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
However, attack in Wasserstein distance and some other methods can also do so.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Especially if the lower-level component of the hierarchy is pre-trained in a non-MARL setup, it can just be seen as part of the environment from the point of view of the MARL training, offerring limited new insight into MARL.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)?' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Finally, the CNN architecture should be compared with modern SAT solvers which have been participating to SAT competitions.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
If the two NNs used in DNN and DNN(resized) are different, I believe it’s still possible to apply the algorithm in DNN(resized) to the NN used in DNN, and get a full trace in the figure as noise and projection changes, which would lead to more fair comparison.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Though Kaleidoscope include these as special cases, it is not clear whether when given the same resources (either memory or computational cost), Kaleidoscope would outperform them.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Have the authors considered to use categorical or binary variables?' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Given that BERT is most robust against their manipulation, it would be good to see a more powerful recurrent model for comparison.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Alternatively why not try using a factorization technique to reduce the rank and then see how well the method does for different ranks?' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
1a. Comparison to other exploration methods.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
- Since the proposed method starts from Laplace transform, it would be helpful to further discuss the connection between other methods that regularises the eigenvalues of the Jacobian (such as spectral-normalisation), which work in the frequency domain from a different perspective.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
But the only negative aspect is the basis competitor algorithms are very simple in nature without any form of learning and that are very old.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Otherwise, it is hard to tell how significant these correlations are. Since the end goal is to determine transferability of tasks and not the methods, it does seem like there are simpler baselines that you could compare against.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
- The paper mentions the MSVV algorithm twice but no reference or explanation is provided.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model.' [SEP] 'rebuttal_by-cr	For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.
Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
As I have mentioned above, actually a lot of methods have been developed to address general image restoration tasks.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Thus I believe authors must compare their method with these state-of-the-art approaches.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
The model-based method achieves better validation error than the other baselines that use actual data.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
There are tons of algorithms for image inpainting, denoising, and super-resolution, but the proposed method was not compared with them.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
"While the authors say ""attributing a deep network’s prediction to its input is well-studied"" they don't compare directly against these methods.' [SEP] 'rebuttal_done"	Complementary experiments have thus been performed, and tables 1, 2 updated.
* Comparison with other methods did not take into account a variety of hyperparameters.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Hence the theoretical sample complexities contributed are not comparable to those of MIME.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
It would have been useful to compare the general models here with some specific math problem-focused ones as well.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
The proposed approach should compared to other stronger methods such as graph convolution neural network/message passing neural networks/structure2vec.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
However, it’s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
For example, in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
My main concerns are about the evaluation and comparison of standard neural models.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
My main concern about this paper is why this algorithm has a better performance than CW attack?' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
I would suggest comparing with CW attack under different sets of hyper-parameters.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
One of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
This is true throughout; as stated before it is not clear how many parameters and how much memory these methods need, which makes it impossible to compare.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Why is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Three of these methods, i.e. Lime, GradCam, PDA, are not designed for producing contrastive explanations, so I am not sure to what extend this comparison is appropriate.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
However, attack in Wasserstein distance and some other methods can also do so.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Especially if the lower-level component of the hierarchy is pre-trained in a non-MARL setup, it can just be seen as part of the environment from the point of view of the MARL training, offerring limited new insight into MARL.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)?' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Finally, the CNN architecture should be compared with modern SAT solvers which have been participating to SAT competitions.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
If the two NNs used in DNN and DNN(resized) are different, I believe it’s still possible to apply the algorithm in DNN(resized) to the NN used in DNN, and get a full trace in the figure as noise and projection changes, which would lead to more fair comparison.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Though Kaleidoscope include these as special cases, it is not clear whether when given the same resources (either memory or computational cost), Kaleidoscope would outperform them.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Have the authors considered to use categorical or binary variables?' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Given that BERT is most robust against their manipulation, it would be good to see a more powerful recurrent model for comparison.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Alternatively why not try using a factorization technique to reduce the rank and then see how well the method does for different ranks?' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
1a. Comparison to other exploration methods.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
- Since the proposed method starts from Laplace transform, it would be helpful to further discuss the connection between other methods that regularises the eigenvalues of the Jacobian (such as spectral-normalisation), which work in the frequency domain from a different perspective.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
But the only negative aspect is the basis competitor algorithms are very simple in nature without any form of learning and that are very old.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Otherwise, it is hard to tell how significant these correlations are. Since the end goal is to determine transferability of tasks and not the methods, it does seem like there are simpler baselines that you could compare against.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
- The paper mentions the MSVV algorithm twice but no reference or explanation is provided.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model.' [SEP] 'rebuttal_done	Complementary experiments have thus been performed, and tables 1, 2 updated.
Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
In summary, I'm inclined to reject this paper given the current version.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
There are a few grammatical/spelling errors that need ironing out.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
I believe that the paper should currently be rejected, but I encourage the authors to revise the paper.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
There are several weaknesses in this paper.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
Nevertheless, I believe that it still has to address some points in order to be better suited for publication:' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
I vote for rejection for four major weaknesses explained as follows.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
"4) There are several typos/grammar issues e.g. ""believed to occurs"", ""important parameters sections"", ""capacity that if efficiently allocated"", etc.).' [SEP] 'rebuttal_structuring"	Q1: “There are a few grammatical/spelling errors that need ironing out.”
- The paper is over the hard page limit for ICLR so needs to be edit to reduce the length.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
I vote to reject the paper at this stage, mainly because of the following three points:' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
The paper would need to be improved substantially in order to appear at a conference like ICLR.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
I do not think this work is ready for publication.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
I think the paper does not fit this conference. It is better to be presented in a Demonstration section at a *ACL conference.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
Overall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
* The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission.' [SEP] 'rebuttal_structuring	Q1: “There are a few grammatical/spelling errors that need ironing out.”
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_concede-criticism"	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_concede-criticism"	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Making this algorithm not very practical.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_concede-criticism"	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
So the closure axiom of a group is violated.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Did you try to have a single network?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_concede-criticism"	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
How does the transformer based method comparing to others?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_concede-criticism"	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_concede-criticism"	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_concede-criticism"	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_concede-criticism"	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_concede-criticism"	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_concede-criticism"	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Lemma 3 is too trivial.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
o feedforward rather than recurrent network;' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
Is that also true in this domain?' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_concede-criticism"	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_concede-criticism	We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Making this algorithm not very practical.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
So the closure axiom of a group is violated.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Did you try to have a single network?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
How does the transformer based method comparing to others?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Lemma 3 is too trivial.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
o feedforward rather than recurrent network;' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
Is that also true in this domain?' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_answer"	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_answer	While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_reject-request"	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_reject-request"	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Making this algorithm not very practical.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_reject-request"	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
So the closure axiom of a group is violated.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Did you try to have a single network?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_reject-request"	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
How does the transformer based method comparing to others?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_reject-request"	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_reject-request"	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_reject-request"	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_reject-request"	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_reject-request"	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_reject-request"	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Lemma 3 is too trivial.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
o feedforward rather than recurrent network;' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
Is that also true in this domain?' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_reject-request"	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_reject-request	Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_structuring"	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_structuring"	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Making this algorithm not very practical.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_structuring"	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
So the closure axiom of a group is violated.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Did you try to have a single network?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_structuring"	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
How does the transformer based method comparing to others?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_structuring"	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_structuring"	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_structuring"	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_structuring"	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_structuring"	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_structuring"	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Lemma 3 is too trivial.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
o feedforward rather than recurrent network;' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
Is that also true in this domain?' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_structuring"	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_structuring	"3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_reject-criticism"	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_reject-criticism"	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Making this algorithm not very practical.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_reject-criticism"	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
So the closure axiom of a group is violated.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Did you try to have a single network?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_reject-criticism"	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
How does the transformer based method comparing to others?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_reject-criticism"	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_reject-criticism"	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_reject-criticism"	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_reject-criticism"	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_reject-criticism"	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_reject-criticism"	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Lemma 3 is too trivial.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
o feedforward rather than recurrent network;' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Is that also true in this domain?' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_reject-criticism"	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_future"	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_future"	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Making this algorithm not very practical.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_future"	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
So the closure axiom of a group is violated.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Did you try to have a single network?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_future"	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
How does the transformer based method comparing to others?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_future"	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_future"	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_future"	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_future"	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_future"	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_future"	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Lemma 3 is too trivial.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
o feedforward rather than recurrent network;' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
Is that also true in this domain?' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_future"	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_future	That said, we agree that using different architectures would strengthen our point and make the paper more convincing.
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_done"	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_done"	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Making this algorithm not very practical.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_done"	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
So the closure axiom of a group is violated.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Did you try to have a single network?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_done"	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
How does the transformer based method comparing to others?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_done"	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_done"	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_done"	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_done"	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_done"	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_done"	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Lemma 3 is too trivial.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
o feedforward rather than recurrent network;' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
Is that also true in this domain?' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_done"	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_done	Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_other"	This comment was also made in the official blind review #2.
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_other"	This comment was also made in the official blind review #2.
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Making this algorithm not very practical.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_other"	This comment was also made in the official blind review #2.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
So the closure axiom of a group is violated.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Did you try to have a single network?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_other"	This comment was also made in the official blind review #2.
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
How does the transformer based method comparing to others?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_other"	This comment was also made in the official blind review #2.
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_other"	This comment was also made in the official blind review #2.
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_other"	This comment was also made in the official blind review #2.
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_other"	This comment was also made in the official blind review #2.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_other"	This comment was also made in the official blind review #2.
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_other"	This comment was also made in the official blind review #2.
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Lemma 3 is too trivial.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
o feedforward rather than recurrent network;' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
Is that also true in this domain?' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_other"	This comment was also made in the official blind review #2.
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_other	This comment was also made in the official blind review #2.
I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
Attacking CRBMs is highly relevant and should be included as a baseline.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
However, the idea is very related to Yeh et al.’s work which has already published but not mentioned at all.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
- For semi-supervised classification, the paper did not report the best results in other baselines.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2].' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
1. The paper should also talk about the details of ARNet and discuss the difference, as I assume they are the most related work' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
It would probably help to position the VAE component more precisely w.r.t. one of the two baselines, by indicating the differences.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
"The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, ""here are some simple functions for which we would need the additional parameters that we define"" makes sense; but arguing that Hartford et al. ""fail approximating rather simple functions"" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting).' [SEP] 'rebuttal_structuring"	[R3: Weakness: It would be good to see some comparison to the state of the art ]
There should be a better discussion of related work on the topic.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
=> Baselines: The comparison provided in the paper is weak.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
Moreover, in spite of the authors writing that their goal is “completely different” from [Lee at al 18a, Ma et al 18a], I found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
A proper baseline should have been compared.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
Third, the comparison to baseline and “DeepSet” is not fair.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
4. Comparison with past works.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
This baseline was also missing in image reconstruction.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
This seems like a very weak baseline---there are any number of other reasonable ways to encode this.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
For e.g. the results on the oscillating behavior of Dirac-GAN are described in related works (e.g. Mescheder et al. 2018), and in practice, WGAN with no regularization is not used (as well as GAN with momentum, as normally beta1=0 in practice).' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
I am also wondering if the comparison with the baselines is fair.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
In other words, although in the present results, the proposed NF-SGAN/WGAN outperforms the baseline, the reported performance of the baselines is worse than in related works on CIFAR10.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it’s being reported as 6.24.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
Weakness: It would be good to see some comparison to the state of the art' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
The main issue of this paper is the fair comparisons with other works.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
- Baseline missing: Random actions from expert' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
- Baseline missing: Simple RNN policies that communicate hidden states.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
- comparison should be made to the linear composition method in the Arora, Liang, Ma ICLR 2017 paper' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
However, there is no comparison against existing work.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
However, memory overhead is still an issue compared to existing method.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
It is important to place the contributions in this paper in context of these other works.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
A number of these references are missing and no experimental comparison to these methods has been made.' [SEP] 'rebuttal_structuring	[R3: Weakness: It would be good to see some comparison to the state of the art ]
"* In related work, no reference to previous work on ""statistical"" approaches to NN' [SEP] 'rebuttal_structuring"	[R3: Weakness: It would be good to see some comparison to the state of the art ]
I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
Attacking CRBMs is highly relevant and should be included as a baseline.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
However, the idea is very related to Yeh et al.’s work which has already published but not mentioned at all.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
- For semi-supervised classification, the paper did not report the best results in other baselines.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2].' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
1. The paper should also talk about the details of ARNet and discuss the difference, as I assume they are the most related work' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
It would probably help to position the VAE component more precisely w.r.t. one of the two baselines, by indicating the differences.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
"The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, ""here are some simple functions for which we would need the additional parameters that we define"" makes sense; but arguing that Hartford et al. ""fail approximating rather simple functions"" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting).' [SEP] 'rebuttal_by-cr"	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
There should be a better discussion of related work on the topic.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
=> Baselines: The comparison provided in the paper is weak.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
Moreover, in spite of the authors writing that their goal is “completely different” from [Lee at al 18a, Ma et al 18a], I found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
A proper baseline should have been compared.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
Third, the comparison to baseline and “DeepSet” is not fair.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
4. Comparison with past works.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
This baseline was also missing in image reconstruction.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
This seems like a very weak baseline---there are any number of other reasonable ways to encode this.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
For e.g. the results on the oscillating behavior of Dirac-GAN are described in related works (e.g. Mescheder et al. 2018), and in practice, WGAN with no regularization is not used (as well as GAN with momentum, as normally beta1=0 in practice).' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
I am also wondering if the comparison with the baselines is fair.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
In other words, although in the present results, the proposed NF-SGAN/WGAN outperforms the baseline, the reported performance of the baselines is worse than in related works on CIFAR10.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it’s being reported as 6.24.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
Weakness: It would be good to see some comparison to the state of the art' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
The main issue of this paper is the fair comparisons with other works.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
- Baseline missing: Random actions from expert' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
- Baseline missing: Simple RNN policies that communicate hidden states.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
- comparison should be made to the linear composition method in the Arora, Liang, Ma ICLR 2017 paper' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
However, there is no comparison against existing work.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
However, memory overhead is still an issue compared to existing method.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
It is important to place the contributions in this paper in context of these other works.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
A number of these references are missing and no experimental comparison to these methods has been made.' [SEP] 'rebuttal_by-cr	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
"* In related work, no reference to previous work on ""statistical"" approaches to NN' [SEP] 'rebuttal_by-cr"	Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.
I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
Attacking CRBMs is highly relevant and should be included as a baseline.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
However, the idea is very related to Yeh et al.’s work which has already published but not mentioned at all.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
- For semi-supervised classification, the paper did not report the best results in other baselines.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2].' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
1. The paper should also talk about the details of ARNet and discuss the difference, as I assume they are the most related work' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
It would probably help to position the VAE component more precisely w.r.t. one of the two baselines, by indicating the differences.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
"The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, ""here are some simple functions for which we would need the additional parameters that we define"" makes sense; but arguing that Hartford et al. ""fail approximating rather simple functions"" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting).' [SEP] 'rebuttal_reject-criticism"	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
There should be a better discussion of related work on the topic.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
=> Baselines: The comparison provided in the paper is weak.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
Moreover, in spite of the authors writing that their goal is “completely different” from [Lee at al 18a, Ma et al 18a], I found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
A proper baseline should have been compared.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
Third, the comparison to baseline and “DeepSet” is not fair.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
4. Comparison with past works.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
This baseline was also missing in image reconstruction.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
This seems like a very weak baseline---there are any number of other reasonable ways to encode this.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
For e.g. the results on the oscillating behavior of Dirac-GAN are described in related works (e.g. Mescheder et al. 2018), and in practice, WGAN with no regularization is not used (as well as GAN with momentum, as normally beta1=0 in practice).' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
I am also wondering if the comparison with the baselines is fair.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
In other words, although in the present results, the proposed NF-SGAN/WGAN outperforms the baseline, the reported performance of the baselines is worse than in related works on CIFAR10.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it’s being reported as 6.24.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
Weakness: It would be good to see some comparison to the state of the art' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
The main issue of this paper is the fair comparisons with other works.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
- Baseline missing: Random actions from expert' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
- Baseline missing: Simple RNN policies that communicate hidden states.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
- comparison should be made to the linear composition method in the Arora, Liang, Ma ICLR 2017 paper' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
However, there is no comparison against existing work.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
However, memory overhead is still an issue compared to existing method.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
It is important to place the contributions in this paper in context of these other works.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
A number of these references are missing and no experimental comparison to these methods has been made.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
"* In related work, no reference to previous work on ""statistical"" approaches to NN' [SEP] 'rebuttal_reject-criticism"	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
Finally, the experimental part is also too weak to evaluate the proposed method.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
However, the experiments feel like they are missing motivation as to why this method is being used.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
The experiments on SHREC17 show all three spherical methods under-performing other approaches.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
However I find the white-box experiments lacking as almost every method has 100% success rate.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
In the current experiments there is a comparison only with CO algorithm and SGDA.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
So overall I have the feeling that the authors have not succeeded to evaluate the model’s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
I also was not convinced by the experiments which are mostly qualitative. I did not find that this set of experiments provide enough support to the proposed method.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class ‘8’). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above — if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?' [SEP] 'rebuttal_reject-criticism	However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.
In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
Finally, the experimental part is also too weak to evaluate the proposed method.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
However, the experiments feel like they are missing motivation as to why this method is being used.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
The experiments on SHREC17 show all three spherical methods under-performing other approaches.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
However I find the white-box experiments lacking as almost every method has 100% success rate.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
In the current experiments there is a comparison only with CO algorithm and SGDA.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
So overall I have the feeling that the authors have not succeeded to evaluate the model’s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
I also was not convinced by the experiments which are mostly qualitative. I did not find that this set of experiments provide enough support to the proposed method.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class ‘8’). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above — if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?' [SEP] 'rebuttal_by-cr	4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.
In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
Finally, the experimental part is also too weak to evaluate the proposed method.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
However, the experiments feel like they are missing motivation as to why this method is being used.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
The experiments on SHREC17 show all three spherical methods under-performing other approaches.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
However I find the white-box experiments lacking as almost every method has 100% success rate.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
In the current experiments there is a comparison only with CO algorithm and SGDA.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
So overall I have the feeling that the authors have not succeeded to evaluate the model’s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
I also was not convinced by the experiments which are mostly qualitative. I did not find that this set of experiments provide enough support to the proposed method.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class ‘8’). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above — if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?' [SEP] 'rebuttal_future	However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.
In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
Finally, the experimental part is also too weak to evaluate the proposed method.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
However, the experiments feel like they are missing motivation as to why this method is being used.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
The experiments on SHREC17 show all three spherical methods under-performing other approaches.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
However I find the white-box experiments lacking as almost every method has 100% success rate.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
In the current experiments there is a comparison only with CO algorithm and SGDA.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
So overall I have the feeling that the authors have not succeeded to evaluate the model’s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
I also was not convinced by the experiments which are mostly qualitative. I did not find that this set of experiments provide enough support to the proposed method.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class ‘8’). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above — if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?' [SEP] 'rebuttal_social	2. Thank you for your suggestion.
5. lack of related work:  4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR 2019.' [SEP] 'rebuttal_summary	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
2. Missed citation: MnasNet [2] also incorporates the cost of architectures in their search process.' [SEP] 'rebuttal_summary	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.' [SEP] 'rebuttal_summary	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
(3) A large body of graph neural network literature is omitted.' [SEP] 'rebuttal_summary	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.' [SEP] 'rebuttal_summary	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
This is a thriving area that requires a careful literature review.' [SEP] 'rebuttal_summary	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
2) Related work, although extensive in terms of the number of references, do not help to place this work in the literature.' [SEP] 'rebuttal_summary	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
Listing related work is no the same as describing similarities and differences compared to previous methods.' [SEP] 'rebuttal_summary	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
The paper misses the key baseline in Bayesian optimisation using tree structure [1] which can perform the prediction under the tree-structure dependencies.' [SEP] 'rebuttal_summary	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
There are some well-cited works that the authors may have missed. These are ultimately different approaches, but perhaps the authors can obtain some inspiration from these:' [SEP] 'rebuttal_summary	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
5, Although there is a notable difference, one may properly mention previous work Yamamoto et al. (2019), which uses GAN as an auxiliary loss within ClariNet and obtains high-fidelity speech ( https://r9y9.github.io/demos/projects/interspeech2019/ ).' [SEP] 'rebuttal_summary	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
"5. The term ""PowerSGD"" seems to have been used by other papers.' [SEP] 'rebuttal_summary"	Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.
5. lack of related work:  4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR 2019.' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
2. Missed citation: MnasNet [2] also incorporates the cost of architectures in their search process.' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
(3) A large body of graph neural network literature is omitted.' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
This is a thriving area that requires a careful literature review.' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
2) Related work, although extensive in terms of the number of references, do not help to place this work in the literature.' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
Listing related work is no the same as describing similarities and differences compared to previous methods.' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
The paper misses the key baseline in Bayesian optimisation using tree structure [1] which can perform the prediction under the tree-structure dependencies.' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
There are some well-cited works that the authors may have missed. These are ultimately different approaches, but perhaps the authors can obtain some inspiration from these:' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
5, Although there is a notable difference, one may properly mention previous work Yamamoto et al. (2019), which uses GAN as an auxiliary loss within ClariNet and obtains high-fidelity speech ( https://r9y9.github.io/demos/projects/interspeech2019/ ).' [SEP] 'rebuttal_done	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
"5. The term ""PowerSGD"" seems to have been used by other papers.' [SEP] 'rebuttal_done"	5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.
- Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch?' [SEP] 'rebuttal_done	We have already uploaded our source codes as well as the demonstration videos to the following sites.
- Although the authors discussed the experiment setting in detail in supplements, I believe open-sourcing the code / software used to conduct the experiments would be greatly help with the reproducibility of the proposed method for researchers or practitioners.' [SEP] 'rebuttal_done	We have already uploaded our source codes as well as the demonstration videos to the following sites.
The results  are overall not very impressive.' [SEP] 'rebuttal_done	We will update the labels in the ablation table to make this more clear.
The result does not at all apply to all of them.' [SEP] 'rebuttal_done	We will update the labels in the ablation table to make this more clear.
Then to state to which of these cases the results of the paper are applicable, allow for an improvement of the variance and at what additional computational cost (considering the cost of evaluating the discrete derivatives).' [SEP] 'rebuttal_done	We will update the labels in the ablation table to make this more clear.
Generally speaking it seems like a lot of technicalities for a relatively simple result:' [SEP] 'rebuttal_done	We will update the labels in the ablation table to make this more clear.
The results of the paper do not give major insights into what are the preferred techniques for training GANs, and certainly not why and under what circumstances they'll work.' [SEP] 'rebuttal_done	We will update the labels in the ablation table to make this more clear.
This is not so interesting, even though results are impressive.' [SEP] 'rebuttal_done	We will update the labels in the ablation table to make this more clear.
The results  are overall not very impressive.' [SEP] 'rebuttal_reject-criticism	[A]  We respectfully disagree.
The result does not at all apply to all of them.' [SEP] 'rebuttal_reject-criticism	[A]  We respectfully disagree.
Then to state to which of these cases the results of the paper are applicable, allow for an improvement of the variance and at what additional computational cost (considering the cost of evaluating the discrete derivatives).' [SEP] 'rebuttal_reject-criticism	[A]  We respectfully disagree.
Generally speaking it seems like a lot of technicalities for a relatively simple result:' [SEP] 'rebuttal_reject-criticism	[A]  We respectfully disagree.
The results of the paper do not give major insights into what are the preferred techniques for training GANs, and certainly not why and under what circumstances they'll work.' [SEP] 'rebuttal_reject-criticism	[A]  We respectfully disagree.
This is not so interesting, even though results are impressive.' [SEP] 'rebuttal_reject-criticism	[A]  We respectfully disagree.
While the authors do report some interesting results, they do a poor job of motivating the proposed extensions.' [SEP] 'rebuttal_structuring	3. Are the authors willing to release the code?
These points remain me puzzled regarding either practical or theoretical application of the result. It would be great if authors could elaborate.' [SEP] 'rebuttal_structuring	3. Are the authors willing to release the code?
Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.' [SEP] 'rebuttal_structuring	3. Are the authors willing to release the code?
"Can you explain somewhere exactly what you mean when you say ""learning dynamics of deep learning""? Given the specific nature of the results presented in the paper it would be nice to be precise also when it comes to the overall topic under study.' [SEP] 'rebuttal_structuring"	3. Are the authors willing to release the code?
"3.	Are the authors willing to release the code? Overall the model looks complicated and the appendix is not sufficient to reproduce the results in the paper.' [SEP] 'rebuttal_structuring"	3. Are the authors willing to release the code?
I trust that the authors did in fact achieve these results but I cannot figure out how or why.' [SEP] 'rebuttal_structuring	3. Are the authors willing to release the code?
While the authors do report some interesting results, they do a poor job of motivating the proposed extensions.' [SEP] 'rebuttal_answer	We based our title on that paper since it extends some of its results to nonlinear neural networks.
These points remain me puzzled regarding either practical or theoretical application of the result. It would be great if authors could elaborate.' [SEP] 'rebuttal_answer	We based our title on that paper since it extends some of its results to nonlinear neural networks.
Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.' [SEP] 'rebuttal_answer	We based our title on that paper since it extends some of its results to nonlinear neural networks.
"Can you explain somewhere exactly what you mean when you say ""learning dynamics of deep learning""? Given the specific nature of the results presented in the paper it would be nice to be precise also when it comes to the overall topic under study.' [SEP] 'rebuttal_answer"	We based our title on that paper since it extends some of its results to nonlinear neural networks.
"3.	Are the authors willing to release the code? Overall the model looks complicated and the appendix is not sufficient to reproduce the results in the paper.' [SEP] 'rebuttal_answer"	We based our title on that paper since it extends some of its results to nonlinear neural networks.
I trust that the authors did in fact achieve these results but I cannot figure out how or why.' [SEP] 'rebuttal_answer	We based our title on that paper since it extends some of its results to nonlinear neural networks.
ii) In table 2, I don’t really see any promising results compared to baselines. There are' [SEP] 'rebuttal_by-cr	Thanks for pointing this out! We will make the presentation of the results consistent by highlighting the respective number.
One thing that puts me off is that, in Table 2, AnyBurl (the single one baseline authors considered other than the original NeuralLP) yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.' [SEP] 'rebuttal_by-cr	Thanks for pointing this out! We will make the presentation of the results consistent by highlighting the respective number.
I do not see much insight into the problem.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
The main problem with this paper is that it is difficult to identify its main and novel contributions.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
At the conceptual level, the idea of jointly modeling local video events is not novel, and can date back to at least ten years ago in the paper “Learning realistic human actions from movies”, where the temporal pyramid matching was combined with the bag-of-visual-words framework to capture long-term temporal structure.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
The idea of having a separate class for out-distribution is a very interesting idea but unfortunately previously explored.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
The idea that introduces labels in VAE is not novel.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
At a high level, the idea of imposing a mixture of gaussian structure in the feature space of a deep neural network classifier is not new.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
The idea to extend the use of VAE-GANs to the video prediction setting is a pretty natural one and not especially novel.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
- The idea is a simple extension of existing work.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
Although the idea seems to be interesting, the paper seems to be a bit incremental and is a simple application of existing GAN techniques.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
ii) Although the idea of generating contrastive explanations is quite interesting, it is not that novel.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
Last comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
* the idea of smoothing gradients is not new' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
- the idea is trivial and simple. I don't think there's significant novelty here: all the components are existing and combining them seems very trivial to me.' [SEP] 'rebuttal_structuring	1. We first want to point out the main contributions of the paper.
I do not see much insight into the problem.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
The main problem with this paper is that it is difficult to identify its main and novel contributions.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
At the conceptual level, the idea of jointly modeling local video events is not novel, and can date back to at least ten years ago in the paper “Learning realistic human actions from movies”, where the temporal pyramid matching was combined with the bag-of-visual-words framework to capture long-term temporal structure.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
The idea of having a separate class for out-distribution is a very interesting idea but unfortunately previously explored.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
The idea that introduces labels in VAE is not novel.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
At a high level, the idea of imposing a mixture of gaussian structure in the feature space of a deep neural network classifier is not new.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
The idea to extend the use of VAE-GANs to the video prediction setting is a pretty natural one and not especially novel.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
- The idea is a simple extension of existing work.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
Although the idea seems to be interesting, the paper seems to be a bit incremental and is a simple application of existing GAN techniques.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
ii) Although the idea of generating contrastive explanations is quite interesting, it is not that novel.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
Last comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
* the idea of smoothing gradients is not new' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
- the idea is trivial and simple. I don't think there's significant novelty here: all the components are existing and combining them seems very trivial to me.' [SEP] 'rebuttal_reject-criticism	Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.
Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.' [SEP] 'rebuttal_structuring	> I am reluctant to give a higher score due to its incremental contribution.
Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.' [SEP] 'rebuttal_structuring	> I am reluctant to give a higher score due to its incremental contribution.
Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).' [SEP] 'rebuttal_structuring	> I am reluctant to give a higher score due to its incremental contribution.
The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper.' [SEP] 'rebuttal_structuring	> I am reluctant to give a higher score due to its incremental contribution.
[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this.' [SEP] 'rebuttal_structuring	> I am reluctant to give a higher score due to its incremental contribution.
"The proposed method adapt a previously presented hierarchical clustering method in the ""standard space"" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model.' [SEP] 'rebuttal_structuring"	> I am reluctant to give a higher score due to its incremental contribution.
The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.' [SEP] 'rebuttal_structuring	> I am reluctant to give a higher score due to its incremental contribution.
Further the contributions are not clear at all, since a large part of the detailed approach/equations relate to the masking which was taken from previous work.' [SEP] 'rebuttal_structuring	> I am reluctant to give a higher score due to its incremental contribution.
Furthermore, there are several similar ideas already published, so comparison against those (for example by J. Peng, N. Heess or J. Mere) either as argument or even better as experiment, would be helpful to evaluate the quality of the proposed hierarchy.' [SEP] 'rebuttal_structuring	> I am reluctant to give a higher score due to its incremental contribution.
The proposed method PowerSGD is an extension of the method in Yuan et al. (extended to handle stochastic gradient and momentum).' [SEP] 'rebuttal_structuring	> I am reluctant to give a higher score due to its incremental contribution.
"A weighted k-means solver is also used in [2], though the ""weighted"" in [2] comes from second-order information instead of minimizing reconstruction error.' [SEP] 'rebuttal_structuring"	> I am reluctant to give a higher score due to its incremental contribution.
Once a low-level walking controller is trained, the high-level multi-agent navigation control is not much different from simple environments, e.g. point mass control, used in the previous works.' [SEP] 'rebuttal_structuring	> I am reluctant to give a higher score due to its incremental contribution.
Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.' [SEP] 'rebuttal_reject-criticism	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.' [SEP] 'rebuttal_reject-criticism	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).' [SEP] 'rebuttal_reject-criticism	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper.' [SEP] 'rebuttal_reject-criticism	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this.' [SEP] 'rebuttal_reject-criticism	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
"The proposed method adapt a previously presented hierarchical clustering method in the ""standard space"" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model.' [SEP] 'rebuttal_reject-criticism"	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.' [SEP] 'rebuttal_reject-criticism	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
Further the contributions are not clear at all, since a large part of the detailed approach/equations relate to the masking which was taken from previous work.' [SEP] 'rebuttal_reject-criticism	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
Furthermore, there are several similar ideas already published, so comparison against those (for example by J. Peng, N. Heess or J. Mere) either as argument or even better as experiment, would be helpful to evaluate the quality of the proposed hierarchy.' [SEP] 'rebuttal_reject-criticism	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
The proposed method PowerSGD is an extension of the method in Yuan et al. (extended to handle stochastic gradient and momentum).' [SEP] 'rebuttal_reject-criticism	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
"A weighted k-means solver is also used in [2], though the ""weighted"" in [2] comes from second-order information instead of minimizing reconstruction error.' [SEP] 'rebuttal_reject-criticism"	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
Once a low-level walking controller is trained, the high-level multi-agent navigation control is not much different from simple environments, e.g. point mass control, used in the previous works.' [SEP] 'rebuttal_reject-criticism	We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.
Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.' [SEP] 'rebuttal_done	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.' [SEP] 'rebuttal_done	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).' [SEP] 'rebuttal_done	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper.' [SEP] 'rebuttal_done	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this.' [SEP] 'rebuttal_done	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
"The proposed method adapt a previously presented hierarchical clustering method in the ""standard space"" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model.' [SEP] 'rebuttal_done"	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.' [SEP] 'rebuttal_done	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
Further the contributions are not clear at all, since a large part of the detailed approach/equations relate to the masking which was taken from previous work.' [SEP] 'rebuttal_done	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
Furthermore, there are several similar ideas already published, so comparison against those (for example by J. Peng, N. Heess or J. Mere) either as argument or even better as experiment, would be helpful to evaluate the quality of the proposed hierarchy.' [SEP] 'rebuttal_done	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
The proposed method PowerSGD is an extension of the method in Yuan et al. (extended to handle stochastic gradient and momentum).' [SEP] 'rebuttal_done	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
"A weighted k-means solver is also used in [2], though the ""weighted"" in [2] comes from second-order information instead of minimizing reconstruction error.' [SEP] 'rebuttal_done"	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
Once a low-level walking controller is trained, the high-level multi-agent navigation control is not much different from simple environments, e.g. point mass control, used in the previous works.' [SEP] 'rebuttal_done	We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).
"4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the ""model complexity"" introduced upto numerical constants.' [SEP] 'rebuttal_done"	We added more baselines to further strengthen the significance of our work with respect to the previous approaches.
2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)?' [SEP] 'rebuttal_done	We added more baselines to further strengthen the significance of our work with respect to the previous approaches.
My main concern about the paper is that, currently, the experiments do not include any strong baseline (the ES currently is not a strong baseline, see comments below).' [SEP] 'rebuttal_done	We added more baselines to further strengthen the significance of our work with respect to the previous approaches.
"4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the ""model complexity"" introduced upto numerical constants.' [SEP] 'rebuttal_structuring"	Missing experiments to validate nature of bounds.
2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)?' [SEP] 'rebuttal_structuring	Missing experiments to validate nature of bounds.
My main concern about the paper is that, currently, the experiments do not include any strong baseline (the ES currently is not a strong baseline, see comments below).' [SEP] 'rebuttal_structuring	Missing experiments to validate nature of bounds.
Since quantitative real-world results are challenging to obtain, improved presentation of the qualitative results would be helpful as well.' [SEP] 'rebuttal_answer	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
A reader’s most natural question is whether there is a large enough improvement to offset the extra computational cost, so the fact that wall-clock times are relegated to the appendix is a significant weakness.' [SEP] 'rebuttal_answer	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
Particularly, the authors mixed their observations up with the results of published works, making it hard to identify the contributions of this paper.' [SEP] 'rebuttal_answer	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
"- There are better words than ""decent"" to describe the performance of DARTS, as it's very similar to the results in this work!' [SEP] 'rebuttal_answer"	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.' [SEP] 'rebuttal_answer	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
- Lack of a strong explanation for the results or a solution to the problem' [SEP] 'rebuttal_answer	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
Since the results are far from state of the art, a clean and neat presentation of the theoretical advantages and contributions is crucial.' [SEP] 'rebuttal_answer	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
It is hard to understand the results without discussing it.' [SEP] 'rebuttal_answer	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
For instance, the omission of a results discussion section or a conclusion are clearly not reader friendly.' [SEP] 'rebuttal_answer	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
They would have a very low weight difference score though they are ideal representations for each other.' [SEP] 'rebuttal_answer	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
"* Some turns of phrase like ""recently gained a flourishing interest"", ""there is still a wide gap in quality of results"", ""which implies a variety of underlying factors"", ... are vague / do not make much sense and should probably be reformulated to enhance readability.' [SEP] 'rebuttal_answer"	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
- Section 1.1 presents results with too many details without introducing the problem.' [SEP] 'rebuttal_answer	We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.
Since quantitative real-world results are challenging to obtain, improved presentation of the qualitative results would be helpful as well.' [SEP] 'rebuttal_done	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
A reader’s most natural question is whether there is a large enough improvement to offset the extra computational cost, so the fact that wall-clock times are relegated to the appendix is a significant weakness.' [SEP] 'rebuttal_done	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
Particularly, the authors mixed their observations up with the results of published works, making it hard to identify the contributions of this paper.' [SEP] 'rebuttal_done	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
"- There are better words than ""decent"" to describe the performance of DARTS, as it's very similar to the results in this work!' [SEP] 'rebuttal_done"	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.' [SEP] 'rebuttal_done	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
- Lack of a strong explanation for the results or a solution to the problem' [SEP] 'rebuttal_done	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
Since the results are far from state of the art, a clean and neat presentation of the theoretical advantages and contributions is crucial.' [SEP] 'rebuttal_done	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
It is hard to understand the results without discussing it.' [SEP] 'rebuttal_done	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
For instance, the omission of a results discussion section or a conclusion are clearly not reader friendly.' [SEP] 'rebuttal_done	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
They would have a very low weight difference score though they are ideal representations for each other.' [SEP] 'rebuttal_done	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
"* Some turns of phrase like ""recently gained a flourishing interest"", ""there is still a wide gap in quality of results"", ""which implies a variety of underlying factors"", ... are vague / do not make much sense and should probably be reformulated to enhance readability.' [SEP] 'rebuttal_done"	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
- Section 1.1 presents results with too many details without introducing the problem.' [SEP] 'rebuttal_done	>> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.
Since quantitative real-world results are challenging to obtain, improved presentation of the qualitative results would be helpful as well.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
A reader’s most natural question is whether there is a large enough improvement to offset the extra computational cost, so the fact that wall-clock times are relegated to the appendix is a significant weakness.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Particularly, the authors mixed their observations up with the results of published works, making it hard to identify the contributions of this paper.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"- There are better words than ""decent"" to describe the performance of DARTS, as it's very similar to the results in this work!' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Lack of a strong explanation for the results or a solution to the problem' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
Since the results are far from state of the art, a clean and neat presentation of the theoretical advantages and contributions is crucial.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
It is hard to understand the results without discussing it.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
For instance, the omission of a results discussion section or a conclusion are clearly not reader friendly.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
They would have a very low weight difference score though they are ideal representations for each other.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
"* Some turns of phrase like ""recently gained a flourishing interest"", ""there is still a wide gap in quality of results"", ""which implies a variety of underlying factors"", ... are vague / do not make much sense and should probably be reformulated to enhance readability.' [SEP] 'rebuttal_concede-criticism"	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
- Section 1.1 presents results with too many details without introducing the problem.' [SEP] 'rebuttal_concede-criticism	Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).
3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
- Does training the generator and interpolation jointly improve the quality of the generator in general ? It would have been nice to run this method on more complicated dataset like CIFAR10 and see if this method increase the overall FID score.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
Although the proposed method does a good job in synthetic experiments, outperforming existing methods by a large margin, its performance on the numerical variants of Freebase/DBPedia dataset does not show consistent significant improvement.' [SEP] 'rebuttal_reject-criticism	Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.
