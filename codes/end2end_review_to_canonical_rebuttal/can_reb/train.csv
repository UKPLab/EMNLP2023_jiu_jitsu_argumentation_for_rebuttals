reviews	canonical_rebuttals
which is much smaller than the number of time series usually involved say in gene regulatory network data' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
"2.	Data size is too small, and the baselines' [SEP] 'rebuttal_concede-criticism"	We apologize for unclear description of experimental settings.
Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
The descriptions of the datasets used are not clear, e.g., the number of classes for each data.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
First, the labeled data portion is fixed and is relatively high' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Is it better to decay learning rates for toy data sets?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
It is difficult to judge the performance of the proposed model based on so small data set.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
- How much does the image matter for the single-image data set?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons"".' [SEP] 'rebuttal_concede-criticism"	We apologize for unclear description of experimental settings.
It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Third, the datasets used in this paper are rather limited.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Thus, it might be helpful to test the result on another dataset (e.g. WikiText).' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
However image captioning datasets are not mentioned.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
It would make sense to use image captioning data to create the image lookup.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
- Lack of an extensive exploration of datasets' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?' [SEP] 'rebuttal_concede-criticism"	We apologize for unclear description of experimental settings.
7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
o use of the Penn Treebank dataset only;' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.' [SEP] 'rebuttal_concede-criticism"	We apologize for unclear description of experimental settings.
1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
which is much smaller than the number of time series usually involved say in gene regulatory network data' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
"2.	Data size is too small, and the baselines' [SEP] 'rebuttal_structuring"	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
The descriptions of the datasets used are not clear, e.g., the number of classes for each data.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
First, the labeled data portion is fixed and is relatively high' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
Is it better to decay learning rates for toy data sets?' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
It is difficult to judge the performance of the proposed model based on so small data set.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
- How much does the image matter for the single-image data set?' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons"".' [SEP] 'rebuttal_structuring"	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
Third, the datasets used in this paper are rather limited.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
Thus, it might be helpful to test the result on another dataset (e.g. WikiText).' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
However image captioning datasets are not mentioned.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
It would make sense to use image captioning data to create the image lookup.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
- Lack of an extensive exploration of datasets' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?' [SEP] 'rebuttal_structuring"	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
o use of the Penn Treebank dataset only;' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.' [SEP] 'rebuttal_structuring"	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?' [SEP] 'rebuttal_structuring	7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
which is much smaller than the number of time series usually involved say in gene regulatory network data' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
"2.	Data size is too small, and the baselines' [SEP] 'rebuttal_done"	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
The descriptions of the datasets used are not clear, e.g., the number of classes for each data.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
First, the labeled data portion is fixed and is relatively high' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Is it better to decay learning rates for toy data sets?' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
It is difficult to judge the performance of the proposed model based on so small data set.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
- How much does the image matter for the single-image data set?' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons"".' [SEP] 'rebuttal_done"	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Third, the datasets used in this paper are rather limited.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Thus, it might be helpful to test the result on another dataset (e.g. WikiText).' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
However image captioning datasets are not mentioned.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
It would make sense to use image captioning data to create the image lookup.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
- Lack of an extensive exploration of datasets' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?' [SEP] 'rebuttal_done"	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
o use of the Penn Treebank dataset only;' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.' [SEP] 'rebuttal_done"	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?' [SEP] 'rebuttal_done	First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.
which is much smaller than the number of time series usually involved say in gene regulatory network data' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
"2.	Data size is too small, and the baselines' [SEP] 'rebuttal_answer"	The reviewer raises an important point about the tested single images.
Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
The descriptions of the datasets used are not clear, e.g., the number of classes for each data.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
First, the labeled data portion is fixed and is relatively high' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
Is it better to decay learning rates for toy data sets?' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
It is difficult to judge the performance of the proposed model based on so small data set.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
- How much does the image matter for the single-image data set?' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons"".' [SEP] 'rebuttal_answer"	The reviewer raises an important point about the tested single images.
It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
Third, the datasets used in this paper are rather limited.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
Thus, it might be helpful to test the result on another dataset (e.g. WikiText).' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
However image captioning datasets are not mentioned.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
It would make sense to use image captioning data to create the image lookup.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
- Lack of an extensive exploration of datasets' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?' [SEP] 'rebuttal_answer"	The reviewer raises an important point about the tested single images.
7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
o use of the Penn Treebank dataset only;' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.' [SEP] 'rebuttal_answer"	The reviewer raises an important point about the tested single images.
1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?' [SEP] 'rebuttal_answer	The reviewer raises an important point about the tested single images.
which is much smaller than the number of time series usually involved say in gene regulatory network data' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
"2.	Data size is too small, and the baselines' [SEP] 'rebuttal_by-cr"	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
The descriptions of the datasets used are not clear, e.g., the number of classes for each data.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
First, the labeled data portion is fixed and is relatively high' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
Is it better to decay learning rates for toy data sets?' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
It is difficult to judge the performance of the proposed model based on so small data set.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
- How much does the image matter for the single-image data set?' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons"".' [SEP] 'rebuttal_by-cr"	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
Third, the datasets used in this paper are rather limited.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
Thus, it might be helpful to test the result on another dataset (e.g. WikiText).' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
However image captioning datasets are not mentioned.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
It would make sense to use image captioning data to create the image lookup.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
- Lack of an extensive exploration of datasets' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?' [SEP] 'rebuttal_by-cr"	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
o use of the Penn Treebank dataset only;' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.' [SEP] 'rebuttal_by-cr"	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?' [SEP] 'rebuttal_by-cr	We are training DeepCluster now on a significantly less busy image and will report results in the coming days.
which is much smaller than the number of time series usually involved say in gene regulatory network data' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
"2.	Data size is too small, and the baselines' [SEP] 'rebuttal_future"	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
The descriptions of the datasets used are not clear, e.g., the number of classes for each data.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
First, the labeled data portion is fixed and is relatively high' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
Is it better to decay learning rates for toy data sets?' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
It is difficult to judge the performance of the proposed model based on so small data set.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
- How much does the image matter for the single-image data set?' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons"".' [SEP] 'rebuttal_future"	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
Third, the datasets used in this paper are rather limited.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
Thus, it might be helpful to test the result on another dataset (e.g. WikiText).' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
However image captioning datasets are not mentioned.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
It would make sense to use image captioning data to create the image lookup.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
- Lack of an extensive exploration of datasets' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?' [SEP] 'rebuttal_future"	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
o use of the Penn Treebank dataset only;' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.' [SEP] 'rebuttal_future"	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?' [SEP] 'rebuttal_future	We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.
which is much smaller than the number of time series usually involved say in gene regulatory network data' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
"2.	Data size is too small, and the baselines' [SEP] 'rebuttal_mitigate-criticism"	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
The descriptions of the datasets used are not clear, e.g., the number of classes for each data.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
First, the labeled data portion is fixed and is relatively high' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
Is it better to decay learning rates for toy data sets?' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
It is difficult to judge the performance of the proposed model based on so small data set.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
- How much does the image matter for the single-image data set?' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons"".' [SEP] 'rebuttal_mitigate-criticism"	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
Third, the datasets used in this paper are rather limited.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
Thus, it might be helpful to test the result on another dataset (e.g. WikiText).' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
However image captioning datasets are not mentioned.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
It would make sense to use image captioning data to create the image lookup.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
- Lack of an extensive exploration of datasets' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?' [SEP] 'rebuttal_mitigate-criticism"	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
o use of the Penn Treebank dataset only;' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.' [SEP] 'rebuttal_mitigate-criticism"	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?' [SEP] 'rebuttal_mitigate-criticism	We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.
which is much smaller than the number of time series usually involved say in gene regulatory network data' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
"2.	Data size is too small, and the baselines' [SEP] 'rebuttal_social"	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
The descriptions of the datasets used are not clear, e.g., the number of classes for each data.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
First, the labeled data portion is fixed and is relatively high' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
Is it better to decay learning rates for toy data sets?' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
It is difficult to judge the performance of the proposed model based on so small data set.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
- How much does the image matter for the single-image data set?' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons"".' [SEP] 'rebuttal_social"	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
Third, the datasets used in this paper are rather limited.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
Thus, it might be helpful to test the result on another dataset (e.g. WikiText).' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
However image captioning datasets are not mentioned.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
It would make sense to use image captioning data to create the image lookup.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
- Lack of an extensive exploration of datasets' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?' [SEP] 'rebuttal_social"	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
o use of the Penn Treebank dataset only;' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.' [SEP] 'rebuttal_social"	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?' [SEP] 'rebuttal_social	We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.
which is much smaller than the number of time series usually involved say in gene regulatory network data' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
"2.	Data size is too small, and the baselines' [SEP] 'rebuttal_summary"	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
The descriptions of the datasets used are not clear, e.g., the number of classes for each data.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
First, the labeled data portion is fixed and is relatively high' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
Is it better to decay learning rates for toy data sets?' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
It is difficult to judge the performance of the proposed model based on so small data set.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
* There is still no comparison with competing nonparametric tests on the fMRI data.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
- How much does the image matter for the single-image data set?' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons"".' [SEP] 'rebuttal_summary"	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
Third, the datasets used in this paper are rather limited.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
Thus, it might be helpful to test the result on another dataset (e.g. WikiText).' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
However image captioning datasets are not mentioned.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
It would make sense to use image captioning data to create the image lookup.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
- Lack of an extensive exploration of datasets' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?' [SEP] 'rebuttal_summary"	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
o use of the Penn Treebank dataset only;' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.' [SEP] 'rebuttal_summary"	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?' [SEP] 'rebuttal_summary	Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.
- In Section 3.1 : “Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.” For me, this is not particularly clear.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- In Appendix D, the Figures could be slightly clarified by using a colored heatmap to color the curve, with colors corresponding to the threshold values.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation?' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- Figures 1-4 are difficult to interpreted on a printed version.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- figures 2 & 3 should be a lot larger in order to be readable' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
4. Fig. 3 (right): It is not clear' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
On the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the Fig. is very confusing.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
2. Figure 3: it is confusing to call the cumulative distribution of the maximum classification score as the CDF of the model (y-axis fig. 3 left) as CDF means something else generally in such contexts, as the CDF of a predictor.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
"2. In the caption of figure 2, there should be a space after `"":"".' [SEP] 'rebuttal_answer"	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
"- In figure 3 (c) ""number |T of input"" should be  ""number |T| of input""' [SEP] 'rebuttal_answer"	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
"- In figure 5 (a) ""cencept"" should be ""concept""' [SEP] 'rebuttal_answer"	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
"- In figure 8 ""Each column corresponds to ..."" should be ""Each row corresponds to ..."".' [SEP] 'rebuttal_answer"	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
Fig 4 is very confusing.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
First, it doesn’t label the X axis.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
Third, I don’t get what is plotted on different subplots.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
The input and output types of each block in Figure 1. should be clearly stated.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
The figures are almost useless, because the captions contain very little information.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
"For example, the authors should at least say that the ""D"" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned.' [SEP] 'rebuttal_answer"	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
Many more can be said in all the figures.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
-What’s the 3d plot supposed to represent?' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
=> The results shown in Figure-4 (Section-4.2) seems unclear to me.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
* Figure 5 should appear after Figure 4.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
The labels of figures are hard to read.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
c. Figure 1 is over-complicated.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
e. What are the two modalities in Table 2? The author should explain.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
2. I don’t understand Figure 4.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
I am confused by Figure 4, and in general with the relative rank metrics.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- Some of the figures your report are compelling but it is a bit unclear to the reader if the results are general (e.g., the examples could have been hand-picked). Are there any quantitative measures you could provide (in addition to Tables 1 and 2 which don't measure the quality of the approach)?' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
The explanation in Fig 2 on why this is the case seem to me not so clear. Are you trying to show that the Wave-U-Net does not work since there is no 1/f^2 law for clean audio signals?' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
Typo:. The “Inf” in Tabel 1' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
If this is what these graphs show, consider using a different visualization to make it clearer that you're improving the final performance, not just the training process.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
2. In Figure 3, the baseline got different perplexity between 3(a) and 3(b).' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- Text on experiment figures is much too small.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
2. table 2: Dynamic -> Adaptive?' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- The aspect ratio in Fig. 5 should be fixed.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
The plot (Figure2 (d)) is very blurry and people cannot really tell local structure from it.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
Some figures, like Figure 3 and 4, are hard to read.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
• Not all of the arrows in Figure 1 are pointing to the right lines.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
With the current table, one has to compare cells in the top half of the table to those in the bottom half of the table, which is quite difficult to do.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
The plots in figure 4 are too small.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- the exposition could be improved, in particular the description of the plots is not very clear, I'm still not sure exactly what they show' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
The graphs were difficult to parse.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
3) I would suggest a different name other than Neural-LP-N, as it is somewhat underselling this work. Also it makes Table 2 not that easy to read.' [SEP] 'rebuttal_answer	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
"-	Figures 1 is way too abstract given the complicated set-up of the proposed architecture.' [SEP] 'rebuttal_answer"	[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.
- In Section 3.1 : “Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.” For me, this is not particularly clear.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- In Appendix D, the Figures could be slightly clarified by using a colored heatmap to color the curve, with colors corresponding to the threshold values.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation?' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- Figures 1-4 are difficult to interpreted on a printed version.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- figures 2 & 3 should be a lot larger in order to be readable' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
4. Fig. 3 (right): It is not clear' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
On the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the Fig. is very confusing.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
2. Figure 3: it is confusing to call the cumulative distribution of the maximum classification score as the CDF of the model (y-axis fig. 3 left) as CDF means something else generally in such contexts, as the CDF of a predictor.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
"2. In the caption of figure 2, there should be a space after `"":"".' [SEP] 'rebuttal_structuring"	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
"- In figure 3 (c) ""number |T of input"" should be  ""number |T| of input""' [SEP] 'rebuttal_structuring"	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
"- In figure 5 (a) ""cencept"" should be ""concept""' [SEP] 'rebuttal_structuring"	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
"- In figure 8 ""Each column corresponds to ..."" should be ""Each row corresponds to ..."".' [SEP] 'rebuttal_structuring"	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
Fig 4 is very confusing.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
First, it doesn’t label the X axis.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
Third, I don’t get what is plotted on different subplots.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
The input and output types of each block in Figure 1. should be clearly stated.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
The figures are almost useless, because the captions contain very little information.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
"For example, the authors should at least say that the ""D"" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned.' [SEP] 'rebuttal_structuring"	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
Many more can be said in all the figures.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
-What’s the 3d plot supposed to represent?' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
=> The results shown in Figure-4 (Section-4.2) seems unclear to me.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
* Figure 5 should appear after Figure 4.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
The labels of figures are hard to read.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
c. Figure 1 is over-complicated.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
e. What are the two modalities in Table 2? The author should explain.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
2. I don’t understand Figure 4.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
I am confused by Figure 4, and in general with the relative rank metrics.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- Some of the figures your report are compelling but it is a bit unclear to the reader if the results are general (e.g., the examples could have been hand-picked). Are there any quantitative measures you could provide (in addition to Tables 1 and 2 which don't measure the quality of the approach)?' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
The explanation in Fig 2 on why this is the case seem to me not so clear. Are you trying to show that the Wave-U-Net does not work since there is no 1/f^2 law for clean audio signals?' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
Typo:. The “Inf” in Tabel 1' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
If this is what these graphs show, consider using a different visualization to make it clearer that you're improving the final performance, not just the training process.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
2. In Figure 3, the baseline got different perplexity between 3(a) and 3(b).' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- Text on experiment figures is much too small.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
2. table 2: Dynamic -> Adaptive?' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- The aspect ratio in Fig. 5 should be fixed.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
The plot (Figure2 (d)) is very blurry and people cannot really tell local structure from it.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
Some figures, like Figure 3 and 4, are hard to read.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
• Not all of the arrows in Figure 1 are pointing to the right lines.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
With the current table, one has to compare cells in the top half of the table to those in the bottom half of the table, which is quite difficult to do.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
The plots in figure 4 are too small.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- the exposition could be improved, in particular the description of the plots is not very clear, I'm still not sure exactly what they show' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
The graphs were difficult to parse.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
3) I would suggest a different name other than Neural-LP-N, as it is somewhat underselling this work. Also it makes Table 2 not that easy to read.' [SEP] 'rebuttal_structuring	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
"-	Figures 1 is way too abstract given the complicated set-up of the proposed architecture.' [SEP] 'rebuttal_structuring"	> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.
- In Section 3.1 : “Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.” For me, this is not particularly clear.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- In Appendix D, the Figures could be slightly clarified by using a colored heatmap to color the curve, with colors corresponding to the threshold values.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation?' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- Figures 1-4 are difficult to interpreted on a printed version.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- figures 2 & 3 should be a lot larger in order to be readable' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
4. Fig. 3 (right): It is not clear' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
On the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the Fig. is very confusing.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
2. Figure 3: it is confusing to call the cumulative distribution of the maximum classification score as the CDF of the model (y-axis fig. 3 left) as CDF means something else generally in such contexts, as the CDF of a predictor.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
"2. In the caption of figure 2, there should be a space after `"":"".' [SEP] 'rebuttal_by-cr"	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
"- In figure 3 (c) ""number |T of input"" should be  ""number |T| of input""' [SEP] 'rebuttal_by-cr"	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
"- In figure 5 (a) ""cencept"" should be ""concept""' [SEP] 'rebuttal_by-cr"	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
"- In figure 8 ""Each column corresponds to ..."" should be ""Each row corresponds to ..."".' [SEP] 'rebuttal_by-cr"	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
Fig 4 is very confusing.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
First, it doesn’t label the X axis.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
Third, I don’t get what is plotted on different subplots.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
The input and output types of each block in Figure 1. should be clearly stated.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
The figures are almost useless, because the captions contain very little information.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
"For example, the authors should at least say that the ""D"" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned.' [SEP] 'rebuttal_by-cr"	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
Many more can be said in all the figures.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
-What’s the 3d plot supposed to represent?' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
=> The results shown in Figure-4 (Section-4.2) seems unclear to me.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
* Figure 5 should appear after Figure 4.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
The labels of figures are hard to read.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
c. Figure 1 is over-complicated.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
e. What are the two modalities in Table 2? The author should explain.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
2. I don’t understand Figure 4.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
I am confused by Figure 4, and in general with the relative rank metrics.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- Some of the figures your report are compelling but it is a bit unclear to the reader if the results are general (e.g., the examples could have been hand-picked). Are there any quantitative measures you could provide (in addition to Tables 1 and 2 which don't measure the quality of the approach)?' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
The explanation in Fig 2 on why this is the case seem to me not so clear. Are you trying to show that the Wave-U-Net does not work since there is no 1/f^2 law for clean audio signals?' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
Typo:. The “Inf” in Tabel 1' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
If this is what these graphs show, consider using a different visualization to make it clearer that you're improving the final performance, not just the training process.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
2. In Figure 3, the baseline got different perplexity between 3(a) and 3(b).' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- Text on experiment figures is much too small.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
2. table 2: Dynamic -> Adaptive?' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- The aspect ratio in Fig. 5 should be fixed.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
The plot (Figure2 (d)) is very blurry and people cannot really tell local structure from it.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
Some figures, like Figure 3 and 4, are hard to read.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
• Not all of the arrows in Figure 1 are pointing to the right lines.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
With the current table, one has to compare cells in the top half of the table to those in the bottom half of the table, which is quite difficult to do.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
The plots in figure 4 are too small.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- the exposition could be improved, in particular the description of the plots is not very clear, I'm still not sure exactly what they show' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
The graphs were difficult to parse.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
3) I would suggest a different name other than Neural-LP-N, as it is somewhat underselling this work. Also it makes Table 2 not that easy to read.' [SEP] 'rebuttal_by-cr	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
"-	Figures 1 is way too abstract given the complicated set-up of the proposed architecture.' [SEP] 'rebuttal_by-cr"	We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.
- In Section 3.1 : “Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.” For me, this is not particularly clear.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- In Appendix D, the Figures could be slightly clarified by using a colored heatmap to color the curve, with colors corresponding to the threshold values.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation?' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- Figures 1-4 are difficult to interpreted on a printed version.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- figures 2 & 3 should be a lot larger in order to be readable' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
4. Fig. 3 (right): It is not clear' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
On the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the Fig. is very confusing.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
2. Figure 3: it is confusing to call the cumulative distribution of the maximum classification score as the CDF of the model (y-axis fig. 3 left) as CDF means something else generally in such contexts, as the CDF of a predictor.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
"2. In the caption of figure 2, there should be a space after `"":"".' [SEP] 'rebuttal_done"	Thanks for this; we have updated the draft to make the presentation clearer.
"- In figure 3 (c) ""number |T of input"" should be  ""number |T| of input""' [SEP] 'rebuttal_done"	Thanks for this; we have updated the draft to make the presentation clearer.
"- In figure 5 (a) ""cencept"" should be ""concept""' [SEP] 'rebuttal_done"	Thanks for this; we have updated the draft to make the presentation clearer.
"- In figure 8 ""Each column corresponds to ..."" should be ""Each row corresponds to ..."".' [SEP] 'rebuttal_done"	Thanks for this; we have updated the draft to make the presentation clearer.
Fig 4 is very confusing.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
First, it doesn’t label the X axis.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
Third, I don’t get what is plotted on different subplots.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
The input and output types of each block in Figure 1. should be clearly stated.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
The figures are almost useless, because the captions contain very little information.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
"For example, the authors should at least say that the ""D"" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned.' [SEP] 'rebuttal_done"	Thanks for this; we have updated the draft to make the presentation clearer.
Many more can be said in all the figures.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
-What’s the 3d plot supposed to represent?' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
=> The results shown in Figure-4 (Section-4.2) seems unclear to me.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
* Figure 5 should appear after Figure 4.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
The labels of figures are hard to read.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
c. Figure 1 is over-complicated.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
e. What are the two modalities in Table 2? The author should explain.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
2. I don’t understand Figure 4.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
I am confused by Figure 4, and in general with the relative rank metrics.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- Some of the figures your report are compelling but it is a bit unclear to the reader if the results are general (e.g., the examples could have been hand-picked). Are there any quantitative measures you could provide (in addition to Tables 1 and 2 which don't measure the quality of the approach)?' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
The explanation in Fig 2 on why this is the case seem to me not so clear. Are you trying to show that the Wave-U-Net does not work since there is no 1/f^2 law for clean audio signals?' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
Typo:. The “Inf” in Tabel 1' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
If this is what these graphs show, consider using a different visualization to make it clearer that you're improving the final performance, not just the training process.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
2. In Figure 3, the baseline got different perplexity between 3(a) and 3(b).' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- Text on experiment figures is much too small.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
2. table 2: Dynamic -> Adaptive?' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- The aspect ratio in Fig. 5 should be fixed.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
The plot (Figure2 (d)) is very blurry and people cannot really tell local structure from it.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
Some figures, like Figure 3 and 4, are hard to read.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
• Not all of the arrows in Figure 1 are pointing to the right lines.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
With the current table, one has to compare cells in the top half of the table to those in the bottom half of the table, which is quite difficult to do.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
The plots in figure 4 are too small.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- the exposition could be improved, in particular the description of the plots is not very clear, I'm still not sure exactly what they show' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
The graphs were difficult to parse.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
3) I would suggest a different name other than Neural-LP-N, as it is somewhat underselling this work. Also it makes Table 2 not that easy to read.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
"-	Figures 1 is way too abstract given the complicated set-up of the proposed architecture.' [SEP] 'rebuttal_done"	Thanks for this; we have updated the draft to make the presentation clearer.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- The proposed approach is a fairly specific form of self-modulation.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Can you clarify how you view the relationship between the approaches mentioned above?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- The randomized weight is not very practical. Though it may be the standard approach of mean field,' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Why should we use embedding to compare the similarity?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I find these assumptions too strong for the task of learning disentangled representation.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The proposed method is very simple and frames the problem basically as a supervised learning problem.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
They can indeed be subsumed by generalization bounds based on VC theory.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
One question which is not addressed is the reason for only one RBM layer.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
My assumption is the visual feature already contains the label information for image captioning.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Is the number right?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The function composition doesn't capture that.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Concerns: I find the claim on deep networks kind of irresponsible.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Second, the conclusion of Theorem 2 seems to be flawed.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This illustrates that the conclusion of Theorem 2 may be wrong.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The paper does not provide very significant evidence that this method is useful.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The definition of “recovering true factor exactly” need to be given.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The descent lemma used by the author is not valid for the stochastic result.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This might be because the Bayes and MAT attacks are too simplistic.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- can you motivate why you are not using perplexity in section 3.2?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- In Theorem 4.7 an expectation on g(x_a) is missing' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Either case, I don't think we can have the inequality in eq. (5).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
1. I had hard time to understand latent canonicalization.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
More explanation of canonicalization is needed.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
3. How can the proposed method be generalized to non-image data?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Is there any explanation for this?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- doesn’t answer the one question regarding observation representation that it set out to evaluate' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- presumably, the sample complexity is ridiculous' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- Sec 4.1:' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
1. The classification of base class into super classes seems questionable to me.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Authors should analyze the stability of their method in details.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
How is this a reasonable assumption?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This hyperparameter itself benefits from (requires?) some scheduling.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
As a matter of good implementation, one never takes the inverse of anything.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
How would the given graph network compare to this?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Could it be that what we are seeing is the attack being denoised?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Does such approximation guarantee the policy improvement?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
There might be other constructions that are more efficient and less restrictive.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Therefore, it is a little misleading to still call it Bayesian active learning.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Please explain the logic for this architectural choice.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
1. The approach is not well justified either by theory or practice.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The algorithm assumptions are strong.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, in this way, when more and more tasks come, the generator will become larger and larger.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The proposed method is also heuristic and lacks promising guarantee.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, the assumption is too strong.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, the proposed technique does not seem to be handling the problem foundationally well.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The paper positions itself generally as dealing with arbitrary transformations T, but really is' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It is not clear to me if NF would improve stability/performances in general games.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
My concern here is that beta might be affecting the result more than the proposed training algorithm.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
How do performance and model size trade off?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
How were the number of layers and kernels chosen?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Was the 5x10x20x10 topology used for MNIST the only topology tried?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Negative aspects: One major concern I have with the paper is the notion of privacy considered.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I do not see the GAN style approach taken by the paper, ensures this.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Without such a guarantee, the proposed method is not very useful because we' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I consider this assumption unrealistic.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
In this case, it is basically a GD, not SGD any more.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
4. The biggest flaw that I see in this method is the practicality of it's use.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
To me these two reasoning statements are not particularly convincing. One could also say:' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I believe the proposed techniques have some flaws which hurt the eventual method.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
My concern is that the flaws in the method do not make it conducive to use as is.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This again greatly concerned me as I am not certain how stable these metrics are.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The proposed  CLF weight difference method has some concerning aspects as well.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Thus the metric is not a proper metric.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
As for GAN, due to the inexact update, it is not really solving the min-max problem.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The question is, why one would exlude the mixture-of-softmax approach here?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
A more general function is' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Then, can the function family the authors used in the paper approximate this function?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- for GN optimization, lambda should be set to 0 - not a constant value.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Eq (2) cannot have Delta Chi on the two sides.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
before Eq (3): the 'photometric ..' -> a 'photometric ..'' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
But there are some minus ones in the random projection?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
So there is no guarantee this algorithm will minimise the overall regret.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- It does not seem necessary to predict cumulative mixture policies (ASN network).' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The only problem I see is that phrase similarity part is not convincing.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
x is already present within the indicator, no need to add yet' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
It is thus unclear if the approach is robust against different hyperparameter settings.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.' [SEP] 'rebuttal_concede-criticism	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.' [SEP] 'rebuttal_concede-criticism"	We focus on simpler domains to provide proof-of-concept results as the first step on this direction.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- The proposed approach is a fairly specific form of self-modulation.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Can you clarify how you view the relationship between the approaches mentioned above?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- The randomized weight is not very practical. Though it may be the standard approach of mean field,' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Why should we use embedding to compare the similarity?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I find these assumptions too strong for the task of learning disentangled representation.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The proposed method is very simple and frames the problem basically as a supervised learning problem.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
They can indeed be subsumed by generalization bounds based on VC theory.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
One question which is not addressed is the reason for only one RBM layer.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
My assumption is the visual feature already contains the label information for image captioning.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Is the number right?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The function composition doesn't capture that.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Concerns: I find the claim on deep networks kind of irresponsible.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Second, the conclusion of Theorem 2 seems to be flawed.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This illustrates that the conclusion of Theorem 2 may be wrong.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The paper does not provide very significant evidence that this method is useful.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The definition of “recovering true factor exactly” need to be given.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The descent lemma used by the author is not valid for the stochastic result.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This might be because the Bayes and MAT attacks are too simplistic.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- can you motivate why you are not using perplexity in section 3.2?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- In Theorem 4.7 an expectation on g(x_a) is missing' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Either case, I don't think we can have the inequality in eq. (5).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
1. I had hard time to understand latent canonicalization.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
More explanation of canonicalization is needed.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
3. How can the proposed method be generalized to non-image data?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Is there any explanation for this?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- doesn’t answer the one question regarding observation representation that it set out to evaluate' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- presumably, the sample complexity is ridiculous' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- Sec 4.1:' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
1. The classification of base class into super classes seems questionable to me.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Authors should analyze the stability of their method in details.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
How is this a reasonable assumption?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This hyperparameter itself benefits from (requires?) some scheduling.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
As a matter of good implementation, one never takes the inverse of anything.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
How would the given graph network compare to this?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Could it be that what we are seeing is the attack being denoised?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Does such approximation guarantee the policy improvement?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
There might be other constructions that are more efficient and less restrictive.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Therefore, it is a little misleading to still call it Bayesian active learning.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Please explain the logic for this architectural choice.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
1. The approach is not well justified either by theory or practice.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The algorithm assumptions are strong.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, in this way, when more and more tasks come, the generator will become larger and larger.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The proposed method is also heuristic and lacks promising guarantee.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, the assumption is too strong.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, the proposed technique does not seem to be handling the problem foundationally well.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The paper positions itself generally as dealing with arbitrary transformations T, but really is' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It is not clear to me if NF would improve stability/performances in general games.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
My concern here is that beta might be affecting the result more than the proposed training algorithm.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
How do performance and model size trade off?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
How were the number of layers and kernels chosen?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Was the 5x10x20x10 topology used for MNIST the only topology tried?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Negative aspects: One major concern I have with the paper is the notion of privacy considered.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I do not see the GAN style approach taken by the paper, ensures this.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Without such a guarantee, the proposed method is not very useful because we' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I consider this assumption unrealistic.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
In this case, it is basically a GD, not SGD any more.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
4. The biggest flaw that I see in this method is the practicality of it's use.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
To me these two reasoning statements are not particularly convincing. One could also say:' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I believe the proposed techniques have some flaws which hurt the eventual method.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
My concern is that the flaws in the method do not make it conducive to use as is.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This again greatly concerned me as I am not certain how stable these metrics are.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The proposed  CLF weight difference method has some concerning aspects as well.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Thus the metric is not a proper metric.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
As for GAN, due to the inexact update, it is not really solving the min-max problem.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The question is, why one would exlude the mixture-of-softmax approach here?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
A more general function is' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Then, can the function family the authors used in the paper approximate this function?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- for GN optimization, lambda should be set to 0 - not a constant value.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Eq (2) cannot have Delta Chi on the two sides.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
before Eq (3): the 'photometric ..' -> a 'photometric ..'' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
But there are some minus ones in the random projection?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
So there is no guarantee this algorithm will minimise the overall regret.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- It does not seem necessary to predict cumulative mixture policies (ASN network).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The only problem I see is that phrase similarity part is not convincing.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
x is already present within the indicator, no need to add yet' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It is thus unclear if the approach is robust against different hyperparameter settings.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- The proposed approach is a fairly specific form of self-modulation.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Can you clarify how you view the relationship between the approaches mentioned above?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- The randomized weight is not very practical. Though it may be the standard approach of mean field,' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Why should we use embedding to compare the similarity?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I find these assumptions too strong for the task of learning disentangled representation.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The proposed method is very simple and frames the problem basically as a supervised learning problem.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
They can indeed be subsumed by generalization bounds based on VC theory.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
One question which is not addressed is the reason for only one RBM layer.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
My assumption is the visual feature already contains the label information for image captioning.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Is the number right?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The function composition doesn't capture that.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Concerns: I find the claim on deep networks kind of irresponsible.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Second, the conclusion of Theorem 2 seems to be flawed.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This illustrates that the conclusion of Theorem 2 may be wrong.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The paper does not provide very significant evidence that this method is useful.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The definition of “recovering true factor exactly” need to be given.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The descent lemma used by the author is not valid for the stochastic result.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This might be because the Bayes and MAT attacks are too simplistic.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- can you motivate why you are not using perplexity in section 3.2?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- In Theorem 4.7 an expectation on g(x_a) is missing' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Either case, I don't think we can have the inequality in eq. (5).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1. I had hard time to understand latent canonicalization.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
More explanation of canonicalization is needed.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
3. How can the proposed method be generalized to non-image data?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Is there any explanation for this?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- doesn’t answer the one question regarding observation representation that it set out to evaluate' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- presumably, the sample complexity is ridiculous' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- Sec 4.1:' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1. The classification of base class into super classes seems questionable to me.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Authors should analyze the stability of their method in details.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
How is this a reasonable assumption?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This hyperparameter itself benefits from (requires?) some scheduling.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
As a matter of good implementation, one never takes the inverse of anything.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
How would the given graph network compare to this?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Could it be that what we are seeing is the attack being denoised?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Does such approximation guarantee the policy improvement?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
There might be other constructions that are more efficient and less restrictive.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Therefore, it is a little misleading to still call it Bayesian active learning.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Please explain the logic for this architectural choice.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1. The approach is not well justified either by theory or practice.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The algorithm assumptions are strong.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, in this way, when more and more tasks come, the generator will become larger and larger.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The proposed method is also heuristic and lacks promising guarantee.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, the assumption is too strong.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, the proposed technique does not seem to be handling the problem foundationally well.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The paper positions itself generally as dealing with arbitrary transformations T, but really is' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is not clear to me if NF would improve stability/performances in general games.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
My concern here is that beta might be affecting the result more than the proposed training algorithm.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
How do performance and model size trade off?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
How were the number of layers and kernels chosen?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Was the 5x10x20x10 topology used for MNIST the only topology tried?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Negative aspects: One major concern I have with the paper is the notion of privacy considered.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I do not see the GAN style approach taken by the paper, ensures this.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Without such a guarantee, the proposed method is not very useful because we' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I consider this assumption unrealistic.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
In this case, it is basically a GD, not SGD any more.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
4. The biggest flaw that I see in this method is the practicality of it's use.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
To me these two reasoning statements are not particularly convincing. One could also say:' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I believe the proposed techniques have some flaws which hurt the eventual method.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
My concern is that the flaws in the method do not make it conducive to use as is.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This again greatly concerned me as I am not certain how stable these metrics are.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The proposed  CLF weight difference method has some concerning aspects as well.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Thus the metric is not a proper metric.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
As for GAN, due to the inexact update, it is not really solving the min-max problem.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The question is, why one would exlude the mixture-of-softmax approach here?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
A more general function is' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Then, can the function family the authors used in the paper approximate this function?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- for GN optimization, lambda should be set to 0 - not a constant value.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Eq (2) cannot have Delta Chi on the two sides.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
before Eq (3): the 'photometric ..' -> a 'photometric ..'' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
But there are some minus ones in the random projection?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
So there is no guarantee this algorithm will minimise the overall regret.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- It does not seem necessary to predict cumulative mixture policies (ASN network).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The only problem I see is that phrase similarity part is not convincing.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
x is already present within the indicator, no need to add yet' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is thus unclear if the approach is robust against different hyperparameter settings.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The proposed approach is a fairly specific form of self-modulation.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Can you clarify how you view the relationship between the approaches mentioned above?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The randomized weight is not very practical. Though it may be the standard approach of mean field,' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Why should we use embedding to compare the similarity?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I find these assumptions too strong for the task of learning disentangled representation.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The proposed method is very simple and frames the problem basically as a supervised learning problem.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
They can indeed be subsumed by generalization bounds based on VC theory.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
One question which is not addressed is the reason for only one RBM layer.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
My assumption is the visual feature already contains the label information for image captioning.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Is the number right?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The function composition doesn't capture that.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Concerns: I find the claim on deep networks kind of irresponsible.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Second, the conclusion of Theorem 2 seems to be flawed.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This illustrates that the conclusion of Theorem 2 may be wrong.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The paper does not provide very significant evidence that this method is useful.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The definition of “recovering true factor exactly” need to be given.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The descent lemma used by the author is not valid for the stochastic result.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This might be because the Bayes and MAT attacks are too simplistic.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- can you motivate why you are not using perplexity in section 3.2?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- In Theorem 4.7 an expectation on g(x_a) is missing' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Either case, I don't think we can have the inequality in eq. (5).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. I had hard time to understand latent canonicalization.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
More explanation of canonicalization is needed.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
3. How can the proposed method be generalized to non-image data?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Is there any explanation for this?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- doesn’t answer the one question regarding observation representation that it set out to evaluate' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- presumably, the sample complexity is ridiculous' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Sec 4.1:' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. The classification of base class into super classes seems questionable to me.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Authors should analyze the stability of their method in details.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
How is this a reasonable assumption?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This hyperparameter itself benefits from (requires?) some scheduling.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
As a matter of good implementation, one never takes the inverse of anything.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
How would the given graph network compare to this?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Could it be that what we are seeing is the attack being denoised?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Does such approximation guarantee the policy improvement?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
There might be other constructions that are more efficient and less restrictive.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Therefore, it is a little misleading to still call it Bayesian active learning.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Please explain the logic for this architectural choice.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. The approach is not well justified either by theory or practice.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The algorithm assumptions are strong.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, in this way, when more and more tasks come, the generator will become larger and larger.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The proposed method is also heuristic and lacks promising guarantee.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, the assumption is too strong.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, the proposed technique does not seem to be handling the problem foundationally well.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The paper positions itself generally as dealing with arbitrary transformations T, but really is' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is not clear to me if NF would improve stability/performances in general games.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
My concern here is that beta might be affecting the result more than the proposed training algorithm.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
How do performance and model size trade off?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
How were the number of layers and kernels chosen?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Was the 5x10x20x10 topology used for MNIST the only topology tried?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Negative aspects: One major concern I have with the paper is the notion of privacy considered.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I do not see the GAN style approach taken by the paper, ensures this.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Without such a guarantee, the proposed method is not very useful because we' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I consider this assumption unrealistic.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In this case, it is basically a GD, not SGD any more.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
4. The biggest flaw that I see in this method is the practicality of it's use.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
To me these two reasoning statements are not particularly convincing. One could also say:' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I believe the proposed techniques have some flaws which hurt the eventual method.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
My concern is that the flaws in the method do not make it conducive to use as is.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This again greatly concerned me as I am not certain how stable these metrics are.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The proposed  CLF weight difference method has some concerning aspects as well.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Thus the metric is not a proper metric.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
As for GAN, due to the inexact update, it is not really solving the min-max problem.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The question is, why one would exlude the mixture-of-softmax approach here?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
A more general function is' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Then, can the function family the authors used in the paper approximate this function?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- for GN optimization, lambda should be set to 0 - not a constant value.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Eq (2) cannot have Delta Chi on the two sides.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
before Eq (3): the 'photometric ..' -> a 'photometric ..'' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
But there are some minus ones in the random projection?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
So there is no guarantee this algorithm will minimise the overall regret.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- It does not seem necessary to predict cumulative mixture policies (ASN network).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The only problem I see is that phrase similarity part is not convincing.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
x is already present within the indicator, no need to add yet' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is thus unclear if the approach is robust against different hyperparameter settings.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- The proposed approach is a fairly specific form of self-modulation.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Can you clarify how you view the relationship between the approaches mentioned above?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- The randomized weight is not very practical. Though it may be the standard approach of mean field,' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Why should we use embedding to compare the similarity?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I find these assumptions too strong for the task of learning disentangled representation.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The proposed method is very simple and frames the problem basically as a supervised learning problem.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
They can indeed be subsumed by generalization bounds based on VC theory.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
One question which is not addressed is the reason for only one RBM layer.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
My assumption is the visual feature already contains the label information for image captioning.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Is the number right?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The function composition doesn't capture that.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Concerns: I find the claim on deep networks kind of irresponsible.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Second, the conclusion of Theorem 2 seems to be flawed.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This illustrates that the conclusion of Theorem 2 may be wrong.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The paper does not provide very significant evidence that this method is useful.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The definition of “recovering true factor exactly” need to be given.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The descent lemma used by the author is not valid for the stochastic result.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This might be because the Bayes and MAT attacks are too simplistic.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- can you motivate why you are not using perplexity in section 3.2?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- In Theorem 4.7 an expectation on g(x_a) is missing' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Either case, I don't think we can have the inequality in eq. (5).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
1. I had hard time to understand latent canonicalization.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
More explanation of canonicalization is needed.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
3. How can the proposed method be generalized to non-image data?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Is there any explanation for this?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- doesn’t answer the one question regarding observation representation that it set out to evaluate' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- presumably, the sample complexity is ridiculous' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- Sec 4.1:' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
1. The classification of base class into super classes seems questionable to me.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Authors should analyze the stability of their method in details.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
How is this a reasonable assumption?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This hyperparameter itself benefits from (requires?) some scheduling.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
As a matter of good implementation, one never takes the inverse of anything.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
How would the given graph network compare to this?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Could it be that what we are seeing is the attack being denoised?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Does such approximation guarantee the policy improvement?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
There might be other constructions that are more efficient and less restrictive.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Therefore, it is a little misleading to still call it Bayesian active learning.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Please explain the logic for this architectural choice.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
1. The approach is not well justified either by theory or practice.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The algorithm assumptions are strong.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, in this way, when more and more tasks come, the generator will become larger and larger.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The proposed method is also heuristic and lacks promising guarantee.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, the assumption is too strong.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, the proposed technique does not seem to be handling the problem foundationally well.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The paper positions itself generally as dealing with arbitrary transformations T, but really is' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It is not clear to me if NF would improve stability/performances in general games.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
My concern here is that beta might be affecting the result more than the proposed training algorithm.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
How do performance and model size trade off?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
How were the number of layers and kernels chosen?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Was the 5x10x20x10 topology used for MNIST the only topology tried?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Negative aspects: One major concern I have with the paper is the notion of privacy considered.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I do not see the GAN style approach taken by the paper, ensures this.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Without such a guarantee, the proposed method is not very useful because we' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I consider this assumption unrealistic.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
In this case, it is basically a GD, not SGD any more.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
4. The biggest flaw that I see in this method is the practicality of it's use.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
To me these two reasoning statements are not particularly convincing. One could also say:' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I believe the proposed techniques have some flaws which hurt the eventual method.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
My concern is that the flaws in the method do not make it conducive to use as is.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This again greatly concerned me as I am not certain how stable these metrics are.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The proposed  CLF weight difference method has some concerning aspects as well.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Thus the metric is not a proper metric.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
As for GAN, due to the inexact update, it is not really solving the min-max problem.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The question is, why one would exlude the mixture-of-softmax approach here?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
A more general function is' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Then, can the function family the authors used in the paper approximate this function?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- for GN optimization, lambda should be set to 0 - not a constant value.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Eq (2) cannot have Delta Chi on the two sides.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
before Eq (3): the 'photometric ..' -> a 'photometric ..'' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
But there are some minus ones in the random projection?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
So there is no guarantee this algorithm will minimise the overall regret.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- It does not seem necessary to predict cumulative mixture policies (ASN network).' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The only problem I see is that phrase similarity part is not convincing.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
x is already present within the indicator, no need to add yet' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
It is thus unclear if the approach is robust against different hyperparameter settings.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.' [SEP] 'rebuttal_social	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.' [SEP] 'rebuttal_social"	[A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- The proposed approach is a fairly specific form of self-modulation.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Can you clarify how you view the relationship between the approaches mentioned above?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- The randomized weight is not very practical. Though it may be the standard approach of mean field,' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Why should we use embedding to compare the similarity?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I find these assumptions too strong for the task of learning disentangled representation.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The proposed method is very simple and frames the problem basically as a supervised learning problem.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
They can indeed be subsumed by generalization bounds based on VC theory.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
One question which is not addressed is the reason for only one RBM layer.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
My assumption is the visual feature already contains the label information for image captioning.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Is the number right?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The function composition doesn't capture that.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Concerns: I find the claim on deep networks kind of irresponsible.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Second, the conclusion of Theorem 2 seems to be flawed.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This illustrates that the conclusion of Theorem 2 may be wrong.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The paper does not provide very significant evidence that this method is useful.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The definition of “recovering true factor exactly” need to be given.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The descent lemma used by the author is not valid for the stochastic result.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This might be because the Bayes and MAT attacks are too simplistic.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- can you motivate why you are not using perplexity in section 3.2?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- In Theorem 4.7 an expectation on g(x_a) is missing' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Either case, I don't think we can have the inequality in eq. (5).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
1. I had hard time to understand latent canonicalization.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
More explanation of canonicalization is needed.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
3. How can the proposed method be generalized to non-image data?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Is there any explanation for this?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- doesn’t answer the one question regarding observation representation that it set out to evaluate' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- presumably, the sample complexity is ridiculous' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- Sec 4.1:' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
1. The classification of base class into super classes seems questionable to me.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Authors should analyze the stability of their method in details.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
How is this a reasonable assumption?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This hyperparameter itself benefits from (requires?) some scheduling.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
As a matter of good implementation, one never takes the inverse of anything.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
How would the given graph network compare to this?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Could it be that what we are seeing is the attack being denoised?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Does such approximation guarantee the policy improvement?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
There might be other constructions that are more efficient and less restrictive.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Therefore, it is a little misleading to still call it Bayesian active learning.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Please explain the logic for this architectural choice.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
1. The approach is not well justified either by theory or practice.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The algorithm assumptions are strong.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, in this way, when more and more tasks come, the generator will become larger and larger.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The proposed method is also heuristic and lacks promising guarantee.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, the assumption is too strong.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, the proposed technique does not seem to be handling the problem foundationally well.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The paper positions itself generally as dealing with arbitrary transformations T, but really is' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It is not clear to me if NF would improve stability/performances in general games.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
My concern here is that beta might be affecting the result more than the proposed training algorithm.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
How do performance and model size trade off?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
How were the number of layers and kernels chosen?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Was the 5x10x20x10 topology used for MNIST the only topology tried?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Negative aspects: One major concern I have with the paper is the notion of privacy considered.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I do not see the GAN style approach taken by the paper, ensures this.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Without such a guarantee, the proposed method is not very useful because we' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I consider this assumption unrealistic.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
In this case, it is basically a GD, not SGD any more.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
4. The biggest flaw that I see in this method is the practicality of it's use.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
To me these two reasoning statements are not particularly convincing. One could also say:' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I believe the proposed techniques have some flaws which hurt the eventual method.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
My concern is that the flaws in the method do not make it conducive to use as is.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This again greatly concerned me as I am not certain how stable these metrics are.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The proposed  CLF weight difference method has some concerning aspects as well.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Thus the metric is not a proper metric.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
As for GAN, due to the inexact update, it is not really solving the min-max problem.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The question is, why one would exlude the mixture-of-softmax approach here?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
A more general function is' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Then, can the function family the authors used in the paper approximate this function?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- for GN optimization, lambda should be set to 0 - not a constant value.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Eq (2) cannot have Delta Chi on the two sides.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
before Eq (3): the 'photometric ..' -> a 'photometric ..'' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
But there are some minus ones in the random projection?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
So there is no guarantee this algorithm will minimise the overall regret.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- It does not seem necessary to predict cumulative mixture policies (ASN network).' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The only problem I see is that phrase similarity part is not convincing.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
x is already present within the indicator, no need to add yet' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
It is thus unclear if the approach is robust against different hyperparameter settings.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.' [SEP] 'rebuttal_structuring	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.' [SEP] 'rebuttal_structuring"	The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- The proposed approach is a fairly specific form of self-modulation.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Can you clarify how you view the relationship between the approaches mentioned above?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- The randomized weight is not very practical. Though it may be the standard approach of mean field,' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Why should we use embedding to compare the similarity?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I find these assumptions too strong for the task of learning disentangled representation.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The proposed method is very simple and frames the problem basically as a supervised learning problem.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
They can indeed be subsumed by generalization bounds based on VC theory.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
One question which is not addressed is the reason for only one RBM layer.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
My assumption is the visual feature already contains the label information for image captioning.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Is the number right?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The function composition doesn't capture that.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Concerns: I find the claim on deep networks kind of irresponsible.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Second, the conclusion of Theorem 2 seems to be flawed.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This illustrates that the conclusion of Theorem 2 may be wrong.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The paper does not provide very significant evidence that this method is useful.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The definition of “recovering true factor exactly” need to be given.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The descent lemma used by the author is not valid for the stochastic result.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This might be because the Bayes and MAT attacks are too simplistic.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- can you motivate why you are not using perplexity in section 3.2?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- In Theorem 4.7 an expectation on g(x_a) is missing' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Either case, I don't think we can have the inequality in eq. (5).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
1. I had hard time to understand latent canonicalization.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
More explanation of canonicalization is needed.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
3. How can the proposed method be generalized to non-image data?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Is there any explanation for this?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- doesn’t answer the one question regarding observation representation that it set out to evaluate' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- presumably, the sample complexity is ridiculous' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- Sec 4.1:' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
1. The classification of base class into super classes seems questionable to me.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Authors should analyze the stability of their method in details.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
How is this a reasonable assumption?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This hyperparameter itself benefits from (requires?) some scheduling.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
As a matter of good implementation, one never takes the inverse of anything.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
How would the given graph network compare to this?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Could it be that what we are seeing is the attack being denoised?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Does such approximation guarantee the policy improvement?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
There might be other constructions that are more efficient and less restrictive.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Therefore, it is a little misleading to still call it Bayesian active learning.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Please explain the logic for this architectural choice.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
1. The approach is not well justified either by theory or practice.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The algorithm assumptions are strong.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, in this way, when more and more tasks come, the generator will become larger and larger.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The proposed method is also heuristic and lacks promising guarantee.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, the assumption is too strong.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, the proposed technique does not seem to be handling the problem foundationally well.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The paper positions itself generally as dealing with arbitrary transformations T, but really is' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It is not clear to me if NF would improve stability/performances in general games.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
My concern here is that beta might be affecting the result more than the proposed training algorithm.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
How do performance and model size trade off?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
How were the number of layers and kernels chosen?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Was the 5x10x20x10 topology used for MNIST the only topology tried?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Negative aspects: One major concern I have with the paper is the notion of privacy considered.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I do not see the GAN style approach taken by the paper, ensures this.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Without such a guarantee, the proposed method is not very useful because we' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I consider this assumption unrealistic.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
In this case, it is basically a GD, not SGD any more.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
4. The biggest flaw that I see in this method is the practicality of it's use.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
To me these two reasoning statements are not particularly convincing. One could also say:' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I believe the proposed techniques have some flaws which hurt the eventual method.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
My concern is that the flaws in the method do not make it conducive to use as is.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This again greatly concerned me as I am not certain how stable these metrics are.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The proposed  CLF weight difference method has some concerning aspects as well.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Thus the metric is not a proper metric.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
As for GAN, due to the inexact update, it is not really solving the min-max problem.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The question is, why one would exlude the mixture-of-softmax approach here?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
A more general function is' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Then, can the function family the authors used in the paper approximate this function?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- for GN optimization, lambda should be set to 0 - not a constant value.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Eq (2) cannot have Delta Chi on the two sides.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
before Eq (3): the 'photometric ..' -> a 'photometric ..'' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
But there are some minus ones in the random projection?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
So there is no guarantee this algorithm will minimise the overall regret.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- It does not seem necessary to predict cumulative mixture policies (ASN network).' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The only problem I see is that phrase similarity part is not convincing.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
x is already present within the indicator, no need to add yet' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
It is thus unclear if the approach is robust against different hyperparameter settings.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.' [SEP] 'rebuttal_by-cr	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.' [SEP] 'rebuttal_by-cr"	Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- The proposed approach is a fairly specific form of self-modulation.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Can you clarify how you view the relationship between the approaches mentioned above?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- The randomized weight is not very practical. Though it may be the standard approach of mean field,' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Why should we use embedding to compare the similarity?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I find these assumptions too strong for the task of learning disentangled representation.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The proposed method is very simple and frames the problem basically as a supervised learning problem.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
They can indeed be subsumed by generalization bounds based on VC theory.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
One question which is not addressed is the reason for only one RBM layer.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
My assumption is the visual feature already contains the label information for image captioning.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Is the number right?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The function composition doesn't capture that.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Concerns: I find the claim on deep networks kind of irresponsible.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Second, the conclusion of Theorem 2 seems to be flawed.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This illustrates that the conclusion of Theorem 2 may be wrong.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The paper does not provide very significant evidence that this method is useful.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The definition of “recovering true factor exactly” need to be given.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The descent lemma used by the author is not valid for the stochastic result.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This might be because the Bayes and MAT attacks are too simplistic.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- can you motivate why you are not using perplexity in section 3.2?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- In Theorem 4.7 an expectation on g(x_a) is missing' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Either case, I don't think we can have the inequality in eq. (5).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
1. I had hard time to understand latent canonicalization.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
More explanation of canonicalization is needed.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
3. How can the proposed method be generalized to non-image data?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Is there any explanation for this?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- doesn’t answer the one question regarding observation representation that it set out to evaluate' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- presumably, the sample complexity is ridiculous' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- Sec 4.1:' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
1. The classification of base class into super classes seems questionable to me.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Authors should analyze the stability of their method in details.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
How is this a reasonable assumption?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This hyperparameter itself benefits from (requires?) some scheduling.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
As a matter of good implementation, one never takes the inverse of anything.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
How would the given graph network compare to this?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Could it be that what we are seeing is the attack being denoised?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Does such approximation guarantee the policy improvement?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
There might be other constructions that are more efficient and less restrictive.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Therefore, it is a little misleading to still call it Bayesian active learning.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Please explain the logic for this architectural choice.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
1. The approach is not well justified either by theory or practice.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The algorithm assumptions are strong.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, in this way, when more and more tasks come, the generator will become larger and larger.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The proposed method is also heuristic and lacks promising guarantee.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, the assumption is too strong.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, the proposed technique does not seem to be handling the problem foundationally well.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The paper positions itself generally as dealing with arbitrary transformations T, but really is' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is not clear to me if NF would improve stability/performances in general games.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
My concern here is that beta might be affecting the result more than the proposed training algorithm.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
How do performance and model size trade off?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
How were the number of layers and kernels chosen?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Was the 5x10x20x10 topology used for MNIST the only topology tried?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Negative aspects: One major concern I have with the paper is the notion of privacy considered.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I do not see the GAN style approach taken by the paper, ensures this.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Without such a guarantee, the proposed method is not very useful because we' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I consider this assumption unrealistic.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
In this case, it is basically a GD, not SGD any more.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
4. The biggest flaw that I see in this method is the practicality of it's use.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
To me these two reasoning statements are not particularly convincing. One could also say:' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I believe the proposed techniques have some flaws which hurt the eventual method.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
My concern is that the flaws in the method do not make it conducive to use as is.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This again greatly concerned me as I am not certain how stable these metrics are.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The proposed  CLF weight difference method has some concerning aspects as well.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Thus the metric is not a proper metric.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
As for GAN, due to the inexact update, it is not really solving the min-max problem.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The question is, why one would exlude the mixture-of-softmax approach here?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
A more general function is' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Then, can the function family the authors used in the paper approximate this function?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- for GN optimization, lambda should be set to 0 - not a constant value.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Eq (2) cannot have Delta Chi on the two sides.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
before Eq (3): the 'photometric ..' -> a 'photometric ..'' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
But there are some minus ones in the random projection?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
So there is no guarantee this algorithm will minimise the overall regret.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- It does not seem necessary to predict cumulative mixture policies (ASN network).' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The only problem I see is that phrase similarity part is not convincing.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
x is already present within the indicator, no need to add yet' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is thus unclear if the approach is robust against different hyperparameter settings.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- The proposed approach is a fairly specific form of self-modulation.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Can you clarify how you view the relationship between the approaches mentioned above?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- The randomized weight is not very practical. Though it may be the standard approach of mean field,' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Why should we use embedding to compare the similarity?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I find these assumptions too strong for the task of learning disentangled representation.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The proposed method is very simple and frames the problem basically as a supervised learning problem.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
They can indeed be subsumed by generalization bounds based on VC theory.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
One question which is not addressed is the reason for only one RBM layer.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
My assumption is the visual feature already contains the label information for image captioning.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Is the number right?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The function composition doesn't capture that.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Concerns: I find the claim on deep networks kind of irresponsible.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Second, the conclusion of Theorem 2 seems to be flawed.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This illustrates that the conclusion of Theorem 2 may be wrong.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The paper does not provide very significant evidence that this method is useful.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The definition of “recovering true factor exactly” need to be given.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The descent lemma used by the author is not valid for the stochastic result.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This might be because the Bayes and MAT attacks are too simplistic.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- can you motivate why you are not using perplexity in section 3.2?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- In Theorem 4.7 an expectation on g(x_a) is missing' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Either case, I don't think we can have the inequality in eq. (5).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
1. I had hard time to understand latent canonicalization.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
More explanation of canonicalization is needed.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
3. How can the proposed method be generalized to non-image data?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Is there any explanation for this?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- doesn’t answer the one question regarding observation representation that it set out to evaluate' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- presumably, the sample complexity is ridiculous' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- Sec 4.1:' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
1. The classification of base class into super classes seems questionable to me.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Authors should analyze the stability of their method in details.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
How is this a reasonable assumption?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This hyperparameter itself benefits from (requires?) some scheduling.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
As a matter of good implementation, one never takes the inverse of anything.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
How would the given graph network compare to this?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Could it be that what we are seeing is the attack being denoised?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Does such approximation guarantee the policy improvement?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
There might be other constructions that are more efficient and less restrictive.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Therefore, it is a little misleading to still call it Bayesian active learning.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Please explain the logic for this architectural choice.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
1. The approach is not well justified either by theory or practice.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The algorithm assumptions are strong.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, in this way, when more and more tasks come, the generator will become larger and larger.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The proposed method is also heuristic and lacks promising guarantee.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, the assumption is too strong.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, the proposed technique does not seem to be handling the problem foundationally well.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The paper positions itself generally as dealing with arbitrary transformations T, but really is' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It is not clear to me if NF would improve stability/performances in general games.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
My concern here is that beta might be affecting the result more than the proposed training algorithm.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
How do performance and model size trade off?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
How were the number of layers and kernels chosen?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Was the 5x10x20x10 topology used for MNIST the only topology tried?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Negative aspects: One major concern I have with the paper is the notion of privacy considered.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I do not see the GAN style approach taken by the paper, ensures this.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Without such a guarantee, the proposed method is not very useful because we' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I consider this assumption unrealistic.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
In this case, it is basically a GD, not SGD any more.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
4. The biggest flaw that I see in this method is the practicality of it's use.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
To me these two reasoning statements are not particularly convincing. One could also say:' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I believe the proposed techniques have some flaws which hurt the eventual method.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
My concern is that the flaws in the method do not make it conducive to use as is.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This again greatly concerned me as I am not certain how stable these metrics are.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The proposed  CLF weight difference method has some concerning aspects as well.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Thus the metric is not a proper metric.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
As for GAN, due to the inexact update, it is not really solving the min-max problem.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The question is, why one would exlude the mixture-of-softmax approach here?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
A more general function is' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Then, can the function family the authors used in the paper approximate this function?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- for GN optimization, lambda should be set to 0 - not a constant value.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Eq (2) cannot have Delta Chi on the two sides.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
before Eq (3): the 'photometric ..' -> a 'photometric ..'' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
But there are some minus ones in the random projection?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
So there is no guarantee this algorithm will minimise the overall regret.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- It does not seem necessary to predict cumulative mixture policies (ASN network).' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The only problem I see is that phrase similarity part is not convincing.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
x is already present within the indicator, no need to add yet' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
It is thus unclear if the approach is robust against different hyperparameter settings.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.' [SEP] 'rebuttal_mitigate-criticism	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.' [SEP] 'rebuttal_mitigate-criticism"	For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- The proposed approach is a fairly specific form of self-modulation.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Can you clarify how you view the relationship between the approaches mentioned above?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- The randomized weight is not very practical. Though it may be the standard approach of mean field,' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Why should we use embedding to compare the similarity?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I find these assumptions too strong for the task of learning disentangled representation.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The proposed method is very simple and frames the problem basically as a supervised learning problem.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
They can indeed be subsumed by generalization bounds based on VC theory.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
One question which is not addressed is the reason for only one RBM layer.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
My assumption is the visual feature already contains the label information for image captioning.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Is the number right?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The function composition doesn't capture that.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Concerns: I find the claim on deep networks kind of irresponsible.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Second, the conclusion of Theorem 2 seems to be flawed.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This illustrates that the conclusion of Theorem 2 may be wrong.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The paper does not provide very significant evidence that this method is useful.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The definition of “recovering true factor exactly” need to be given.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The descent lemma used by the author is not valid for the stochastic result.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This might be because the Bayes and MAT attacks are too simplistic.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- can you motivate why you are not using perplexity in section 3.2?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- In Theorem 4.7 an expectation on g(x_a) is missing' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Either case, I don't think we can have the inequality in eq. (5).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
1. I had hard time to understand latent canonicalization.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
More explanation of canonicalization is needed.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
3. How can the proposed method be generalized to non-image data?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Is there any explanation for this?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- doesn’t answer the one question regarding observation representation that it set out to evaluate' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- presumably, the sample complexity is ridiculous' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- Sec 4.1:' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
1. The classification of base class into super classes seems questionable to me.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Authors should analyze the stability of their method in details.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
How is this a reasonable assumption?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This hyperparameter itself benefits from (requires?) some scheduling.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
As a matter of good implementation, one never takes the inverse of anything.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
How would the given graph network compare to this?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Could it be that what we are seeing is the attack being denoised?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Does such approximation guarantee the policy improvement?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
There might be other constructions that are more efficient and less restrictive.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Therefore, it is a little misleading to still call it Bayesian active learning.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Please explain the logic for this architectural choice.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
1. The approach is not well justified either by theory or practice.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The algorithm assumptions are strong.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, in this way, when more and more tasks come, the generator will become larger and larger.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The proposed method is also heuristic and lacks promising guarantee.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, the assumption is too strong.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, the proposed technique does not seem to be handling the problem foundationally well.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The paper positions itself generally as dealing with arbitrary transformations T, but really is' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It is not clear to me if NF would improve stability/performances in general games.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
My concern here is that beta might be affecting the result more than the proposed training algorithm.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
How do performance and model size trade off?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
How were the number of layers and kernels chosen?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Was the 5x10x20x10 topology used for MNIST the only topology tried?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Negative aspects: One major concern I have with the paper is the notion of privacy considered.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I do not see the GAN style approach taken by the paper, ensures this.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Without such a guarantee, the proposed method is not very useful because we' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I consider this assumption unrealistic.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
In this case, it is basically a GD, not SGD any more.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
4. The biggest flaw that I see in this method is the practicality of it's use.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
To me these two reasoning statements are not particularly convincing. One could also say:' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I believe the proposed techniques have some flaws which hurt the eventual method.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
My concern is that the flaws in the method do not make it conducive to use as is.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This again greatly concerned me as I am not certain how stable these metrics are.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The proposed  CLF weight difference method has some concerning aspects as well.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Thus the metric is not a proper metric.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
As for GAN, due to the inexact update, it is not really solving the min-max problem.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The question is, why one would exlude the mixture-of-softmax approach here?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
A more general function is' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Then, can the function family the authors used in the paper approximate this function?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- for GN optimization, lambda should be set to 0 - not a constant value.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Eq (2) cannot have Delta Chi on the two sides.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
before Eq (3): the 'photometric ..' -> a 'photometric ..'' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
But there are some minus ones in the random projection?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
So there is no guarantee this algorithm will minimise the overall regret.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- It does not seem necessary to predict cumulative mixture policies (ASN network).' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The only problem I see is that phrase similarity part is not convincing.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
x is already present within the indicator, no need to add yet' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
It is thus unclear if the approach is robust against different hyperparameter settings.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.' [SEP] 'rebuttal_future	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.' [SEP] 'rebuttal_future"	=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- The proposed approach is a fairly specific form of self-modulation.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Can you clarify how you view the relationship between the approaches mentioned above?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- The randomized weight is not very practical. Though it may be the standard approach of mean field,' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Why should we use embedding to compare the similarity?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I find these assumptions too strong for the task of learning disentangled representation.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The proposed method is very simple and frames the problem basically as a supervised learning problem.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
They can indeed be subsumed by generalization bounds based on VC theory.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
One question which is not addressed is the reason for only one RBM layer.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
My assumption is the visual feature already contains the label information for image captioning.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Is the number right?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The function composition doesn't capture that.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Concerns: I find the claim on deep networks kind of irresponsible.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Second, the conclusion of Theorem 2 seems to be flawed.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This illustrates that the conclusion of Theorem 2 may be wrong.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The paper does not provide very significant evidence that this method is useful.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The definition of “recovering true factor exactly” need to be given.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The descent lemma used by the author is not valid for the stochastic result.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This might be because the Bayes and MAT attacks are too simplistic.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- can you motivate why you are not using perplexity in section 3.2?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- In Theorem 4.7 an expectation on g(x_a) is missing' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Either case, I don't think we can have the inequality in eq. (5).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
1. I had hard time to understand latent canonicalization.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
More explanation of canonicalization is needed.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
3. How can the proposed method be generalized to non-image data?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Is there any explanation for this?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- doesn’t answer the one question regarding observation representation that it set out to evaluate' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- presumably, the sample complexity is ridiculous' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- Sec 4.1:' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
1. The classification of base class into super classes seems questionable to me.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Authors should analyze the stability of their method in details.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
How is this a reasonable assumption?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This hyperparameter itself benefits from (requires?) some scheduling.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
As a matter of good implementation, one never takes the inverse of anything.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
How would the given graph network compare to this?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Could it be that what we are seeing is the attack being denoised?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Does such approximation guarantee the policy improvement?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
There might be other constructions that are more efficient and less restrictive.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Therefore, it is a little misleading to still call it Bayesian active learning.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Please explain the logic for this architectural choice.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
1. The approach is not well justified either by theory or practice.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The algorithm assumptions are strong.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, in this way, when more and more tasks come, the generator will become larger and larger.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The proposed method is also heuristic and lacks promising guarantee.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, the assumption is too strong.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, the proposed technique does not seem to be handling the problem foundationally well.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The paper positions itself generally as dealing with arbitrary transformations T, but really is' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It is not clear to me if NF would improve stability/performances in general games.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
My concern here is that beta might be affecting the result more than the proposed training algorithm.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
How do performance and model size trade off?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
How were the number of layers and kernels chosen?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Was the 5x10x20x10 topology used for MNIST the only topology tried?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Negative aspects: One major concern I have with the paper is the notion of privacy considered.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I do not see the GAN style approach taken by the paper, ensures this.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Without such a guarantee, the proposed method is not very useful because we' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I consider this assumption unrealistic.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
In this case, it is basically a GD, not SGD any more.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
4. The biggest flaw that I see in this method is the practicality of it's use.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
To me these two reasoning statements are not particularly convincing. One could also say:' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I believe the proposed techniques have some flaws which hurt the eventual method.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
My concern is that the flaws in the method do not make it conducive to use as is.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This again greatly concerned me as I am not certain how stable these metrics are.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The proposed  CLF weight difference method has some concerning aspects as well.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Thus the metric is not a proper metric.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
As for GAN, due to the inexact update, it is not really solving the min-max problem.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The question is, why one would exlude the mixture-of-softmax approach here?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
A more general function is' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Then, can the function family the authors used in the paper approximate this function?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- for GN optimization, lambda should be set to 0 - not a constant value.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Eq (2) cannot have Delta Chi on the two sides.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
before Eq (3): the 'photometric ..' -> a 'photometric ..'' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
But there are some minus ones in the random projection?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
So there is no guarantee this algorithm will minimise the overall regret.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- It does not seem necessary to predict cumulative mixture policies (ASN network).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The only problem I see is that phrase similarity part is not convincing.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
x is already present within the indicator, no need to add yet' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It is thus unclear if the approach is robust against different hyperparameter settings.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- The proposed approach is a fairly specific form of self-modulation.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Can you clarify how you view the relationship between the approaches mentioned above?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- The randomized weight is not very practical. Though it may be the standard approach of mean field,' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Why should we use embedding to compare the similarity?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I find these assumptions too strong for the task of learning disentangled representation.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The proposed method is very simple and frames the problem basically as a supervised learning problem.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
They can indeed be subsumed by generalization bounds based on VC theory.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
One question which is not addressed is the reason for only one RBM layer.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
My assumption is the visual feature already contains the label information for image captioning.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Is the number right?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The function composition doesn't capture that.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Concerns: I find the claim on deep networks kind of irresponsible.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Second, the conclusion of Theorem 2 seems to be flawed.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This illustrates that the conclusion of Theorem 2 may be wrong.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The paper does not provide very significant evidence that this method is useful.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including ""selection network"" with threshold) is not very principled.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The definition of “recovering true factor exactly” need to be given.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The descent lemma used by the author is not valid for the stochastic result.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This might be because the Bayes and MAT attacks are too simplistic.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- can you motivate why you are not using perplexity in section 3.2?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- In remark 4.8 in the end option I and II are inverted by mistake' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- In Theorem 4.7 an expectation on g(x_a) is missing' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Either case, I don't think we can have the inequality in eq. (5).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
1. I had hard time to understand latent canonicalization.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
More explanation of canonicalization is needed.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
3. How can the proposed method be generalized to non-image data?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Is there any explanation for this?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- doesn’t answer the one question regarding observation representation that it set out to evaluate' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- presumably, the sample complexity is ridiculous' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- Sec 4.1:' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
1. The classification of base class into super classes seems questionable to me.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- Some assumptions are not explicitly stated.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Authors should analyze the stability of their method in details.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"").' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The role of \sigma seems very redundant given \omega.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
How is this a reasonable assumption?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This hyperparameter itself benefits from (requires?) some scheduling.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
As a matter of good implementation, one never takes the inverse of anything.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
How would the given graph network compare to this?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Could it be that what we are seeing is the attack being denoised?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Does such approximation guarantee the policy improvement?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
There might be other constructions that are more efficient and less restrictive.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Therefore, it is a little misleading to still call it Bayesian active learning.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- why do you need a conditional GAN discriminator, if you already model similarity by L1?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Please explain the logic for this architectural choice.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Therefore, it is not clear how the proposed framework is helping the model compression techniques.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
1. The approach is not well justified either by theory or practice.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
In some RL tasks, it is not allowed to access the RAM state.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The algorithm assumptions are strong.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, in this way, when more and more tasks come, the generator will become larger and larger.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The proposed method is also heuristic and lacks promising guarantee.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, the assumption is too strong.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, the proposed technique does not seem to be handling the problem foundationally well.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The paper positions itself generally as dealing with arbitrary transformations T, but really is' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It is not clear to me if NF would improve stability/performances in general games.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
My concern here is that beta might be affecting the result more than the proposed training algorithm.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
How do performance and model size trade off?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
How were the number of layers and kernels chosen?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Was the 5x10x20x10 topology used for MNIST the only topology tried?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Negative aspects: One major concern I have with the paper is the notion of privacy considered.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I do not see the GAN style approach taken by the paper, ensures this.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Without such a guarantee, the proposed method is not very useful because we' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I consider this assumption unrealistic.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
In this case, it is basically a GD, not SGD any more.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
4. The biggest flaw that I see in this method is the practicality of it's use.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
To me these two reasoning statements are not particularly convincing. One could also say:' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I believe the proposed techniques have some flaws which hurt the eventual method.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
My concern is that the flaws in the method do not make it conducive to use as is.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This again greatly concerned me as I am not certain how stable these metrics are.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The proposed  CLF weight difference method has some concerning aspects as well.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Thus the metric is not a proper metric.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
As for GAN, due to the inexact update, it is not really solving the min-max problem.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The question is, why one would exlude the mixture-of-softmax approach here?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
A more general function is' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Then, can the function family the authors used in the paper approximate this function?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- for GN optimization, lambda should be set to 0 - not a constant value.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Thus, the advantage of using a LM optimization scheme is not very convincing.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Eq (2) cannot have Delta Chi on the two sides.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
before Eq (3): the 'photometric ..' -> a 'photometric ..'' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
But there are some minus ones in the random projection?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However in the approach proposed here, the negative examples are missing.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
So there is no guarantee this algorithm will minimise the overall regret.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
- It does not seem necessary to predict cumulative mixture policies (ASN network).' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The only problem I see is that phrase similarity part is not convincing.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
x is already present within the indicator, no need to add yet' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
It is thus unclear if the approach is robust against different hyperparameter settings.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.' [SEP] 'rebuttal_followup	Could you please elaborate on the comment ’the current design […] simply sums them up’?
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.' [SEP] 'rebuttal_followup"	Could you please elaborate on the comment ’the current design […] simply sums them up’?
The same holds for the type of data, since the paper only shows results for image classification benchmarks.' [SEP] 'rebuttal_done	We will incorporate these results in the experiments section and post the updated manuscript shortly.
As for the experimental results, the paper only provides results on the sentimental analysis results and digit datasets, which are small benchmarks.' [SEP] 'rebuttal_done	We will incorporate these results in the experiments section and post the updated manuscript shortly.
The results on real datasets are similar to the regular GCN.' [SEP] 'rebuttal_done	We will incorporate these results in the experiments section and post the updated manuscript shortly.
They might be more convincing if there were results on a very large corpus such as 1 billion word corpus.' [SEP] 'rebuttal_done	We will incorporate these results in the experiments section and post the updated manuscript shortly.
My overall concern is that the experiment result doesn’t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn’t really fit in the “transfer” learning scenario.' [SEP] 'rebuttal_done	We will incorporate these results in the experiments section and post the updated manuscript shortly.
"With this dataset, it's a bit of a stretch to say there was ""only a 1 point drop in BLEU score"". That's a significant drop, and in fact the DynamicConv paper goes to significant lengths to make a smaller 0.8 point improvement.' [SEP] 'rebuttal_done"	We will incorporate these results in the experiments section and post the updated manuscript shortly.
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.' [SEP] 'rebuttal_done	We will incorporate these results in the experiments section and post the updated manuscript shortly.
The same holds for the type of data, since the paper only shows results for image classification benchmarks.' [SEP] 'rebuttal_answer	As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.
As for the experimental results, the paper only provides results on the sentimental analysis results and digit datasets, which are small benchmarks.' [SEP] 'rebuttal_answer	As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.
The results on real datasets are similar to the regular GCN.' [SEP] 'rebuttal_answer	As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.
They might be more convincing if there were results on a very large corpus such as 1 billion word corpus.' [SEP] 'rebuttal_answer	As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.
My overall concern is that the experiment result doesn’t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn’t really fit in the “transfer” learning scenario.' [SEP] 'rebuttal_answer	As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.
"With this dataset, it's a bit of a stretch to say there was ""only a 1 point drop in BLEU score"". That's a significant drop, and in fact the DynamicConv paper goes to significant lengths to make a smaller 0.8 point improvement.' [SEP] 'rebuttal_answer"	As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.' [SEP] 'rebuttal_answer	As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.
3.  Or simply to a well tuned \lambda, chosen on a per dataset basis? From the text it appears that \lambda is manually selected to trade off accuracy against uncertainty on OOD data.' [SEP] 'rebuttal_done	We added a new Section 4 in the revised version of the paper discussing these differences.
I am wondering this method can be applied to other complex datasets whose latent factors are unknown.' [SEP] 'rebuttal_done	We added a new Section 4 in the revised version of the paper discussing these differences.
They start from Equation (4) which is incorrectly denoted as the log-marginal distribution while it is the same conditional distribution introduced in Equation (3) with the extra summation for all the available data points.' [SEP] 'rebuttal_done	We added a new Section 4 in the revised version of the paper discussing these differences.
It seems to me; the author assumed data from one modality is generated by all the latent factors (see Eq. (11)), then what is the point for assuming the prior of the latent factor is factorized (see Eq. (4) and (5))?' [SEP] 'rebuttal_done	We added a new Section 4 in the revised version of the paper discussing these differences.
However, I can not agree because you can simply generate poisoned data and train the neural networks on the poisoned data regardless of the underlying approach that is targeted in generating the poisoned data.' [SEP] 'rebuttal_done	We added a new Section 4 in the revised version of the paper discussing these differences.
3.  Or simply to a well tuned \lambda, chosen on a per dataset basis? From the text it appears that \lambda is manually selected to trade off accuracy against uncertainty on OOD data.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
I am wondering this method can be applied to other complex datasets whose latent factors are unknown.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
They start from Equation (4) which is incorrectly denoted as the log-marginal distribution while it is the same conditional distribution introduced in Equation (3) with the extra summation for all the available data points.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
It seems to me; the author assumed data from one modality is generated by all the latent factors (see Eq. (11)), then what is the point for assuming the prior of the latent factor is factorized (see Eq. (4) and (5))?' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
However, I can not agree because you can simply generate poisoned data and train the neural networks on the poisoned data regardless of the underlying approach that is targeted in generating the poisoned data.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
The contributions of the method could also be underlined more clearly in the abstract and introduction.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
- The digression at the bottom of the first page about neural architecture search seem out of context and interrupts the flow of the introduction.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
First, it is somewhat misleading about its contributions: it's not obvious from abstract/introduction that the whole model is the same as DIP except for the proposed architecture.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
The introduction can start at a lower level (such as flat/hyperbolic neural networks).' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
"10.	The title suggests that the paper studies multiple VQA models but only a single model is studied.' [SEP] 'rebuttal_done"	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
"I do not understand the ""deep integration of MARL and HRL"" that is claimed in the Introduction.' [SEP] 'rebuttal_done"	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
"* In the introduction, ""the classical approach"" is mentioned but to be the latter is' [SEP] 'rebuttal_done"	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
I enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
Overall, I find the paper a promising contribution. But until the authors provide a more thorough experimental evaluation, I hesitate to recommend acceptance.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
In its current state, I am not sure that it adds a lot to the manuscript.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
It reads very much like a STOC theory paper, and a lot of the key ML details that would be relevant to audience at this conference seem to have been shoved under the rug in a way.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
Overall, I think this paper is below the borderline of acceptance due to insufficient comparison with Sparsely-Gated MoE.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
While I like the premise of the paper, I feel that it needs more work.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
Overall, I quite liked the paper and think it is well-written, but I believe the authors need to highlight at least one practical advance introduced by the CW distance (in which case I will raise my score).' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
Ultimately this paper is interesting but falls well below the standards of exposition that I expect from a theory paper and doesn’t go very far at connecting the analysis back to the claimed motivation of investigating practical invariances. If the authors significantly improve the quality of the draft, I’ll be happy to revisit it and re-evaluate my score.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
Overall, the paper requires significant improvement.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
5. The paper is imprecise and unpolished and the presentation needs improvement.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
However, I found that the contribution of this paper is fairly small.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
The paper would gain in clarity' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
However, I think the relative novelty of the paper does not meet ICLR standards, and it’s better suited as a whitepaper attached to an open dataset release.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
The paper has few really minor grammatical errors and typos. Please fix those before uploading the final draft.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
I believe that this paper is thus not in its final form and could be largely improved.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
A good paper overall, but the experiments were relatively weak (common for most ICLR submissions) and the novelty was somewhat limited.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
The paper has an interesting potential but seems a bit limited in its present form.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
In conclusion, I suggest a reject of this paper due to the lacks of comprehensive study and evaluation.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
The paper is not very self contained.' [SEP] 'rebuttal_followup	Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?
I enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
Overall, I find the paper a promising contribution. But until the authors provide a more thorough experimental evaluation, I hesitate to recommend acceptance.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
In its current state, I am not sure that it adds a lot to the manuscript.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
It reads very much like a STOC theory paper, and a lot of the key ML details that would be relevant to audience at this conference seem to have been shoved under the rug in a way.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
Overall, I think this paper is below the borderline of acceptance due to insufficient comparison with Sparsely-Gated MoE.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
While I like the premise of the paper, I feel that it needs more work.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
Overall, I quite liked the paper and think it is well-written, but I believe the authors need to highlight at least one practical advance introduced by the CW distance (in which case I will raise my score).' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
Ultimately this paper is interesting but falls well below the standards of exposition that I expect from a theory paper and doesn’t go very far at connecting the analysis back to the claimed motivation of investigating practical invariances. If the authors significantly improve the quality of the draft, I’ll be happy to revisit it and re-evaluate my score.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
Overall, the paper requires significant improvement.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
5. The paper is imprecise and unpolished and the presentation needs improvement.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
However, I found that the contribution of this paper is fairly small.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
The paper would gain in clarity' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
However, I think the relative novelty of the paper does not meet ICLR standards, and it’s better suited as a whitepaper attached to an open dataset release.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
The paper has few really minor grammatical errors and typos. Please fix those before uploading the final draft.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
I believe that this paper is thus not in its final form and could be largely improved.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
A good paper overall, but the experiments were relatively weak (common for most ICLR submissions) and the novelty was somewhat limited.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
The paper has an interesting potential but seems a bit limited in its present form.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
In conclusion, I suggest a reject of this paper due to the lacks of comprehensive study and evaluation.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
The paper is not very self contained.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.
I enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
Overall, I find the paper a promising contribution. But until the authors provide a more thorough experimental evaluation, I hesitate to recommend acceptance.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
In its current state, I am not sure that it adds a lot to the manuscript.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
It reads very much like a STOC theory paper, and a lot of the key ML details that would be relevant to audience at this conference seem to have been shoved under the rug in a way.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
Overall, I think this paper is below the borderline of acceptance due to insufficient comparison with Sparsely-Gated MoE.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
While I like the premise of the paper, I feel that it needs more work.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
Overall, I quite liked the paper and think it is well-written, but I believe the authors need to highlight at least one practical advance introduced by the CW distance (in which case I will raise my score).' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
Ultimately this paper is interesting but falls well below the standards of exposition that I expect from a theory paper and doesn’t go very far at connecting the analysis back to the claimed motivation of investigating practical invariances. If the authors significantly improve the quality of the draft, I’ll be happy to revisit it and re-evaluate my score.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
Overall, the paper requires significant improvement.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
5. The paper is imprecise and unpolished and the presentation needs improvement.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
However, I found that the contribution of this paper is fairly small.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
The paper would gain in clarity' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
However, I think the relative novelty of the paper does not meet ICLR standards, and it’s better suited as a whitepaper attached to an open dataset release.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
The paper has few really minor grammatical errors and typos. Please fix those before uploading the final draft.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
I believe that this paper is thus not in its final form and could be largely improved.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
A good paper overall, but the experiments were relatively weak (common for most ICLR submissions) and the novelty was somewhat limited.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
The paper has an interesting potential but seems a bit limited in its present form.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
In conclusion, I suggest a reject of this paper due to the lacks of comprehensive study and evaluation.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
The paper is not very self contained.' [SEP] 'rebuttal_structuring	We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.
Simply because for continuous variables similar experiments have been reported before' [SEP] 'rebuttal_structuring	"3. ""Given the existing body of literature, I found the technical novelty of this paper rather weak"""
Even though the proposed approach seems to have significant potential, the experimental' [SEP] 'rebuttal_structuring	"3. ""Given the existing body of literature, I found the technical novelty of this paper rather weak"""
Also, I find the experiments done in section 3 and 4 are similar to previous works and even the conclusions are similar.' [SEP] 'rebuttal_structuring	"3. ""Given the existing body of literature, I found the technical novelty of this paper rather weak"""
The experiments currently demonstrate that optimizing both controller and hardware is better than optimizing just the controller, which is not surprising and is a phenomenon which has been previously studied in the literature.' [SEP] 'rebuttal_structuring	"3. ""Given the existing body of literature, I found the technical novelty of this paper rather weak"""
Originality: Given the existing body of literature, I found the technical novelty of this paper rather weak. However, it seems the experiments are thoroughly conducted.' [SEP] 'rebuttal_structuring	"3. ""Given the existing body of literature, I found the technical novelty of this paper rather weak"""
If this all is indeed the case, it is not surprising that the numbers the authors get in the experiments are so similar to WAE-MMD, because CWAE would be exactly WAE-MMD with a specific choice of the kernel.' [SEP] 'rebuttal_structuring	"3. ""Given the existing body of literature, I found the technical novelty of this paper rather weak"""
However the theoretical contribution is poor. And the experiment uses a very classical benchmark providing simulated data.' [SEP] 'rebuttal_structuring	"3. ""Given the existing body of literature, I found the technical novelty of this paper rather weak"""
2. Although the experiments are detailed and interesting they support poor theoretical developments and use a very classical benchmark' [SEP] 'rebuttal_structuring	"3. ""Given the existing body of literature, I found the technical novelty of this paper rather weak"""
Simply because for continuous variables similar experiments have been reported before' [SEP] 'rebuttal_reject-criticism	We have added a sentence in the introduction emphasizing this crucial point.
Even though the proposed approach seems to have significant potential, the experimental' [SEP] 'rebuttal_reject-criticism	We have added a sentence in the introduction emphasizing this crucial point.
Also, I find the experiments done in section 3 and 4 are similar to previous works and even the conclusions are similar.' [SEP] 'rebuttal_reject-criticism	We have added a sentence in the introduction emphasizing this crucial point.
The experiments currently demonstrate that optimizing both controller and hardware is better than optimizing just the controller, which is not surprising and is a phenomenon which has been previously studied in the literature.' [SEP] 'rebuttal_reject-criticism	We have added a sentence in the introduction emphasizing this crucial point.
Originality: Given the existing body of literature, I found the technical novelty of this paper rather weak. However, it seems the experiments are thoroughly conducted.' [SEP] 'rebuttal_reject-criticism	We have added a sentence in the introduction emphasizing this crucial point.
If this all is indeed the case, it is not surprising that the numbers the authors get in the experiments are so similar to WAE-MMD, because CWAE would be exactly WAE-MMD with a specific choice of the kernel.' [SEP] 'rebuttal_reject-criticism	We have added a sentence in the introduction emphasizing this crucial point.
However the theoretical contribution is poor. And the experiment uses a very classical benchmark providing simulated data.' [SEP] 'rebuttal_reject-criticism	We have added a sentence in the introduction emphasizing this crucial point.
2. Although the experiments are detailed and interesting they support poor theoretical developments and use a very classical benchmark' [SEP] 'rebuttal_reject-criticism	We have added a sentence in the introduction emphasizing this crucial point.
The authors do not thoroughly explain the motivation of this paper.' [SEP] 'rebuttal_reject-criticism	We are of course willing to further specify any details that the referee misses in the current paper. We would therefore like to kindly invite the referee to be specific about the details that he/she would like to be added to the manuscript.
It is also not clear to me why these problems are important.' [SEP] 'rebuttal_reject-criticism	We are of course willing to further specify any details that the referee misses in the current paper. We would therefore like to kindly invite the referee to be specific about the details that he/she would like to be added to the manuscript.
"-	While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited.' [SEP] 'rebuttal_reject-criticism"	We are of course willing to further specify any details that the referee misses in the current paper. We would therefore like to kindly invite the referee to be specific about the details that he/she would like to be added to the manuscript.
2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem  or sample selection problem)' [SEP] 'rebuttal_reject-criticism	We are of course willing to further specify any details that the referee misses in the current paper. We would therefore like to kindly invite the referee to be specific about the details that he/she would like to be added to the manuscript.
The idea is an interesting one, but' [SEP] 'rebuttal_reject-criticism	We are of course willing to further specify any details that the referee misses in the current paper. We would therefore like to kindly invite the referee to be specific about the details that he/she would like to be added to the manuscript.
The authors do not thoroughly explain the motivation of this paper.' [SEP] 'rebuttal_answer	We think generating such justifications is a great next step and hope that our work will foster such interesting future research.
It is also not clear to me why these problems are important.' [SEP] 'rebuttal_answer	We think generating such justifications is a great next step and hope that our work will foster such interesting future research.
"-	While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited.' [SEP] 'rebuttal_answer"	We think generating such justifications is a great next step and hope that our work will foster such interesting future research.
2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem  or sample selection problem)' [SEP] 'rebuttal_answer	We think generating such justifications is a great next step and hope that our work will foster such interesting future research.
The idea is an interesting one, but' [SEP] 'rebuttal_answer	We think generating such justifications is a great next step and hope that our work will foster such interesting future research.
The authors do not thoroughly explain the motivation of this paper.' [SEP] 'rebuttal_structuring	Re. somewhat limited contribution:
It is also not clear to me why these problems are important.' [SEP] 'rebuttal_structuring	Re. somewhat limited contribution:
"-	While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited.' [SEP] 'rebuttal_structuring"	Re. somewhat limited contribution:
2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem  or sample selection problem)' [SEP] 'rebuttal_structuring	Re. somewhat limited contribution:
The idea is an interesting one, but' [SEP] 'rebuttal_structuring	Re. somewhat limited contribution:
The authors do not thoroughly explain the motivation of this paper.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
It is also not clear to me why these problems are important.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"-	While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem  or sample selection problem)' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
The idea is an interesting one, but' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
However, it looks to me that the authors need to better explain the motivation of DiVA, the differences of DiVA from existing supervised VAE, and the experimental settings, before the acceptance of this paper.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
If faster training of dictionary learning models was a bottleneck in practical applications, this might be of interest, but it is not.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
However, it looks to me that the authors need to better explain the motivation of DiVA, the differences of DiVA from existing supervised VAE, and the experimental settings, before the acceptance of this paper.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
If faster training of dictionary learning models was a bottleneck in practical applications, this might be of interest, but it is not.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
However, it looks to me that the authors need to better explain the motivation of DiVA, the differences of DiVA from existing supervised VAE, and the experimental settings, before the acceptance of this paper.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
If faster training of dictionary learning models was a bottleneck in practical applications, this might be of interest, but it is not.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
Although experiments show that learning such representations are beneficial for low-shot setting of SVHN, it is not clear whether such improvement generalizes to more realistic datasets such as ImageNet.' [SEP] 'rebuttal_done	In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.
Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?' [SEP] 'rebuttal_done	In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.
In the real world, one would not have access to OOD data during training, how is one to pick \lambda in such cases?' [SEP] 'rebuttal_done	In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.
The experiments were only done on simple image datasets.' [SEP] 'rebuttal_done	In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.
This also becomes apparent in the experiments section, where rotational data augmentation is found to be necessary.' [SEP] 'rebuttal_done	In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.
This is also lacking from the description of the experimental protocol, which does not address the data-splits (how many classes were used for each) and size of the unlabelled test set.' [SEP] 'rebuttal_done	In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.
- How many samples did you use from p(theta|x) during training?' [SEP] 'rebuttal_done	In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.
1. Experimental settings are clear, however, what makes me confused is that the construction for p_{\bar{d}} is straightforward for simple distribution like 2D points dataset, however, it might be intractable for complex high dimensional data such as images.' [SEP] 'rebuttal_done	In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.
Although experiments show that learning such representations are beneficial for low-shot setting of SVHN, it is not clear whether such improvement generalizes to more realistic datasets such as ImageNet.' [SEP] 'rebuttal_answer	In responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.
Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?' [SEP] 'rebuttal_answer	In responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.
In the real world, one would not have access to OOD data during training, how is one to pick \lambda in such cases?' [SEP] 'rebuttal_answer	In responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.
The experiments were only done on simple image datasets.' [SEP] 'rebuttal_answer	In responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.
This also becomes apparent in the experiments section, where rotational data augmentation is found to be necessary.' [SEP] 'rebuttal_answer	In responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.
This is also lacking from the description of the experimental protocol, which does not address the data-splits (how many classes were used for each) and size of the unlabelled test set.' [SEP] 'rebuttal_answer	In responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.
- How many samples did you use from p(theta|x) during training?' [SEP] 'rebuttal_answer	In responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.
1. Experimental settings are clear, however, what makes me confused is that the construction for p_{\bar{d}} is straightforward for simple distribution like 2D points dataset, however, it might be intractable for complex high dimensional data such as images.' [SEP] 'rebuttal_answer	In responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.
- very incremental improvements on PTB over a very simple baseline (far from SotA)' [SEP] 'rebuttal_done	In addition to implementation details, the appendix has a rather detailed table of the architecture parameters.
3. The graph neural networks used in the model are not described in the paper, only a reference to Paliwal et al (2019) is given.' [SEP] 'rebuttal_done	In addition to implementation details, the appendix has a rather detailed table of the architecture parameters.
It would be helpful to have a brief paragraph describing this architecture, for readers not familiar with the referenced paper.' [SEP] 'rebuttal_done	In addition to implementation details, the appendix has a rather detailed table of the architecture parameters.
1) I wonder if the proposed method work for most GAN models, more experiments evaluated on more recent GAN-based  models should be added to verify the superiority claimed in this paper, e.g., TP-GAN [Huang et al., ICCV 2017], PIM [Zhao et al., CVPR 2018], DR-GAN [Tran et al., CVPR 2017], DA-GAN [Zhao et al., NIPS 2017], MH-Parser [Li et al., 2017], 3D-PIM [Zhao et al., IJCAI 2018], SimGAN [Shrivastava et al., CVPR 2016], AIM [Zhao et al., AAAI 2019].' [SEP] 'rebuttal_done	In addition to implementation details, the appendix has a rather detailed table of the architecture parameters.
Some details are missing, which is hardly reproduced by the other researchers.' [SEP] 'rebuttal_done	In addition to implementation details, the appendix has a rather detailed table of the architecture parameters.
- very incremental improvements on PTB over a very simple baseline (far from SotA)' [SEP] 'rebuttal_answer	As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.
3. The graph neural networks used in the model are not described in the paper, only a reference to Paliwal et al (2019) is given.' [SEP] 'rebuttal_answer	As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.
It would be helpful to have a brief paragraph describing this architecture, for readers not familiar with the referenced paper.' [SEP] 'rebuttal_answer	As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.
1) I wonder if the proposed method work for most GAN models, more experiments evaluated on more recent GAN-based  models should be added to verify the superiority claimed in this paper, e.g., TP-GAN [Huang et al., ICCV 2017], PIM [Zhao et al., CVPR 2018], DR-GAN [Tran et al., CVPR 2017], DA-GAN [Zhao et al., NIPS 2017], MH-Parser [Li et al., 2017], 3D-PIM [Zhao et al., IJCAI 2018], SimGAN [Shrivastava et al., CVPR 2016], AIM [Zhao et al., AAAI 2019].' [SEP] 'rebuttal_answer	As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.
Some details are missing, which is hardly reproduced by the other researchers.' [SEP] 'rebuttal_answer	As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.
- very incremental improvements on PTB over a very simple baseline (far from SotA)' [SEP] 'rebuttal_by-cr	Q1: We are evaluating the proposed metrics on more recent GAN-based models you suggested and will update the results once the results become available.
3. The graph neural networks used in the model are not described in the paper, only a reference to Paliwal et al (2019) is given.' [SEP] 'rebuttal_by-cr	Q1: We are evaluating the proposed metrics on more recent GAN-based models you suggested and will update the results once the results become available.
It would be helpful to have a brief paragraph describing this architecture, for readers not familiar with the referenced paper.' [SEP] 'rebuttal_by-cr	Q1: We are evaluating the proposed metrics on more recent GAN-based models you suggested and will update the results once the results become available.
1) I wonder if the proposed method work for most GAN models, more experiments evaluated on more recent GAN-based  models should be added to verify the superiority claimed in this paper, e.g., TP-GAN [Huang et al., ICCV 2017], PIM [Zhao et al., CVPR 2018], DR-GAN [Tran et al., CVPR 2017], DA-GAN [Zhao et al., NIPS 2017], MH-Parser [Li et al., 2017], 3D-PIM [Zhao et al., IJCAI 2018], SimGAN [Shrivastava et al., CVPR 2016], AIM [Zhao et al., AAAI 2019].' [SEP] 'rebuttal_by-cr	Q1: We are evaluating the proposed metrics on more recent GAN-based models you suggested and will update the results once the results become available.
Some details are missing, which is hardly reproduced by the other researchers.' [SEP] 'rebuttal_by-cr	Q1: We are evaluating the proposed metrics on more recent GAN-based models you suggested and will update the results once the results become available.
The introduction contains a few statements that may paint an incomplete or confusing picture of the current literature in adversarial attacks on neural networks:' [SEP] 'rebuttal_done	results in the updated paper.
"1. The title of the paper is ""visual reasoning by progressive module networks."" The title may be a little overstated since the major task is focused on visual question answering (VQA).' [SEP] 'rebuttal_done"	results in the updated paper.
The authors spend two paragraphs in the introduction trying to draw a distinction but I am still not convinced.' [SEP] 'rebuttal_done	results in the updated paper.
"The title claims way more than what is actually delivered in the paper, despite the fact that the authors have put in an ""On"" in the beginning of the title.' [SEP] 'rebuttal_done"	results in the updated paper.
So, the motivating assertion “[...] state-of-the-art solvers do not yet scale to large, difficult formulas, such as ones with hundreds of variables and thousands of clauses” in the introduction of the paper, is not totally correct.' [SEP] 'rebuttal_done	results in the updated paper.
* Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model.' [SEP] 'rebuttal_done	results in the updated paper.
Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training.' [SEP] 'rebuttal_structuring	Following the reviewer’s suggestion, we have added a figure that shows the dynamics of neuromodulation in the cue-response task (Figure 3, in the Appendix).
If the novelty is in applying to continual learning and new datasets, it is not clear that this is sufficient.' [SEP] 'rebuttal_reject-criticism	On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models.
With lack of clear novel insights, or at least more systematic study on additional datasets of the 'winning' techniques and a sensitivity analysis, the paper does not give a valuable enough contribution to the field to merit publication.' [SEP] 'rebuttal_reject-criticism	On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models.
However, the real-world experiments are not necessarily the easiest to read.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
Authors should clarify the justification behind experimenting only on 'first 500 test images'.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
It seems there is no conclusion to take away from the experiments in section 5 (convolutions).' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
In the experiment there is no details on how you set the hyperparameters of CW and EAD.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
"In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have ""training algorithms that are exactly equivalent."" I think this example needs to be clarified.' [SEP] 'rebuttal_answer"	We added new experiments showing the agent can learn to manipulate two balls.
5. In fact, I don't know why \omega needs to output p. It's never mentioned in the experiment section.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
10. Reading the baselines before the experiments is very confusing.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
3. This apart, I think that the experiment section is pretty hard to read, given all the metrics and methodology is in the Appendix.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
The imagenet experiment lacks details.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
The experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
Cons:  unclear transfer learning model, insufficient experiments.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
section 6.3, the authors show an experiments in this case, but only on a dense' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
1. the difficult to train the network' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
This paper has problems with clarity/polish and experimental design that are sufficiently severe' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
5. Clarity: The first half of this paper was easy to follow and clear. The experimental section had a couple of areas that left me confused. In particular:' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
"8. ""Beyond fixed schedules, automatically adjusting the training of G and D remains untacked"" -- this is not 100% true.' [SEP] 'rebuttal_answer"	We added new experiments showing the agent can learn to manipulate two balls.
There were some experimental details that were poorly explained but in general the paper was readable.' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
"Also, the sentence, ""We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy."", is confusing. If I use data parallelization, the gain should be also around 2.' [SEP] 'rebuttal_answer"	We added new experiments showing the agent can learn to manipulate two balls.
"2) It would be great if the paper can clearly define the experiments: ""waypoint"", ""oncoming"", ""mall"", and ""bottleneck"".' [SEP] 'rebuttal_answer"	We added new experiments showing the agent can learn to manipulate two balls.
This needs more elaboration. Is this way of training results expected? What is the lesson learned?' [SEP] 'rebuttal_answer	We added new experiments showing the agent can learn to manipulate two balls.
However, the real-world experiments are not necessarily the easiest to read.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
Authors should clarify the justification behind experimenting only on 'first 500 test images'.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
It seems there is no conclusion to take away from the experiments in section 5 (convolutions).' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
In the experiment there is no details on how you set the hyperparameters of CW and EAD.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
"In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have ""training algorithms that are exactly equivalent."" I think this example needs to be clarified.' [SEP] 'rebuttal_structuring"	Remark 1. Expression and detail
5. In fact, I don't know why \omega needs to output p. It's never mentioned in the experiment section.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
10. Reading the baselines before the experiments is very confusing.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
3. This apart, I think that the experiment section is pretty hard to read, given all the metrics and methodology is in the Appendix.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
The imagenet experiment lacks details.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
The experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
Cons:  unclear transfer learning model, insufficient experiments.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
section 6.3, the authors show an experiments in this case, but only on a dense' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
1. the difficult to train the network' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
This paper has problems with clarity/polish and experimental design that are sufficiently severe' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
5. Clarity: The first half of this paper was easy to follow and clear. The experimental section had a couple of areas that left me confused. In particular:' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
"8. ""Beyond fixed schedules, automatically adjusting the training of G and D remains untacked"" -- this is not 100% true.' [SEP] 'rebuttal_structuring"	Remark 1. Expression and detail
There were some experimental details that were poorly explained but in general the paper was readable.' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
"Also, the sentence, ""We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy."", is confusing. If I use data parallelization, the gain should be also around 2.' [SEP] 'rebuttal_structuring"	Remark 1. Expression and detail
"2) It would be great if the paper can clearly define the experiments: ""waypoint"", ""oncoming"", ""mall"", and ""bottleneck"".' [SEP] 'rebuttal_structuring"	Remark 1. Expression and detail
This needs more elaboration. Is this way of training results expected? What is the lesson learned?' [SEP] 'rebuttal_structuring	Remark 1. Expression and detail
However, the real-world experiments are not necessarily the easiest to read.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
Authors should clarify the justification behind experimenting only on 'first 500 test images'.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
It seems there is no conclusion to take away from the experiments in section 5 (convolutions).' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
In the experiment there is no details on how you set the hyperparameters of CW and EAD.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
"In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have ""training algorithms that are exactly equivalent."" I think this example needs to be clarified.' [SEP] 'rebuttal_by-cr"	5. We will add more details about the experiments to the appendix.
5. In fact, I don't know why \omega needs to output p. It's never mentioned in the experiment section.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
10. Reading the baselines before the experiments is very confusing.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
3. This apart, I think that the experiment section is pretty hard to read, given all the metrics and methodology is in the Appendix.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
The imagenet experiment lacks details.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
The experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
Cons:  unclear transfer learning model, insufficient experiments.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
section 6.3, the authors show an experiments in this case, but only on a dense' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
1. the difficult to train the network' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
This paper has problems with clarity/polish and experimental design that are sufficiently severe' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
5. Clarity: The first half of this paper was easy to follow and clear. The experimental section had a couple of areas that left me confused. In particular:' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
"8. ""Beyond fixed schedules, automatically adjusting the training of G and D remains untacked"" -- this is not 100% true.' [SEP] 'rebuttal_by-cr"	5. We will add more details about the experiments to the appendix.
There were some experimental details that were poorly explained but in general the paper was readable.' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
"Also, the sentence, ""We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy."", is confusing. If I use data parallelization, the gain should be also around 2.' [SEP] 'rebuttal_by-cr"	5. We will add more details about the experiments to the appendix.
"2) It would be great if the paper can clearly define the experiments: ""waypoint"", ""oncoming"", ""mall"", and ""bottleneck"".' [SEP] 'rebuttal_by-cr"	5. We will add more details about the experiments to the appendix.
This needs more elaboration. Is this way of training results expected? What is the lesson learned?' [SEP] 'rebuttal_by-cr	5. We will add more details about the experiments to the appendix.
However, the real-world experiments are not necessarily the easiest to read.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
Authors should clarify the justification behind experimenting only on 'first 500 test images'.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
It seems there is no conclusion to take away from the experiments in section 5 (convolutions).' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
In the experiment there is no details on how you set the hyperparameters of CW and EAD.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
"In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have ""training algorithms that are exactly equivalent."" I think this example needs to be clarified.' [SEP] 'rebuttal_done"	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
5. In fact, I don't know why \omega needs to output p. It's never mentioned in the experiment section.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
10. Reading the baselines before the experiments is very confusing.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
3. This apart, I think that the experiment section is pretty hard to read, given all the metrics and methodology is in the Appendix.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
The imagenet experiment lacks details.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
The experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
Cons:  unclear transfer learning model, insufficient experiments.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
section 6.3, the authors show an experiments in this case, but only on a dense' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
1. the difficult to train the network' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
This paper has problems with clarity/polish and experimental design that are sufficiently severe' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
5. Clarity: The first half of this paper was easy to follow and clear. The experimental section had a couple of areas that left me confused. In particular:' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
"8. ""Beyond fixed schedules, automatically adjusting the training of G and D remains untacked"" -- this is not 100% true.' [SEP] 'rebuttal_done"	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
There were some experimental details that were poorly explained but in general the paper was readable.' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
"Also, the sentence, ""We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy."", is confusing. If I use data parallelization, the gain should be also around 2.' [SEP] 'rebuttal_done"	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
"2) It would be great if the paper can clearly define the experiments: ""waypoint"", ""oncoming"", ""mall"", and ""bottleneck"".' [SEP] 'rebuttal_done"	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
This needs more elaboration. Is this way of training results expected? What is the lesson learned?' [SEP] 'rebuttal_done	(6) We updated the appendix to address this. See “Training convergence” in Appendix D.2
However, the real-world experiments are not necessarily the easiest to read.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
Authors should clarify the justification behind experimenting only on 'first 500 test images'.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
It seems there is no conclusion to take away from the experiments in section 5 (convolutions).' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
In the experiment there is no details on how you set the hyperparameters of CW and EAD.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
"In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have ""training algorithms that are exactly equivalent."" I think this example needs to be clarified.' [SEP] 'rebuttal_concede-criticism"	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
5. In fact, I don't know why \omega needs to output p. It's never mentioned in the experiment section.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
10. Reading the baselines before the experiments is very confusing.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
3. This apart, I think that the experiment section is pretty hard to read, given all the metrics and methodology is in the Appendix.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
The imagenet experiment lacks details.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
The experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
Cons:  unclear transfer learning model, insufficient experiments.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
section 6.3, the authors show an experiments in this case, but only on a dense' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
1. the difficult to train the network' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
This paper has problems with clarity/polish and experimental design that are sufficiently severe' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
5. Clarity: The first half of this paper was easy to follow and clear. The experimental section had a couple of areas that left me confused. In particular:' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
"8. ""Beyond fixed schedules, automatically adjusting the training of G and D remains untacked"" -- this is not 100% true.' [SEP] 'rebuttal_concede-criticism"	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
There were some experimental details that were poorly explained but in general the paper was readable.' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
"Also, the sentence, ""We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy."", is confusing. If I use data parallelization, the gain should be also around 2.' [SEP] 'rebuttal_concede-criticism"	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
"2) It would be great if the paper can clearly define the experiments: ""waypoint"", ""oncoming"", ""mall"", and ""bottleneck"".' [SEP] 'rebuttal_concede-criticism"	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
This needs more elaboration. Is this way of training results expected? What is the lesson learned?' [SEP] 'rebuttal_concede-criticism	A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.
A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018).' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
Compressability is evaluated, but that was already present in the previous work.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018).' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
The testbeds all existed previously and this is mostly the effort of pulling then together.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
Beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scaling-up of [Pathak, et al., ICML17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like ‘curiosity honeypots’ is interesting).' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
All of the testbeds have been used previously.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
Spectrum pooling has been used in the community of computer vision and machine learning.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
"Taking a random example (there are others by simple searching), in the ECCV paper ""DFT-based Transformation Invariant Pooling Layer for Visual Classification, Ryu et al., 2018"" The DFT magnitude pooling is almost the same as the authors' propositions, where the ""Fourier coefficients are cropped by cutting off high-frequency components"".' [SEP] 'rebuttal_concede-criticism"	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
The work is rather incremental from current state-of-the-art methods.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
For example, Narayanaswamy et al. [1] also propose to utilize labels to VAE.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
However, there are a few, though not many, works in the literature trying to combine Choquet integral with deep learning.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
For instance, using codes and codebooks to compress the weights has already been used in [1,2].' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
It is thus very hard to know if this new approach brings any improvement to previous work.' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018).' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
Compressability is evaluated, but that was already present in the previous work.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018).' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
The testbeds all existed previously and this is mostly the effort of pulling then together.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
Beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scaling-up of [Pathak, et al., ICML17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like ‘curiosity honeypots’ is interesting).' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
All of the testbeds have been used previously.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
Spectrum pooling has been used in the community of computer vision and machine learning.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
"Taking a random example (there are others by simple searching), in the ECCV paper ""DFT-based Transformation Invariant Pooling Layer for Visual Classification, Ryu et al., 2018"" The DFT magnitude pooling is almost the same as the authors' propositions, where the ""Fourier coefficients are cropped by cutting off high-frequency components"".' [SEP] 'rebuttal_mitigate-criticism"	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
The work is rather incremental from current state-of-the-art methods.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
For example, Narayanaswamy et al. [1] also propose to utilize labels to VAE.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
However, there are a few, though not many, works in the literature trying to combine Choquet integral with deep learning.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
For instance, using codes and codebooks to compress the weights has already been used in [1,2].' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
It is thus very hard to know if this new approach brings any improvement to previous work.' [SEP] 'rebuttal_mitigate-criticism	Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).
A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018).' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
Compressability is evaluated, but that was already present in the previous work.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018).' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
The testbeds all existed previously and this is mostly the effort of pulling then together.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
Beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scaling-up of [Pathak, et al., ICML17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like ‘curiosity honeypots’ is interesting).' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
All of the testbeds have been used previously.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
Spectrum pooling has been used in the community of computer vision and machine learning.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
"Taking a random example (there are others by simple searching), in the ECCV paper ""DFT-based Transformation Invariant Pooling Layer for Visual Classification, Ryu et al., 2018"" The DFT magnitude pooling is almost the same as the authors' propositions, where the ""Fourier coefficients are cropped by cutting off high-frequency components"".' [SEP] 'rebuttal_done"	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
The work is rather incremental from current state-of-the-art methods.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
For example, Narayanaswamy et al. [1] also propose to utilize labels to VAE.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
However, there are a few, though not many, works in the literature trying to combine Choquet integral with deep learning.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
For instance, using codes and codebooks to compress the weights has already been used in [1,2].' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
It is thus very hard to know if this new approach brings any improvement to previous work.' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018).' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
Compressability is evaluated, but that was already present in the previous work.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018).' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
The testbeds all existed previously and this is mostly the effort of pulling then together.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
Beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scaling-up of [Pathak, et al., ICML17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like ‘curiosity honeypots’ is interesting).' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
All of the testbeds have been used previously.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
Spectrum pooling has been used in the community of computer vision and machine learning.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
"Taking a random example (there are others by simple searching), in the ECCV paper ""DFT-based Transformation Invariant Pooling Layer for Visual Classification, Ryu et al., 2018"" The DFT magnitude pooling is almost the same as the authors' propositions, where the ""Fourier coefficients are cropped by cutting off high-frequency components"".' [SEP] 'rebuttal_answer"	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
The work is rather incremental from current state-of-the-art methods.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
For example, Narayanaswamy et al. [1] also propose to utilize labels to VAE.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
However, there are a few, though not many, works in the literature trying to combine Choquet integral with deep learning.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
For instance, using codes and codebooks to compress the weights has already been used in [1,2].' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
It is thus very hard to know if this new approach brings any improvement to previous work.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018).' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
Compressability is evaluated, but that was already present in the previous work.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018).' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
The testbeds all existed previously and this is mostly the effort of pulling then together.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
Beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scaling-up of [Pathak, et al., ICML17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like ‘curiosity honeypots’ is interesting).' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
All of the testbeds have been used previously.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
Spectrum pooling has been used in the community of computer vision and machine learning.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
"Taking a random example (there are others by simple searching), in the ECCV paper ""DFT-based Transformation Invariant Pooling Layer for Visual Classification, Ryu et al., 2018"" The DFT magnitude pooling is almost the same as the authors' propositions, where the ""Fourier coefficients are cropped by cutting off high-frequency components"".' [SEP] 'rebuttal_structuring"	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
The work is rather incremental from current state-of-the-art methods.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
For example, Narayanaswamy et al. [1] also propose to utilize labels to VAE.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
However, there are a few, though not many, works in the literature trying to combine Choquet integral with deep learning.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
For instance, using codes and codebooks to compress the weights has already been used in [1,2].' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
It is thus very hard to know if this new approach brings any improvement to previous work.' [SEP] 'rebuttal_structuring	"In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."
A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Compressability is evaluated, but that was already present in the previous work.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The testbeds all existed previously and this is mostly the effort of pulling then together.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scaling-up of [Pathak, et al., ICML17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like ‘curiosity honeypots’ is interesting).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
All of the testbeds have been used previously.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Spectrum pooling has been used in the community of computer vision and machine learning.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
"Taking a random example (there are others by simple searching), in the ECCV paper ""DFT-based Transformation Invariant Pooling Layer for Visual Classification, Ryu et al., 2018"" The DFT magnitude pooling is almost the same as the authors' propositions, where the ""Fourier coefficients are cropped by cutting off high-frequency components"".' [SEP] 'rebuttal_summary"	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The work is rather incremental from current state-of-the-art methods.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
For example, Narayanaswamy et al. [1] also propose to utilize labels to VAE.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
However, there are a few, though not many, works in the literature trying to combine Choquet integral with deep learning.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
For instance, using codes and codebooks to compress the weights has already been used in [1,2].' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It is thus very hard to know if this new approach brings any improvement to previous work.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
Besides, as the base classifier is different for various baselines, it is hard to compare the methods.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
2) Applying the model of Hartford et al’18 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
Furthermore, no comparisons were provided to any baselines/alternative methods.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
But there are no comparisons between the proposed training method and previous related works.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
The paper currently only mentions the most related work for the proposed method,  using the whole section 2 to describe VCL and use section 3 to describe FSM and half of section 5 to describe SSR.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
While the section 4 has a discussion on related papers, there's no systematic experimental comparison across these methods.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
RNN-based approaches are with better “complexity” comparing to your sum baseline and “Deepset” approach.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
There is no experiment clearly shows convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches (Xie & Ermon (2019); Kool et al. (2019); PlÂšotz & Roth (2018) ).' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
Since this work takes the approach of allowing stale weight updates, the author should also compare with existing distributed training approaches that use asynchronous updates, with or without model parallelism, for example, Dean et al., 2012.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
- No comparison has been made between their approach and other previous approaches.' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred' [SEP] 'rebuttal_by-cr	We plan to make our code public to aid research in the area.
Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
Besides, as the base classifier is different for various baselines, it is hard to compare the methods.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
2) Applying the model of Hartford et al’18 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
Furthermore, no comparisons were provided to any baselines/alternative methods.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
But there are no comparisons between the proposed training method and previous related works.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
The paper currently only mentions the most related work for the proposed method,  using the whole section 2 to describe VCL and use section 3 to describe FSM and half of section 5 to describe SSR.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
While the section 4 has a discussion on related papers, there's no systematic experimental comparison across these methods.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
RNN-based approaches are with better “complexity” comparing to your sum baseline and “Deepset” approach.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
There is no experiment clearly shows convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches (Xie & Ermon (2019); Kool et al. (2019); PlÂšotz & Roth (2018) ).' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
Since this work takes the approach of allowing stale weight updates, the author should also compare with existing distributed training approaches that use asynchronous updates, with or without model parallelism, for example, Dean et al., 2012.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
- No comparison has been made between their approach and other previous approaches.' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred' [SEP] 'rebuttal_reject-criticism	While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.
Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
Besides, as the base classifier is different for various baselines, it is hard to compare the methods.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
2) Applying the model of Hartford et al’18 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
Furthermore, no comparisons were provided to any baselines/alternative methods.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
But there are no comparisons between the proposed training method and previous related works.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
The paper currently only mentions the most related work for the proposed method,  using the whole section 2 to describe VCL and use section 3 to describe FSM and half of section 5 to describe SSR.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
While the section 4 has a discussion on related papers, there's no systematic experimental comparison across these methods.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
RNN-based approaches are with better “complexity” comparing to your sum baseline and “Deepset” approach.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
There is no experiment clearly shows convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches (Xie & Ermon (2019); Kool et al. (2019); PlÂšotz & Roth (2018) ).' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
Since this work takes the approach of allowing stale weight updates, the author should also compare with existing distributed training approaches that use asynchronous updates, with or without model parallelism, for example, Dean et al., 2012.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
- No comparison has been made between their approach and other previous approaches.' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred' [SEP] 'rebuttal_done	In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.
- It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.' [SEP] 'rebuttal_structuring	Q2. It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.
Although the idea behind this paper is fairly simple, the paper is very difficult to understand.' [SEP] 'rebuttal_structuring	Q2. It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.
- The problem setting description is neither formal nor intuitive which made it very hard for me to understand exactly the problem you are trying to solve.' [SEP] 'rebuttal_structuring	Q2. It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.
In Figure 5, a legend indicating the relationship between color intensity and distance would be helpful.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
In Figure 6 there seem to be unnecessary discrepancies between the y-axis and colorbar of subplots (a) and (b), and keeping those more consistent would improve readability.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples).' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
Furthermore, it may be informative to show the saliency maps in Figure 5 not only for cases in which the learner classified the image correctly in both time steps, but also cases in which the learner classified the image correctly the first time and incorrectly the second time.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
From the plots in Figure 2 and 3, it is hard to find the convergence of the method within 100 iterations.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
Also, the claim in Fig. 1 that the transition from ‘’high capacity’’ to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand, and should be toned down. (*)' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
Also, from the three Tables in the experimental part, the improvement of F-pooling over AA-pooling (developed by the main reference of this work) does not seem to be significant or consistent.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
For example, in Table 2, the F-pooling only wins at either accuracy (marginally) or consistency, but not both.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
Yet, in Fig.1 some difference is observed between the methods, why is that so?' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
To continue with the experimental evaluation, I found the plots with the predictive uncertainty in Figure 3 a bit confusing.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
There is no value in being very confident if you are wrong and vice-versa, so unless there is an accompanying plot/table reporting the accuracy I see not much value from this plot alone.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
And finally, the discussion of this figure makes claims about the behaviour of the model that seems to be too strong to be based on a single image experiment.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
- In Table 4, it is hard to compare DeepTwist with the other methods because activation quantization is not used.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
An example is presented in Figure 3 but is not expanded upon in the main text.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
In my opinion, to support the above claim, shouldn’t the authors provide a similar table as Table 1, directly comparing the certified radii of the natural images and adversarial images?' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
- From Figure 9, we see the certificate radii of the natural have at least two peaks. Though on average the certificate radii of the adversarial attacks is higher than that of the natural images, it is smaller than the right peak. Could the authors elaborate more of the results?' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled “Generative Ensembles for Robust Anomaly Detection” makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
2. Even though Figure 2b shows that SVHN test likelihoods are higher than CIFAR test likelihoods, the overlap in the histograms of CIFAR-train and CIFAR-test is much higher than the overlap in CIFAR-train and SVHN-test.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
I don’t see a significant difference between RAN and DNN in Figure 5. Maybe more explanation or better visualization would help.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
It is not clear how the compression ratio in table 1 is obtained.' [SEP] 'rebuttal_structuring	R4: An example is presented in Figure 3 but is not expanded upon in the main text.
In Figure 5, a legend indicating the relationship between color intensity and distance would be helpful.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
In Figure 6 there seem to be unnecessary discrepancies between the y-axis and colorbar of subplots (a) and (b), and keeping those more consistent would improve readability.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples).' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
Furthermore, it may be informative to show the saliency maps in Figure 5 not only for cases in which the learner classified the image correctly in both time steps, but also cases in which the learner classified the image correctly the first time and incorrectly the second time.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
From the plots in Figure 2 and 3, it is hard to find the convergence of the method within 100 iterations.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
Also, the claim in Fig. 1 that the transition from ‘’high capacity’’ to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand, and should be toned down. (*)' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
Also, from the three Tables in the experimental part, the improvement of F-pooling over AA-pooling (developed by the main reference of this work) does not seem to be significant or consistent.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
For example, in Table 2, the F-pooling only wins at either accuracy (marginally) or consistency, but not both.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
Yet, in Fig.1 some difference is observed between the methods, why is that so?' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
To continue with the experimental evaluation, I found the plots with the predictive uncertainty in Figure 3 a bit confusing.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
There is no value in being very confident if you are wrong and vice-versa, so unless there is an accompanying plot/table reporting the accuracy I see not much value from this plot alone.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
And finally, the discussion of this figure makes claims about the behaviour of the model that seems to be too strong to be based on a single image experiment.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
- In Table 4, it is hard to compare DeepTwist with the other methods because activation quantization is not used.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
An example is presented in Figure 3 but is not expanded upon in the main text.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
In my opinion, to support the above claim, shouldn’t the authors provide a similar table as Table 1, directly comparing the certified radii of the natural images and adversarial images?' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
- From Figure 9, we see the certificate radii of the natural have at least two peaks. Though on average the certificate radii of the adversarial attacks is higher than that of the natural images, it is smaller than the right peak. Could the authors elaborate more of the results?' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled “Generative Ensembles for Robust Anomaly Detection” makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
2. Even though Figure 2b shows that SVHN test likelihoods are higher than CIFAR test likelihoods, the overlap in the histograms of CIFAR-train and CIFAR-test is much higher than the overlap in CIFAR-train and SVHN-test.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
I don’t see a significant difference between RAN and DNN in Figure 5. Maybe more explanation or better visualization would help.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
It is not clear how the compression ratio in table 1 is obtained.' [SEP] 'rebuttal_done	We have modified parts of our paper to reflect these arguments better.
In Figure 5, a legend indicating the relationship between color intensity and distance would be helpful.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
In Figure 6 there seem to be unnecessary discrepancies between the y-axis and colorbar of subplots (a) and (b), and keeping those more consistent would improve readability.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples).' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
Furthermore, it may be informative to show the saliency maps in Figure 5 not only for cases in which the learner classified the image correctly in both time steps, but also cases in which the learner classified the image correctly the first time and incorrectly the second time.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
From the plots in Figure 2 and 3, it is hard to find the convergence of the method within 100 iterations.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
Also, the claim in Fig. 1 that the transition from ‘’high capacity’’ to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand, and should be toned down. (*)' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
Also, from the three Tables in the experimental part, the improvement of F-pooling over AA-pooling (developed by the main reference of this work) does not seem to be significant or consistent.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
For example, in Table 2, the F-pooling only wins at either accuracy (marginally) or consistency, but not both.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
Yet, in Fig.1 some difference is observed between the methods, why is that so?' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
To continue with the experimental evaluation, I found the plots with the predictive uncertainty in Figure 3 a bit confusing.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
There is no value in being very confident if you are wrong and vice-versa, so unless there is an accompanying plot/table reporting the accuracy I see not much value from this plot alone.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
And finally, the discussion of this figure makes claims about the behaviour of the model that seems to be too strong to be based on a single image experiment.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
- In Table 4, it is hard to compare DeepTwist with the other methods because activation quantization is not used.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
An example is presented in Figure 3 but is not expanded upon in the main text.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
In my opinion, to support the above claim, shouldn’t the authors provide a similar table as Table 1, directly comparing the certified radii of the natural images and adversarial images?' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
- From Figure 9, we see the certificate radii of the natural have at least two peaks. Though on average the certificate radii of the adversarial attacks is higher than that of the natural images, it is smaller than the right peak. Could the authors elaborate more of the results?' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled “Generative Ensembles for Robust Anomaly Detection” makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
2. Even though Figure 2b shows that SVHN test likelihoods are higher than CIFAR test likelihoods, the overlap in the histograms of CIFAR-train and CIFAR-test is much higher than the overlap in CIFAR-train and SVHN-test.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
I don’t see a significant difference between RAN and DNN in Figure 5. Maybe more explanation or better visualization would help.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
It is not clear how the compression ratio in table 1 is obtained.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
-3 For image-to-image translation experiments, no quantitative analysis whatsoever is offered so the reader can't really conclude anything about the effect of the proposed method in this domain.' [SEP] 'rebuttal_answer	Given the limited space provided, it would be difficult to fit a convergence analysis in our paper.
That said, I would like to see more analysis on the behavior of the proposed method under various interesting cases not tested yet.' [SEP] 'rebuttal_answer	Given the limited space provided, it would be difficult to fit a convergence analysis in our paper.
5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.' [SEP] 'rebuttal_answer	Given the limited space provided, it would be difficult to fit a convergence analysis in our paper.
-3 For image-to-image translation experiments, no quantitative analysis whatsoever is offered so the reader can't really conclude anything about the effect of the proposed method in this domain.' [SEP] 'rebuttal_social	We appreciate such detailed and rigorous convergence analysis provided in [1] and [2].
That said, I would like to see more analysis on the behavior of the proposed method under various interesting cases not tested yet.' [SEP] 'rebuttal_social	We appreciate such detailed and rigorous convergence analysis provided in [1] and [2].
5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.' [SEP] 'rebuttal_social	We appreciate such detailed and rigorous convergence analysis provided in [1] and [2].
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches).' [SEP] 'rebuttal_concede-criticism	We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches).' [SEP] 'rebuttal_done	It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches).' [SEP] 'rebuttal_mitigate-criticism	3. We clarify the differences to some other recent papers in our reply to reviewer 1.
I would have liked to see results for both trivial baselines like random ranking as well as more informed baselines where we can estimate transfer potential using say k representation techniques, and then use that to help us understand how well it would do on the other representations.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
Even though the authors answer positively to each of their four questions in the experiments section' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
However, I don’t know how effective this is in practice.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
The negation example is nice but this doesn't seem to display the potential power of the method to understand a neural network.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
3) The overall performance of the proposed SST in the experiments is not convincing and not promising.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
However, all experiments only show results in the MCAR setting, so the claim is not experimentally validated.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
* The experiments do not demonstrate that the model learns a meaningful *conditional* distribution for the missing modalities, since the provided figures show just one sample per conditioning image.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images’’ -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen’’ images which it labels as the negative class, thus the negative class is also seen by the memorization model.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4).' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
2. Though seemingly very important to the architecture, the purpose of constructing the super-graph g^{sup} in the training of C^{CAT} seems to be unclear to me.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
B. You don't schedule learning rates for your baseline methods except for a single experiment for some initial learning rate.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Overall, this is a reasonable paper but experimental section needs much more attention.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Regarding the experiments on adversarial examples, I am not convinced of their relevance at all.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
4. [Experiments.] The author presented a multimodal representation learning framework for partially-observable multimodal data, while the experiments cannot corraborrate the claim.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
These synthetic setting can be used for sanity check, but cannot be the main part of the experiments.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
"From the experimental perspective, the experimental evidence on ""invertible boolean logic"" does not seem to be very convincing for validating the approach.' [SEP] 'rebuttal_concede-criticism"	We apologize for unclear description of experimental settings.
The experiments are not very convincing or illustrative of the theoretical results in my opinion.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
"If that's true, then this distribution would be very bad for training a value function, which is supposed to involve an expectation over ""nature""'s choices in the MDP.' [SEP] 'rebuttal_concede-criticism"	We apologize for unclear description of experimental settings.
Second, the experiments are (as I understand it, but I may be wrong) in deterministic domains, which definitely does not constitute a general test of a proposed RL  method.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
On the other hand, it is hard to say what the _main_ contribution is, which in turn makes it difficult to evaluate whether the experimental evaluation is sufficient:' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Given the fact that it applies the same MLP independently to individual set members, it is obvious that it is not universal equivariant (for example, consider a function that performs a fixed permutation to its input), and I fail to see why the paper goes into the trouble of having theorems and experiments just to demonstrate this point. If there were any other objectives beyond this in the experiments could you please clarify?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
3) In the experiments, the accuracy values are too low for me.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Actually in the experiments the authors never use an increasing batch size.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Viewing it as a “duality gap” seems to be far from the practical training.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
I’m not sure whether it is due to parameter choice or due to weak D/G networks used in the simulation.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Another unjustified choice by the authors is their choice of weighing the Tensor term (when it is being added to two base embedding vectors) in the phrase similarity experiment.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Even though the authors answer positively to each of their four questions in the experiments section' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
However, I don’t know how effective this is in practice.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
The negation example is nice but this doesn't seem to display the potential power of the method to understand a neural network.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
3) The overall performance of the proposed SST in the experiments is not convincing and not promising.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
However, all experiments only show results in the MCAR setting, so the claim is not experimentally validated.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
* The experiments do not demonstrate that the model learns a meaningful *conditional* distribution for the missing modalities, since the provided figures show just one sample per conditioning image.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images’’ -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen’’ images which it labels as the negative class, thus the negative class is also seen by the memorization model.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4).' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
2. Though seemingly very important to the architecture, the purpose of constructing the super-graph g^{sup} in the training of C^{CAT} seems to be unclear to me.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
B. You don't schedule learning rates for your baseline methods except for a single experiment for some initial learning rate.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
Overall, this is a reasonable paper but experimental section needs much more attention.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
Regarding the experiments on adversarial examples, I am not convinced of their relevance at all.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
4. [Experiments.] The author presented a multimodal representation learning framework for partially-observable multimodal data, while the experiments cannot corraborrate the claim.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
These synthetic setting can be used for sanity check, but cannot be the main part of the experiments.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
"From the experimental perspective, the experimental evidence on ""invertible boolean logic"" does not seem to be very convincing for validating the approach.' [SEP] 'rebuttal_structuring"	Please refer to Table 3 and Appendix C.4 for updated comparison results.
The experiments are not very convincing or illustrative of the theoretical results in my opinion.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
"If that's true, then this distribution would be very bad for training a value function, which is supposed to involve an expectation over ""nature""'s choices in the MDP.' [SEP] 'rebuttal_structuring"	Please refer to Table 3 and Appendix C.4 for updated comparison results.
Second, the experiments are (as I understand it, but I may be wrong) in deterministic domains, which definitely does not constitute a general test of a proposed RL  method.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
On the other hand, it is hard to say what the _main_ contribution is, which in turn makes it difficult to evaluate whether the experimental evaluation is sufficient:' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
Given the fact that it applies the same MLP independently to individual set members, it is obvious that it is not universal equivariant (for example, consider a function that performs a fixed permutation to its input), and I fail to see why the paper goes into the trouble of having theorems and experiments just to demonstrate this point. If there were any other objectives beyond this in the experiments could you please clarify?' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
3) In the experiments, the accuracy values are too low for me.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
Actually in the experiments the authors never use an increasing batch size.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
Viewing it as a “duality gap” seems to be far from the practical training.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
I’m not sure whether it is due to parameter choice or due to weak D/G networks used in the simulation.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
Another unjustified choice by the authors is their choice of weighing the Tensor term (when it is being added to two base embedding vectors) in the phrase similarity experiment.' [SEP] 'rebuttal_structuring	Please refer to Table 3 and Appendix C.4 for updated comparison results.
Even though the authors answer positively to each of their four questions in the experiments section' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
However, I don’t know how effective this is in practice.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
The negation example is nice but this doesn't seem to display the potential power of the method to understand a neural network.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
3) The overall performance of the proposed SST in the experiments is not convincing and not promising.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
However, all experiments only show results in the MCAR setting, so the claim is not experimentally validated.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
* The experiments do not demonstrate that the model learns a meaningful *conditional* distribution for the missing modalities, since the provided figures show just one sample per conditioning image.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images’’ -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen’’ images which it labels as the negative class, thus the negative class is also seen by the memorization model.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4).' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
2. Though seemingly very important to the architecture, the purpose of constructing the super-graph g^{sup} in the training of C^{CAT} seems to be unclear to me.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
B. You don't schedule learning rates for your baseline methods except for a single experiment for some initial learning rate.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
Overall, this is a reasonable paper but experimental section needs much more attention.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
Regarding the experiments on adversarial examples, I am not convinced of their relevance at all.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
4. [Experiments.] The author presented a multimodal representation learning framework for partially-observable multimodal data, while the experiments cannot corraborrate the claim.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
These synthetic setting can be used for sanity check, but cannot be the main part of the experiments.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
"From the experimental perspective, the experimental evidence on ""invertible boolean logic"" does not seem to be very convincing for validating the approach.' [SEP] 'rebuttal_done"	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
The experiments are not very convincing or illustrative of the theoretical results in my opinion.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
"If that's true, then this distribution would be very bad for training a value function, which is supposed to involve an expectation over ""nature""'s choices in the MDP.' [SEP] 'rebuttal_done"	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
Second, the experiments are (as I understand it, but I may be wrong) in deterministic domains, which definitely does not constitute a general test of a proposed RL  method.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
On the other hand, it is hard to say what the _main_ contribution is, which in turn makes it difficult to evaluate whether the experimental evaluation is sufficient:' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
Given the fact that it applies the same MLP independently to individual set members, it is obvious that it is not universal equivariant (for example, consider a function that performs a fixed permutation to its input), and I fail to see why the paper goes into the trouble of having theorems and experiments just to demonstrate this point. If there were any other objectives beyond this in the experiments could you please clarify?' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
3) In the experiments, the accuracy values are too low for me.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
Actually in the experiments the authors never use an increasing batch size.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
Viewing it as a “duality gap” seems to be far from the practical training.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
I’m not sure whether it is due to parameter choice or due to weak D/G networks used in the simulation.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
Another unjustified choice by the authors is their choice of weighing the Tensor term (when it is being added to two base embedding vectors) in the phrase similarity experiment.' [SEP] 'rebuttal_done	We have addressed your concern about the baseline models and learning rate schedules in our updated paper.
Even though the authors answer positively to each of their four questions in the experiments section' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
However, I don’t know how effective this is in practice.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
The negation example is nice but this doesn't seem to display the potential power of the method to understand a neural network.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
3) The overall performance of the proposed SST in the experiments is not convincing and not promising.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
However, all experiments only show results in the MCAR setting, so the claim is not experimentally validated.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
* The experiments do not demonstrate that the model learns a meaningful *conditional* distribution for the missing modalities, since the provided figures show just one sample per conditioning image.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images’’ -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen’’ images which it labels as the negative class, thus the negative class is also seen by the memorization model.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4).' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
2. Though seemingly very important to the architecture, the purpose of constructing the super-graph g^{sup} in the training of C^{CAT} seems to be unclear to me.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
B. You don't schedule learning rates for your baseline methods except for a single experiment for some initial learning rate.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
Overall, this is a reasonable paper but experimental section needs much more attention.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
Regarding the experiments on adversarial examples, I am not convinced of their relevance at all.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
4. [Experiments.] The author presented a multimodal representation learning framework for partially-observable multimodal data, while the experiments cannot corraborrate the claim.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
These synthetic setting can be used for sanity check, but cannot be the main part of the experiments.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
"From the experimental perspective, the experimental evidence on ""invertible boolean logic"" does not seem to be very convincing for validating the approach.' [SEP] 'rebuttal_answer"	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
The experiments are not very convincing or illustrative of the theoretical results in my opinion.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
"If that's true, then this distribution would be very bad for training a value function, which is supposed to involve an expectation over ""nature""'s choices in the MDP.' [SEP] 'rebuttal_answer"	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
Second, the experiments are (as I understand it, but I may be wrong) in deterministic domains, which definitely does not constitute a general test of a proposed RL  method.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
On the other hand, it is hard to say what the _main_ contribution is, which in turn makes it difficult to evaluate whether the experimental evaluation is sufficient:' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
Given the fact that it applies the same MLP independently to individual set members, it is obvious that it is not universal equivariant (for example, consider a function that performs a fixed permutation to its input), and I fail to see why the paper goes into the trouble of having theorems and experiments just to demonstrate this point. If there were any other objectives beyond this in the experiments could you please clarify?' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
3) In the experiments, the accuracy values are too low for me.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
Actually in the experiments the authors never use an increasing batch size.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
Viewing it as a “duality gap” seems to be far from the practical training.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
I’m not sure whether it is due to parameter choice or due to weak D/G networks used in the simulation.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
Another unjustified choice by the authors is their choice of weighing the Tensor term (when it is being added to two base embedding vectors) in the phrase similarity experiment.' [SEP] 'rebuttal_answer	Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.
Even though the authors answer positively to each of their four questions in the experiments section' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
However, I don’t know how effective this is in practice.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
The negation example is nice but this doesn't seem to display the potential power of the method to understand a neural network.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
3) The overall performance of the proposed SST in the experiments is not convincing and not promising.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
However, all experiments only show results in the MCAR setting, so the claim is not experimentally validated.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
* The experiments do not demonstrate that the model learns a meaningful *conditional* distribution for the missing modalities, since the provided figures show just one sample per conditioning image.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images’’ -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen’’ images which it labels as the negative class, thus the negative class is also seen by the memorization model.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4).' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
2. Though seemingly very important to the architecture, the purpose of constructing the super-graph g^{sup} in the training of C^{CAT} seems to be unclear to me.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
B. You don't schedule learning rates for your baseline methods except for a single experiment for some initial learning rate.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
Overall, this is a reasonable paper but experimental section needs much more attention.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
Regarding the experiments on adversarial examples, I am not convinced of their relevance at all.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
4. [Experiments.] The author presented a multimodal representation learning framework for partially-observable multimodal data, while the experiments cannot corraborrate the claim.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
These synthetic setting can be used for sanity check, but cannot be the main part of the experiments.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
"From the experimental perspective, the experimental evidence on ""invertible boolean logic"" does not seem to be very convincing for validating the approach.' [SEP] 'rebuttal_refute-question"	Though not explained in Section 4.
The experiments are not very convincing or illustrative of the theoretical results in my opinion.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
"If that's true, then this distribution would be very bad for training a value function, which is supposed to involve an expectation over ""nature""'s choices in the MDP.' [SEP] 'rebuttal_refute-question"	Though not explained in Section 4.
Second, the experiments are (as I understand it, but I may be wrong) in deterministic domains, which definitely does not constitute a general test of a proposed RL  method.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
On the other hand, it is hard to say what the _main_ contribution is, which in turn makes it difficult to evaluate whether the experimental evaluation is sufficient:' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
Given the fact that it applies the same MLP independently to individual set members, it is obvious that it is not universal equivariant (for example, consider a function that performs a fixed permutation to its input), and I fail to see why the paper goes into the trouble of having theorems and experiments just to demonstrate this point. If there were any other objectives beyond this in the experiments could you please clarify?' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
3) In the experiments, the accuracy values are too low for me.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
Actually in the experiments the authors never use an increasing batch size.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
Viewing it as a “duality gap” seems to be far from the practical training.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
I’m not sure whether it is due to parameter choice or due to weak D/G networks used in the simulation.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
Another unjustified choice by the authors is their choice of weighing the Tensor term (when it is being added to two base embedding vectors) in the phrase similarity experiment.' [SEP] 'rebuttal_refute-question	Though not explained in Section 4.
The main problem is that directly predicting the context is intractable because of combinatorial explosion.' [SEP] 'rebuttal_concede-criticism	We apologize if we painted an incorrect picture by calling it a “simple example” and a “staple introductory problem”.
- the more interesting problem, RL + auxiliary loss isn’t evaluated in detail' [SEP] 'rebuttal_concede-criticism	We apologize if we painted an incorrect picture by calling it a “simple example” and a “staple introductory problem”.
But the problem settings are not clear to me.' [SEP] 'rebuttal_concede-criticism	We apologize if we painted an incorrect picture by calling it a “simple example” and a “staple introductory problem”.
"The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to ""directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems."" Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.' [SEP] 'rebuttal_concede-criticism"	We apologize if we painted an incorrect picture by calling it a “simple example” and a “staple introductory problem”.
The paper motivates the problem that we need to pick out an exploration sequence that optimizes learning progress, but then approximates it as simply measuring the return.' [SEP] 'rebuttal_concede-criticism	We apologize if we painted an incorrect picture by calling it a “simple example” and a “staple introductory problem”.
It would be nice to position the ideas from the paper w.r.t. this line of research too.' [SEP] 'rebuttal_concede-criticism	We apologize if we painted an incorrect picture by calling it a “simple example” and a “staple introductory problem”.
"The basic idea is very interesting, but I would have expect more interesting use cases as teased by the first sentence of the abstract ""find algorithms with strong worst-case guarantees for online combinatorial optimization problems"".' [SEP] 'rebuttal_concede-criticism"	We apologize if we painted an incorrect picture by calling it a “simple example” and a “staple introductory problem”.
So at the end, I am a bit puzzled. I really like the idea, but I have the feeling that this technique should have been developed for more complicated setting. Or maybe it is actually not working on more difficult combinatorial problem (and this is hidden in the paper).' [SEP] 'rebuttal_concede-criticism	We apologize if we painted an incorrect picture by calling it a “simple example” and a “staple introductory problem”.
The idea in this paper is novel but experiments do not seem to be enough.' [SEP] 'rebuttal_concede-criticism	We apologize if we painted an incorrect picture by calling it a “simple example” and a “staple introductory problem”.
The main problem is that directly predicting the context is intractable because of combinatorial explosion.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer's main comment that the experiments are not large scale.
- the more interesting problem, RL + auxiliary loss isn’t evaluated in detail' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer's main comment that the experiments are not large scale.
But the problem settings are not clear to me.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer's main comment that the experiments are not large scale.
"The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to ""directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems."" Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.' [SEP] 'rebuttal_reject-criticism"	We respectfully disagree with the reviewer's main comment that the experiments are not large scale.
The paper motivates the problem that we need to pick out an exploration sequence that optimizes learning progress, but then approximates it as simply measuring the return.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer's main comment that the experiments are not large scale.
It would be nice to position the ideas from the paper w.r.t. this line of research too.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer's main comment that the experiments are not large scale.
"The basic idea is very interesting, but I would have expect more interesting use cases as teased by the first sentence of the abstract ""find algorithms with strong worst-case guarantees for online combinatorial optimization problems"".' [SEP] 'rebuttal_reject-criticism"	We respectfully disagree with the reviewer's main comment that the experiments are not large scale.
So at the end, I am a bit puzzled. I really like the idea, but I have the feeling that this technique should have been developed for more complicated setting. Or maybe it is actually not working on more difficult combinatorial problem (and this is hidden in the paper).' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer's main comment that the experiments are not large scale.
The idea in this paper is novel but experiments do not seem to be enough.' [SEP] 'rebuttal_reject-criticism	We respectfully disagree with the reviewer's main comment that the experiments are not large scale.
The main problem is that directly predicting the context is intractable because of combinatorial explosion.' [SEP] 'rebuttal_by-cr	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
- the more interesting problem, RL + auxiliary loss isn’t evaluated in detail' [SEP] 'rebuttal_by-cr	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
But the problem settings are not clear to me.' [SEP] 'rebuttal_by-cr	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
"The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to ""directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems."" Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.' [SEP] 'rebuttal_by-cr"	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
The paper motivates the problem that we need to pick out an exploration sequence that optimizes learning progress, but then approximates it as simply measuring the return.' [SEP] 'rebuttal_by-cr	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
It would be nice to position the ideas from the paper w.r.t. this line of research too.' [SEP] 'rebuttal_by-cr	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
"The basic idea is very interesting, but I would have expect more interesting use cases as teased by the first sentence of the abstract ""find algorithms with strong worst-case guarantees for online combinatorial optimization problems"".' [SEP] 'rebuttal_by-cr"	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
So at the end, I am a bit puzzled. I really like the idea, but I have the feeling that this technique should have been developed for more complicated setting. Or maybe it is actually not working on more difficult combinatorial problem (and this is hidden in the paper).' [SEP] 'rebuttal_by-cr	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
The idea in this paper is novel but experiments do not seem to be enough.' [SEP] 'rebuttal_by-cr	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
The main problem is that directly predicting the context is intractable because of combinatorial explosion.' [SEP] 'rebuttal_mitigate-criticism	This is, once again, motivated by making something work with modest amount of computation.
- the more interesting problem, RL + auxiliary loss isn’t evaluated in detail' [SEP] 'rebuttal_mitigate-criticism	This is, once again, motivated by making something work with modest amount of computation.
But the problem settings are not clear to me.' [SEP] 'rebuttal_mitigate-criticism	This is, once again, motivated by making something work with modest amount of computation.
"The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to ""directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems."" Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.' [SEP] 'rebuttal_mitigate-criticism"	This is, once again, motivated by making something work with modest amount of computation.
The paper motivates the problem that we need to pick out an exploration sequence that optimizes learning progress, but then approximates it as simply measuring the return.' [SEP] 'rebuttal_mitigate-criticism	This is, once again, motivated by making something work with modest amount of computation.
It would be nice to position the ideas from the paper w.r.t. this line of research too.' [SEP] 'rebuttal_mitigate-criticism	This is, once again, motivated by making something work with modest amount of computation.
"The basic idea is very interesting, but I would have expect more interesting use cases as teased by the first sentence of the abstract ""find algorithms with strong worst-case guarantees for online combinatorial optimization problems"".' [SEP] 'rebuttal_mitigate-criticism"	This is, once again, motivated by making something work with modest amount of computation.
So at the end, I am a bit puzzled. I really like the idea, but I have the feeling that this technique should have been developed for more complicated setting. Or maybe it is actually not working on more difficult combinatorial problem (and this is hidden in the paper).' [SEP] 'rebuttal_mitigate-criticism	This is, once again, motivated by making something work with modest amount of computation.
The idea in this paper is novel but experiments do not seem to be enough.' [SEP] 'rebuttal_mitigate-criticism	This is, once again, motivated by making something work with modest amount of computation.
However, the experiments conducted generally show that SAVP offers only a trade-off between the visual quality of GANs and the coverage of VAEs, and does not show a clear advantage over current VAE models (Denton & Fergus, 2018) that with simpler architectures obtain similar results.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
With regards to the fMRI experiments, good baselines are missing: DEMINE is compared to Pearson correlation.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
However, the paper contains only little novelty and does not provide sufficiently new scientific insights.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
Originality: The work is moderately original.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
But I'm concerned with the novelty and contributions of this paper.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
In summary, the quality of the paper is poor and the originality of the work is low.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
Overall, this paper is good, but is not novel or important enough for acceptance.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
Overall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
I believe the primary claim of this paper is neither surprising nor novel.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
Assessment: Overall, this is a borderline paper, as the task is interesting and novel, but the presentation is lacking in technical detail and there is a lack of novelty on the modeling side.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
The only issue with this paper is its degree of novelty, which is narrow.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
Hence, I am not very sure whether the novelty of the paper is significant.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
Overall, this appears to be a board-line paper with weak novelty.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
This was a fun, albeit incremental paper.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
The paper is overall well written and intuitive but limited in evaluation and novelty (see e.g. [1,2] ) with only limited modifications (sharing low-level controller) for the multi agent case.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
My main concern comes from the novelty of this paper.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
However, the paper contains only little novelty and does not provide sufficiently new scientific insights.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Originality: The work is moderately original.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
But I'm concerned with the novelty and contributions of this paper.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
In summary, the quality of the paper is poor and the originality of the work is low.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Overall, this paper is good, but is not novel or important enough for acceptance.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Overall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
I believe the primary claim of this paper is neither surprising nor novel.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Assessment: Overall, this is a borderline paper, as the task is interesting and novel, but the presentation is lacking in technical detail and there is a lack of novelty on the modeling side.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The only issue with this paper is its degree of novelty, which is narrow.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Hence, I am not very sure whether the novelty of the paper is significant.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Overall, this appears to be a board-line paper with weak novelty.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
This was a fun, albeit incremental paper.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The paper is overall well written and intuitive but limited in evaluation and novelty (see e.g. [1,2] ) with only limited modifications (sharing low-level controller) for the multi agent case.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
My main concern comes from the novelty of this paper.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
However, the paper contains only little novelty and does not provide sufficiently new scientific insights.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
Originality: The work is moderately original.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
But I'm concerned with the novelty and contributions of this paper.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
In summary, the quality of the paper is poor and the originality of the work is low.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
Overall, this paper is good, but is not novel or important enough for acceptance.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
Overall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
I believe the primary claim of this paper is neither surprising nor novel.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
Assessment: Overall, this is a borderline paper, as the task is interesting and novel, but the presentation is lacking in technical detail and there is a lack of novelty on the modeling side.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
The only issue with this paper is its degree of novelty, which is narrow.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
Hence, I am not very sure whether the novelty of the paper is significant.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
Overall, this appears to be a board-line paper with weak novelty.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
This was a fun, albeit incremental paper.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
The paper is overall well written and intuitive but limited in evaluation and novelty (see e.g. [1,2] ) with only limited modifications (sharing low-level controller) for the multi agent case.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
My main concern comes from the novelty of this paper.' [SEP] 'rebuttal_reject-criticism	However, our work differs in two major ways:
However, the paper contains only little novelty and does not provide sufficiently new scientific insights.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
Originality: The work is moderately original.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
But I'm concerned with the novelty and contributions of this paper.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
In summary, the quality of the paper is poor and the originality of the work is low.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
Overall, this paper is good, but is not novel or important enough for acceptance.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
Overall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
I believe the primary claim of this paper is neither surprising nor novel.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
Assessment: Overall, this is a borderline paper, as the task is interesting and novel, but the presentation is lacking in technical detail and there is a lack of novelty on the modeling side.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
The only issue with this paper is its degree of novelty, which is narrow.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
Hence, I am not very sure whether the novelty of the paper is significant.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
Overall, this appears to be a board-line paper with weak novelty.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
This was a fun, albeit incremental paper.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
The paper is overall well written and intuitive but limited in evaluation and novelty (see e.g. [1,2] ) with only limited modifications (sharing low-level controller) for the multi agent case.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
My main concern comes from the novelty of this paper.' [SEP] 'rebuttal_structuring	[Q] The only issue with this paper is its degree of novelty, which is narrow.
This poses a challenge in evaluating this paper.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
- some parts of the paper are quite unclear' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
The primary difficulty in reviewing this paper is the poor presentation of the paper.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
This issues makes reviewing this paper very difficult.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
There are many typos and grammar errors' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Weaknesses: Paper could have been written better. I had hard time understanding it.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
The notations are overall confusing and not explained well.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
I also had a hard time going through the paper - there aren't many details.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
The paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
However, there are some key issues with the paper that are not clear.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
- This paper is a slightly difficult read - not because of the' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
"- Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)' [SEP] 'rebuttal_concede-criticism"	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
2) The writing is poor and hard to follow.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
"- the paper is poorly written and sentences are generally very hard to parse. For example, section 3.1 is opened by statements such as ""(we use) a multi-task evaluator which trains on the principal and auxiliary tasks, and evaluates the performance of the auxiliary tasks on a meta set""??' [SEP] 'rebuttal_concede-criticism"	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
- Paper is often hard to follow, and contains a significant number of typos.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
The paper can benefit from a proofreading.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
There are a few typos throughout the paper such as:' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
* The text is quite hard to read.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
There are many typos (see below).' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
I think this is a very interesting direction, but the present paper is somewhat unclear.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Overall, I think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
The paper is relatively well-written, although the description of the neural models can be improved.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
The main criticism I have is that I found the paper harder to read.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
(4) The writing quality is not satisfactory.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Overall the paper, while interesting is unacceptably messy.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
The paper is also littered with typos and vague statements (many enumerated below under *small issues*).' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
3. The paper is not nicely written or rather easy to follow.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
I recommend adjust the language to be more consistent throughout.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Especially given the very specific nature of the topic I miss a strong and clear path through the paper.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
In terms of writing, the paper is a bit confusing in terms of motivations and notations.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
I think the paper could benefit from having this in the earlier sections.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
First, this paper is not easy to follow.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Fourth, there are some grammar mistakes and typos.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Third, the writing in the paper has some significant lapses in clarity.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
I found the paper confusing at times.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
It is well-written from a syntactical and grammatical point of view, but some key concepts are stated without being explained, which gives the impression that the authors have a clear understanding of the material presented in the paper but communicate only part of the full picture to the reader.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Clarity: The clarity is below average.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
The clarity of this paper needs to be strengthened.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
2) The main contributions of this paper are not quite clear to me.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
In terms of presentation, the notation and statement of theorems are precise, however, the presentation is rather dry, and I think the paper can be significantly more accessible.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
- Some parts of the paper feel long-winded and aimless.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Honestly, this paper is very difficult to follow.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
2) The presentation is not professional, hard to follow and the submission overall looks very rushed:' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
- The writing looks very rushed, and should be improved.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
This paper looks very hastily put together, especially pages 7 and 8.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
There are many typos and unclear statements.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
To change to a firm accept, I think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area, which can easily be implemented in the camera ready stage of paper preparation.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Overall, the paper is a little confusing.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
- Grammatical errors and odd formulations all over the place' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Current representation is difficult to read / parse.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Second, the writing can be greatly improved.' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
Therefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).' [SEP] 'rebuttal_concede-criticism	In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.
This poses a challenge in evaluating this paper.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
- some parts of the paper are quite unclear' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
The primary difficulty in reviewing this paper is the poor presentation of the paper.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
This issues makes reviewing this paper very difficult.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
There are many typos and grammar errors' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Weaknesses: Paper could have been written better. I had hard time understanding it.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
The notations are overall confusing and not explained well.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
I also had a hard time going through the paper - there aren't many details.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
The paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
However, there are some key issues with the paper that are not clear.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
- This paper is a slightly difficult read - not because of the' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
"- Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)' [SEP] 'rebuttal_structuring"	"R: ""The paper can benefit from a proofreading."""
2) The writing is poor and hard to follow.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
"- the paper is poorly written and sentences are generally very hard to parse. For example, section 3.1 is opened by statements such as ""(we use) a multi-task evaluator which trains on the principal and auxiliary tasks, and evaluates the performance of the auxiliary tasks on a meta set""??' [SEP] 'rebuttal_structuring"	"R: ""The paper can benefit from a proofreading."""
- Paper is often hard to follow, and contains a significant number of typos.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
The paper can benefit from a proofreading.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
There are a few typos throughout the paper such as:' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
* The text is quite hard to read.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
There are many typos (see below).' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
I think this is a very interesting direction, but the present paper is somewhat unclear.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Overall, I think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
The paper is relatively well-written, although the description of the neural models can be improved.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
The main criticism I have is that I found the paper harder to read.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
(4) The writing quality is not satisfactory.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Overall the paper, while interesting is unacceptably messy.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
The paper is also littered with typos and vague statements (many enumerated below under *small issues*).' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
3. The paper is not nicely written or rather easy to follow.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
I recommend adjust the language to be more consistent throughout.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Especially given the very specific nature of the topic I miss a strong and clear path through the paper.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
In terms of writing, the paper is a bit confusing in terms of motivations and notations.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
I think the paper could benefit from having this in the earlier sections.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
First, this paper is not easy to follow.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Fourth, there are some grammar mistakes and typos.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Third, the writing in the paper has some significant lapses in clarity.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
I found the paper confusing at times.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
It is well-written from a syntactical and grammatical point of view, but some key concepts are stated without being explained, which gives the impression that the authors have a clear understanding of the material presented in the paper but communicate only part of the full picture to the reader.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Clarity: The clarity is below average.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
The clarity of this paper needs to be strengthened.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
2) The main contributions of this paper are not quite clear to me.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
In terms of presentation, the notation and statement of theorems are precise, however, the presentation is rather dry, and I think the paper can be significantly more accessible.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
- Some parts of the paper feel long-winded and aimless.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Honestly, this paper is very difficult to follow.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
2) The presentation is not professional, hard to follow and the submission overall looks very rushed:' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
- The writing looks very rushed, and should be improved.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
This paper looks very hastily put together, especially pages 7 and 8.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
There are many typos and unclear statements.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
To change to a firm accept, I think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area, which can easily be implemented in the camera ready stage of paper preparation.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Overall, the paper is a little confusing.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
- Grammatical errors and odd formulations all over the place' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Current representation is difficult to read / parse.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Second, the writing can be greatly improved.' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
Therefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).' [SEP] 'rebuttal_structuring	"R: ""The paper can benefit from a proofreading."""
This poses a challenge in evaluating this paper.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
- some parts of the paper are quite unclear' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
The primary difficulty in reviewing this paper is the poor presentation of the paper.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
This issues makes reviewing this paper very difficult.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
There are many typos and grammar errors' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Weaknesses: Paper could have been written better. I had hard time understanding it.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
The notations are overall confusing and not explained well.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
I also had a hard time going through the paper - there aren't many details.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
The paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
However, there are some key issues with the paper that are not clear.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
- This paper is a slightly difficult read - not because of the' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
"- Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)' [SEP] 'rebuttal_reject-criticism"	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
2) The writing is poor and hard to follow.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
"- the paper is poorly written and sentences are generally very hard to parse. For example, section 3.1 is opened by statements such as ""(we use) a multi-task evaluator which trains on the principal and auxiliary tasks, and evaluates the performance of the auxiliary tasks on a meta set""??' [SEP] 'rebuttal_reject-criticism"	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
- Paper is often hard to follow, and contains a significant number of typos.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
The paper can benefit from a proofreading.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
There are a few typos throughout the paper such as:' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
* The text is quite hard to read.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
There are many typos (see below).' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
I think this is a very interesting direction, but the present paper is somewhat unclear.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Overall, I think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
The paper is relatively well-written, although the description of the neural models can be improved.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
The main criticism I have is that I found the paper harder to read.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
(4) The writing quality is not satisfactory.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Overall the paper, while interesting is unacceptably messy.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
The paper is also littered with typos and vague statements (many enumerated below under *small issues*).' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
3. The paper is not nicely written or rather easy to follow.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
I recommend adjust the language to be more consistent throughout.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Especially given the very specific nature of the topic I miss a strong and clear path through the paper.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
In terms of writing, the paper is a bit confusing in terms of motivations and notations.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
I think the paper could benefit from having this in the earlier sections.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
First, this paper is not easy to follow.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Fourth, there are some grammar mistakes and typos.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Third, the writing in the paper has some significant lapses in clarity.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
I found the paper confusing at times.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
It is well-written from a syntactical and grammatical point of view, but some key concepts are stated without being explained, which gives the impression that the authors have a clear understanding of the material presented in the paper but communicate only part of the full picture to the reader.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Clarity: The clarity is below average.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
The clarity of this paper needs to be strengthened.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
2) The main contributions of this paper are not quite clear to me.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
In terms of presentation, the notation and statement of theorems are precise, however, the presentation is rather dry, and I think the paper can be significantly more accessible.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
- Some parts of the paper feel long-winded and aimless.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Honestly, this paper is very difficult to follow.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
2) The presentation is not professional, hard to follow and the submission overall looks very rushed:' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
- The writing looks very rushed, and should be improved.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
This paper looks very hastily put together, especially pages 7 and 8.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
There are many typos and unclear statements.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
To change to a firm accept, I think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area, which can easily be implemented in the camera ready stage of paper preparation.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Overall, the paper is a little confusing.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
- Grammatical errors and odd formulations all over the place' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Current representation is difficult to read / parse.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Second, the writing can be greatly improved.' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
Therefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).' [SEP] 'rebuttal_reject-criticism	We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.
This poses a challenge in evaluating this paper.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
- some parts of the paper are quite unclear' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
The primary difficulty in reviewing this paper is the poor presentation of the paper.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
This issues makes reviewing this paper very difficult.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
There are many typos and grammar errors' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Weaknesses: Paper could have been written better. I had hard time understanding it.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
The notations are overall confusing and not explained well.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
I also had a hard time going through the paper - there aren't many details.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
The paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
However, there are some key issues with the paper that are not clear.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
- This paper is a slightly difficult read - not because of the' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
"- Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)' [SEP] 'rebuttal_done"	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
2) The writing is poor and hard to follow.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
"- the paper is poorly written and sentences are generally very hard to parse. For example, section 3.1 is opened by statements such as ""(we use) a multi-task evaluator which trains on the principal and auxiliary tasks, and evaluates the performance of the auxiliary tasks on a meta set""??' [SEP] 'rebuttal_done"	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
- Paper is often hard to follow, and contains a significant number of typos.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
The paper can benefit from a proofreading.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
There are a few typos throughout the paper such as:' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
* The text is quite hard to read.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
There are many typos (see below).' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
I think this is a very interesting direction, but the present paper is somewhat unclear.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Overall, I think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
The paper is relatively well-written, although the description of the neural models can be improved.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
The main criticism I have is that I found the paper harder to read.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
(4) The writing quality is not satisfactory.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Overall the paper, while interesting is unacceptably messy.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
The paper is also littered with typos and vague statements (many enumerated below under *small issues*).' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
3. The paper is not nicely written or rather easy to follow.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
I recommend adjust the language to be more consistent throughout.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Especially given the very specific nature of the topic I miss a strong and clear path through the paper.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
In terms of writing, the paper is a bit confusing in terms of motivations and notations.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
I think the paper could benefit from having this in the earlier sections.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
First, this paper is not easy to follow.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Fourth, there are some grammar mistakes and typos.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Third, the writing in the paper has some significant lapses in clarity.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
I found the paper confusing at times.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
It is well-written from a syntactical and grammatical point of view, but some key concepts are stated without being explained, which gives the impression that the authors have a clear understanding of the material presented in the paper but communicate only part of the full picture to the reader.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Clarity: The clarity is below average.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
The clarity of this paper needs to be strengthened.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
2) The main contributions of this paper are not quite clear to me.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
In terms of presentation, the notation and statement of theorems are precise, however, the presentation is rather dry, and I think the paper can be significantly more accessible.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
- Some parts of the paper feel long-winded and aimless.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Honestly, this paper is very difficult to follow.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
2) The presentation is not professional, hard to follow and the submission overall looks very rushed:' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
- The writing looks very rushed, and should be improved.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
This paper looks very hastily put together, especially pages 7 and 8.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
There are many typos and unclear statements.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
To change to a firm accept, I think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area, which can easily be implemented in the camera ready stage of paper preparation.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Overall, the paper is a little confusing.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
- Grammatical errors and odd formulations all over the place' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Current representation is difficult to read / parse.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Second, the writing can be greatly improved.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Therefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
This poses a challenge in evaluating this paper.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
- some parts of the paper are quite unclear' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
The primary difficulty in reviewing this paper is the poor presentation of the paper.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
This issues makes reviewing this paper very difficult.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
There are many typos and grammar errors' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Weaknesses: Paper could have been written better. I had hard time understanding it.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
The notations are overall confusing and not explained well.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
I also had a hard time going through the paper - there aren't many details.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
The paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
However, there are some key issues with the paper that are not clear.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
- This paper is a slightly difficult read - not because of the' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
"- Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)' [SEP] 'rebuttal_answer"	Q2: The main contribution is listed as follows:
2) The writing is poor and hard to follow.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
"- the paper is poorly written and sentences are generally very hard to parse. For example, section 3.1 is opened by statements such as ""(we use) a multi-task evaluator which trains on the principal and auxiliary tasks, and evaluates the performance of the auxiliary tasks on a meta set""??' [SEP] 'rebuttal_answer"	Q2: The main contribution is listed as follows:
- Paper is often hard to follow, and contains a significant number of typos.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
The paper can benefit from a proofreading.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
There are a few typos throughout the paper such as:' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
* The text is quite hard to read.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
There are many typos (see below).' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
I think this is a very interesting direction, but the present paper is somewhat unclear.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Overall, I think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
The paper is relatively well-written, although the description of the neural models can be improved.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
The main criticism I have is that I found the paper harder to read.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
(4) The writing quality is not satisfactory.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Overall the paper, while interesting is unacceptably messy.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
The paper is also littered with typos and vague statements (many enumerated below under *small issues*).' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
3. The paper is not nicely written or rather easy to follow.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
I recommend adjust the language to be more consistent throughout.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Especially given the very specific nature of the topic I miss a strong and clear path through the paper.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
In terms of writing, the paper is a bit confusing in terms of motivations and notations.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
I think the paper could benefit from having this in the earlier sections.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
First, this paper is not easy to follow.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Fourth, there are some grammar mistakes and typos.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Third, the writing in the paper has some significant lapses in clarity.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
I found the paper confusing at times.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
It is well-written from a syntactical and grammatical point of view, but some key concepts are stated without being explained, which gives the impression that the authors have a clear understanding of the material presented in the paper but communicate only part of the full picture to the reader.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Clarity: The clarity is below average.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
The clarity of this paper needs to be strengthened.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
2) The main contributions of this paper are not quite clear to me.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
In terms of presentation, the notation and statement of theorems are precise, however, the presentation is rather dry, and I think the paper can be significantly more accessible.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
- Some parts of the paper feel long-winded and aimless.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Honestly, this paper is very difficult to follow.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
2) The presentation is not professional, hard to follow and the submission overall looks very rushed:' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
- The writing looks very rushed, and should be improved.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
This paper looks very hastily put together, especially pages 7 and 8.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
There are many typos and unclear statements.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
To change to a firm accept, I think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area, which can easily be implemented in the camera ready stage of paper preparation.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Overall, the paper is a little confusing.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
- Grammatical errors and odd formulations all over the place' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Current representation is difficult to read / parse.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Second, the writing can be greatly improved.' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
Therefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).' [SEP] 'rebuttal_answer	Q2: The main contribution is listed as follows:
However, it seems the only Figure showing D’s loss when unconstrained is Figure 26, in which it is hard to notice any significant jump in the loss.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
- In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says “losses”.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
a) The uncertainties produced by CDN in Figure 2 seems strange.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
"- p8par1: ""approximate embedding $\alpha(e(\gamma'(...)))$ - $e$ is undefined and should probably be $e'$ (this is also the case in the caption of Fig. 5), and $c'$ should probably be included as well.' [SEP] 'rebuttal_concede-criticism"	We apologize for unclear description of experimental settings.
The samples from MNIST in Figure 3 are indeed very blurry, supporting this.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
It did also not match any numbers in Tab. 4 of the appendix.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Table 8 rises some concerns.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Also, since the synthetic image pairs are not multimodal in nature, it is unclear to me for what the messages are conveyed in Figure 3 and 4.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
(b) a significant clarification of Figure 4.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
When I look at Figure 4abcd, it appears that the Convolution and Dilated Convolutions fit a clean signal faster (it is just not as clean.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Perhaps I am misreading this plot, but it is not obvious to me that this plot supports the claims the authors are making.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
"Also, Figure 6 is referenced in the text in the context of binary multiplication (""[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6""), but presents results for addition and factorization only.' [SEP] 'rebuttal_concede-criticism"	We apologize for unclear description of experimental settings.
- In Table 1, for ImageNet, Shadow Attach does not always generate adversarial examples that have on average larger certified radii than the natural parallel, at least for sigma=0.5 and 1.0. Could the authors explain the reason?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
- In Table 2, it is not clear to me what is the point for comparing errors of the natural images (which measures the misclassification rate of a natural image) and that of the adversarial images (which measures successful attacks rate), and why this comparison helps to support the claim that the attack results in a stronger certificates.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
- Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
The transfer metrics (MMD and kNN) are opaque to the reader: for instance, in table 1, is a knn score of 43173 qualitatively different than 43180?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
- Why are there missing BLEU scores and the number of parameters in Table 1?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Table 1 tries to provide some comparisons on Atari, however number of samples is different for different methods making the comparisons invalid.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
caption of Fig 1: extractS' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
typo in absolute in caption of Fig 4' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
"* The descriptor distributions in Figure 3 don't look like an ""almost exact match"" to me (as claimed in the text).' [SEP] 'rebuttal_concede-criticism"	We apologize for unclear description of experimental settings.
In Table 2 I saw some optimizers end up with much lower test accuracy.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
"-	In Figure 2.b I’m surprised by the difference obtained in the feature maps for images which seems very similar (only the lighting seems to be different). Is it three consecutive frames?' [SEP] 'rebuttal_concede-criticism"	We apologize for unclear description of experimental settings.
-  Is the romantic/ horror flips and their absence the only spurious thing in Figure 4 ?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
However, it seems the only Figure showing D’s loss when unconstrained is Figure 26, in which it is hard to notice any significant jump in the loss.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
- In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says “losses”.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
a) The uncertainties produced by CDN in Figure 2 seems strange.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
"- p8par1: ""approximate embedding $\alpha(e(\gamma'(...)))$ - $e$ is undefined and should probably be $e'$ (this is also the case in the caption of Fig. 5), and $c'$ should probably be included as well.' [SEP] 'rebuttal_answer"	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
The samples from MNIST in Figure 3 are indeed very blurry, supporting this.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
It did also not match any numbers in Tab. 4 of the appendix.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
Table 8 rises some concerns.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
Also, since the synthetic image pairs are not multimodal in nature, it is unclear to me for what the messages are conveyed in Figure 3 and 4.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
(b) a significant clarification of Figure 4.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
When I look at Figure 4abcd, it appears that the Convolution and Dilated Convolutions fit a clean signal faster (it is just not as clean.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
Perhaps I am misreading this plot, but it is not obvious to me that this plot supports the claims the authors are making.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
"Also, Figure 6 is referenced in the text in the context of binary multiplication (""[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6""), but presents results for addition and factorization only.' [SEP] 'rebuttal_answer"	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
- In Table 1, for ImageNet, Shadow Attach does not always generate adversarial examples that have on average larger certified radii than the natural parallel, at least for sigma=0.5 and 1.0. Could the authors explain the reason?' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
- In Table 2, it is not clear to me what is the point for comparing errors of the natural images (which measures the misclassification rate of a natural image) and that of the adversarial images (which measures successful attacks rate), and why this comparison helps to support the claim that the attack results in a stronger certificates.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
- Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
The transfer metrics (MMD and kNN) are opaque to the reader: for instance, in table 1, is a knn score of 43173 qualitatively different than 43180?' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
- Why are there missing BLEU scores and the number of parameters in Table 1?' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
Table 1 tries to provide some comparisons on Atari, however number of samples is different for different methods making the comparisons invalid.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
caption of Fig 1: extractS' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
typo in absolute in caption of Fig 4' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
"* The descriptor distributions in Figure 3 don't look like an ""almost exact match"" to me (as claimed in the text).' [SEP] 'rebuttal_answer"	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
In Table 2 I saw some optimizers end up with much lower test accuracy.' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
"-	In Figure 2.b I’m surprised by the difference obtained in the feature maps for images which seems very similar (only the lighting seems to be different). Is it three consecutive frames?' [SEP] 'rebuttal_answer"	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
-  Is the romantic/ horror flips and their absence the only spurious thing in Figure 4 ?' [SEP] 'rebuttal_answer	We discuss this point at the bottom of page 4 after equation 6 and will further clarify.
However, it seems the only Figure showing D’s loss when unconstrained is Figure 26, in which it is hard to notice any significant jump in the loss.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
- In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says “losses”.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
a) The uncertainties produced by CDN in Figure 2 seems strange.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
"- p8par1: ""approximate embedding $\alpha(e(\gamma'(...)))$ - $e$ is undefined and should probably be $e'$ (this is also the case in the caption of Fig. 5), and $c'$ should probably be included as well.' [SEP] 'rebuttal_done"	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
The samples from MNIST in Figure 3 are indeed very blurry, supporting this.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
It did also not match any numbers in Tab. 4 of the appendix.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
Table 8 rises some concerns.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
Also, since the synthetic image pairs are not multimodal in nature, it is unclear to me for what the messages are conveyed in Figure 3 and 4.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
(b) a significant clarification of Figure 4.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
When I look at Figure 4abcd, it appears that the Convolution and Dilated Convolutions fit a clean signal faster (it is just not as clean.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
Perhaps I am misreading this plot, but it is not obvious to me that this plot supports the claims the authors are making.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
"Also, Figure 6 is referenced in the text in the context of binary multiplication (""[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6""), but presents results for addition and factorization only.' [SEP] 'rebuttal_done"	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
- In Table 1, for ImageNet, Shadow Attach does not always generate adversarial examples that have on average larger certified radii than the natural parallel, at least for sigma=0.5 and 1.0. Could the authors explain the reason?' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
- In Table 2, it is not clear to me what is the point for comparing errors of the natural images (which measures the misclassification rate of a natural image) and that of the adversarial images (which measures successful attacks rate), and why this comparison helps to support the claim that the attack results in a stronger certificates.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
- Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
The transfer metrics (MMD and kNN) are opaque to the reader: for instance, in table 1, is a knn score of 43173 qualitatively different than 43180?' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
- Why are there missing BLEU scores and the number of parameters in Table 1?' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
Table 1 tries to provide some comparisons on Atari, however number of samples is different for different methods making the comparisons invalid.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
caption of Fig 1: extractS' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
typo in absolute in caption of Fig 4' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
"* The descriptor distributions in Figure 3 don't look like an ""almost exact match"" to me (as claimed in the text).' [SEP] 'rebuttal_done"	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
In Table 2 I saw some optimizers end up with much lower test accuracy.' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
"-	In Figure 2.b I’m surprised by the difference obtained in the feature maps for images which seems very similar (only the lighting seems to be different). Is it three consecutive frames?' [SEP] 'rebuttal_done"	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
-  Is the romantic/ horror flips and their absence the only spurious thing in Figure 4 ?' [SEP] 'rebuttal_done	-Thanks! This was indeed an error, which we’ve corrected in the updated draft.
However, it seems the only Figure showing D’s loss when unconstrained is Figure 26, in which it is hard to notice any significant jump in the loss.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
- In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says “losses”.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
a) The uncertainties produced by CDN in Figure 2 seems strange.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
"- p8par1: ""approximate embedding $\alpha(e(\gamma'(...)))$ - $e$ is undefined and should probably be $e'$ (this is also the case in the caption of Fig. 5), and $c'$ should probably be included as well.' [SEP] 'rebuttal_structuring"	Q4. The top row of Figure 2b is confusing:
The samples from MNIST in Figure 3 are indeed very blurry, supporting this.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
It did also not match any numbers in Tab. 4 of the appendix.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
Table 8 rises some concerns.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
Also, since the synthetic image pairs are not multimodal in nature, it is unclear to me for what the messages are conveyed in Figure 3 and 4.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
(b) a significant clarification of Figure 4.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
When I look at Figure 4abcd, it appears that the Convolution and Dilated Convolutions fit a clean signal faster (it is just not as clean.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
Perhaps I am misreading this plot, but it is not obvious to me that this plot supports the claims the authors are making.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
"Also, Figure 6 is referenced in the text in the context of binary multiplication (""[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6""), but presents results for addition and factorization only.' [SEP] 'rebuttal_structuring"	Q4. The top row of Figure 2b is confusing:
- In Table 1, for ImageNet, Shadow Attach does not always generate adversarial examples that have on average larger certified radii than the natural parallel, at least for sigma=0.5 and 1.0. Could the authors explain the reason?' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
- In Table 2, it is not clear to me what is the point for comparing errors of the natural images (which measures the misclassification rate of a natural image) and that of the adversarial images (which measures successful attacks rate), and why this comparison helps to support the claim that the attack results in a stronger certificates.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
- Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
The transfer metrics (MMD and kNN) are opaque to the reader: for instance, in table 1, is a knn score of 43173 qualitatively different than 43180?' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
- Why are there missing BLEU scores and the number of parameters in Table 1?' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
Table 1 tries to provide some comparisons on Atari, however number of samples is different for different methods making the comparisons invalid.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
caption of Fig 1: extractS' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
typo in absolute in caption of Fig 4' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
"* The descriptor distributions in Figure 3 don't look like an ""almost exact match"" to me (as claimed in the text).' [SEP] 'rebuttal_structuring"	Q4. The top row of Figure 2b is confusing:
In Table 2 I saw some optimizers end up with much lower test accuracy.' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
"-	In Figure 2.b I’m surprised by the difference obtained in the feature maps for images which seems very similar (only the lighting seems to be different). Is it three consecutive frames?' [SEP] 'rebuttal_structuring"	Q4. The top row of Figure 2b is confusing:
-  Is the romantic/ horror flips and their absence the only spurious thing in Figure 4 ?' [SEP] 'rebuttal_structuring	Q4. The top row of Figure 2b is confusing:
- There is no motivations for the use of $\lambda >1$ neither practical or theoretical since the results are only proven for $\lambda =1$ whereas the experiments are done with \lambda = 5,20 or 30.' [SEP] 'rebuttal_structuring	Comment: The paper does not provide any quantitatively convincing results
Furthermore, in experiments, the paper does not provide any quantitatively convincing results to suggest the generator in use is a good one.' [SEP] 'rebuttal_structuring	Comment: The paper does not provide any quantitatively convincing results
- I'm not sure we can conclude much from the results on fetchSlide (and it would make sense not to use the last set of parameters but the best one encountered during training)' [SEP] 'rebuttal_structuring	Comment: The paper does not provide any quantitatively convincing results
It is also not clear why Table. 3 does not report the Bayes baseline results.' [SEP] 'rebuttal_answer	"5. ""Additionally, in section 6.4, the results in Figure 2 also does not look very"
Additionally, in section 6.4, the results in Figure 2 also does not look very' [SEP] 'rebuttal_answer	"5. ""Additionally, in section 6.4, the results in Figure 2 also does not look very"
It would be useful to have a table, like the one on the last page, which clearly shows the baseline vs. the random-projection model, with some description of the results in the main body of the text.' [SEP] 'rebuttal_answer	"5. ""Additionally, in section 6.4, the results in Figure 2 also does not look very"
While this is a reasonable assumption, it does not necessarily seem to be the case that a larger, more disconnected saliency map indicates worse classification performance.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
Only some heuristic results are obtained for them without rigorous theory.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
This claim is simply not true, and more care is needed in reporting the results here in the wider context of the literature.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
However, I have a few concerns about the results.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
"The column ""FLOPS"" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases.' [SEP] 'rebuttal_answer"	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
Indeed, the manuscript introduces sample complexity results to justify the benefits of the out-of-sample procedure (th 1), but it seems to me that these give an incomplete picture.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
The proposed sampling distributions assumes independence between the random variables over which the authors optimize — I find it surprising that this leads to good empirical results' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
It is unclear how important this particular objective is to the results.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
The authors claim that some amount of noise can be tolerated, but do not quantify how much.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
"The paper also lacks experimental results, and the main conclusion from these results seems to be ""MNIST is not suitable for benchmarking of adversarial attacks"".' [SEP] 'rebuttal_answer"	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
"""Table 4 shows that our first results are promising, even though they are not as good as the state of the art."" The state of the art on LibriSpeech is not Mohamed at al. 2019. See e.g. Irie et al. Interspeech 2019 for better result' [SEP] 'rebuttal_answer"	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
Overall, I feel that the result is interesting but it depends on a strong assumption and doesn't capture all interesting cases.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
It is also not clear how this theoretical result can shed insight on the empirical study of neural networks.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
have no idea how confident the sampling based result is.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
The ResNet on Cifar-10 results are not convincing.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
Perhaps the authors had no salient observations for loss, but explicitly stating such would be useful to the reader.' [SEP] 'rebuttal_answer	Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.
While this is a reasonable assumption, it does not necessarily seem to be the case that a larger, more disconnected saliency map indicates worse classification performance.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Only some heuristic results are obtained for them without rigorous theory.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
This claim is simply not true, and more care is needed in reporting the results here in the wider context of the literature.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
However, I have a few concerns about the results.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"The column ""FLOPS"" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases.' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Indeed, the manuscript introduces sample complexity results to justify the benefits of the out-of-sample procedure (th 1), but it seems to me that these give an incomplete picture.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The proposed sampling distributions assumes independence between the random variables over which the authors optimize — I find it surprising that this leads to good empirical results' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is unclear how important this particular objective is to the results.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The authors claim that some amount of noise can be tolerated, but do not quantify how much.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"The paper also lacks experimental results, and the main conclusion from these results seems to be ""MNIST is not suitable for benchmarking of adversarial attacks"".' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
"""Table 4 shows that our first results are promising, even though they are not as good as the state of the art."" The state of the art on LibriSpeech is not Mohamed at al. 2019. See e.g. Irie et al. Interspeech 2019 for better result' [SEP] 'rebuttal_reject-criticism"	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Overall, I feel that the result is interesting but it depends on a strong assumption and doesn't capture all interesting cases.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
It is also not clear how this theoretical result can shed insight on the empirical study of neural networks.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
have no idea how confident the sampling based result is.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
The ResNet on Cifar-10 results are not convincing.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
Perhaps the authors had no salient observations for loss, but explicitly stating such would be useful to the reader.' [SEP] 'rebuttal_reject-criticism	The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.
While this is a reasonable assumption, it does not necessarily seem to be the case that a larger, more disconnected saliency map indicates worse classification performance.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
Only some heuristic results are obtained for them without rigorous theory.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
This claim is simply not true, and more care is needed in reporting the results here in the wider context of the literature.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
However, I have a few concerns about the results.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
"The column ""FLOPS"" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases.' [SEP] 'rebuttal_future"	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
Indeed, the manuscript introduces sample complexity results to justify the benefits of the out-of-sample procedure (th 1), but it seems to me that these give an incomplete picture.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
The proposed sampling distributions assumes independence between the random variables over which the authors optimize — I find it surprising that this leads to good empirical results' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
It is unclear how important this particular objective is to the results.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
The authors claim that some amount of noise can be tolerated, but do not quantify how much.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
"The paper also lacks experimental results, and the main conclusion from these results seems to be ""MNIST is not suitable for benchmarking of adversarial attacks"".' [SEP] 'rebuttal_future"	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
"""Table 4 shows that our first results are promising, even though they are not as good as the state of the art."" The state of the art on LibriSpeech is not Mohamed at al. 2019. See e.g. Irie et al. Interspeech 2019 for better result' [SEP] 'rebuttal_future"	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
Overall, I feel that the result is interesting but it depends on a strong assumption and doesn't capture all interesting cases.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
It is also not clear how this theoretical result can shed insight on the empirical study of neural networks.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
have no idea how confident the sampling based result is.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
The ResNet on Cifar-10 results are not convincing.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
Perhaps the authors had no salient observations for loss, but explicitly stating such would be useful to the reader.' [SEP] 'rebuttal_future	For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.
While this is a reasonable assumption, it does not necessarily seem to be the case that a larger, more disconnected saliency map indicates worse classification performance.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
Only some heuristic results are obtained for them without rigorous theory.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
This claim is simply not true, and more care is needed in reporting the results here in the wider context of the literature.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
However, I have a few concerns about the results.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
"The column ""FLOPS"" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases.' [SEP] 'rebuttal_done"	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
Indeed, the manuscript introduces sample complexity results to justify the benefits of the out-of-sample procedure (th 1), but it seems to me that these give an incomplete picture.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
The proposed sampling distributions assumes independence between the random variables over which the authors optimize — I find it surprising that this leads to good empirical results' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
It is unclear how important this particular objective is to the results.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
The authors claim that some amount of noise can be tolerated, but do not quantify how much.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
"The paper also lacks experimental results, and the main conclusion from these results seems to be ""MNIST is not suitable for benchmarking of adversarial attacks"".' [SEP] 'rebuttal_done"	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
"""Table 4 shows that our first results are promising, even though they are not as good as the state of the art."" The state of the art on LibriSpeech is not Mohamed at al. 2019. See e.g. Irie et al. Interspeech 2019 for better result' [SEP] 'rebuttal_done"	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
Overall, I feel that the result is interesting but it depends on a strong assumption and doesn't capture all interesting cases.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
It is also not clear how this theoretical result can shed insight on the empirical study of neural networks.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
have no idea how confident the sampling based result is.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
The ResNet on Cifar-10 results are not convincing.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
Perhaps the authors had no salient observations for loss, but explicitly stating such would be useful to the reader.' [SEP] 'rebuttal_done	We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).
While this is a reasonable assumption, it does not necessarily seem to be the case that a larger, more disconnected saliency map indicates worse classification performance.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
Only some heuristic results are obtained for them without rigorous theory.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
This claim is simply not true, and more care is needed in reporting the results here in the wider context of the literature.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
However, I have a few concerns about the results.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
"The column ""FLOPS"" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases.' [SEP] 'rebuttal_by-cr"	- Table 3 is indeed confusing, this is a good point. We will correct it.
Indeed, the manuscript introduces sample complexity results to justify the benefits of the out-of-sample procedure (th 1), but it seems to me that these give an incomplete picture.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
The proposed sampling distributions assumes independence between the random variables over which the authors optimize — I find it surprising that this leads to good empirical results' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
It is unclear how important this particular objective is to the results.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
The authors claim that some amount of noise can be tolerated, but do not quantify how much.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
"The paper also lacks experimental results, and the main conclusion from these results seems to be ""MNIST is not suitable for benchmarking of adversarial attacks"".' [SEP] 'rebuttal_by-cr"	- Table 3 is indeed confusing, this is a good point. We will correct it.
"""Table 4 shows that our first results are promising, even though they are not as good as the state of the art."" The state of the art on LibriSpeech is not Mohamed at al. 2019. See e.g. Irie et al. Interspeech 2019 for better result' [SEP] 'rebuttal_by-cr"	- Table 3 is indeed confusing, this is a good point. We will correct it.
Overall, I feel that the result is interesting but it depends on a strong assumption and doesn't capture all interesting cases.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
It is also not clear how this theoretical result can shed insight on the empirical study of neural networks.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
have no idea how confident the sampling based result is.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
The ResNet on Cifar-10 results are not convincing.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
Perhaps the authors had no salient observations for loss, but explicitly stating such would be useful to the reader.' [SEP] 'rebuttal_by-cr	- Table 3 is indeed confusing, this is a good point. We will correct it.
While this is a reasonable assumption, it does not necessarily seem to be the case that a larger, more disconnected saliency map indicates worse classification performance.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
Only some heuristic results are obtained for them without rigorous theory.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
This claim is simply not true, and more care is needed in reporting the results here in the wider context of the literature.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
However, I have a few concerns about the results.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
"The column ""FLOPS"" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases.' [SEP] 'rebuttal_social"	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
Indeed, the manuscript introduces sample complexity results to justify the benefits of the out-of-sample procedure (th 1), but it seems to me that these give an incomplete picture.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
The proposed sampling distributions assumes independence between the random variables over which the authors optimize — I find it surprising that this leads to good empirical results' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
It is unclear how important this particular objective is to the results.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
The authors claim that some amount of noise can be tolerated, but do not quantify how much.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
"The paper also lacks experimental results, and the main conclusion from these results seems to be ""MNIST is not suitable for benchmarking of adversarial attacks"".' [SEP] 'rebuttal_social"	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
"""Table 4 shows that our first results are promising, even though they are not as good as the state of the art."" The state of the art on LibriSpeech is not Mohamed at al. 2019. See e.g. Irie et al. Interspeech 2019 for better result' [SEP] 'rebuttal_social"	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
Overall, I feel that the result is interesting but it depends on a strong assumption and doesn't capture all interesting cases.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
It is also not clear how this theoretical result can shed insight on the empirical study of neural networks.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
have no idea how confident the sampling based result is.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
The ResNet on Cifar-10 results are not convincing.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
Perhaps the authors had no salient observations for loss, but explicitly stating such would be useful to the reader.' [SEP] 'rebuttal_social	Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).
While this is a reasonable assumption, it does not necessarily seem to be the case that a larger, more disconnected saliency map indicates worse classification performance.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
Only some heuristic results are obtained for them without rigorous theory.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
This claim is simply not true, and more care is needed in reporting the results here in the wider context of the literature.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
However, I have a few concerns about the results.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
"The column ""FLOPS"" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases.' [SEP] 'rebuttal_structuring"	-Q: Actionable consequences from paper:
Indeed, the manuscript introduces sample complexity results to justify the benefits of the out-of-sample procedure (th 1), but it seems to me that these give an incomplete picture.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
The proposed sampling distributions assumes independence between the random variables over which the authors optimize — I find it surprising that this leads to good empirical results' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
It is unclear how important this particular objective is to the results.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
The authors claim that some amount of noise can be tolerated, but do not quantify how much.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
"The paper also lacks experimental results, and the main conclusion from these results seems to be ""MNIST is not suitable for benchmarking of adversarial attacks"".' [SEP] 'rebuttal_structuring"	-Q: Actionable consequences from paper:
"""Table 4 shows that our first results are promising, even though they are not as good as the state of the art."" The state of the art on LibriSpeech is not Mohamed at al. 2019. See e.g. Irie et al. Interspeech 2019 for better result' [SEP] 'rebuttal_structuring"	-Q: Actionable consequences from paper:
Overall, I feel that the result is interesting but it depends on a strong assumption and doesn't capture all interesting cases.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
It is also not clear how this theoretical result can shed insight on the empirical study of neural networks.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
have no idea how confident the sampling based result is.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
The ResNet on Cifar-10 results are not convincing.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
Perhaps the authors had no salient observations for loss, but explicitly stating such would be useful to the reader.' [SEP] 'rebuttal_structuring	-Q: Actionable consequences from paper:
How do we take a limit of M -> ∞ ? Does k also go ∞?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
How does the proposed method perform in more complicated tasks such as' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Dual-1 and Dual-5 are introduced without explanation.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
It is unclear whether the data augmentation techniques is applied only at training time or also at test time.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- it wasn't clear how the sparsity percentage on page 3 was defined?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- Architecture choice unclear: Why are $\sigma$ and $\omega$ separate networks.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
The paper used very restricted Gaussian distributions for the formulation.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
First of all, the setup for the AE and VAE is not specified.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
3. Scenario discussed in Sec. 4 seems somewhat impractical.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
In addition, the paper would also need to show that such a model does not generalize to a validation set of images.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
"- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001' [SEP] 'rebuttal_concede-criticism"	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
The proofs are quite dense and I was unable to verify them carefully.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
"- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"".' [SEP] 'rebuttal_concede-criticism"	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Specifically, how can mutual information in this context be formally linked to generalization/overfitting?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
I also find weird the way that the authors arrive to their final objective in Equation (5).' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi).' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Is there a reason why the authors do not introduce their objective by following the variational framework?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
What is the purpose then for introducing the matrix variate Gaussian?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
It is also not clear to me how domain translation is relevant to continual learning.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
I do not understand how the model is trained to solve multiple tasks.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
6. The rationale of the two tower design (why not combine two) is not clearly explained.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Doesn't the classification loss have a dependency on the input condition?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
"--What does a ""heavy classifier"" imply concretely?' [SEP] 'rebuttal_concede-criticism"	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
In contrast, the studied learning rates are asymptotic and there is a big discrepancy.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
However, it is not obvious that how to move from line 3 to line 4 at Eq 15.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
(5) Due to the mean policy approximation, does the mean policy depend on \phi?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
The authors should clearly explain how to update \phi when optimizing Eq 12.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
However, the derivations about \phi are missing.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
"As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of ""edge-to-edge"" convolutions and generally the architectural choice related to the conditional GAN discriminator.' [SEP] 'rebuttal_concede-criticism"	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
There is not sufficient detail to reproduce the models based on the paper alone.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
For example, one question is how often a single partial tree has multiple possible completions in the data.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Do they here refer to the gradients with respect to the weights ONLY?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.' [SEP] 'rebuttal_concede-criticism"	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
"3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?' [SEP] 'rebuttal_concede-criticism"	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- It would greatly benefit the reader if eq. 5 were expanded.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- the method is not applicable to episodes of different length' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Is Harmonic Convolution applicable to complex STFT coefficients as well?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
If so it would be better to define the operator in a more general notation.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Towards this, how does the computational complexity scale wrt to the connectedness?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
What is the L1 norm applied on?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- Trick is specific to LM.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
It seems heavily dependent on GBDT.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Was crossvalidation used to select the topology?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
If so, what was the methodology.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
1 The implementation steps of the proposed method (MoVE) are not clear.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
To be honest, the theoretical contribution of the paper is limited.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
More discussions on these questions can be very helpful to further understand the proposed method.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
"It should be better motivated why one should use the duality gap as an upper bound for the ""F-distance"".' [SEP] 'rebuttal_concede-criticism"	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Minimizing the F-distance as is usually done seems like the more direct and simple approach.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
How do “we choose a specific number of assignments based on prediction probabilities”?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
"- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.' [SEP] 'rebuttal_concede-criticism"	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Negative points: (1) The authors should provide more justification on equation-3.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Why do the authors directly average different loss for the discriminator and the classifer?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
(2) The function of the discriminator is not very clear, especially for the classification error test.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
2. The learning procedure is confusing.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
It is highly recommended to provide the pseudocode of the proposed method.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
The reasons for the use of the energy-based formulation are not clear to me.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- It is not clear how the initialisation (10) is implemented.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
"Particularly the ""fusion"" module remains extremely unclear.' [SEP] 'rebuttal_concede-criticism"	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
It is not clear to me that the classifier difference metric is well-defined.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
"-	How the first camera pose is initialized?' [SEP] 'rebuttal_concede-criticism"	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.' [SEP] 'rebuttal_concede-criticism	Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.
How do we take a limit of M -> ∞ ? Does k also go ∞?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
How does the proposed method perform in more complicated tasks such as' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Dual-1 and Dual-5 are introduced without explanation.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
It is unclear whether the data augmentation techniques is applied only at training time or also at test time.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
- it wasn't clear how the sparsity percentage on page 3 was defined?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
- Architecture choice unclear: Why are $\sigma$ and $\omega$ separate networks.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
The paper used very restricted Gaussian distributions for the formulation.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
First of all, the setup for the AE and VAE is not specified.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
3. Scenario discussed in Sec. 4 seems somewhat impractical.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
In addition, the paper would also need to show that such a model does not generalize to a validation set of images.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
"- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001' [SEP] 'rebuttal_summary"	Thus, we suggested applying the domain translation to address this issue.
In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
The proofs are quite dense and I was unable to verify them carefully.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
"- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"".' [SEP] 'rebuttal_summary"	Thus, we suggested applying the domain translation to address this issue.
In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Specifically, how can mutual information in this context be formally linked to generalization/overfitting?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
I also find weird the way that the authors arrive to their final objective in Equation (5).' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi).' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Is there a reason why the authors do not introduce their objective by following the variational framework?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
What is the purpose then for introducing the matrix variate Gaussian?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
It is also not clear to me how domain translation is relevant to continual learning.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
I do not understand how the model is trained to solve multiple tasks.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
6. The rationale of the two tower design (why not combine two) is not clearly explained.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Doesn't the classification loss have a dependency on the input condition?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
"--What does a ""heavy classifier"" imply concretely?' [SEP] 'rebuttal_summary"	Thus, we suggested applying the domain translation to address this issue.
In contrast, the studied learning rates are asymptotic and there is a big discrepancy.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
However, it is not obvious that how to move from line 3 to line 4 at Eq 15.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
(5) Due to the mean policy approximation, does the mean policy depend on \phi?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
The authors should clearly explain how to update \phi when optimizing Eq 12.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
However, the derivations about \phi are missing.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
"As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of ""edge-to-edge"" convolutions and generally the architectural choice related to the conditional GAN discriminator.' [SEP] 'rebuttal_summary"	Thus, we suggested applying the domain translation to address this issue.
- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
There is not sufficient detail to reproduce the models based on the paper alone.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
For example, one question is how often a single partial tree has multiple possible completions in the data.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Do they here refer to the gradients with respect to the weights ONLY?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.' [SEP] 'rebuttal_summary"	Thus, we suggested applying the domain translation to address this issue.
"3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?' [SEP] 'rebuttal_summary"	Thus, we suggested applying the domain translation to address this issue.
- It would greatly benefit the reader if eq. 5 were expanded.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
- the method is not applicable to episodes of different length' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Is Harmonic Convolution applicable to complex STFT coefficients as well?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
If so it would be better to define the operator in a more general notation.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Towards this, how does the computational complexity scale wrt to the connectedness?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
What is the L1 norm applied on?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
- Trick is specific to LM.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
It seems heavily dependent on GBDT.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Was crossvalidation used to select the topology?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
If so, what was the methodology.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
1 The implementation steps of the proposed method (MoVE) are not clear.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
To be honest, the theoretical contribution of the paper is limited.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
More discussions on these questions can be very helpful to further understand the proposed method.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
"It should be better motivated why one should use the duality gap as an upper bound for the ""F-distance"".' [SEP] 'rebuttal_summary"	Thus, we suggested applying the domain translation to address this issue.
Minimizing the F-distance as is usually done seems like the more direct and simple approach.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
How do “we choose a specific number of assignments based on prediction probabilities”?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
"- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.' [SEP] 'rebuttal_summary"	Thus, we suggested applying the domain translation to address this issue.
13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Negative points: (1) The authors should provide more justification on equation-3.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Why do the authors directly average different loss for the discriminator and the classifer?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
(2) The function of the discriminator is not very clear, especially for the classification error test.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
2. The learning procedure is confusing.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
It is highly recommended to provide the pseudocode of the proposed method.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
The reasons for the use of the energy-based formulation are not clear to me.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
- It is not clear how the initialisation (10) is implemented.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
"Particularly the ""fusion"" module remains extremely unclear.' [SEP] 'rebuttal_summary"	Thus, we suggested applying the domain translation to address this issue.
It is not clear to me that the classifier difference metric is well-defined.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
"-	How the first camera pose is initialized?' [SEP] 'rebuttal_summary"	Thus, we suggested applying the domain translation to address this issue.
This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.' [SEP] 'rebuttal_summary	Thus, we suggested applying the domain translation to address this issue.
How do we take a limit of M -> ∞ ? Does k also go ∞?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
How does the proposed method perform in more complicated tasks such as' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Dual-1 and Dual-5 are introduced without explanation.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is unclear whether the data augmentation techniques is applied only at training time or also at test time.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- it wasn't clear how the sparsity percentage on page 3 was defined?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- Architecture choice unclear: Why are $\sigma$ and $\omega$ separate networks.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The paper used very restricted Gaussian distributions for the formulation.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
First of all, the setup for the AE and VAE is not specified.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
3. Scenario discussed in Sec. 4 seems somewhat impractical.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
In addition, the paper would also need to show that such a model does not generalize to a validation set of images.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The proofs are quite dense and I was unable to verify them carefully.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"".' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Specifically, how can mutual information in this context be formally linked to generalization/overfitting?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I also find weird the way that the authors arrive to their final objective in Equation (5).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Is there a reason why the authors do not introduce their objective by following the variational framework?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
What is the purpose then for introducing the matrix variate Gaussian?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is also not clear to me how domain translation is relevant to continual learning.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I do not understand how the model is trained to solve multiple tasks.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
6. The rationale of the two tower design (why not combine two) is not clearly explained.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Doesn't the classification loss have a dependency on the input condition?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"--What does a ""heavy classifier"" imply concretely?' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
In contrast, the studied learning rates are asymptotic and there is a big discrepancy.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, it is not obvious that how to move from line 3 to line 4 at Eq 15.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
(5) Due to the mean policy approximation, does the mean policy depend on \phi?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The authors should clearly explain how to update \phi when optimizing Eq 12.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, the derivations about \phi are missing.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of ""edge-to-edge"" convolutions and generally the architectural choice related to the conditional GAN discriminator.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
There is not sufficient detail to reproduce the models based on the paper alone.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
For example, one question is how often a single partial tree has multiple possible completions in the data.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Do they here refer to the gradients with respect to the weights ONLY?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- It would greatly benefit the reader if eq. 5 were expanded.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- the method is not applicable to episodes of different length' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Is Harmonic Convolution applicable to complex STFT coefficients as well?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
If so it would be better to define the operator in a more general notation.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Towards this, how does the computational complexity scale wrt to the connectedness?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
What is the L1 norm applied on?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- Trick is specific to LM.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It seems heavily dependent on GBDT.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Was crossvalidation used to select the topology?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
If so, what was the methodology.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1 The implementation steps of the proposed method (MoVE) are not clear.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
To be honest, the theoretical contribution of the paper is limited.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
More discussions on these questions can be very helpful to further understand the proposed method.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"It should be better motivated why one should use the duality gap as an upper bound for the ""F-distance"".' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Minimizing the F-distance as is usually done seems like the more direct and simple approach.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
How do “we choose a specific number of assignments based on prediction probabilities”?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Negative points: (1) The authors should provide more justification on equation-3.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Why do the authors directly average different loss for the discriminator and the classifer?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
(2) The function of the discriminator is not very clear, especially for the classification error test.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
2. The learning procedure is confusing.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is highly recommended to provide the pseudocode of the proposed method.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The reasons for the use of the energy-based formulation are not clear to me.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- It is not clear how the initialisation (10) is implemented.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"Particularly the ""fusion"" module remains extremely unclear.' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It is not clear to me that the classifier difference metric is well-defined.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
"-	How the first camera pose is initialized?' [SEP] 'rebuttal_answer"	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.' [SEP] 'rebuttal_answer	All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.
How do we take a limit of M -> ∞ ? Does k also go ∞?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
How does the proposed method perform in more complicated tasks such as' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Dual-1 and Dual-5 are introduced without explanation.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It is unclear whether the data augmentation techniques is applied only at training time or also at test time.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- it wasn't clear how the sparsity percentage on page 3 was defined?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- Architecture choice unclear: Why are $\sigma$ and $\omega$ separate networks.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The paper used very restricted Gaussian distributions for the formulation.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
First of all, the setup for the AE and VAE is not specified.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
3. Scenario discussed in Sec. 4 seems somewhat impractical.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
In addition, the paper would also need to show that such a model does not generalize to a validation set of images.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The proofs are quite dense and I was unable to verify them carefully.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"".' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Specifically, how can mutual information in this context be formally linked to generalization/overfitting?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
I also find weird the way that the authors arrive to their final objective in Equation (5).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Is there a reason why the authors do not introduce their objective by following the variational framework?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
What is the purpose then for introducing the matrix variate Gaussian?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It is also not clear to me how domain translation is relevant to continual learning.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
I do not understand how the model is trained to solve multiple tasks.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
6. The rationale of the two tower design (why not combine two) is not clearly explained.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Doesn't the classification loss have a dependency on the input condition?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"--What does a ""heavy classifier"" imply concretely?' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
In contrast, the studied learning rates are asymptotic and there is a big discrepancy.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
However, it is not obvious that how to move from line 3 to line 4 at Eq 15.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
(5) Due to the mean policy approximation, does the mean policy depend on \phi?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The authors should clearly explain how to update \phi when optimizing Eq 12.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
However, the derivations about \phi are missing.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of ""edge-to-edge"" convolutions and generally the architectural choice related to the conditional GAN discriminator.' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
There is not sufficient detail to reproduce the models based on the paper alone.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
For example, one question is how often a single partial tree has multiple possible completions in the data.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Do they here refer to the gradients with respect to the weights ONLY?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- It would greatly benefit the reader if eq. 5 were expanded.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- the method is not applicable to episodes of different length' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Is Harmonic Convolution applicable to complex STFT coefficients as well?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
If so it would be better to define the operator in a more general notation.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Towards this, how does the computational complexity scale wrt to the connectedness?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
What is the L1 norm applied on?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- Trick is specific to LM.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It seems heavily dependent on GBDT.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Was crossvalidation used to select the topology?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
If so, what was the methodology.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
1 The implementation steps of the proposed method (MoVE) are not clear.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
To be honest, the theoretical contribution of the paper is limited.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
More discussions on these questions can be very helpful to further understand the proposed method.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"It should be better motivated why one should use the duality gap as an upper bound for the ""F-distance"".' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Minimizing the F-distance as is usually done seems like the more direct and simple approach.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
How do “we choose a specific number of assignments based on prediction probabilities”?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Negative points: (1) The authors should provide more justification on equation-3.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Why do the authors directly average different loss for the discriminator and the classifer?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
(2) The function of the discriminator is not very clear, especially for the classification error test.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
2. The learning procedure is confusing.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It is highly recommended to provide the pseudocode of the proposed method.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The reasons for the use of the energy-based formulation are not clear to me.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- It is not clear how the initialisation (10) is implemented.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"Particularly the ""fusion"" module remains extremely unclear.' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It is not clear to me that the classifier difference metric is well-defined.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"-	How the first camera pose is initialized?' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
How do we take a limit of M -> ∞ ? Does k also go ∞?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
How does the proposed method perform in more complicated tasks such as' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Dual-1 and Dual-5 are introduced without explanation.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
It is unclear whether the data augmentation techniques is applied only at training time or also at test time.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- it wasn't clear how the sparsity percentage on page 3 was defined?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- Architecture choice unclear: Why are $\sigma$ and $\omega$ separate networks.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
The paper used very restricted Gaussian distributions for the formulation.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
First of all, the setup for the AE and VAE is not specified.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
3. Scenario discussed in Sec. 4 seems somewhat impractical.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
In addition, the paper would also need to show that such a model does not generalize to a validation set of images.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
"- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001' [SEP] 'rebuttal_structuring"	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
The proofs are quite dense and I was unable to verify them carefully.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
"- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"".' [SEP] 'rebuttal_structuring"	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Specifically, how can mutual information in this context be formally linked to generalization/overfitting?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
I also find weird the way that the authors arrive to their final objective in Equation (5).' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi).' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Is there a reason why the authors do not introduce their objective by following the variational framework?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
What is the purpose then for introducing the matrix variate Gaussian?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
It is also not clear to me how domain translation is relevant to continual learning.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
I do not understand how the model is trained to solve multiple tasks.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
6. The rationale of the two tower design (why not combine two) is not clearly explained.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Doesn't the classification loss have a dependency on the input condition?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
"--What does a ""heavy classifier"" imply concretely?' [SEP] 'rebuttal_structuring"	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
In contrast, the studied learning rates are asymptotic and there is a big discrepancy.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
However, it is not obvious that how to move from line 3 to line 4 at Eq 15.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
(5) Due to the mean policy approximation, does the mean policy depend on \phi?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
The authors should clearly explain how to update \phi when optimizing Eq 12.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
However, the derivations about \phi are missing.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
"As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of ""edge-to-edge"" convolutions and generally the architectural choice related to the conditional GAN discriminator.' [SEP] 'rebuttal_structuring"	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
There is not sufficient detail to reproduce the models based on the paper alone.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
For example, one question is how often a single partial tree has multiple possible completions in the data.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Do they here refer to the gradients with respect to the weights ONLY?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.' [SEP] 'rebuttal_structuring"	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
"3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?' [SEP] 'rebuttal_structuring"	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- It would greatly benefit the reader if eq. 5 were expanded.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- the method is not applicable to episodes of different length' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Is Harmonic Convolution applicable to complex STFT coefficients as well?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
If so it would be better to define the operator in a more general notation.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Towards this, how does the computational complexity scale wrt to the connectedness?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
What is the L1 norm applied on?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- Trick is specific to LM.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
It seems heavily dependent on GBDT.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Was crossvalidation used to select the topology?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
If so, what was the methodology.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
1 The implementation steps of the proposed method (MoVE) are not clear.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
To be honest, the theoretical contribution of the paper is limited.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
More discussions on these questions can be very helpful to further understand the proposed method.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
"It should be better motivated why one should use the duality gap as an upper bound for the ""F-distance"".' [SEP] 'rebuttal_structuring"	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Minimizing the F-distance as is usually done seems like the more direct and simple approach.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
How do “we choose a specific number of assignments based on prediction probabilities”?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
"- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.' [SEP] 'rebuttal_structuring"	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Negative points: (1) The authors should provide more justification on equation-3.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Why do the authors directly average different loss for the discriminator and the classifer?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
(2) The function of the discriminator is not very clear, especially for the classification error test.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
2. The learning procedure is confusing.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
It is highly recommended to provide the pseudocode of the proposed method.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
The reasons for the use of the energy-based formulation are not clear to me.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- It is not clear how the initialisation (10) is implemented.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
"Particularly the ""fusion"" module remains extremely unclear.' [SEP] 'rebuttal_structuring"	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
It is not clear to me that the classifier difference metric is well-defined.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
"-	How the first camera pose is initialized?' [SEP] 'rebuttal_structuring"	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.' [SEP] 'rebuttal_structuring	Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
How do we take a limit of M -> ∞ ? Does k also go ∞?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
How does the proposed method perform in more complicated tasks such as' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Dual-1 and Dual-5 are introduced without explanation.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
It is unclear whether the data augmentation techniques is applied only at training time or also at test time.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- it wasn't clear how the sparsity percentage on page 3 was defined?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- Architecture choice unclear: Why are $\sigma$ and $\omega$ separate networks.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
The paper used very restricted Gaussian distributions for the formulation.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
First of all, the setup for the AE and VAE is not specified.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
3. Scenario discussed in Sec. 4 seems somewhat impractical.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
In addition, the paper would also need to show that such a model does not generalize to a validation set of images.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
"- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001' [SEP] 'rebuttal_by-cr"	We will make all code and models trained in this paper available for reproducibility.
In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
The proofs are quite dense and I was unable to verify them carefully.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
"- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"".' [SEP] 'rebuttal_by-cr"	We will make all code and models trained in this paper available for reproducibility.
In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Specifically, how can mutual information in this context be formally linked to generalization/overfitting?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
I also find weird the way that the authors arrive to their final objective in Equation (5).' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi).' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Is there a reason why the authors do not introduce their objective by following the variational framework?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
What is the purpose then for introducing the matrix variate Gaussian?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
It is also not clear to me how domain translation is relevant to continual learning.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
I do not understand how the model is trained to solve multiple tasks.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
6. The rationale of the two tower design (why not combine two) is not clearly explained.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Doesn't the classification loss have a dependency on the input condition?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
"--What does a ""heavy classifier"" imply concretely?' [SEP] 'rebuttal_by-cr"	We will make all code and models trained in this paper available for reproducibility.
In contrast, the studied learning rates are asymptotic and there is a big discrepancy.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
However, it is not obvious that how to move from line 3 to line 4 at Eq 15.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
(5) Due to the mean policy approximation, does the mean policy depend on \phi?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
The authors should clearly explain how to update \phi when optimizing Eq 12.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
However, the derivations about \phi are missing.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
"As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of ""edge-to-edge"" convolutions and generally the architectural choice related to the conditional GAN discriminator.' [SEP] 'rebuttal_by-cr"	We will make all code and models trained in this paper available for reproducibility.
- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
There is not sufficient detail to reproduce the models based on the paper alone.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
For example, one question is how often a single partial tree has multiple possible completions in the data.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Do they here refer to the gradients with respect to the weights ONLY?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.' [SEP] 'rebuttal_by-cr"	We will make all code and models trained in this paper available for reproducibility.
"3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?' [SEP] 'rebuttal_by-cr"	We will make all code and models trained in this paper available for reproducibility.
- It would greatly benefit the reader if eq. 5 were expanded.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- the method is not applicable to episodes of different length' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Is Harmonic Convolution applicable to complex STFT coefficients as well?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
If so it would be better to define the operator in a more general notation.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Towards this, how does the computational complexity scale wrt to the connectedness?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
What is the L1 norm applied on?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- Trick is specific to LM.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
It seems heavily dependent on GBDT.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Was crossvalidation used to select the topology?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
If so, what was the methodology.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
1 The implementation steps of the proposed method (MoVE) are not clear.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
To be honest, the theoretical contribution of the paper is limited.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
More discussions on these questions can be very helpful to further understand the proposed method.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
"It should be better motivated why one should use the duality gap as an upper bound for the ""F-distance"".' [SEP] 'rebuttal_by-cr"	We will make all code and models trained in this paper available for reproducibility.
Minimizing the F-distance as is usually done seems like the more direct and simple approach.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
How do “we choose a specific number of assignments based on prediction probabilities”?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
"- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.' [SEP] 'rebuttal_by-cr"	We will make all code and models trained in this paper available for reproducibility.
13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Negative points: (1) The authors should provide more justification on equation-3.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Why do the authors directly average different loss for the discriminator and the classifer?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
(2) The function of the discriminator is not very clear, especially for the classification error test.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
2. The learning procedure is confusing.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
It is highly recommended to provide the pseudocode of the proposed method.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
The reasons for the use of the energy-based formulation are not clear to me.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- It is not clear how the initialisation (10) is implemented.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
"Particularly the ""fusion"" module remains extremely unclear.' [SEP] 'rebuttal_by-cr"	We will make all code and models trained in this paper available for reproducibility.
It is not clear to me that the classifier difference metric is well-defined.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
"-	How the first camera pose is initialized?' [SEP] 'rebuttal_by-cr"	We will make all code and models trained in this paper available for reproducibility.
This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.' [SEP] 'rebuttal_by-cr	We will make all code and models trained in this paper available for reproducibility.
How do we take a limit of M -> ∞ ? Does k also go ∞?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
How does the proposed method perform in more complicated tasks such as' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Dual-1 and Dual-5 are introduced without explanation.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
It is unclear whether the data augmentation techniques is applied only at training time or also at test time.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
- it wasn't clear how the sparsity percentage on page 3 was defined?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
- Architecture choice unclear: Why are $\sigma$ and $\omega$ separate networks.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
The paper used very restricted Gaussian distributions for the formulation.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
First of all, the setup for the AE and VAE is not specified.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
3. Scenario discussed in Sec. 4 seems somewhat impractical.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
In addition, the paper would also need to show that such a model does not generalize to a validation set of images.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
"- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001' [SEP] 'rebuttal_future"	We leave it as a future work to study where the clear boundary is.
In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
The proofs are quite dense and I was unable to verify them carefully.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
"- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"".' [SEP] 'rebuttal_future"	We leave it as a future work to study where the clear boundary is.
In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Specifically, how can mutual information in this context be formally linked to generalization/overfitting?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
I also find weird the way that the authors arrive to their final objective in Equation (5).' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi).' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Is there a reason why the authors do not introduce their objective by following the variational framework?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
What is the purpose then for introducing the matrix variate Gaussian?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
It is also not clear to me how domain translation is relevant to continual learning.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
I do not understand how the model is trained to solve multiple tasks.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
6. The rationale of the two tower design (why not combine two) is not clearly explained.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Doesn't the classification loss have a dependency on the input condition?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
"--What does a ""heavy classifier"" imply concretely?' [SEP] 'rebuttal_future"	We leave it as a future work to study where the clear boundary is.
In contrast, the studied learning rates are asymptotic and there is a big discrepancy.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
However, it is not obvious that how to move from line 3 to line 4 at Eq 15.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
(5) Due to the mean policy approximation, does the mean policy depend on \phi?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
The authors should clearly explain how to update \phi when optimizing Eq 12.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
However, the derivations about \phi are missing.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
"As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of ""edge-to-edge"" convolutions and generally the architectural choice related to the conditional GAN discriminator.' [SEP] 'rebuttal_future"	We leave it as a future work to study where the clear boundary is.
- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
There is not sufficient detail to reproduce the models based on the paper alone.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
For example, one question is how often a single partial tree has multiple possible completions in the data.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Do they here refer to the gradients with respect to the weights ONLY?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.' [SEP] 'rebuttal_future"	We leave it as a future work to study where the clear boundary is.
"3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?' [SEP] 'rebuttal_future"	We leave it as a future work to study where the clear boundary is.
- It would greatly benefit the reader if eq. 5 were expanded.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
- the method is not applicable to episodes of different length' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Is Harmonic Convolution applicable to complex STFT coefficients as well?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
If so it would be better to define the operator in a more general notation.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Towards this, how does the computational complexity scale wrt to the connectedness?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
What is the L1 norm applied on?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
- Trick is specific to LM.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
It seems heavily dependent on GBDT.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Was crossvalidation used to select the topology?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
If so, what was the methodology.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
1 The implementation steps of the proposed method (MoVE) are not clear.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
To be honest, the theoretical contribution of the paper is limited.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
More discussions on these questions can be very helpful to further understand the proposed method.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
"It should be better motivated why one should use the duality gap as an upper bound for the ""F-distance"".' [SEP] 'rebuttal_future"	We leave it as a future work to study where the clear boundary is.
Minimizing the F-distance as is usually done seems like the more direct and simple approach.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
How do “we choose a specific number of assignments based on prediction probabilities”?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
"- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.' [SEP] 'rebuttal_future"	We leave it as a future work to study where the clear boundary is.
13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Negative points: (1) The authors should provide more justification on equation-3.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Why do the authors directly average different loss for the discriminator and the classifer?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
(2) The function of the discriminator is not very clear, especially for the classification error test.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
2. The learning procedure is confusing.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
It is highly recommended to provide the pseudocode of the proposed method.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
The reasons for the use of the energy-based formulation are not clear to me.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
- It is not clear how the initialisation (10) is implemented.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
"Particularly the ""fusion"" module remains extremely unclear.' [SEP] 'rebuttal_future"	We leave it as a future work to study where the clear boundary is.
It is not clear to me that the classifier difference metric is well-defined.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
"-	How the first camera pose is initialized?' [SEP] 'rebuttal_future"	We leave it as a future work to study where the clear boundary is.
This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
How do we take a limit of M -> ∞ ? Does k also go ∞?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
How does the proposed method perform in more complicated tasks such as' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Dual-1 and Dual-5 are introduced without explanation.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
It is unclear whether the data augmentation techniques is applied only at training time or also at test time.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- it wasn't clear how the sparsity percentage on page 3 was defined?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- Architecture choice unclear: Why are $\sigma$ and $\omega$ separate networks.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
The paper used very restricted Gaussian distributions for the formulation.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
First of all, the setup for the AE and VAE is not specified.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
3. Scenario discussed in Sec. 4 seems somewhat impractical.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
In addition, the paper would also need to show that such a model does not generalize to a validation set of images.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
"- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001' [SEP] 'rebuttal_done"	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
The proofs are quite dense and I was unable to verify them carefully.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
"- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"".' [SEP] 'rebuttal_done"	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Specifically, how can mutual information in this context be formally linked to generalization/overfitting?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
I also find weird the way that the authors arrive to their final objective in Equation (5).' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi).' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Is there a reason why the authors do not introduce their objective by following the variational framework?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
What is the purpose then for introducing the matrix variate Gaussian?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
It is also not clear to me how domain translation is relevant to continual learning.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
I do not understand how the model is trained to solve multiple tasks.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
6. The rationale of the two tower design (why not combine two) is not clearly explained.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Doesn't the classification loss have a dependency on the input condition?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
"--What does a ""heavy classifier"" imply concretely?' [SEP] 'rebuttal_done"	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
In contrast, the studied learning rates are asymptotic and there is a big discrepancy.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
However, it is not obvious that how to move from line 3 to line 4 at Eq 15.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
(5) Due to the mean policy approximation, does the mean policy depend on \phi?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
The authors should clearly explain how to update \phi when optimizing Eq 12.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
However, the derivations about \phi are missing.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
"As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of ""edge-to-edge"" convolutions and generally the architectural choice related to the conditional GAN discriminator.' [SEP] 'rebuttal_done"	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
There is not sufficient detail to reproduce the models based on the paper alone.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
For example, one question is how often a single partial tree has multiple possible completions in the data.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Do they here refer to the gradients with respect to the weights ONLY?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.' [SEP] 'rebuttal_done"	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
"3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?' [SEP] 'rebuttal_done"	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- It would greatly benefit the reader if eq. 5 were expanded.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- the method is not applicable to episodes of different length' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Is Harmonic Convolution applicable to complex STFT coefficients as well?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
If so it would be better to define the operator in a more general notation.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Towards this, how does the computational complexity scale wrt to the connectedness?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
What is the L1 norm applied on?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- Trick is specific to LM.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
It seems heavily dependent on GBDT.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Was crossvalidation used to select the topology?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
If so, what was the methodology.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
1 The implementation steps of the proposed method (MoVE) are not clear.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
To be honest, the theoretical contribution of the paper is limited.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
More discussions on these questions can be very helpful to further understand the proposed method.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
"It should be better motivated why one should use the duality gap as an upper bound for the ""F-distance"".' [SEP] 'rebuttal_done"	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Minimizing the F-distance as is usually done seems like the more direct and simple approach.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
How do “we choose a specific number of assignments based on prediction probabilities”?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
"- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.' [SEP] 'rebuttal_done"	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Negative points: (1) The authors should provide more justification on equation-3.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Why do the authors directly average different loss for the discriminator and the classifer?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
(2) The function of the discriminator is not very clear, especially for the classification error test.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
2. The learning procedure is confusing.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
It is highly recommended to provide the pseudocode of the proposed method.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
The reasons for the use of the energy-based formulation are not clear to me.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- It is not clear how the initialisation (10) is implemented.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
"Particularly the ""fusion"" module remains extremely unclear.' [SEP] 'rebuttal_done"	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
It is not clear to me that the classifier difference metric is well-defined.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
"-	How the first camera pose is initialized?' [SEP] 'rebuttal_done"	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.' [SEP] 'rebuttal_done	As suggested, we have utilized the appendices to give detailed information about the experimental setup.
How do we take a limit of M -> ∞ ? Does k also go ∞?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
How does the proposed method perform in more complicated tasks such as' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Dual-1 and Dual-5 are introduced without explanation.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
It is unclear whether the data augmentation techniques is applied only at training time or also at test time.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
- it wasn't clear how the sparsity percentage on page 3 was defined?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
- Architecture choice unclear: Why are $\sigma$ and $\omega$ separate networks.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
The paper used very restricted Gaussian distributions for the formulation.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
First of all, the setup for the AE and VAE is not specified.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
3. Scenario discussed in Sec. 4 seems somewhat impractical.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
In addition, the paper would also need to show that such a model does not generalize to a validation set of images.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
"- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001' [SEP] 'rebuttal_reject-request"	Footnote 2 warned the reader about this, as we know it is unusual.
In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
The proofs are quite dense and I was unable to verify them carefully.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
"- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"".' [SEP] 'rebuttal_reject-request"	Footnote 2 warned the reader about this, as we know it is unusual.
In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Specifically, how can mutual information in this context be formally linked to generalization/overfitting?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
I also find weird the way that the authors arrive to their final objective in Equation (5).' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi).' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Is there a reason why the authors do not introduce their objective by following the variational framework?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
What is the purpose then for introducing the matrix variate Gaussian?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
It is also not clear to me how domain translation is relevant to continual learning.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
I do not understand how the model is trained to solve multiple tasks.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
6. The rationale of the two tower design (why not combine two) is not clearly explained.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Doesn't the classification loss have a dependency on the input condition?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
"--What does a ""heavy classifier"" imply concretely?' [SEP] 'rebuttal_reject-request"	Footnote 2 warned the reader about this, as we know it is unusual.
In contrast, the studied learning rates are asymptotic and there is a big discrepancy.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
However, it is not obvious that how to move from line 3 to line 4 at Eq 15.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
(5) Due to the mean policy approximation, does the mean policy depend on \phi?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
The authors should clearly explain how to update \phi when optimizing Eq 12.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
However, the derivations about \phi are missing.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
"As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of ""edge-to-edge"" convolutions and generally the architectural choice related to the conditional GAN discriminator.' [SEP] 'rebuttal_reject-request"	Footnote 2 warned the reader about this, as we know it is unusual.
- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
There is not sufficient detail to reproduce the models based on the paper alone.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
For example, one question is how often a single partial tree has multiple possible completions in the data.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Do they here refer to the gradients with respect to the weights ONLY?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.' [SEP] 'rebuttal_reject-request"	Footnote 2 warned the reader about this, as we know it is unusual.
"3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?' [SEP] 'rebuttal_reject-request"	Footnote 2 warned the reader about this, as we know it is unusual.
- It would greatly benefit the reader if eq. 5 were expanded.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
- the method is not applicable to episodes of different length' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Is Harmonic Convolution applicable to complex STFT coefficients as well?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
If so it would be better to define the operator in a more general notation.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Towards this, how does the computational complexity scale wrt to the connectedness?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
What is the L1 norm applied on?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
- Trick is specific to LM.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
It seems heavily dependent on GBDT.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Was crossvalidation used to select the topology?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
If so, what was the methodology.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
1 The implementation steps of the proposed method (MoVE) are not clear.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
To be honest, the theoretical contribution of the paper is limited.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
More discussions on these questions can be very helpful to further understand the proposed method.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
"It should be better motivated why one should use the duality gap as an upper bound for the ""F-distance"".' [SEP] 'rebuttal_reject-request"	Footnote 2 warned the reader about this, as we know it is unusual.
Minimizing the F-distance as is usually done seems like the more direct and simple approach.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
How do “we choose a specific number of assignments based on prediction probabilities”?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
"- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.' [SEP] 'rebuttal_reject-request"	Footnote 2 warned the reader about this, as we know it is unusual.
13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Negative points: (1) The authors should provide more justification on equation-3.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Why do the authors directly average different loss for the discriminator and the classifer?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
(2) The function of the discriminator is not very clear, especially for the classification error test.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
2. The learning procedure is confusing.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
It is highly recommended to provide the pseudocode of the proposed method.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
The reasons for the use of the energy-based formulation are not clear to me.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
- It is not clear how the initialisation (10) is implemented.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
"Particularly the ""fusion"" module remains extremely unclear.' [SEP] 'rebuttal_reject-request"	Footnote 2 warned the reader about this, as we know it is unusual.
It is not clear to me that the classifier difference metric is well-defined.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
"-	How the first camera pose is initialized?' [SEP] 'rebuttal_reject-request"	Footnote 2 warned the reader about this, as we know it is unusual.
This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.' [SEP] 'rebuttal_reject-request	Footnote 2 warned the reader about this, as we know it is unusual.
Significance: It is hard to assess given the current submission.' [SEP] 'rebuttal_done	We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.
In sum, the paper has a very good application but not good enough as a research paper.' [SEP] 'rebuttal_done	We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.
Due to several shortcomings of the paper, most important of which is on presentation of the paper, this manuscript requires a significant revision by the authors to reach the necessary standards for publication, moreover it would be helpful to clarify the modeling choices and consequences of these choices more clearly.' [SEP] 'rebuttal_done	We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.
While sensible, this seems to me to be too minor a contribution to stand alone as a paper.' [SEP] 'rebuttal_done	We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.
This could have made the paper much stronger.' [SEP] 'rebuttal_done	We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.
As such the paper is not convincing.' [SEP] 'rebuttal_done	We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.
Overall, I am not sure what we could gain from this research direction.' [SEP] 'rebuttal_done	We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.
1) The motivation is unclear and overall structure of the paper is confusing.' [SEP] 'rebuttal_done	We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.
- Contribution overall may be a bit limited' [SEP] 'rebuttal_done	We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.
Significance: Below average' [SEP] 'rebuttal_done	We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.
The writing is understandable for the most part, but the paper seems to lack focus - there is no clear take home message.' [SEP] 'rebuttal_done	We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.
The MTurk experiment gives a qualitative picture, but it could be improved with comparisons to pairwise distances learned through alternative means using the RGB image itself (given that images would permit such a comparison).' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
The message of synthetic experiments would be stronger if more of them were available and if the comparison between LOE, TSTE, and OENN was made on more of them.' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below.' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
The experimental results are not very convincing because many importance baselines are neglected.' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
The comparisons are also absent in experiments.' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
3) The experiments lack comparisons to several important baselines from self-supervised learning community, and methods using soft labels for training (as mentioned in 2) above).' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance.' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
Without a proper comparison (formal and experimental) with these lines of work, the paper is incomplete.' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
So, I have some doubts about the experimental results.' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
However, there is no comparison with ENAS and DARTS in experiments.' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN.' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
- For the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbel-sinkhorn.' [SEP] 'rebuttal_by-cr	4. We will include more related works to our paper.
The MTurk experiment gives a qualitative picture, but it could be improved with comparisons to pairwise distances learned through alternative means using the RGB image itself (given that images would permit such a comparison).' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
The message of synthetic experiments would be stronger if more of them were available and if the comparison between LOE, TSTE, and OENN was made on more of them.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
The experimental results are not very convincing because many importance baselines are neglected.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
The comparisons are also absent in experiments.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
3) The experiments lack comparisons to several important baselines from self-supervised learning community, and methods using soft labels for training (as mentioned in 2) above).' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
Without a proper comparison (formal and experimental) with these lines of work, the paper is incomplete.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
So, I have some doubts about the experimental results.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
However, there is no comparison with ENAS and DARTS in experiments.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
- For the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbel-sinkhorn.' [SEP] 'rebuttal_reject-criticism	We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.
The MTurk experiment gives a qualitative picture, but it could be improved with comparisons to pairwise distances learned through alternative means using the RGB image itself (given that images would permit such a comparison).' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
The message of synthetic experiments would be stronger if more of them were available and if the comparison between LOE, TSTE, and OENN was made on more of them.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
The experimental results are not very convincing because many importance baselines are neglected.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
The comparisons are also absent in experiments.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
3) The experiments lack comparisons to several important baselines from self-supervised learning community, and methods using soft labels for training (as mentioned in 2) above).' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
Without a proper comparison (formal and experimental) with these lines of work, the paper is incomplete.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
So, I have some doubts about the experimental results.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
However, there is no comparison with ENAS and DARTS in experiments.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
- For the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbel-sinkhorn.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
1: The authors show no benefit of this scheme except perhaps faster convergence.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
The method provides impressive compression ratios (in the order of x20-30) but at the cost of a lower performance.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
If the test set is not shuffled (by emphasis on first I assume not) these images are from training NIST (cleaner) set and may not include samples of all digits.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
One observation from the submission is that the token set may need to very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive (I noticed that the BERT model is trained on 128 GPUs)' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
3. One concern I have with discrete representation is how robust they are wrt different dataset.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
1. The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage. After all, the paper says BERT scores 88% before the dataset is attacked.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Why does it go to nearly zero around x = 0, while being higher in surrounding regions with more data?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
"Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being ""meta domain adaptation"".' [SEP] 'rebuttal_concede-criticism"	We apologize for unclear description of experimental settings.
- in section 4.3, there is no guarantee that the intersection between the training set and test set is empty.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Optimizing compression rates should be done on the training set with a separate development set.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
The test set should not used before the best compression scheme is selected.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
First, I consider the tabular features as multi-feature data and less to be the multimodal data.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Also, the fact that the accuracy on the Kaggle non-doctored test set is low is simply because the test set is not coming from the same distribution of the training set.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
The setup where labeled data (c) also seems a bit unnatural (this also seems to be confirmed by the fact that the authors had to build datasets for the problem).' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
A bigger source of uncertainty is likely due to there being a limited amount of data to fit the coefficients to.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
5. One of the anonymous comments on OpenReview is very interesting: samples from a CIFAR model look nothing like SVHN.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
"7.	The study is only performed on symbols which a very large training set (given the difficulty of the problem) and it not clear how well this generalizes to real images or scenarios with less training data.' [SEP] 'rebuttal_concede-criticism"	We apologize for unclear description of experimental settings.
Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4).' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets.' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
3. Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
In this case, A: PointNet, B: DeepSet' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
It looks like all the results are given on the test set. Did you not do any tuning on the validation data?' [SEP] 'rebuttal_concede-criticism	We apologize for unclear description of experimental settings.
If the test set is not shuffled (by emphasis on first I assume not) these images are from training NIST (cleaner) set and may not include samples of all digits.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
One observation from the submission is that the token set may need to very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive (I noticed that the BERT model is trained on 128 GPUs)' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
3. One concern I have with discrete representation is how robust they are wrt different dataset.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
1. The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage. After all, the paper says BERT scores 88% before the dataset is attacked.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
Why does it go to nearly zero around x = 0, while being higher in surrounding regions with more data?' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
"Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being ""meta domain adaptation"".' [SEP] 'rebuttal_structuring"	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
- in section 4.3, there is no guarantee that the intersection between the training set and test set is empty.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
Optimizing compression rates should be done on the training set with a separate development set.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
The test set should not used before the best compression scheme is selected.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
First, I consider the tabular features as multi-feature data and less to be the multimodal data.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
Also, the fact that the accuracy on the Kaggle non-doctored test set is low is simply because the test set is not coming from the same distribution of the training set.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
The setup where labeled data (c) also seems a bit unnatural (this also seems to be confirmed by the fact that the authors had to build datasets for the problem).' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
A bigger source of uncertainty is likely due to there being a limited amount of data to fit the coefficients to.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
5. One of the anonymous comments on OpenReview is very interesting: samples from a CIFAR model look nothing like SVHN.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
"7.	The study is only performed on symbols which a very large training set (given the difficulty of the problem) and it not clear how well this generalizes to real images or scenarios with less training data.' [SEP] 'rebuttal_structuring"	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4).' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
3. Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
In this case, A: PointNet, B: DeepSet' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
It looks like all the results are given on the test set. Did you not do any tuning on the validation data?' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
If the test set is not shuffled (by emphasis on first I assume not) these images are from training NIST (cleaner) set and may not include samples of all digits.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
One observation from the submission is that the token set may need to very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive (I noticed that the BERT model is trained on 128 GPUs)' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
3. One concern I have with discrete representation is how robust they are wrt different dataset.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
1. The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage. After all, the paper says BERT scores 88% before the dataset is attacked.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
Why does it go to nearly zero around x = 0, while being higher in surrounding regions with more data?' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
"Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being ""meta domain adaptation"".' [SEP] 'rebuttal_done"	Fig. 3 and other evaluations have been updated for the new test set.
- in section 4.3, there is no guarantee that the intersection between the training set and test set is empty.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
Optimizing compression rates should be done on the training set with a separate development set.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
The test set should not used before the best compression scheme is selected.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
First, I consider the tabular features as multi-feature data and less to be the multimodal data.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
Also, the fact that the accuracy on the Kaggle non-doctored test set is low is simply because the test set is not coming from the same distribution of the training set.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
The setup where labeled data (c) also seems a bit unnatural (this also seems to be confirmed by the fact that the authors had to build datasets for the problem).' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
A bigger source of uncertainty is likely due to there being a limited amount of data to fit the coefficients to.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
5. One of the anonymous comments on OpenReview is very interesting: samples from a CIFAR model look nothing like SVHN.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
"7.	The study is only performed on symbols which a very large training set (given the difficulty of the problem) and it not clear how well this generalizes to real images or scenarios with less training data.' [SEP] 'rebuttal_done"	Fig. 3 and other evaluations have been updated for the new test set.
Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4).' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets.' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
3. Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
In this case, A: PointNet, B: DeepSet' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
It looks like all the results are given on the test set. Did you not do any tuning on the validation data?' [SEP] 'rebuttal_done	Fig. 3 and other evaluations have been updated for the new test set.
If the test set is not shuffled (by emphasis on first I assume not) these images are from training NIST (cleaner) set and may not include samples of all digits.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
One observation from the submission is that the token set may need to very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive (I noticed that the BERT model is trained on 128 GPUs)' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
3. One concern I have with discrete representation is how robust they are wrt different dataset.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
1. The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage. After all, the paper says BERT scores 88% before the dataset is attacked.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
Why does it go to nearly zero around x = 0, while being higher in surrounding regions with more data?' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
"Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being ""meta domain adaptation"".' [SEP] 'rebuttal_future"	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
- in section 4.3, there is no guarantee that the intersection between the training set and test set is empty.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
Optimizing compression rates should be done on the training set with a separate development set.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
The test set should not used before the best compression scheme is selected.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
First, I consider the tabular features as multi-feature data and less to be the multimodal data.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
Also, the fact that the accuracy on the Kaggle non-doctored test set is low is simply because the test set is not coming from the same distribution of the training set.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
The setup where labeled data (c) also seems a bit unnatural (this also seems to be confirmed by the fact that the authors had to build datasets for the problem).' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
A bigger source of uncertainty is likely due to there being a limited amount of data to fit the coefficients to.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
5. One of the anonymous comments on OpenReview is very interesting: samples from a CIFAR model look nothing like SVHN.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
"7.	The study is only performed on symbols which a very large training set (given the difficulty of the problem) and it not clear how well this generalizes to real images or scenarios with less training data.' [SEP] 'rebuttal_future"	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4).' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets.' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
3. Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
In this case, A: PointNet, B: DeepSet' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
It looks like all the results are given on the test set. Did you not do any tuning on the validation data?' [SEP] 'rebuttal_future	We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.
If the test set is not shuffled (by emphasis on first I assume not) these images are from training NIST (cleaner) set and may not include samples of all digits.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
One observation from the submission is that the token set may need to very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive (I noticed that the BERT model is trained on 128 GPUs)' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
3. One concern I have with discrete representation is how robust they are wrt different dataset.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
1. The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage. After all, the paper says BERT scores 88% before the dataset is attacked.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
Why does it go to nearly zero around x = 0, while being higher in surrounding regions with more data?' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
"Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being ""meta domain adaptation"".' [SEP] 'rebuttal_summary"	Note the compression rates are the same as the data in Table 3 in the original manuscript.
- in section 4.3, there is no guarantee that the intersection between the training set and test set is empty.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
Optimizing compression rates should be done on the training set with a separate development set.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
The test set should not used before the best compression scheme is selected.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
First, I consider the tabular features as multi-feature data and less to be the multimodal data.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
Also, the fact that the accuracy on the Kaggle non-doctored test set is low is simply because the test set is not coming from the same distribution of the training set.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
The setup where labeled data (c) also seems a bit unnatural (this also seems to be confirmed by the fact that the authors had to build datasets for the problem).' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
A bigger source of uncertainty is likely due to there being a limited amount of data to fit the coefficients to.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
5. One of the anonymous comments on OpenReview is very interesting: samples from a CIFAR model look nothing like SVHN.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
"7.	The study is only performed on symbols which a very large training set (given the difficulty of the problem) and it not clear how well this generalizes to real images or scenarios with less training data.' [SEP] 'rebuttal_summary"	Note the compression rates are the same as the data in Table 3 in the original manuscript.
Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4).' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
3. Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
In this case, A: PointNet, B: DeepSet' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
It looks like all the results are given on the test set. Did you not do any tuning on the validation data?' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
For example, “Fuzzy Choquet Integration of Deep Convolutional Neural Networks for Remote Sensing” by Derek T. Anderson et al.' [SEP] 'rebuttal_answer	See ensuing discussion in p.18 following equation 36.
See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.' [SEP] 'rebuttal_answer	See ensuing discussion in p.18 following equation 36.
- Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016' [SEP] 'rebuttal_answer	See ensuing discussion in p.18 following equation 36.
For example, “Fuzzy Choquet Integration of Deep Convolutional Neural Networks for Remote Sensing” by Derek T. Anderson et al.' [SEP] 'rebuttal_done	* In relation to the connection to IWAE, we have included a detailed discussion in Appendix E.1.
See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.' [SEP] 'rebuttal_done	* In relation to the connection to IWAE, we have included a detailed discussion in Appendix E.1.
- Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016' [SEP] 'rebuttal_done	* In relation to the connection to IWAE, we have included a detailed discussion in Appendix E.1.
3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
2) The experimental results provided in this paper are weak.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
"-	The experimental results of section 5.2 are somewhat disappointing.' [SEP] 'rebuttal_summary"	Note the compression rates are the same as the data in Table 3 in the original manuscript.
Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
The experimental results are actually less impressive than what are claimed in contribution and conclusion.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
So this work has to be supported with more detailed experimental results to express the potential of this approach fully.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
Both the results on the development set and on the test set should be reported for the validity of the experiments.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
(1) The experimental results cannot show the usefulness of the proposed GCN.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
- The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
- Experimental results are provided only on MNIST and Fashion-MNIST.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
- The experiments show good results compared to existing algorithms, but not impressively so.' [SEP] 'rebuttal_summary	Note the compression rates are the same as the data in Table 3 in the original manuscript.
3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
2) The experimental results provided in this paper are weak.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
"-	The experimental results of section 5.2 are somewhat disappointing.' [SEP] 'rebuttal_structuring"	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
The experimental results are actually less impressive than what are claimed in contribution and conclusion.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
So this work has to be supported with more detailed experimental results to express the potential of this approach fully.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
Both the results on the development set and on the test set should be reported for the validity of the experiments.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
(1) The experimental results cannot show the usefulness of the proposed GCN.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
- The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
- Experimental results are provided only on MNIST and Fashion-MNIST.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
- The experiments show good results compared to existing algorithms, but not impressively so.' [SEP] 'rebuttal_structuring	We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.
3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
2) The experimental results provided in this paper are weak.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
"-	The experimental results of section 5.2 are somewhat disappointing.' [SEP] 'rebuttal_done"	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
The experimental results are actually less impressive than what are claimed in contribution and conclusion.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
So this work has to be supported with more detailed experimental results to express the potential of this approach fully.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
Both the results on the development set and on the test set should be reported for the validity of the experiments.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
(1) The experimental results cannot show the usefulness of the proposed GCN.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
- The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
- Experimental results are provided only on MNIST and Fashion-MNIST.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
- The experiments show good results compared to existing algorithms, but not impressively so.' [SEP] 'rebuttal_done	Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.
3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
2) The experimental results provided in this paper are weak.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
"-	The experimental results of section 5.2 are somewhat disappointing.' [SEP] 'rebuttal_concede-criticism"	We agree that any improvements compared to RGCN are marginal.
Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
The experimental results are actually less impressive than what are claimed in contribution and conclusion.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
So this work has to be supported with more detailed experimental results to express the potential of this approach fully.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
Both the results on the development set and on the test set should be reported for the validity of the experiments.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
(1) The experimental results cannot show the usefulness of the proposed GCN.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
- The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
- Experimental results are provided only on MNIST and Fashion-MNIST.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
- The experiments show good results compared to existing algorithms, but not impressively so.' [SEP] 'rebuttal_concede-criticism	We agree that any improvements compared to RGCN are marginal.
3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
2) The experimental results provided in this paper are weak.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
"-	The experimental results of section 5.2 are somewhat disappointing.' [SEP] 'rebuttal_reject-request"	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The experimental results are actually less impressive than what are claimed in contribution and conclusion.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
So this work has to be supported with more detailed experimental results to express the potential of this approach fully.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Both the results on the development set and on the test set should be reported for the validity of the experiments.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
(1) The experimental results cannot show the usefulness of the proposed GCN.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- Experimental results are provided only on MNIST and Fashion-MNIST.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- The experiments show good results compared to existing algorithms, but not impressively so.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018).' [SEP] 'rebuttal_answer	In addition, this work provides a detailed comparison of the effect of the losses on the various metrics.
In terms of method, the guidance for learning Siamese networks are designed heuristically (e.g. edges, colors, etc.) which limits its applicability over various datasets; I think that designing more principled approach to build such guidances from data should be one of the key contributions of the paper.' [SEP] 'rebuttal_mitigate-criticism	Further, we added our unsupervised analyses to show that the method works even without explicit guidance on all tested datasets.
This paper is very well written, although very dense and not easy to follows, as many methods are referenced and assume that the reviewer is highly familiar with the related works.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
Hence it is unclear how large the running time improvement is compared to a well-tuned baseline.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
This is not to say that this way of doing things is wrong, but rather that it is misleading in the context of prior work.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
I find the background on ELBO and GANs unnecessary occluding the clarity at this point.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
"11. Baseline 2 is actually referred to as ""usage baseline"" but this name is not introduced in the itemized part.' [SEP] 'rebuttal_concede-criticism"	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
Indeed, without referencing the original Pointer Network and (and especially the) Transformer papers, it would not be possible to understand this paper at all.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
Also, please place the related work earlier on in the paper.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
"For example, there are two ""the"" in the end of the third paragraph in Related Work.' [SEP] 'rebuttal_concede-criticism"	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
"In the last paragraph in Related Work, ""provide"" should be ""provides"".' [SEP] 'rebuttal_concede-criticism"	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
The related work section looks incomplete with some missing related references as mentioned above, and copy of a segment that appears in the introduction.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
2. First paragraph in related work is very unrelated to the current subject, please remove.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
While it’s great that so much prior work was acknowledged, mentioning a paper once per paragraph is (usually) sufficient and increases readability.' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
For example, the related works paper would read better if it wasn't one long block (i.e. break it into several paragraphs)' [SEP] 'rebuttal_concede-criticism	In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.
This paper is very well written, although very dense and not easy to follows, as many methods are referenced and assume that the reviewer is highly familiar with the related works.' [SEP] 'rebuttal_done	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
Hence it is unclear how large the running time improvement is compared to a well-tuned baseline.' [SEP] 'rebuttal_done	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
This is not to say that this way of doing things is wrong, but rather that it is misleading in the context of prior work.' [SEP] 'rebuttal_done	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
I find the background on ELBO and GANs unnecessary occluding the clarity at this point.' [SEP] 'rebuttal_done	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
"11. Baseline 2 is actually referred to as ""usage baseline"" but this name is not introduced in the itemized part.' [SEP] 'rebuttal_done"	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.' [SEP] 'rebuttal_done	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
Indeed, without referencing the original Pointer Network and (and especially the) Transformer papers, it would not be possible to understand this paper at all.' [SEP] 'rebuttal_done	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
Also, please place the related work earlier on in the paper.' [SEP] 'rebuttal_done	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
"For example, there are two ""the"" in the end of the third paragraph in Related Work.' [SEP] 'rebuttal_done"	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
"In the last paragraph in Related Work, ""provide"" should be ""provides"".' [SEP] 'rebuttal_done"	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
The related work section looks incomplete with some missing related references as mentioned above, and copy of a segment that appears in the introduction.' [SEP] 'rebuttal_done	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
2. First paragraph in related work is very unrelated to the current subject, please remove.' [SEP] 'rebuttal_done	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
While it’s great that so much prior work was acknowledged, mentioning a paper once per paragraph is (usually) sufficient and increases readability.' [SEP] 'rebuttal_done	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
For example, the related works paper would read better if it wasn't one long block (i.e. break it into several paragraphs)' [SEP] 'rebuttal_done	1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.
Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
I also wonder if the video data will be released, which could be important for the following comparisons.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
4. How large is the training set of (T, P) pairs? I don't think this is mentioned in the paper.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
It is also not clear to me why CIFAR datasets involve two domains and how these domains are relevant in each of the tasks.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
It is necessary to test it on datasets with much more fine classes and much-complicated hierarchy, e.g., ImageNet, MS COCO or their subsets, which have ideal class hierarchy structures.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
- Lack of sufficient technical detail on models and dataset' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
The IDF scores would be stronger if they were computed on a bigger in-domain corpus than the gold test set.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
While reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets.' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
I also wonder if the video data will be released, which could be important for the following comparisons.' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
4. How large is the training set of (T, P) pairs? I don't think this is mentioned in the paper.' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
It is also not clear to me why CIFAR datasets involve two domains and how these domains are relevant in each of the tasks.' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
It is necessary to test it on datasets with much more fine classes and much-complicated hierarchy, e.g., ImageNet, MS COCO or their subsets, which have ideal class hierarchy structures.' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
- Lack of sufficient technical detail on models and dataset' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
The IDF scores would be stronger if they were computed on a bigger in-domain corpus than the gold test set.' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small.' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator.' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
While reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets.' [SEP] 'rebuttal_reject-request	Due to the space restriction, however, we cannot present them all in the paper.
Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets.' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
I also wonder if the video data will be released, which could be important for the following comparisons.' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
4. How large is the training set of (T, P) pairs? I don't think this is mentioned in the paper.' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
It is also not clear to me why CIFAR datasets involve two domains and how these domains are relevant in each of the tasks.' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
It is necessary to test it on datasets with much more fine classes and much-complicated hierarchy, e.g., ImageNet, MS COCO or their subsets, which have ideal class hierarchy structures.' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
- Lack of sufficient technical detail on models and dataset' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
The IDF scores would be stronger if they were computed on a bigger in-domain corpus than the gold test set.' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small.' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator.' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
While reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets.' [SEP] 'rebuttal_done	We believe that, together with other architectural details present in the paper, it makes our work reproducible.
Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets.' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
I also wonder if the video data will be released, which could be important for the following comparisons.' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
4. How large is the training set of (T, P) pairs? I don't think this is mentioned in the paper.' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
It is also not clear to me why CIFAR datasets involve two domains and how these domains are relevant in each of the tasks.' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
It is necessary to test it on datasets with much more fine classes and much-complicated hierarchy, e.g., ImageNet, MS COCO or their subsets, which have ideal class hierarchy structures.' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
- Lack of sufficient technical detail on models and dataset' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
The IDF scores would be stronger if they were computed on a bigger in-domain corpus than the gold test set.' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small.' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator.' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
While reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets.' [SEP] 'rebuttal_answer	We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.
Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets.' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
I also wonder if the video data will be released, which could be important for the following comparisons.' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
4. How large is the training set of (T, P) pairs? I don't think this is mentioned in the paper.' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
It is also not clear to me why CIFAR datasets involve two domains and how these domains are relevant in each of the tasks.' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
It is necessary to test it on datasets with much more fine classes and much-complicated hierarchy, e.g., ImageNet, MS COCO or their subsets, which have ideal class hierarchy structures.' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
- Lack of sufficient technical detail on models and dataset' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
The IDF scores would be stronger if they were computed on a bigger in-domain corpus than the gold test set.' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small.' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator.' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
While reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets.' [SEP] 'rebuttal_reject-criticism	Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.
First, In Theorem 2, which seems to be a main result of the paper, the authors were concerned with the condition when W_{ji} >0, but there is not conclusion if W_{ji} =0.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
2. As to the results of the Pose2Pose network, I wonder if there are some artifacts that will affect the performance of the Pose2Frame network.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
However, this only affects the method of drawing the samples from a fixed known distribution and should have no more effect on the results than say a choice of a pseudo-random number generator.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
On the other hand, the obtained results are very weak: only one layered version of the paper is analysed and the theorem applies only to networks with less than some threshold of parameters.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
Overall, while I find the proposed approach simple -- the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
This shows itself in the results; i.e., the proposed algorithm is either negligibly performing better than GBDT or when  GBDT dependence removed, it performs worse. It seems to me' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
Though promising results have been demonstrated, a drawback of the proposed method is that it introduces more memory overhead compared to GPipe.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
Even with the hybrid method, the accuracy still drops.' [SEP] 'rebuttal_answer	Please also refer to the main contribution (ii) of our response to Reviewer 2.
First, In Theorem 2, which seems to be a main result of the paper, the authors were concerned with the condition when W_{ji} >0, but there is not conclusion if W_{ji} =0.' [SEP] 'rebuttal_done	To show that we can easily include these features, we have included in our appendix some results including non-structural features.
Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.' [SEP] 'rebuttal_done	To show that we can easily include these features, we have included in our appendix some results including non-structural features.
2. As to the results of the Pose2Pose network, I wonder if there are some artifacts that will affect the performance of the Pose2Frame network.' [SEP] 'rebuttal_done	To show that we can easily include these features, we have included in our appendix some results including non-structural features.
However, this only affects the method of drawing the samples from a fixed known distribution and should have no more effect on the results than say a choice of a pseudo-random number generator.' [SEP] 'rebuttal_done	To show that we can easily include these features, we have included in our appendix some results including non-structural features.
On the other hand, the obtained results are very weak: only one layered version of the paper is analysed and the theorem applies only to networks with less than some threshold of parameters.' [SEP] 'rebuttal_done	To show that we can easily include these features, we have included in our appendix some results including non-structural features.
Overall, while I find the proposed approach simple -- the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same.' [SEP] 'rebuttal_done	To show that we can easily include these features, we have included in our appendix some results including non-structural features.
This shows itself in the results; i.e., the proposed algorithm is either negligibly performing better than GBDT or when  GBDT dependence removed, it performs worse. It seems to me' [SEP] 'rebuttal_done	To show that we can easily include these features, we have included in our appendix some results including non-structural features.
Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.' [SEP] 'rebuttal_done	To show that we can easily include these features, we have included in our appendix some results including non-structural features.
2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper.' [SEP] 'rebuttal_done	To show that we can easily include these features, we have included in our appendix some results including non-structural features.
Though promising results have been demonstrated, a drawback of the proposed method is that it introduces more memory overhead compared to GPipe.' [SEP] 'rebuttal_done	To show that we can easily include these features, we have included in our appendix some results including non-structural features.
Even with the hybrid method, the accuracy still drops.' [SEP] 'rebuttal_done	To show that we can easily include these features, we have included in our appendix some results including non-structural features.
First, In Theorem 2, which seems to be a main result of the paper, the authors were concerned with the condition when W_{ji} >0, but there is not conclusion if W_{ji} =0.' [SEP] 'rebuttal_structuring	So, two responses are given below.
Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.' [SEP] 'rebuttal_structuring	So, two responses are given below.
2. As to the results of the Pose2Pose network, I wonder if there are some artifacts that will affect the performance of the Pose2Frame network.' [SEP] 'rebuttal_structuring	So, two responses are given below.
However, this only affects the method of drawing the samples from a fixed known distribution and should have no more effect on the results than say a choice of a pseudo-random number generator.' [SEP] 'rebuttal_structuring	So, two responses are given below.
On the other hand, the obtained results are very weak: only one layered version of the paper is analysed and the theorem applies only to networks with less than some threshold of parameters.' [SEP] 'rebuttal_structuring	So, two responses are given below.
Overall, while I find the proposed approach simple -- the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same.' [SEP] 'rebuttal_structuring	So, two responses are given below.
This shows itself in the results; i.e., the proposed algorithm is either negligibly performing better than GBDT or when  GBDT dependence removed, it performs worse. It seems to me' [SEP] 'rebuttal_structuring	So, two responses are given below.
Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.' [SEP] 'rebuttal_structuring	So, two responses are given below.
2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper.' [SEP] 'rebuttal_structuring	So, two responses are given below.
Though promising results have been demonstrated, a drawback of the proposed method is that it introduces more memory overhead compared to GPipe.' [SEP] 'rebuttal_structuring	So, two responses are given below.
Even with the hybrid method, the accuracy still drops.' [SEP] 'rebuttal_structuring	So, two responses are given below.
Therefore, an interesting baseline for the evaluation of the ICL approach would be a predefined, fixed attention map consisting of concentric circles with the image center as their center, to show that the proposed approach does more than just deemphasizing the corners of the image)' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
However, in its current state the work lacks sufficiently strong baselines to support the paper’s claims; thus, the merits of this approach cannot yet be properly assessed.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
This is a very good point, however the paper do not compare or contrast with existing methods.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
Second, SST itself is only comparable with or even worse than the state-of-art methods.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches?' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
- It would be also better to show the coefficient of existing methods that have no theoretical justification.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
- How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
(a) a comparison to other methods (outside the current framework) for sound separation' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
Similarly, I'd have expected baselines that included those models in the evaluation section showing the differences in performance between the newly proposed Transformer model for trees and previously used methods.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
This latter baseline is a zero-cost baseline as it is not even dependent on the method.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
The connections of the proposed approach with existing literature should be better explained.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
Therefore, an interesting baseline for the evaluation of the ICL approach would be a predefined, fixed attention map consisting of concentric circles with the image center as their center, to show that the proposed approach does more than just deemphasizing the corners of the image)' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
However, in its current state the work lacks sufficiently strong baselines to support the paper’s claims; thus, the merits of this approach cannot yet be properly assessed.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
This is a very good point, however the paper do not compare or contrast with existing methods.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Second, SST itself is only comparable with or even worse than the state-of-art methods.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches?' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
- It would be also better to show the coefficient of existing methods that have no theoretical justification.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
- How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
(a) a comparison to other methods (outside the current framework) for sound separation' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Similarly, I'd have expected baselines that included those models in the evaluation section showing the differences in performance between the newly proposed Transformer model for trees and previously used methods.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
This latter baseline is a zero-cost baseline as it is not even dependent on the method.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
The connections of the proposed approach with existing literature should be better explained.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Therefore, an interesting baseline for the evaluation of the ICL approach would be a predefined, fixed attention map consisting of concentric circles with the image center as their center, to show that the proposed approach does more than just deemphasizing the corners of the image)' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
However, in its current state the work lacks sufficiently strong baselines to support the paper’s claims; thus, the merits of this approach cannot yet be properly assessed.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
This is a very good point, however the paper do not compare or contrast with existing methods.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
Second, SST itself is only comparable with or even worse than the state-of-art methods.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches?' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
- It would be also better to show the coefficient of existing methods that have no theoretical justification.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
- How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
(a) a comparison to other methods (outside the current framework) for sound separation' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
Similarly, I'd have expected baselines that included those models in the evaluation section showing the differences in performance between the newly proposed Transformer model for trees and previously used methods.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
This latter baseline is a zero-cost baseline as it is not even dependent on the method.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
The connections of the proposed approach with existing literature should be better explained.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.' [SEP] 'rebuttal_answer	Upon your suggestion, we would also add this random baseline in table 2 as well.
Therefore, an interesting baseline for the evaluation of the ICL approach would be a predefined, fixed attention map consisting of concentric circles with the image center as their center, to show that the proposed approach does more than just deemphasizing the corners of the image)' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
However, in its current state the work lacks sufficiently strong baselines to support the paper’s claims; thus, the merits of this approach cannot yet be properly assessed.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
This is a very good point, however the paper do not compare or contrast with existing methods.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
Second, SST itself is only comparable with or even worse than the state-of-art methods.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches?' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
- It would be also better to show the coefficient of existing methods that have no theoretical justification.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
- How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
(a) a comparison to other methods (outside the current framework) for sound separation' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
Similarly, I'd have expected baselines that included those models in the evaluation section showing the differences in performance between the newly proposed Transformer model for trees and previously used methods.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
This latter baseline is a zero-cost baseline as it is not even dependent on the method.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
The connections of the proposed approach with existing literature should be better explained.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
Therefore, an interesting baseline for the evaluation of the ICL approach would be a predefined, fixed attention map consisting of concentric circles with the image center as their center, to show that the proposed approach does more than just deemphasizing the corners of the image)' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
However, in its current state the work lacks sufficiently strong baselines to support the paper’s claims; thus, the merits of this approach cannot yet be properly assessed.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
This is a very good point, however the paper do not compare or contrast with existing methods.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
Second, SST itself is only comparable with or even worse than the state-of-art methods.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches?' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
- It would be also better to show the coefficient of existing methods that have no theoretical justification.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
- How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
(a) a comparison to other methods (outside the current framework) for sound separation' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
Similarly, I'd have expected baselines that included those models in the evaluation section showing the differences in performance between the newly proposed Transformer model for trees and previously used methods.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
This latter baseline is a zero-cost baseline as it is not even dependent on the method.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
The connections of the proposed approach with existing literature should be better explained.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.' [SEP] 'rebuttal_by-cr	We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.
Therefore, an interesting baseline for the evaluation of the ICL approach would be a predefined, fixed attention map consisting of concentric circles with the image center as their center, to show that the proposed approach does more than just deemphasizing the corners of the image)' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
However, in its current state the work lacks sufficiently strong baselines to support the paper’s claims; thus, the merits of this approach cannot yet be properly assessed.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
This is a very good point, however the paper do not compare or contrast with existing methods.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
Second, SST itself is only comparable with or even worse than the state-of-art methods.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches?' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
- It would be also better to show the coefficient of existing methods that have no theoretical justification.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
- How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
(a) a comparison to other methods (outside the current framework) for sound separation' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
Similarly, I'd have expected baselines that included those models in the evaluation section showing the differences in performance between the newly proposed Transformer model for trees and previously used methods.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
This latter baseline is a zero-cost baseline as it is not even dependent on the method.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
The connections of the proposed approach with existing literature should be better explained.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.' [SEP] 'rebuttal_future	This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.
Therefore, an interesting baseline for the evaluation of the ICL approach would be a predefined, fixed attention map consisting of concentric circles with the image center as their center, to show that the proposed approach does more than just deemphasizing the corners of the image)' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
However, in its current state the work lacks sufficiently strong baselines to support the paper’s claims; thus, the merits of this approach cannot yet be properly assessed.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
This is a very good point, however the paper do not compare or contrast with existing methods.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
Second, SST itself is only comparable with or even worse than the state-of-art methods.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches?' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
- It would be also better to show the coefficient of existing methods that have no theoretical justification.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
- How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
(a) a comparison to other methods (outside the current framework) for sound separation' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
Similarly, I'd have expected baselines that included those models in the evaluation section showing the differences in performance between the newly proposed Transformer model for trees and previously used methods.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
This latter baseline is a zero-cost baseline as it is not even dependent on the method.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
The connections of the proposed approach with existing literature should be better explained.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.' [SEP] 'rebuttal_done	We revised the notations in the paper to make formulation clearer.
Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
- no qualitative analysis on how modulation is actually use by the systems.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
"-2 Diversity of additional ""agents"" not analyzed (more below).' [SEP] 'rebuttal_concede-criticism"	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
I would have liked to see a bit more analysis as to why some pre-training strategies work over others.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
- It would be nice if different stopping criteria were analysed.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?)' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
This may also help to understand some of the limitations of this analysis.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
"And the analysis of the ""dynamic range"" of the algorithim is missing.' [SEP] 'rebuttal_concede-criticism"	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
- The premises of the analyses are not very convincing, limiting the significance of the paper.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
2. The authors should provide ablation study and analysis of their CTAugment.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
4. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.” However, they do not show any analysis to support this hypothesis.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
If that's the goal, however, a more detailed error analysis would need to be included.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well)' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
- The analysis has focused on a very simple case of having a linear discriminator which for example in WGAN, forces the first moments to match. How does the analysis extend to more realistic cases?' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
- no qualitative analysis on how modulation is actually use by the systems.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
"-2 Diversity of additional ""agents"" not analyzed (more below).' [SEP] 'rebuttal_structuring"	[The analysis is relatively straightforward]
- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
I would have liked to see a bit more analysis as to why some pre-training strategies work over others.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
- It would be nice if different stopping criteria were analysed.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?)' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
This may also help to understand some of the limitations of this analysis.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
"And the analysis of the ""dynamic range"" of the algorithim is missing.' [SEP] 'rebuttal_structuring"	[The analysis is relatively straightforward]
- The premises of the analyses are not very convincing, limiting the significance of the paper.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
2. The authors should provide ablation study and analysis of their CTAugment.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
4. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.” However, they do not show any analysis to support this hypothesis.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
If that's the goal, however, a more detailed error analysis would need to be included.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well)' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
- The analysis has focused on a very simple case of having a linear discriminator which for example in WGAN, forces the first moments to match. How does the analysis extend to more realistic cases?' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.' [SEP] 'rebuttal_structuring	[The analysis is relatively straightforward]
Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
- no qualitative analysis on how modulation is actually use by the systems.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
"-2 Diversity of additional ""agents"" not analyzed (more below).' [SEP] 'rebuttal_reject-criticism"	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
I would have liked to see a bit more analysis as to why some pre-training strategies work over others.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
- It would be nice if different stopping criteria were analysed.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?)' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
This may also help to understand some of the limitations of this analysis.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
"And the analysis of the ""dynamic range"" of the algorithim is missing.' [SEP] 'rebuttal_reject-criticism"	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
- The premises of the analyses are not very convincing, limiting the significance of the paper.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
2. The authors should provide ablation study and analysis of their CTAugment.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
4. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.” However, they do not show any analysis to support this hypothesis.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
If that's the goal, however, a more detailed error analysis would need to be included.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well)' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
- The analysis has focused on a very simple case of having a linear discriminator which for example in WGAN, forces the first moments to match. How does the analysis extend to more realistic cases?' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.' [SEP] 'rebuttal_reject-criticism	Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.
Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
- no qualitative analysis on how modulation is actually use by the systems.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
"-2 Diversity of additional ""agents"" not analyzed (more below).' [SEP] 'rebuttal_future"	We leave more comprehensive studies on diversity to future work.
- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
I would have liked to see a bit more analysis as to why some pre-training strategies work over others.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
- It would be nice if different stopping criteria were analysed.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?)' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
This may also help to understand some of the limitations of this analysis.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
"And the analysis of the ""dynamic range"" of the algorithim is missing.' [SEP] 'rebuttal_future"	We leave more comprehensive studies on diversity to future work.
- The premises of the analyses are not very convincing, limiting the significance of the paper.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
2. The authors should provide ablation study and analysis of their CTAugment.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
4. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.” However, they do not show any analysis to support this hypothesis.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
If that's the goal, however, a more detailed error analysis would need to be included.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well)' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
- The analysis has focused on a very simple case of having a linear discriminator which for example in WGAN, forces the first moments to match. How does the analysis extend to more realistic cases?' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.' [SEP] 'rebuttal_future	We leave more comprehensive studies on diversity to future work.
Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
- no qualitative analysis on how modulation is actually use by the systems.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
"-2 Diversity of additional ""agents"" not analyzed (more below).' [SEP] 'rebuttal_done"	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
I would have liked to see a bit more analysis as to why some pre-training strategies work over others.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
- It would be nice if different stopping criteria were analysed.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?)' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
This may also help to understand some of the limitations of this analysis.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
"And the analysis of the ""dynamic range"" of the algorithim is missing.' [SEP] 'rebuttal_done"	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
- The premises of the analyses are not very convincing, limiting the significance of the paper.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
2. The authors should provide ablation study and analysis of their CTAugment.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
4. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.” However, they do not show any analysis to support this hypothesis.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
If that's the goal, however, a more detailed error analysis would need to be included.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well)' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
- The analysis has focused on a very simple case of having a linear discriminator which for example in WGAN, forces the first moments to match. How does the analysis extend to more realistic cases?' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
- no qualitative analysis on how modulation is actually use by the systems.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
"-2 Diversity of additional ""agents"" not analyzed (more below).' [SEP] 'rebuttal_answer"	Your interpretation of section 3 is exactly right.
- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
I would have liked to see a bit more analysis as to why some pre-training strategies work over others.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
- It would be nice if different stopping criteria were analysed.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?)' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
This may also help to understand some of the limitations of this analysis.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
"And the analysis of the ""dynamic range"" of the algorithim is missing.' [SEP] 'rebuttal_answer"	Your interpretation of section 3 is exactly right.
- The premises of the analyses are not very convincing, limiting the significance of the paper.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
2. The authors should provide ablation study and analysis of their CTAugment.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
4. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.” However, they do not show any analysis to support this hypothesis.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
If that's the goal, however, a more detailed error analysis would need to be included.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well)' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
- The analysis has focused on a very simple case of having a linear discriminator which for example in WGAN, forces the first moments to match. How does the analysis extend to more realistic cases?' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.' [SEP] 'rebuttal_answer	Your interpretation of section 3 is exactly right.
Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
- no qualitative analysis on how modulation is actually use by the systems.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
"-2 Diversity of additional ""agents"" not analyzed (more below).' [SEP] 'rebuttal_by-cr"	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
I would have liked to see a bit more analysis as to why some pre-training strategies work over others.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
- It would be nice if different stopping criteria were analysed.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?)' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
This may also help to understand some of the limitations of this analysis.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
"And the analysis of the ""dynamic range"" of the algorithim is missing.' [SEP] 'rebuttal_by-cr"	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
- The premises of the analyses are not very convincing, limiting the significance of the paper.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
2. The authors should provide ablation study and analysis of their CTAugment.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
4. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.” However, they do not show any analysis to support this hypothesis.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
If that's the goal, however, a more detailed error analysis would need to be included.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well)' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
- The analysis has focused on a very simple case of having a linear discriminator which for example in WGAN, forces the first moments to match. How does the analysis extend to more realistic cases?' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.' [SEP] 'rebuttal_by-cr	Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.
2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
The selected baselines are not sufficient.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
The improvement from the baselines is also limited.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
little improvements over the baselines or even significantly worse. More importantly,' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
"- Pioneering work is not necessarily equivalent to ""using all the GPUs""' [SEP] 'rebuttal_concede-criticism"	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
I feel the baseline in domain adaptation area is a bit limited.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
For example, [1] could be used to reduce the variance of gradient w.r.t. \phi.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
I believe it will not be great, but I think for completeness, you should add such a baseline.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
6. On CIFAR10 the results seem to be worse that other methods.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9).' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
Minor comment: An interesting line of work is that of [3] which could be included in the discussion.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
This would be an effective baseline to compare. (Correct me if I am wrong here.)' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
- (W3) Baselines for transfer learning: I felt this was another notable oversight.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
- The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP).' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
The selected baselines are not sufficient.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
The improvement from the baselines is also limited.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
little improvements over the baselines or even significantly worse. More importantly,' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
"- Pioneering work is not necessarily equivalent to ""using all the GPUs""' [SEP] 'rebuttal_summary"	As the reviewer can observe the scale of the experimental evaluation is significantly different.
I feel the baseline in domain adaptation area is a bit limited.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
For example, [1] could be used to reduce the variance of gradient w.r.t. \phi.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
I believe it will not be great, but I think for completeness, you should add such a baseline.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
6. On CIFAR10 the results seem to be worse that other methods.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9).' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
Minor comment: An interesting line of work is that of [3] which could be included in the discussion.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
This would be an effective baseline to compare. (Correct me if I am wrong here.)' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
- (W3) Baselines for transfer learning: I felt this was another notable oversight.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
- The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP).' [SEP] 'rebuttal_summary	As the reviewer can observe the scale of the experimental evaluation is significantly different.
2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
The selected baselines are not sufficient.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
The improvement from the baselines is also limited.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
little improvements over the baselines or even significantly worse. More importantly,' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
"- Pioneering work is not necessarily equivalent to ""using all the GPUs""' [SEP] 'rebuttal_answer"	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
I feel the baseline in domain adaptation area is a bit limited.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
For example, [1] could be used to reduce the variance of gradient w.r.t. \phi.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
I believe it will not be great, but I think for completeness, you should add such a baseline.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
6. On CIFAR10 the results seem to be worse that other methods.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9).' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
Minor comment: An interesting line of work is that of [3] which could be included in the discussion.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
This would be an effective baseline to compare. (Correct me if I am wrong here.)' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
- (W3) Baselines for transfer learning: I felt this was another notable oversight.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
- The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP).' [SEP] 'rebuttal_answer	It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.
2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
The selected baselines are not sufficient.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
The improvement from the baselines is also limited.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
little improvements over the baselines or even significantly worse. More importantly,' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
"- Pioneering work is not necessarily equivalent to ""using all the GPUs""' [SEP] 'rebuttal_structuring"	Q4. Updated abstract and performance evaluation.
I feel the baseline in domain adaptation area is a bit limited.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
For example, [1] could be used to reduce the variance of gradient w.r.t. \phi.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
I believe it will not be great, but I think for completeness, you should add such a baseline.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
6. On CIFAR10 the results seem to be worse that other methods.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9).' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
Minor comment: An interesting line of work is that of [3] which could be included in the discussion.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
This would be an effective baseline to compare. (Correct me if I am wrong here.)' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
- (W3) Baselines for transfer learning: I felt this was another notable oversight.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
- The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP).' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
The selected baselines are not sufficient.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
The improvement from the baselines is also limited.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
little improvements over the baselines or even significantly worse. More importantly,' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
"- Pioneering work is not necessarily equivalent to ""using all the GPUs""' [SEP] 'rebuttal_by-cr"	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
I feel the baseline in domain adaptation area is a bit limited.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
For example, [1] could be used to reduce the variance of gradient w.r.t. \phi.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
I believe it will not be great, but I think for completeness, you should add such a baseline.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
6. On CIFAR10 the results seem to be worse that other methods.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9).' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
Minor comment: An interesting line of work is that of [3] which could be included in the discussion.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
This would be an effective baseline to compare. (Correct me if I am wrong here.)' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
- (W3) Baselines for transfer learning: I felt this was another notable oversight.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
- The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP).' [SEP] 'rebuttal_by-cr	Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.
2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
The selected baselines are not sufficient.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
The improvement from the baselines is also limited.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
little improvements over the baselines or even significantly worse. More importantly,' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
"- Pioneering work is not necessarily equivalent to ""using all the GPUs""' [SEP] 'rebuttal_done"	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
I feel the baseline in domain adaptation area is a bit limited.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
For example, [1] could be used to reduce the variance of gradient w.r.t. \phi.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
I believe it will not be great, but I think for completeness, you should add such a baseline.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
6. On CIFAR10 the results seem to be worse that other methods.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9).' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
Minor comment: An interesting line of work is that of [3] which could be included in the discussion.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
This would be an effective baseline to compare. (Correct me if I am wrong here.)' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
- (W3) Baselines for transfer learning: I felt this was another notable oversight.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
- The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP).' [SEP] 'rebuttal_done	"We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."
2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
The selected baselines are not sufficient.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
The improvement from the baselines is also limited.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
little improvements over the baselines or even significantly worse. More importantly,' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
"- Pioneering work is not necessarily equivalent to ""using all the GPUs""' [SEP] 'rebuttal_social"	Thank you for pointing out paper [3].
I feel the baseline in domain adaptation area is a bit limited.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
For example, [1] could be used to reduce the variance of gradient w.r.t. \phi.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
I believe it will not be great, but I think for completeness, you should add such a baseline.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
6. On CIFAR10 the results seem to be worse that other methods.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9).' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
Minor comment: An interesting line of work is that of [3] which could be included in the discussion.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
This would be an effective baseline to compare. (Correct me if I am wrong here.)' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
- (W3) Baselines for transfer learning: I felt this was another notable oversight.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
- The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP).' [SEP] 'rebuttal_social	Thank you for pointing out paper [3].
In general, the space that was used to explain the Transformer baselines---which are essentially straightforward ways to adapt transformers to this task---could have been used to give more detail on the dataset.' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
1. What are the key limitations of AutoLoss ? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ?' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
Does the discriminator exclude the poisoning data according to certain rule?' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
But there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.), not clear' [SEP] 'rebuttal_concede-criticism	We agree much detail on embeddings can be condensed or moved to Appendix.
In general, the space that was used to explain the Transformer baselines---which are essentially straightforward ways to adapt transformers to this task---could have been used to give more detail on the dataset.' [SEP] 'rebuttal_done	We haved add the above discussion to the latest version as Appendix A.9.
1. What are the key limitations of AutoLoss ? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ?' [SEP] 'rebuttal_done	We haved add the above discussion to the latest version as Appendix A.9.
Does the discriminator exclude the poisoning data according to certain rule?' [SEP] 'rebuttal_done	We haved add the above discussion to the latest version as Appendix A.9.
But there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.), not clear' [SEP] 'rebuttal_done	We haved add the above discussion to the latest version as Appendix A.9.
In general, the space that was used to explain the Transformer baselines---which are essentially straightforward ways to adapt transformers to this task---could have been used to give more detail on the dataset.' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
1. What are the key limitations of AutoLoss ? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
Does the discriminator exclude the poisoning data according to certain rule?' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
But there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.), not clear' [SEP] 'rebuttal_future	We leave it as a future work to study where the clear boundary is.
"Also, a ""1x"" label seems to be missing in for the full softmax, so that the reference is clearly specified.' [SEP] 'rebuttal_answer"	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
In a related note, using MF for training BMs have been proposed previously and found to not work due to various reasons:' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
- The paper makes use of a result from the David MacKay textbook' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
- It is not clear why authors did not follow the evaluation protocol of [Achlioptas’17] or [Wu’16] more closely.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
You probably have to limit the operation to a half-sphere (there's some ideas for this in Gu et al).' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
* The paper states multiple times that VAEAC [Ivanov et al., 2019] cannot handle partially missing data, but I don’t think this is true, since their missing features imputation experiment uses the setup of 50% truly missing features.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
What is the point that you are trying to make? Also, note that some of the algorithms that you are citing there have indeed applied beyond architecture search, eg. Bayesian optimization is used for gait optimization in robotics, and Genetic algorithms have been used for automatic robot design.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
#3 is so generic that a large part of the previous literature on the topic fall under this category -- not new.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
The cited Berrett and Samworth MI test uses a permutation approach to obtaining the test threshold, not an asymptotic approach  (see the results of Section 4 of that paper).' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
"8. The itemized part in 5.3, ""...carefully selected baselines: 1.xxx, 2.xxx, 3. xxx, 4. xxx"". However, both 3 and 4 are not baselines!' [SEP] 'rebuttal_answer"	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
Lee et al. seem to make similar mistakes, and it is likely that their experimental design is also flawed.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
Specifically, prior work considered non-missing data during training, while we can't always guarantee that all the modalities are available.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
Specifically, the author mentioned Tsai et al. assumed factorized latent variables from the multimodal data, while Tsai et al. actually assumed the generation of multimodal data consists of disentangled modality-specific and multimodal factors.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
It is not even clear that the final compression of the baselines would not be better.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
Finally yet importantly, though a large number of works have been proposed to try to solve this problem especially the catastrophic forgetting, most of these works are heuristic and lack mathematical proof, and thus have no guarantee on new tasks or scenarios.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
There has been a large body of work which shows that weaker attacks like membership attacks can be equally damaging, ii) Privacy is a worst-case guarantee.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
The reported FID for CIFAR10 using WGAN-GP is 54.4, which seems to be a bit high.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
Also, the authors should compare with few more graphnet + transmission layer (GraphNetST) baselines with the graph layers: $P(X)_i = Ax_i + \sum_{j \in N(x_i, X)} Bx_j + c$ and the same single transmission layer $\mathbf{1}\mathbf{1}^TXB$ in PointNetST.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
PointNet is a specialization of graphnets and GraphNetST should be added as a baseline with reasonable adjacency.' [SEP] 'rebuttal_answer	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
"* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a ""domain confusion loss""), contrary to what is claimed in the introduction.' [SEP] 'rebuttal_answer"	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
"For example, ""The old system of private arbitration courts is off the table"" from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture.' [SEP] 'rebuttal_answer"	"Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."
"Also, a ""1x"" label seems to be missing in for the full softmax, so that the reference is clearly specified.' [SEP] 'rebuttal_structuring"	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
In a related note, using MF for training BMs have been proposed previously and found to not work due to various reasons:' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
- The paper makes use of a result from the David MacKay textbook' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
- It is not clear why authors did not follow the evaluation protocol of [Achlioptas’17] or [Wu’16] more closely.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
You probably have to limit the operation to a half-sphere (there's some ideas for this in Gu et al).' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
* The paper states multiple times that VAEAC [Ivanov et al., 2019] cannot handle partially missing data, but I don’t think this is true, since their missing features imputation experiment uses the setup of 50% truly missing features.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
What is the point that you are trying to make? Also, note that some of the algorithms that you are citing there have indeed applied beyond architecture search, eg. Bayesian optimization is used for gait optimization in robotics, and Genetic algorithms have been used for automatic robot design.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
#3 is so generic that a large part of the previous literature on the topic fall under this category -- not new.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
The cited Berrett and Samworth MI test uses a permutation approach to obtaining the test threshold, not an asymptotic approach  (see the results of Section 4 of that paper).' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
"8. The itemized part in 5.3, ""...carefully selected baselines: 1.xxx, 2.xxx, 3. xxx, 4. xxx"". However, both 3 and 4 are not baselines!' [SEP] 'rebuttal_structuring"	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
Lee et al. seem to make similar mistakes, and it is likely that their experimental design is also flawed.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
Specifically, prior work considered non-missing data during training, while we can't always guarantee that all the modalities are available.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
Specifically, the author mentioned Tsai et al. assumed factorized latent variables from the multimodal data, while Tsai et al. actually assumed the generation of multimodal data consists of disentangled modality-specific and multimodal factors.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
It is not even clear that the final compression of the baselines would not be better.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
Finally yet importantly, though a large number of works have been proposed to try to solve this problem especially the catastrophic forgetting, most of these works are heuristic and lack mathematical proof, and thus have no guarantee on new tasks or scenarios.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
There has been a large body of work which shows that weaker attacks like membership attacks can be equally damaging, ii) Privacy is a worst-case guarantee.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
The reported FID for CIFAR10 using WGAN-GP is 54.4, which seems to be a bit high.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
Also, the authors should compare with few more graphnet + transmission layer (GraphNetST) baselines with the graph layers: $P(X)_i = Ax_i + \sum_{j \in N(x_i, X)} Bx_j + c$ and the same single transmission layer $\mathbf{1}\mathbf{1}^TXB$ in PointNetST.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
PointNet is a specialization of graphnets and GraphNetST should be added as a baseline with reasonable adjacency.' [SEP] 'rebuttal_structuring	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
"* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a ""domain confusion loss""), contrary to what is claimed in the introduction.' [SEP] 'rebuttal_structuring"	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
"For example, ""The old system of private arbitration courts is off the table"" from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture.' [SEP] 'rebuttal_structuring"	"Concerning the point "" It is not even clear that the final compression of the baselines would not be better."
"Also, a ""1x"" label seems to be missing in for the full softmax, so that the reference is clearly specified.' [SEP] 'rebuttal_concede-criticism"	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
In a related note, using MF for training BMs have been proposed previously and found to not work due to various reasons:' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
- The paper makes use of a result from the David MacKay textbook' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
- It is not clear why authors did not follow the evaluation protocol of [Achlioptas’17] or [Wu’16] more closely.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
You probably have to limit the operation to a half-sphere (there's some ideas for this in Gu et al).' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
* The paper states multiple times that VAEAC [Ivanov et al., 2019] cannot handle partially missing data, but I don’t think this is true, since their missing features imputation experiment uses the setup of 50% truly missing features.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
What is the point that you are trying to make? Also, note that some of the algorithms that you are citing there have indeed applied beyond architecture search, eg. Bayesian optimization is used for gait optimization in robotics, and Genetic algorithms have been used for automatic robot design.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
#3 is so generic that a large part of the previous literature on the topic fall under this category -- not new.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
The cited Berrett and Samworth MI test uses a permutation approach to obtaining the test threshold, not an asymptotic approach  (see the results of Section 4 of that paper).' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
"8. The itemized part in 5.3, ""...carefully selected baselines: 1.xxx, 2.xxx, 3. xxx, 4. xxx"". However, both 3 and 4 are not baselines!' [SEP] 'rebuttal_concede-criticism"	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
Lee et al. seem to make similar mistakes, and it is likely that their experimental design is also flawed.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
Specifically, prior work considered non-missing data during training, while we can't always guarantee that all the modalities are available.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
Specifically, the author mentioned Tsai et al. assumed factorized latent variables from the multimodal data, while Tsai et al. actually assumed the generation of multimodal data consists of disentangled modality-specific and multimodal factors.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
It is not even clear that the final compression of the baselines would not be better.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
Finally yet importantly, though a large number of works have been proposed to try to solve this problem especially the catastrophic forgetting, most of these works are heuristic and lack mathematical proof, and thus have no guarantee on new tasks or scenarios.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
There has been a large body of work which shows that weaker attacks like membership attacks can be equally damaging, ii) Privacy is a worst-case guarantee.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
The reported FID for CIFAR10 using WGAN-GP is 54.4, which seems to be a bit high.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
Also, the authors should compare with few more graphnet + transmission layer (GraphNetST) baselines with the graph layers: $P(X)_i = Ax_i + \sum_{j \in N(x_i, X)} Bx_j + c$ and the same single transmission layer $\mathbf{1}\mathbf{1}^TXB$ in PointNetST.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
PointNet is a specialization of graphnets and GraphNetST should be added as a baseline with reasonable adjacency.' [SEP] 'rebuttal_concede-criticism	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
"* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a ""domain confusion loss""), contrary to what is claimed in the introduction.' [SEP] 'rebuttal_concede-criticism"	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
"For example, ""The old system of private arbitration courts is off the table"" from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture.' [SEP] 'rebuttal_concede-criticism"	However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.
"Also, a ""1x"" label seems to be missing in for the full softmax, so that the reference is clearly specified.' [SEP] 'rebuttal_done"	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
In a related note, using MF for training BMs have been proposed previously and found to not work due to various reasons:' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
- The paper makes use of a result from the David MacKay textbook' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
- It is not clear why authors did not follow the evaluation protocol of [Achlioptas’17] or [Wu’16] more closely.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
You probably have to limit the operation to a half-sphere (there's some ideas for this in Gu et al).' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
* The paper states multiple times that VAEAC [Ivanov et al., 2019] cannot handle partially missing data, but I don’t think this is true, since their missing features imputation experiment uses the setup of 50% truly missing features.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
What is the point that you are trying to make? Also, note that some of the algorithms that you are citing there have indeed applied beyond architecture search, eg. Bayesian optimization is used for gait optimization in robotics, and Genetic algorithms have been used for automatic robot design.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
#3 is so generic that a large part of the previous literature on the topic fall under this category -- not new.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
The cited Berrett and Samworth MI test uses a permutation approach to obtaining the test threshold, not an asymptotic approach  (see the results of Section 4 of that paper).' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
"8. The itemized part in 5.3, ""...carefully selected baselines: 1.xxx, 2.xxx, 3. xxx, 4. xxx"". However, both 3 and 4 are not baselines!' [SEP] 'rebuttal_done"	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
Lee et al. seem to make similar mistakes, and it is likely that their experimental design is also flawed.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
Specifically, prior work considered non-missing data during training, while we can't always guarantee that all the modalities are available.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
Specifically, the author mentioned Tsai et al. assumed factorized latent variables from the multimodal data, while Tsai et al. actually assumed the generation of multimodal data consists of disentangled modality-specific and multimodal factors.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
It is not even clear that the final compression of the baselines would not be better.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
Finally yet importantly, though a large number of works have been proposed to try to solve this problem especially the catastrophic forgetting, most of these works are heuristic and lack mathematical proof, and thus have no guarantee on new tasks or scenarios.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
There has been a large body of work which shows that weaker attacks like membership attacks can be equally damaging, ii) Privacy is a worst-case guarantee.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
The reported FID for CIFAR10 using WGAN-GP is 54.4, which seems to be a bit high.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
Also, the authors should compare with few more graphnet + transmission layer (GraphNetST) baselines with the graph layers: $P(X)_i = Ax_i + \sum_{j \in N(x_i, X)} Bx_j + c$ and the same single transmission layer $\mathbf{1}\mathbf{1}^TXB$ in PointNetST.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
PointNet is a specialization of graphnets and GraphNetST should be added as a baseline with reasonable adjacency.' [SEP] 'rebuttal_done	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
"* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a ""domain confusion loss""), contrary to what is claimed in the introduction.' [SEP] 'rebuttal_done"	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
"For example, ""The old system of private arbitration courts is off the table"" from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture.' [SEP] 'rebuttal_done"	We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.
- The hyperparameter selection regime (and the experiments used to find them) is not described' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
3) The paper only conducts comparison experiments with fixed-alpha baselines.' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
4. Can the authors show concrete examples on how the attacks are generated?' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
However it's not clear how is this range used in practice ? Do you sample uniformly $\alpha$ in this range to train the linear interpolation ?' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
It would be nice if the authors pointed to a git repository with their code an experiments.' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
"2)	It is not clear what the “replicates” refer to in the experiments.' [SEP] 'rebuttal_structuring"	It would be nice if the authors pointed to a git repository with their code an experiments.
Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
I think all claims about running time should be corroborated by controlled experiments.' [SEP] 'rebuttal_structuring	It would be nice if the authors pointed to a git repository with their code an experiments.
- The hyperparameter selection regime (and the experiments used to find them) is not described' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
3) The paper only conducts comparison experiments with fixed-alpha baselines.' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
4. Can the authors show concrete examples on how the attacks are generated?' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
However it's not clear how is this range used in practice ? Do you sample uniformly $\alpha$ in this range to train the linear interpolation ?' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
It would be nice if the authors pointed to a git repository with their code an experiments.' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
"2)	It is not clear what the “replicates” refer to in the experiments.' [SEP] 'rebuttal_answer"	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
I think all claims about running time should be corroborated by controlled experiments.' [SEP] 'rebuttal_answer	A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;
- The hyperparameter selection regime (and the experiments used to find them) is not described' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
3) The paper only conducts comparison experiments with fixed-alpha baselines.' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
4. Can the authors show concrete examples on how the attacks are generated?' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
However it's not clear how is this range used in practice ? Do you sample uniformly $\alpha$ in this range to train the linear interpolation ?' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
It would be nice if the authors pointed to a git repository with their code an experiments.' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
"2)	It is not clear what the “replicates” refer to in the experiments.' [SEP] 'rebuttal_by-cr"	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
I think all claims about running time should be corroborated by controlled experiments.' [SEP] 'rebuttal_by-cr	We’ll include more details about hyperparameters and hyperparameter selection in any future revision.
- The hyperparameter selection regime (and the experiments used to find them) is not described' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
3) The paper only conducts comparison experiments with fixed-alpha baselines.' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
4. Can the authors show concrete examples on how the attacks are generated?' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
However it's not clear how is this range used in practice ? Do you sample uniformly $\alpha$ in this range to train the linear interpolation ?' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
It would be nice if the authors pointed to a git repository with their code an experiments.' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
"2)	It is not clear what the “replicates” refer to in the experiments.' [SEP] 'rebuttal_done"	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
I think all claims about running time should be corroborated by controlled experiments.' [SEP] 'rebuttal_done	We have also provided more details about LOAN dataset and how we attack in Appendix A.1.
"* It is unclear to me whether the ""efficient method for SN in convolutional nets"" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides.' [SEP] 'rebuttal_done"	We renamed all the models based on the original papers and their properties.
The difference with the other reference model (SVG) is less clear.' [SEP] 'rebuttal_done	We renamed all the models based on the original papers and their properties.
I agree that local SO(2) invariance is too limiting. But it is not true that rotating filters is not effective in planar/volumetric CNNs, as shown by many recent papers on equivariant networks.' [SEP] 'rebuttal_done	We renamed all the models based on the original papers and their properties.
Since the paper relies so heavily on Lee et al., the authors should make an effort to summarize the approach in a mathematically precise way.' [SEP] 'rebuttal_done	We renamed all the models based on the original papers and their properties.
In particular, the exact difference between the proposed method and the ES baseline is not as clear as it could be.' [SEP] 'rebuttal_done	We renamed all the models based on the original papers and their properties.
Otherwise, it is impossible for a reader to correctly and objectively relate your proposed approach to previous literature.' [SEP] 'rebuttal_done	We renamed all the models based on the original papers and their properties.
"- The parameter count of a 2-layer network with $h$ hidden units and input dimension $d$ is $O(dh)$. So perhaps it makes sense to study an asymptotic regime where $dh/n$ approaches $\gamma$, instead of both d and h growing linearly in n. While this issue is hinted at in the discussion section, I don't understand the statement ""the mechanism that provably gives rise to double descent from previous works Hastie et al. (2019); Belkin et al. (2019) might not translate to optimizing two-layer neural networks.""' [SEP] 'rebuttal_done"	We renamed all the models based on the original papers and their properties.
Without the comparison it’s not clear how much improvement this approach provides compared to existing work that perform stale updates.' [SEP] 'rebuttal_done	We renamed all the models based on the original papers and their properties.
Although some promising' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
Hence, I kindly do not think the outcome is truly a research result.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
As the authors admit, the main result is not especially surprising.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
The results are not strong. And, unfortunately, the model contribution currently is too modest.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
Although I personally enjoyed reading the results that from control theory perspective are inline with GAN literature, the paper does not provide novel surprising results.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
However, this limits the novelty of the results relative to existing literature.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors:' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
It was kind of surprising that the authors did not cite this paper given the results are pretty much the same.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
- Page 7 offers a result (expression for the Dirichlet form), which is hardly more than an exercise for anybody familiar with Markov Chains theory.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
Again, this follows from known results.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
Many of the results have been already presented in' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
While I wish there were more such studies -- as I believe reproducing past results experimentally is important, and so is providing practical advice for practitioners -- this work in many parts hard to follow, and it is hard to get lot of new insight from the results, or a better understanding of GANs.' [SEP] 'rebuttal_answer	We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.
Although some promising' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
Hence, I kindly do not think the outcome is truly a research result.' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
As the authors admit, the main result is not especially surprising.' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
The results are not strong. And, unfortunately, the model contribution currently is too modest.' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations.' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
Although I personally enjoyed reading the results that from control theory perspective are inline with GAN literature, the paper does not provide novel surprising results.' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
However, this limits the novelty of the results relative to existing literature.' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors:' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
It was kind of surprising that the authors did not cite this paper given the results are pretty much the same.' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper.' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
- Page 7 offers a result (expression for the Dirichlet form), which is hardly more than an exercise for anybody familiar with Markov Chains theory.' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
Again, this follows from known results.' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
Many of the results have been already presented in' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
While I wish there were more such studies -- as I believe reproducing past results experimentally is important, and so is providing practical advice for practitioners -- this work in many parts hard to follow, and it is hard to get lot of new insight from the results, or a better understanding of GANs.' [SEP] 'rebuttal_structuring	> “The results are not strong. And, unfortunately, the model contribution currently is too modest.”
Although some promising' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
Hence, I kindly do not think the outcome is truly a research result.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
As the authors admit, the main result is not especially surprising.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
The results are not strong. And, unfortunately, the model contribution currently is too modest.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
Although I personally enjoyed reading the results that from control theory perspective are inline with GAN literature, the paper does not provide novel surprising results.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
However, this limits the novelty of the results relative to existing literature.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors:' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
It was kind of surprising that the authors did not cite this paper given the results are pretty much the same.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
- Page 7 offers a result (expression for the Dirichlet form), which is hardly more than an exercise for anybody familiar with Markov Chains theory.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
Again, this follows from known results.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
Many of the results have been already presented in' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
While I wish there were more such studies -- as I believe reproducing past results experimentally is important, and so is providing practical advice for practitioners -- this work in many parts hard to follow, and it is hard to get lot of new insight from the results, or a better understanding of GANs.' [SEP] 'rebuttal_reject-criticism	Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.
Although some promising' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
Hence, I kindly do not think the outcome is truly a research result.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
As the authors admit, the main result is not especially surprising.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
The results are not strong. And, unfortunately, the model contribution currently is too modest.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
Although I personally enjoyed reading the results that from control theory perspective are inline with GAN literature, the paper does not provide novel surprising results.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
However, this limits the novelty of the results relative to existing literature.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors:' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
It was kind of surprising that the authors did not cite this paper given the results are pretty much the same.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
- Page 7 offers a result (expression for the Dirichlet form), which is hardly more than an exercise for anybody familiar with Markov Chains theory.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
Again, this follows from known results.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
Many of the results have been already presented in' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
While I wish there were more such studies -- as I believe reproducing past results experimentally is important, and so is providing practical advice for practitioners -- this work in many parts hard to follow, and it is hard to get lot of new insight from the results, or a better understanding of GANs.' [SEP] 'rebuttal_accept-praise	A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.
Although some promising' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
Hence, I kindly do not think the outcome is truly a research result.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
As the authors admit, the main result is not especially surprising.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
The results are not strong. And, unfortunately, the model contribution currently is too modest.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
Although I personally enjoyed reading the results that from control theory perspective are inline with GAN literature, the paper does not provide novel surprising results.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
However, this limits the novelty of the results relative to existing literature.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors:' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
It was kind of surprising that the authors did not cite this paper given the results are pretty much the same.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
- Page 7 offers a result (expression for the Dirichlet form), which is hardly more than an exercise for anybody familiar with Markov Chains theory.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
Again, this follows from known results.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
Many of the results have been already presented in' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
While I wish there were more such studies -- as I believe reproducing past results experimentally is important, and so is providing practical advice for practitioners -- this work in many parts hard to follow, and it is hard to get lot of new insight from the results, or a better understanding of GANs.' [SEP] 'rebuttal_mitigate-criticism	As we explained at the common response, we started our research from clear open questions.
- In Table 2 and 3, how are the degree and block information leveraged into the model?' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
"It is also unclear how the calculation of relative entropy ""D"" was performed in figure 3.' [SEP] 'rebuttal_done"	Thanks for this; we have updated the draft to make the presentation clearer.
1. Where is L_da in Figure 2? In Figure 2, what’s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- for Figure 6, there is not a clear conclusion.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- in section 4.3 how is the reconstruction built (Figure 3b)?' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
(d) In Figure 2, it is unclear to me how the 1/f^2 law is observed in (a) but not in (c) or (e).' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
It is not clear how the noise is introduced in the graphs.' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
* \sigma is not given in Figure 3(a)' [SEP] 'rebuttal_done	Thanks for this; we have updated the draft to make the presentation clearer.
- In Table 2 and 3, how are the degree and block information leveraged into the model?' [SEP] 'rebuttal_summary	There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.
"It is also unclear how the calculation of relative entropy ""D"" was performed in figure 3.' [SEP] 'rebuttal_summary"	There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.
1. Where is L_da in Figure 2? In Figure 2, what’s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?' [SEP] 'rebuttal_summary	There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.
- for Figure 6, there is not a clear conclusion.' [SEP] 'rebuttal_summary	There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.
- in section 4.3 how is the reconstruction built (Figure 3b)?' [SEP] 'rebuttal_summary	There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.
I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?' [SEP] 'rebuttal_summary	There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.
(d) In Figure 2, it is unclear to me how the 1/f^2 law is observed in (a) but not in (c) or (e).' [SEP] 'rebuttal_summary	There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.
It is not clear how the noise is introduced in the graphs.' [SEP] 'rebuttal_summary	There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.
* \sigma is not given in Figure 3(a)' [SEP] 'rebuttal_summary	There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.
First, many details are missing, especially in the experiments, which makes the proposed method suspicious and non-convincing.' [SEP] 'rebuttal_answer	Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)
1. While I see the value in designing an automatic exploration mechanism, the complexity of the underlying approach makes the contribution of the bandit-based algorithm difficult to discern from the large number of other bells and whistles in the experiments.' [SEP] 'rebuttal_answer	Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)
-The experimental section do not clarify the benefits of the proposed approach.' [SEP] 'rebuttal_answer	Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)
Why is the random seed being used to compare the performance of different arms? Do you instead mean that s and s’ are two values of the arm in Figure 4?' [SEP] 'rebuttal_structuring	“Judging from Table 1, the proposed method does not seem to provide a large contribution.
- Judging from Table 1, the proposed method does not seem to provide a large contribution.' [SEP] 'rebuttal_structuring	“Judging from Table 1, the proposed method does not seem to provide a large contribution.
-  From the plots of learning curves in appendix, the proposed methods doesn’t seem to show a huge boost of performance comparing to the uniform bandit. Could you show aggregated comparison between the proposed method and uniform bandit similarly to what is done in Figure 4 ?' [SEP] 'rebuttal_structuring	“Judging from Table 1, the proposed method does not seem to provide a large contribution.
In Fig. 3 the author claim that the proposed method dominates the other methods in terms of privacy and utility but this is not correct.' [SEP] 'rebuttal_structuring	“Judging from Table 1, the proposed method does not seem to provide a large contribution.
1. The presentation is somewhat convoluted.' [SEP] 'rebuttal_by-cr	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
I have some concerns on this paper:' [SEP] 'rebuttal_by-cr	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
Quality: Below average' [SEP] 'rebuttal_by-cr	The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
In addition, the results seem very weak.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Perhaps the results would be a little more convincing if additional common word embeddings were also tested.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
But the paper only provides empirical results on sentimental analysis and digit recognition.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
* The biggest problem for me was the unconvincing results.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
- The conclusions focus on the importance of section 3 and' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Results on more scenes will make the performance more convincing.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
- it's better to show time v.s. testing accuracy as well.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
From the perspective of a purely technical contribution, there are fewer exciting results.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Thus, it is hard to say whether the results are applicable in practice.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
The unsupervised results are more interesting but not very much explored (a single set of sampled faces).' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
iv) Finally, the reported results are mostly qualitative.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
- No large corpus results.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
However such problems are entirely missing in the results section.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
1) Provide stronger empirical results (these are not too convincing).' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
something that is either deterministic, or a probabilistic result with a small' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
4. How sensitive are the results to the number of adaptive kernels in the layers.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
However, the results are not enough to be accepted to ICLR having a very high standard.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
- The shown inception scores are far from state-of-the-art.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Unfortunately this paper offers only weak results.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
In particular, the qualitative results are too limited and no quantitative evaluations is provided.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
My major concern is whether the results are significant enough to deserve acceptance.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
However, the results are a bit misleading in their reporting of the std error.' [SEP] 'rebuttal_concede-criticism	We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
In addition, the results seem very weak.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Perhaps the results would be a little more convincing if additional common word embeddings were also tested.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
But the paper only provides empirical results on sentimental analysis and digit recognition.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
* The biggest problem for me was the unconvincing results.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
- The conclusions focus on the importance of section 3 and' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Results on more scenes will make the performance more convincing.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
- it's better to show time v.s. testing accuracy as well.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
From the perspective of a purely technical contribution, there are fewer exciting results.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Thus, it is hard to say whether the results are applicable in practice.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
The unsupervised results are more interesting but not very much explored (a single set of sampled faces).' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
iv) Finally, the reported results are mostly qualitative.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
- No large corpus results.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
However such problems are entirely missing in the results section.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
1) Provide stronger empirical results (these are not too convincing).' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
something that is either deterministic, or a probabilistic result with a small' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
4. How sensitive are the results to the number of adaptive kernels in the layers.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
However, the results are not enough to be accepted to ICLR having a very high standard.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
- The shown inception scores are far from state-of-the-art.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Unfortunately this paper offers only weak results.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
In particular, the qualitative results are too limited and no quantitative evaluations is provided.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
My major concern is whether the results are significant enough to deserve acceptance.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
However, the results are a bit misleading in their reporting of the std error.' [SEP] 'rebuttal_answer	Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
In addition, the results seem very weak.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Perhaps the results would be a little more convincing if additional common word embeddings were also tested.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
But the paper only provides empirical results on sentimental analysis and digit recognition.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
* The biggest problem for me was the unconvincing results.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
- The conclusions focus on the importance of section 3 and' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Results on more scenes will make the performance more convincing.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
- it's better to show time v.s. testing accuracy as well.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
From the perspective of a purely technical contribution, there are fewer exciting results.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Thus, it is hard to say whether the results are applicable in practice.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
The unsupervised results are more interesting but not very much explored (a single set of sampled faces).' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
iv) Finally, the reported results are mostly qualitative.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
- No large corpus results.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
However such problems are entirely missing in the results section.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
1) Provide stronger empirical results (these are not too convincing).' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
something that is either deterministic, or a probabilistic result with a small' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
4. How sensitive are the results to the number of adaptive kernels in the layers.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
However, the results are not enough to be accepted to ICLR having a very high standard.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
- The shown inception scores are far from state-of-the-art.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Unfortunately this paper offers only weak results.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
In particular, the qualitative results are too limited and no quantitative evaluations is provided.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
My major concern is whether the results are significant enough to deserve acceptance.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
However, the results are a bit misleading in their reporting of the std error.' [SEP] 'rebuttal_structuring	Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
In addition, the results seem very weak.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Perhaps the results would be a little more convincing if additional common word embeddings were also tested.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
But the paper only provides empirical results on sentimental analysis and digit recognition.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
* The biggest problem for me was the unconvincing results.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
- The conclusions focus on the importance of section 3 and' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Results on more scenes will make the performance more convincing.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
- it's better to show time v.s. testing accuracy as well.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
From the perspective of a purely technical contribution, there are fewer exciting results.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Thus, it is hard to say whether the results are applicable in practice.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
The unsupervised results are more interesting but not very much explored (a single set of sampled faces).' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
iv) Finally, the reported results are mostly qualitative.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
- No large corpus results.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
However such problems are entirely missing in the results section.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
1) Provide stronger empirical results (these are not too convincing).' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
something that is either deterministic, or a probabilistic result with a small' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
4. How sensitive are the results to the number of adaptive kernels in the layers.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
However, the results are not enough to be accepted to ICLR having a very high standard.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
- The shown inception scores are far from state-of-the-art.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Unfortunately this paper offers only weak results.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
In particular, the qualitative results are too limited and no quantitative evaluations is provided.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
My major concern is whether the results are significant enough to deserve acceptance.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
However, the results are a bit misleading in their reporting of the std error.' [SEP] 'rebuttal_reject-criticism	For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
In addition, the results seem very weak.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Perhaps the results would be a little more convincing if additional common word embeddings were also tested.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
But the paper only provides empirical results on sentimental analysis and digit recognition.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
* The biggest problem for me was the unconvincing results.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
- The conclusions focus on the importance of section 3 and' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Results on more scenes will make the performance more convincing.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
- it's better to show time v.s. testing accuracy as well.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
From the perspective of a purely technical contribution, there are fewer exciting results.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Thus, it is hard to say whether the results are applicable in practice.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
The unsupervised results are more interesting but not very much explored (a single set of sampled faces).' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
iv) Finally, the reported results are mostly qualitative.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
- No large corpus results.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
However such problems are entirely missing in the results section.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
1) Provide stronger empirical results (these are not too convincing).' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
something that is either deterministic, or a probabilistic result with a small' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
4. How sensitive are the results to the number of adaptive kernels in the layers.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
However, the results are not enough to be accepted to ICLR having a very high standard.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
- The shown inception scores are far from state-of-the-art.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Unfortunately this paper offers only weak results.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
In particular, the qualitative results are too limited and no quantitative evaluations is provided.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
My major concern is whether the results are significant enough to deserve acceptance.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
However, the results are a bit misleading in their reporting of the std error.' [SEP] 'rebuttal_done	We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
In addition, the results seem very weak.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Perhaps the results would be a little more convincing if additional common word embeddings were also tested.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
But the paper only provides empirical results on sentimental analysis and digit recognition.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
* The biggest problem for me was the unconvincing results.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
- The conclusions focus on the importance of section 3 and' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Results on more scenes will make the performance more convincing.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
- it's better to show time v.s. testing accuracy as well.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
From the perspective of a purely technical contribution, there are fewer exciting results.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Thus, it is hard to say whether the results are applicable in practice.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
The unsupervised results are more interesting but not very much explored (a single set of sampled faces).' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
iv) Finally, the reported results are mostly qualitative.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
- No large corpus results.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
However such problems are entirely missing in the results section.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
1) Provide stronger empirical results (these are not too convincing).' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
something that is either deterministic, or a probabilistic result with a small' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
4. How sensitive are the results to the number of adaptive kernels in the layers.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
However, the results are not enough to be accepted to ICLR having a very high standard.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
- The shown inception scores are far from state-of-the-art.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Unfortunately this paper offers only weak results.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
In particular, the qualitative results are too limited and no quantitative evaluations is provided.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
My major concern is whether the results are significant enough to deserve acceptance.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
However, the results are a bit misleading in their reporting of the std error.' [SEP] 'rebuttal_by-cr	3)For the experiment: we will spend some time to train GANs with more iteration and modify it.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
In addition, the results seem very weak.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Perhaps the results would be a little more convincing if additional common word embeddings were also tested.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
But the paper only provides empirical results on sentimental analysis and digit recognition.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
* The biggest problem for me was the unconvincing results.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
- The conclusions focus on the importance of section 3 and' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Results on more scenes will make the performance more convincing.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
- it's better to show time v.s. testing accuracy as well.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
From the perspective of a purely technical contribution, there are fewer exciting results.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Thus, it is hard to say whether the results are applicable in practice.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
The unsupervised results are more interesting but not very much explored (a single set of sampled faces).' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
iv) Finally, the reported results are mostly qualitative.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
- No large corpus results.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
However such problems are entirely missing in the results section.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
1) Provide stronger empirical results (these are not too convincing).' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
something that is either deterministic, or a probabilistic result with a small' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
4. How sensitive are the results to the number of adaptive kernels in the layers.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
However, the results are not enough to be accepted to ICLR having a very high standard.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
- The shown inception scores are far from state-of-the-art.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Unfortunately this paper offers only weak results.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
In particular, the qualitative results are too limited and no quantitative evaluations is provided.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
My major concern is whether the results are significant enough to deserve acceptance.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
However, the results are a bit misleading in their reporting of the std error.' [SEP] 'rebuttal_contradict-assertion	Please check the degraded images in Table 3.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:' [SEP] 'rebuttal_social	A: Thanks for the review comment.
In addition, the results seem very weak.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Perhaps the results would be a little more convincing if additional common word embeddings were also tested.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
But the paper only provides empirical results on sentimental analysis and digit recognition.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
* The biggest problem for me was the unconvincing results.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
- The conclusions focus on the importance of section 3 and' [SEP] 'rebuttal_social	A: Thanks for the review comment.
If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).' [SEP] 'rebuttal_social	A: Thanks for the review comment.
I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Results on more scenes will make the performance more convincing.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
- it's better to show time v.s. testing accuracy as well.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
From the perspective of a purely technical contribution, there are fewer exciting results.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Thus, it is hard to say whether the results are applicable in practice.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
The unsupervised results are more interesting but not very much explored (a single set of sampled faces).' [SEP] 'rebuttal_social	A: Thanks for the review comment.
It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
iv) Finally, the reported results are mostly qualitative.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
- No large corpus results.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
However such problems are entirely missing in the results section.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].' [SEP] 'rebuttal_social	A: Thanks for the review comment.
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).' [SEP] 'rebuttal_social	A: Thanks for the review comment.
1) Provide stronger empirical results (these are not too convincing).' [SEP] 'rebuttal_social	A: Thanks for the review comment.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
something that is either deterministic, or a probabilistic result with a small' [SEP] 'rebuttal_social	A: Thanks for the review comment.
4. How sensitive are the results to the number of adaptive kernels in the layers.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
However, the results are not enough to be accepted to ICLR having a very high standard.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
- The shown inception scores are far from state-of-the-art.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Unfortunately this paper offers only weak results.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
In particular, the qualitative results are too limited and no quantitative evaluations is provided.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
My major concern is whether the results are significant enough to deserve acceptance.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).' [SEP] 'rebuttal_social	A: Thanks for the review comment.
I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
However, the results are a bit misleading in their reporting of the std error.' [SEP] 'rebuttal_social	A: Thanks for the review comment.
The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
In addition, the results seem very weak.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Perhaps the results would be a little more convincing if additional common word embeddings were also tested.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
But the paper only provides empirical results on sentimental analysis and digit recognition.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
* The biggest problem for me was the unconvincing results.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
- The conclusions focus on the importance of section 3 and' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Results on more scenes will make the performance more convincing.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
- it's better to show time v.s. testing accuracy as well.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
From the perspective of a purely technical contribution, there are fewer exciting results.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Thus, it is hard to say whether the results are applicable in practice.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
The unsupervised results are more interesting but not very much explored (a single set of sampled faces).' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
iv) Finally, the reported results are mostly qualitative.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
- No large corpus results.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
However such problems are entirely missing in the results section.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
1) Provide stronger empirical results (these are not too convincing).' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
More importantly, the results presented are quite meager.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
something that is either deterministic, or a probabilistic result with a small' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
4. How sensitive are the results to the number of adaptive kernels in the layers.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
However, the results are not enough to be accepted to ICLR having a very high standard.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
- The shown inception scores are far from state-of-the-art.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Unfortunately this paper offers only weak results.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
In particular, the qualitative results are too limited and no quantitative evaluations is provided.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
My major concern is whether the results are significant enough to deserve acceptance.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
However, the results are a bit misleading in their reporting of the std error.' [SEP] 'rebuttal_reject-request	Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.
- the data sets used in the experiments are very small' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion).' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
"2.	The experimental data set is too small, with only 635 problems.' [SEP] 'rebuttal_summary"	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, the selected experiments on the cubic regression toy data (Section 5.1) and the out-of-distribution classification (Section 5.2) are clear examples of system’s noise, i.e. epistemic uncertainty.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
I believe that a more challenging experiment should be conducted e.g. using celebA dataset.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
The empirical evaluation is quite weak- one sparsity setting, two baselines, one dataset' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Since the paper focuses explicitly on providing contrastive explanations for choosing a class A over another class B -- experiments on datasets which do not have real-images seem insufficient.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Regarding contrastive explanations, experiments on datasets where distractor classes (y_probe) are present in addition to the class interest (y_true) seem important -- PASCAL VOC, COCO, etc.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
Three datasets cannot make the experiments convincing.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
It would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
More experiments based on other types of data sets with clear global structures such as faces or stop signs will' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
More experiments on datasets' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
2. Were there experiments that applies your metric to the training datasets like CelebA and FFHQ? In theory your metric should show no gap between N_R_obs and N_R_ref measured on the training dataset since that's the sampled ground truth.' [SEP] 'rebuttal_summary	This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.
- the data sets used in the experiments are very small' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion).' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
"2.	The experimental data set is too small, with only 635 problems.' [SEP] 'rebuttal_structuring"	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
However, the selected experiments on the cubic regression toy data (Section 5.1) and the out-of-distribution classification (Section 5.2) are clear examples of system’s noise, i.e. epistemic uncertainty.' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
I believe that a more challenging experiment should be conducted e.g. using celebA dataset.' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
The empirical evaluation is quite weak- one sparsity setting, two baselines, one dataset' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
Since the paper focuses explicitly on providing contrastive explanations for choosing a class A over another class B -- experiments on datasets which do not have real-images seem insufficient.' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
Regarding contrastive explanations, experiments on datasets where distractor classes (y_probe) are present in addition to the class interest (y_true) seem important -- PASCAL VOC, COCO, etc.' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
Three datasets cannot make the experiments convincing.' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
It would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance.' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
More experiments based on other types of data sets with clear global structures such as faces or stop signs will' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
More experiments on datasets' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
2. Were there experiments that applies your metric to the training datasets like CelebA and FFHQ? In theory your metric should show no gap between N_R_obs and N_R_ref measured on the training dataset since that's the sampled ground truth.' [SEP] 'rebuttal_structuring	> The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.
- the data sets used in the experiments are very small' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion).' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
"2.	The experimental data set is too small, with only 635 problems.' [SEP] 'rebuttal_reject-criticism"	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
However, the selected experiments on the cubic regression toy data (Section 5.1) and the out-of-distribution classification (Section 5.2) are clear examples of system’s noise, i.e. epistemic uncertainty.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
I believe that a more challenging experiment should be conducted e.g. using celebA dataset.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
The empirical evaluation is quite weak- one sparsity setting, two baselines, one dataset' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
Since the paper focuses explicitly on providing contrastive explanations for choosing a class A over another class B -- experiments on datasets which do not have real-images seem insufficient.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
Regarding contrastive explanations, experiments on datasets where distractor classes (y_probe) are present in addition to the class interest (y_true) seem important -- PASCAL VOC, COCO, etc.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
Three datasets cannot make the experiments convincing.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
It would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
More experiments based on other types of data sets with clear global structures such as faces or stop signs will' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
More experiments on datasets' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
2. Were there experiments that applies your metric to the training datasets like CelebA and FFHQ? In theory your metric should show no gap between N_R_obs and N_R_ref measured on the training dataset since that's the sampled ground truth.' [SEP] 'rebuttal_reject-criticism	We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.
- the data sets used in the experiments are very small' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion).' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
"2.	The experimental data set is too small, with only 635 problems.' [SEP] 'rebuttal_done"	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
However, the selected experiments on the cubic regression toy data (Section 5.1) and the out-of-distribution classification (Section 5.2) are clear examples of system’s noise, i.e. epistemic uncertainty.' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
I believe that a more challenging experiment should be conducted e.g. using celebA dataset.' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
The empirical evaluation is quite weak- one sparsity setting, two baselines, one dataset' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
Since the paper focuses explicitly on providing contrastive explanations for choosing a class A over another class B -- experiments on datasets which do not have real-images seem insufficient.' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
Regarding contrastive explanations, experiments on datasets where distractor classes (y_probe) are present in addition to the class interest (y_true) seem important -- PASCAL VOC, COCO, etc.' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
Three datasets cannot make the experiments convincing.' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
It would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance.' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
More experiments based on other types of data sets with clear global structures such as faces or stop signs will' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
More experiments on datasets' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
2. Were there experiments that applies your metric to the training datasets like CelebA and FFHQ? In theory your metric should show no gap between N_R_obs and N_R_ref measured on the training dataset since that's the sampled ground truth.' [SEP] 'rebuttal_done	"You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."
- the data sets used in the experiments are very small' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion).' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
"2.	The experimental data set is too small, with only 635 problems.' [SEP] 'rebuttal_answer"	Moreover, we investigate the mixing distribution learned in Appendix G.
However, the selected experiments on the cubic regression toy data (Section 5.1) and the out-of-distribution classification (Section 5.2) are clear examples of system’s noise, i.e. epistemic uncertainty.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
I believe that a more challenging experiment should be conducted e.g. using celebA dataset.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
The empirical evaluation is quite weak- one sparsity setting, two baselines, one dataset' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
Since the paper focuses explicitly on providing contrastive explanations for choosing a class A over another class B -- experiments on datasets which do not have real-images seem insufficient.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
Regarding contrastive explanations, experiments on datasets where distractor classes (y_probe) are present in addition to the class interest (y_true) seem important -- PASCAL VOC, COCO, etc.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
Three datasets cannot make the experiments convincing.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
It would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
More experiments based on other types of data sets with clear global structures such as faces or stop signs will' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
More experiments on datasets' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
2. Were there experiments that applies your metric to the training datasets like CelebA and FFHQ? In theory your metric should show no gap between N_R_obs and N_R_ref measured on the training dataset since that's the sampled ground truth.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
In particular, it is unclear what the assumption on the size of the unlabelled test set is.' [SEP] 'rebuttal_answer	Please see the last paragraph in page 5.
* The issue of possible false positives due to sample dependence in fMRI data has again been ignored in the rebuttal.' [SEP] 'rebuttal_answer	Please see the last paragraph in page 5.
"For example, I don't understand what does it mean in ""However, if training data is complete, ..... handle during missing data during test."" Another example would be the last few paragraphs on page 4; they are very unclear.' [SEP] 'rebuttal_answer"	Please see the last paragraph in page 5.
"For example, I have trouble understanding the sentence ""So the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets."" in the introduction.' [SEP] 'rebuttal_answer"	Please see the last paragraph in page 5.
6. Validation data / test sets: Throughout this work, it is unclear what / how validation is performed.' [SEP] 'rebuttal_answer	Please see the last paragraph in page 5.
You have 3 datasets worth of data for the regression task (it is still unclear, however, what is being used for evaluation), but it doesn't look like this is addressed in the larger scale experiments at all.' [SEP] 'rebuttal_answer	Please see the last paragraph in page 5.
The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms.' [SEP] 'rebuttal_answer	Please see the last paragraph in page 5.
In particular, it is unclear what the assumption on the size of the unlabelled test set is.' [SEP] 'rebuttal_mitigate-criticism	So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.
* The issue of possible false positives due to sample dependence in fMRI data has again been ignored in the rebuttal.' [SEP] 'rebuttal_mitigate-criticism	So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.
"For example, I don't understand what does it mean in ""However, if training data is complete, ..... handle during missing data during test."" Another example would be the last few paragraphs on page 4; they are very unclear.' [SEP] 'rebuttal_mitigate-criticism"	So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.
"For example, I have trouble understanding the sentence ""So the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets."" in the introduction.' [SEP] 'rebuttal_mitigate-criticism"	So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.
6. Validation data / test sets: Throughout this work, it is unclear what / how validation is performed.' [SEP] 'rebuttal_mitigate-criticism	So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.
You have 3 datasets worth of data for the regression task (it is still unclear, however, what is being used for evaluation), but it doesn't look like this is addressed in the larger scale experiments at all.' [SEP] 'rebuttal_mitigate-criticism	So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.
The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms.' [SEP] 'rebuttal_mitigate-criticism	So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.
The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
"1.	Lack of technical novelty.' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
It seems to me just a combination of several mature techniques.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Although this extension seems to be easily derived using the contributions made at point 2.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information.' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
"In the current form of evaluation, it is hard to say if there is any benefit of using the ""selection network"" that is the main novelty of the paper.' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The authors need to describe in detail the algorithmic novelty of their work.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Therefore, the paper cannot be qualified for ``meta domain adaptation’’ and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The reviewer votes for rejection as the method has limited novelty.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The novelty of this method is minimal.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The theoretical contribution is very limited.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Overall, I like the approach of the proposed method, especially tuning coefficient during training procedure although novelty in the theoretical analysis is somewhat limited.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
It is also not clear how the loss function proposed differs from that of the CDVAE, etc.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- Incremental modeling contribution' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
While I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
However, I believe this is also the case in the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
After all, the former approach gets a lot more knowledge about the target function built into it.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Is it a combination of DGR and HAT with some capacity expansion?' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The remaining components of the proposed method are not very new.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
In summary, I find there is no novelty involved apart from combining the already existing SOTA model in disentangled feature learning (beta-VAE) and image generation (StackGAN).' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
"(1) My main problem with this paper is that the novel objective proposed by the authors in Eq. 7 is equivalent to the objective of WAEs appearing in Eq. 4 of [1] (up to a heuristic of applying logarithm to the divergence measure, which is not justified but meant to ""improve the balance between two terms"", see footnote 2), where the authors use the newly introduced Cramer-Wold divergence as a choice of the penalty term in Eq. 4 of [1].' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
(2) When viewed in this way, CW-distance introduced in Eq (2) closely resembles the unbiased U-statistic estimate of the MMD used in WAE-MMD [1, Algorithm 2].' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times).' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
1) the proof techniques are very standard' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification).' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
It is unclear, why one should use the proposed duality gap GAN.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
(1) using codes and codebooks to compress weights; and' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The combination of these two methods seems straightforward.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
"1.	Lack of technical novelty.' [SEP] 'rebuttal_answer"	We provide clarification for the two main questions of the Reviewer below.
It seems to me just a combination of several mature techniques.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
- The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Although this extension seems to be easily derived using the contributions made at point 2.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information.' [SEP] 'rebuttal_answer"	We provide clarification for the two main questions of the Reviewer below.
"In the current form of evaluation, it is hard to say if there is any benefit of using the ""selection network"" that is the main novelty of the paper.' [SEP] 'rebuttal_answer"	We provide clarification for the two main questions of the Reviewer below.
First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The authors need to describe in detail the algorithmic novelty of their work.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Therefore, the paper cannot be qualified for ``meta domain adaptation’’ and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The reviewer votes for rejection as the method has limited novelty.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The novelty of this method is minimal.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The theoretical contribution is very limited.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Overall, I like the approach of the proposed method, especially tuning coefficient during training procedure although novelty in the theoretical analysis is somewhat limited.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
It is also not clear how the loss function proposed differs from that of the CDVAE, etc.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
- Incremental modeling contribution' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
While I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
However, I believe this is also the case in the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
After all, the former approach gets a lot more knowledge about the target function built into it.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Is it a combination of DGR and HAT with some capacity expansion?' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The remaining components of the proposed method are not very new.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
In summary, I find there is no novelty involved apart from combining the already existing SOTA model in disentangled feature learning (beta-VAE) and image generation (StackGAN).' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
"(1) My main problem with this paper is that the novel objective proposed by the authors in Eq. 7 is equivalent to the objective of WAEs appearing in Eq. 4 of [1] (up to a heuristic of applying logarithm to the divergence measure, which is not justified but meant to ""improve the balance between two terms"", see footnote 2), where the authors use the newly introduced Cramer-Wold divergence as a choice of the penalty term in Eq. 4 of [1].' [SEP] 'rebuttal_answer"	We provide clarification for the two main questions of the Reviewer below.
(2) When viewed in this way, CW-distance introduced in Eq (2) closely resembles the unbiased U-statistic estimate of the MMD used in WAE-MMD [1, Algorithm 2].' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times).' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
1) the proof techniques are very standard' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification).' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
It is unclear, why one should use the proposed duality gap GAN.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
(1) using codes and codebooks to compress weights; and' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The combination of these two methods seems straightforward.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
- The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.' [SEP] 'rebuttal_answer	We provide clarification for the two main questions of the Reviewer below.
The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
"1.	Lack of technical novelty.' [SEP] 'rebuttal_structuring"	1. Comments about the contributions and novelty
It seems to me just a combination of several mature techniques.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
- The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
Although this extension seems to be easily derived using the contributions made at point 2.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information.' [SEP] 'rebuttal_structuring"	1. Comments about the contributions and novelty
"In the current form of evaluation, it is hard to say if there is any benefit of using the ""selection network"" that is the main novelty of the paper.' [SEP] 'rebuttal_structuring"	1. Comments about the contributions and novelty
First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The authors need to describe in detail the algorithmic novelty of their work.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
Therefore, the paper cannot be qualified for ``meta domain adaptation’’ and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The reviewer votes for rejection as the method has limited novelty.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The novelty of this method is minimal.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The theoretical contribution is very limited.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
Overall, I like the approach of the proposed method, especially tuning coefficient during training procedure although novelty in the theoretical analysis is somewhat limited.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
It is also not clear how the loss function proposed differs from that of the CDVAE, etc.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
- Incremental modeling contribution' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
While I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
However, I believe this is also the case in the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
After all, the former approach gets a lot more knowledge about the target function built into it.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
Is it a combination of DGR and HAT with some capacity expansion?' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The remaining components of the proposed method are not very new.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
In summary, I find there is no novelty involved apart from combining the already existing SOTA model in disentangled feature learning (beta-VAE) and image generation (StackGAN).' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
"(1) My main problem with this paper is that the novel objective proposed by the authors in Eq. 7 is equivalent to the objective of WAEs appearing in Eq. 4 of [1] (up to a heuristic of applying logarithm to the divergence measure, which is not justified but meant to ""improve the balance between two terms"", see footnote 2), where the authors use the newly introduced Cramer-Wold divergence as a choice of the penalty term in Eq. 4 of [1].' [SEP] 'rebuttal_structuring"	1. Comments about the contributions and novelty
(2) When viewed in this way, CW-distance introduced in Eq (2) closely resembles the unbiased U-statistic estimate of the MMD used in WAE-MMD [1, Algorithm 2].' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times).' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
1) the proof techniques are very standard' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification).' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
It is unclear, why one should use the proposed duality gap GAN.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
(1) using codes and codebooks to compress weights; and' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The combination of these two methods seems straightforward.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
- The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.' [SEP] 'rebuttal_structuring	1. Comments about the contributions and novelty
The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
"1.	Lack of technical novelty.' [SEP] 'rebuttal_reject-criticism"	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
It seems to me just a combination of several mature techniques.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
- The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
Although this extension seems to be easily derived using the contributions made at point 2.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information.' [SEP] 'rebuttal_reject-criticism"	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
"In the current form of evaluation, it is hard to say if there is any benefit of using the ""selection network"" that is the main novelty of the paper.' [SEP] 'rebuttal_reject-criticism"	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The authors need to describe in detail the algorithmic novelty of their work.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
Therefore, the paper cannot be qualified for ``meta domain adaptation’’ and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The reviewer votes for rejection as the method has limited novelty.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The novelty of this method is minimal.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The theoretical contribution is very limited.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
Overall, I like the approach of the proposed method, especially tuning coefficient during training procedure although novelty in the theoretical analysis is somewhat limited.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
It is also not clear how the loss function proposed differs from that of the CDVAE, etc.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
- Incremental modeling contribution' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
While I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
However, I believe this is also the case in the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
After all, the former approach gets a lot more knowledge about the target function built into it.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
Is it a combination of DGR and HAT with some capacity expansion?' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The remaining components of the proposed method are not very new.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
In summary, I find there is no novelty involved apart from combining the already existing SOTA model in disentangled feature learning (beta-VAE) and image generation (StackGAN).' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
"(1) My main problem with this paper is that the novel objective proposed by the authors in Eq. 7 is equivalent to the objective of WAEs appearing in Eq. 4 of [1] (up to a heuristic of applying logarithm to the divergence measure, which is not justified but meant to ""improve the balance between two terms"", see footnote 2), where the authors use the newly introduced Cramer-Wold divergence as a choice of the penalty term in Eq. 4 of [1].' [SEP] 'rebuttal_reject-criticism"	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
(2) When viewed in this way, CW-distance introduced in Eq (2) closely resembles the unbiased U-statistic estimate of the MMD used in WAE-MMD [1, Algorithm 2].' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times).' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
1) the proof techniques are very standard' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification).' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
It is unclear, why one should use the proposed duality gap GAN.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
(1) using codes and codebooks to compress weights; and' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The combination of these two methods seems straightforward.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
- The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.' [SEP] 'rebuttal_reject-criticism	We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.
The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
"1.	Lack of technical novelty.' [SEP] 'rebuttal_mitigate-criticism"	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
It seems to me just a combination of several mature techniques.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
- The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
Although this extension seems to be easily derived using the contributions made at point 2.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information.' [SEP] 'rebuttal_mitigate-criticism"	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
"In the current form of evaluation, it is hard to say if there is any benefit of using the ""selection network"" that is the main novelty of the paper.' [SEP] 'rebuttal_mitigate-criticism"	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The authors need to describe in detail the algorithmic novelty of their work.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
Therefore, the paper cannot be qualified for ``meta domain adaptation’’ and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The reviewer votes for rejection as the method has limited novelty.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The novelty of this method is minimal.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The theoretical contribution is very limited.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
Overall, I like the approach of the proposed method, especially tuning coefficient during training procedure although novelty in the theoretical analysis is somewhat limited.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
It is also not clear how the loss function proposed differs from that of the CDVAE, etc.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
- Incremental modeling contribution' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
While I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
However, I believe this is also the case in the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
After all, the former approach gets a lot more knowledge about the target function built into it.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
Is it a combination of DGR and HAT with some capacity expansion?' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The remaining components of the proposed method are not very new.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
In summary, I find there is no novelty involved apart from combining the already existing SOTA model in disentangled feature learning (beta-VAE) and image generation (StackGAN).' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
"(1) My main problem with this paper is that the novel objective proposed by the authors in Eq. 7 is equivalent to the objective of WAEs appearing in Eq. 4 of [1] (up to a heuristic of applying logarithm to the divergence measure, which is not justified but meant to ""improve the balance between two terms"", see footnote 2), where the authors use the newly introduced Cramer-Wold divergence as a choice of the penalty term in Eq. 4 of [1].' [SEP] 'rebuttal_mitigate-criticism"	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
(2) When viewed in this way, CW-distance introduced in Eq (2) closely resembles the unbiased U-statistic estimate of the MMD used in WAE-MMD [1, Algorithm 2].' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times).' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
1) the proof techniques are very standard' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification).' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
It is unclear, why one should use the proposed duality gap GAN.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
(1) using codes and codebooks to compress weights; and' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The combination of these two methods seems straightforward.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
- The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.' [SEP] 'rebuttal_mitigate-criticism	We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).
The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
"1.	Lack of technical novelty.' [SEP] 'rebuttal_done"	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
It seems to me just a combination of several mature techniques.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
- The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Although this extension seems to be easily derived using the contributions made at point 2.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information.' [SEP] 'rebuttal_done"	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
"In the current form of evaluation, it is hard to say if there is any benefit of using the ""selection network"" that is the main novelty of the paper.' [SEP] 'rebuttal_done"	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The authors need to describe in detail the algorithmic novelty of their work.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Therefore, the paper cannot be qualified for ``meta domain adaptation’’ and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The reviewer votes for rejection as the method has limited novelty.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The novelty of this method is minimal.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The theoretical contribution is very limited.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Overall, I like the approach of the proposed method, especially tuning coefficient during training procedure although novelty in the theoretical analysis is somewhat limited.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
It is also not clear how the loss function proposed differs from that of the CDVAE, etc.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
- Incremental modeling contribution' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
While I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
However, I believe this is also the case in the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
After all, the former approach gets a lot more knowledge about the target function built into it.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Is it a combination of DGR and HAT with some capacity expansion?' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The remaining components of the proposed method are not very new.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
In summary, I find there is no novelty involved apart from combining the already existing SOTA model in disentangled feature learning (beta-VAE) and image generation (StackGAN).' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
"(1) My main problem with this paper is that the novel objective proposed by the authors in Eq. 7 is equivalent to the objective of WAEs appearing in Eq. 4 of [1] (up to a heuristic of applying logarithm to the divergence measure, which is not justified but meant to ""improve the balance between two terms"", see footnote 2), where the authors use the newly introduced Cramer-Wold divergence as a choice of the penalty term in Eq. 4 of [1].' [SEP] 'rebuttal_done"	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
(2) When viewed in this way, CW-distance introduced in Eq (2) closely resembles the unbiased U-statistic estimate of the MMD used in WAE-MMD [1, Algorithm 2].' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times).' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
1) the proof techniques are very standard' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification).' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
It is unclear, why one should use the proposed duality gap GAN.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
(1) using codes and codebooks to compress weights; and' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The combination of these two methods seems straightforward.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
- The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
"1.	Lack of technical novelty.' [SEP] 'rebuttal_concede-criticism"	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
It seems to me just a combination of several mature techniques.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
- The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Although this extension seems to be easily derived using the contributions made at point 2.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information.' [SEP] 'rebuttal_concede-criticism"	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
"In the current form of evaluation, it is hard to say if there is any benefit of using the ""selection network"" that is the main novelty of the paper.' [SEP] 'rebuttal_concede-criticism"	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The authors need to describe in detail the algorithmic novelty of their work.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Therefore, the paper cannot be qualified for ``meta domain adaptation’’ and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The reviewer votes for rejection as the method has limited novelty.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The novelty of this method is minimal.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The theoretical contribution is very limited.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Overall, I like the approach of the proposed method, especially tuning coefficient during training procedure although novelty in the theoretical analysis is somewhat limited.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
It is also not clear how the loss function proposed differs from that of the CDVAE, etc.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
- Incremental modeling contribution' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
While I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
However, I believe this is also the case in the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
After all, the former approach gets a lot more knowledge about the target function built into it.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Is it a combination of DGR and HAT with some capacity expansion?' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The remaining components of the proposed method are not very new.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
In summary, I find there is no novelty involved apart from combining the already existing SOTA model in disentangled feature learning (beta-VAE) and image generation (StackGAN).' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
"(1) My main problem with this paper is that the novel objective proposed by the authors in Eq. 7 is equivalent to the objective of WAEs appearing in Eq. 4 of [1] (up to a heuristic of applying logarithm to the divergence measure, which is not justified but meant to ""improve the balance between two terms"", see footnote 2), where the authors use the newly introduced Cramer-Wold divergence as a choice of the penalty term in Eq. 4 of [1].' [SEP] 'rebuttal_concede-criticism"	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
(2) When viewed in this way, CW-distance introduced in Eq (2) closely resembles the unbiased U-statistic estimate of the MMD used in WAE-MMD [1, Algorithm 2].' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times).' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
1) the proof techniques are very standard' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification).' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
It is unclear, why one should use the proposed duality gap GAN.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
(1) using codes and codebooks to compress weights; and' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The combination of these two methods seems straightforward.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
- The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
That is, authors assumed that they have a degradation function F and all the inference process is just based on this known function.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
Thus we may only apply the proposed model on a few tasks with exactly known F.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
If the authors don't discuss a motivation then how will a reader know how to apply the tool?' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
- My biggest concern is that the technical contributions of the paper are not clear at all.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
This very much limits the utility of the method.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
Hence, the effectiveness and advantage of the proposed methods are not clear.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
I don’t think it is of any practical use to show that a new algorithm (such at DDGD) provide some defence compared to no defence.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
(2) The method is not well motivated.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
The model is not well motivated and the optimization algorithm is also not well described.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
I like the area of research the authors are looking into and I think it's an important application. However, the paper doesn't answer key questions about both the application and the models:' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
One drawback is that it is highly specific to language models.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
In addition, there is not much theoretical justification for it, it seems like a one-off trick.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
The main drawback of the paper is that it seems to be more engineering-focused, and doesn’t provide much insight into semi-supervised learning.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
The privacy definition employed in this work is problematic.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
There are also concerns about the motivations behind parts of the technique.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
Although the paper proposes an interesting framework I would argue that it is a “green apple” in the sense that authors need to motivate the approach better and expand the contribution beyond solving a particular instance.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
- limited amount of new insight, which is limiting as new and better understanding of GANs and practical guidelines are arguably the main contribution of a work of this type' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
* I found it difficult to follow the theoretical motivation for performing the work.' [SEP] 'rebuttal_answer	We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.
My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
That is, authors assumed that they have a degradation function F and all the inference process is just based on this known function.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Thus we may only apply the proposed model on a few tasks with exactly known F.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
If the authors don't discuss a motivation then how will a reader know how to apply the tool?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- My biggest concern is that the technical contributions of the paper are not clear at all.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This very much limits the utility of the method.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Hence, the effectiveness and advantage of the proposed methods are not clear.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I don’t think it is of any practical use to show that a new algorithm (such at DDGD) provide some defence compared to no defence.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
(2) The method is not well motivated.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The model is not well motivated and the optimization algorithm is also not well described.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I like the area of research the authors are looking into and I think it's an important application. However, the paper doesn't answer key questions about both the application and the models:' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
One drawback is that it is highly specific to language models.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In addition, there is not much theoretical justification for it, it seems like a one-off trick.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The main drawback of the paper is that it seems to be more engineering-focused, and doesn’t provide much insight into semi-supervised learning.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The privacy definition employed in this work is problematic.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
There are also concerns about the motivations behind parts of the technique.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Although the paper proposes an interesting framework I would argue that it is a “green apple” in the sense that authors need to motivate the approach better and expand the contribution beyond solving a particular instance.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- limited amount of new insight, which is limiting as new and better understanding of GANs and practical guidelines are arguably the main contribution of a work of this type' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
* I found it difficult to follow the theoretical motivation for performing the work.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
"The performance of the proposed method is worse than the previous work but they claimed ""state-of-the-art"" results.' [SEP] 'rebuttal_concede-criticism"	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
"- In the appendix, the statement ""Sarkar (2011) show that a similar statement as in Theorem 2 holds for a very general class of trees"" is confusing to me. The ""general class"", as far as I know, is actually *all* trees, weighted or unweighted.' [SEP] 'rebuttal_concede-criticism"	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
I wonder the motivation of analyzing generalization of RNNs by the techniques established by Bartlett.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
Therefore, I don’t find interesting to report how DDGC improve upon “no baseline”, because known methods do even better.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
While better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
As the approach uses NF is derived specifically for unstable dynamics, it is not clear to me how adding it would affect the training if the dynamics *is stable*. In this context, I consider that for example, the work by Balduzzi et al. 2018 may be relevant as it describes that the dynamics of games (the Jacobian) has two components, one of which describes the oscillating behavior (Hamiltonian game); whereas most games are a mix of oscillating and non-oscillating dynamics.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
The justification that the method of Khadka & Tumer (2018) cannot be extended to use CEM, since the RL policies do not comply with the covariance matrix is unclear to me.' [SEP] 'rebuttal_concede-criticism	Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.
The results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
"The performance of the proposed method is worse than the previous work but they claimed ""state-of-the-art"" results.' [SEP] 'rebuttal_structuring"	Q4. Updated abstract and performance evaluation.
"- In the appendix, the statement ""Sarkar (2011) show that a similar statement as in Theorem 2 holds for a very general class of trees"" is confusing to me. The ""general class"", as far as I know, is actually *all* trees, weighted or unweighted.' [SEP] 'rebuttal_structuring"	Q4. Updated abstract and performance evaluation.
I wonder the motivation of analyzing generalization of RNNs by the techniques established by Bartlett.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
Therefore, I don’t find interesting to report how DDGC improve upon “no baseline”, because known methods do even better.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
While better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
As the approach uses NF is derived specifically for unstable dynamics, it is not clear to me how adding it would affect the training if the dynamics *is stable*. In this context, I consider that for example, the work by Balduzzi et al. 2018 may be relevant as it describes that the dynamics of games (the Jacobian) has two components, one of which describes the oscillating behavior (Hamiltonian game); whereas most games are a mix of oscillating and non-oscillating dynamics.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
The justification that the method of Khadka & Tumer (2018) cannot be extended to use CEM, since the RL policies do not comply with the covariance matrix is unclear to me.' [SEP] 'rebuttal_structuring	Q4. Updated abstract and performance evaluation.
The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Yet the experiments considered in the paper are limited to very few time series.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The only set of experiments are comparisons on first 500 MNIST test images.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Moreover, I don't think some of the presented experiments are necessary.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
We think that this is not enough, and more extensive experimental results would provide a better paper.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
It would be nice to see a better case made for spherical convolutions within the experimental section.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
* The connections to deep learning seem arbitrary in some of the experiments.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Given the paper title, the reviewer would have expected more experiments in a multiple domain context.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- The performance gain is not substantial in experiments.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
* The baselines in the experiments could be improved.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
3. The experimental study is weak.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- Detailed experimental setups are missing.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
However, the experimental results are weak in justifying the paper's claims.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The main problems come from the experiments, which I would ask for more things.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Regarding the experimental evaluation of the model rather confusing.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Experimental results itself are fine but not complete.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The second weakness is experimental design.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
=> Environment: The experimental section of the paper can be further improved.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
6, the experimental design of Sec. 4.2 is also a bit unfair.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
So the experiments in this paper is also not convincing.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
However, my concern is about the experiments.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The paper’s primary drawback is the restrictive setting under which the experiments are performed.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Please run at least 10 experiments.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Additional experiments on at least ImageNet would have made the paper stronger.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
"2.	The experiments are rather insufficient.' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
However, it seems the experiments do not seem to support this.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
In fact, the separate training seems to make this unlikely.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Thus the experiment comparison is not really fair.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
2 The experimental settings are not reasonable.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The current experimental settings are not matched with the practice environment.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The experiments of this paper lack comparisons to certified verification' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
It might be beneficial to include comparison to this approach in the experimental section.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
3) The experiments are completely preliminary and not reasonable:' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
These issues would maybe be excusable if not for the totally inadequate experimental validation.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The weight sharing was also needed further investigation and experimental data on sharing different parts.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The rest experiments' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- The setup for the learning to permute experiment is not as general as it would imply in the main text.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
(4) For the error-specific attack task, it would be better to provide an ablation experiment.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
*The experimental section is too limited.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
3) The simulation is not convincing.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
"- at the start of section 3: what is an ""experiment""?' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.' [SEP] 'rebuttal_summary"	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
this is important since all your experiments rely on that assumption.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.' [SEP] 'rebuttal_summary	Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.
The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Yet the experiments considered in the paper are limited to very few time series.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The only set of experiments are comparisons on first 500 MNIST test images.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Moreover, I don't think some of the presented experiments are necessary.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.' [SEP] 'rebuttal_reject-request"	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.' [SEP] 'rebuttal_reject-request"	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
We think that this is not enough, and more extensive experimental results would provide a better paper.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
It would be nice to see a better case made for spherical convolutions within the experimental section.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
* The connections to deep learning seem arbitrary in some of the experiments.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Given the paper title, the reviewer would have expected more experiments in a multiple domain context.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- The performance gain is not substantial in experiments.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
* The baselines in the experiments could be improved.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
3. The experimental study is weak.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- Detailed experimental setups are missing.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
However, the experimental results are weak in justifying the paper's claims.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The main problems come from the experiments, which I would ask for more things.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.' [SEP] 'rebuttal_reject-request"	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Regarding the experimental evaluation of the model rather confusing.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Experimental results itself are fine but not complete.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The second weakness is experimental design.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
=> Environment: The experimental section of the paper can be further improved.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
6, the experimental design of Sec. 4.2 is also a bit unfair.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
So the experiments in this paper is also not convincing.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
However, my concern is about the experiments.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The paper’s primary drawback is the restrictive setting under which the experiments are performed.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Please run at least 10 experiments.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Additional experiments on at least ImageNet would have made the paper stronger.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
"2.	The experiments are rather insufficient.' [SEP] 'rebuttal_reject-request"	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
However, it seems the experiments do not seem to support this.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
In fact, the separate training seems to make this unlikely.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Thus the experiment comparison is not really fair.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
2 The experimental settings are not reasonable.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The current experimental settings are not matched with the practice environment.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The experiments of this paper lack comparisons to certified verification' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
It might be beneficial to include comparison to this approach in the experimental section.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
3) The experiments are completely preliminary and not reasonable:' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
These issues would maybe be excusable if not for the totally inadequate experimental validation.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The weight sharing was also needed further investigation and experimental data on sharing different parts.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The rest experiments' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- The setup for the learning to permute experiment is not as general as it would imply in the main text.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
(4) For the error-specific attack task, it would be better to provide an ablation experiment.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
*The experimental section is too limited.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
3) The simulation is not convincing.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
"- at the start of section 3: what is an ""experiment""?' [SEP] 'rebuttal_reject-request"	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?' [SEP] 'rebuttal_reject-request"	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.' [SEP] 'rebuttal_reject-request"	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
this is important since all your experiments rely on that assumption.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.' [SEP] 'rebuttal_reject-request	We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.
The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Yet the experiments considered in the paper are limited to very few time series.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The only set of experiments are comparisons on first 500 MNIST test images.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Moreover, I don't think some of the presented experiments are necessary.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
We think that this is not enough, and more extensive experimental results would provide a better paper.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It would be nice to see a better case made for spherical convolutions within the experimental section.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
* The connections to deep learning seem arbitrary in some of the experiments.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Given the paper title, the reviewer would have expected more experiments in a multiple domain context.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The performance gain is not substantial in experiments.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
* The baselines in the experiments could be improved.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
3. The experimental study is weak.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Detailed experimental setups are missing.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, the experimental results are weak in justifying the paper's claims.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The main problems come from the experiments, which I would ask for more things.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Regarding the experimental evaluation of the model rather confusing.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Experimental results itself are fine but not complete.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The second weakness is experimental design.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
=> Environment: The experimental section of the paper can be further improved.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
6, the experimental design of Sec. 4.2 is also a bit unfair.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
So the experiments in this paper is also not convincing.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, my concern is about the experiments.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The paper’s primary drawback is the restrictive setting under which the experiments are performed.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Please run at least 10 experiments.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Additional experiments on at least ImageNet would have made the paper stronger.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"2.	The experiments are rather insufficient.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, it seems the experiments do not seem to support this.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In fact, the separate training seems to make this unlikely.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Thus the experiment comparison is not really fair.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
2 The experimental settings are not reasonable.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The current experimental settings are not matched with the practice environment.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The experiments of this paper lack comparisons to certified verification' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It might be beneficial to include comparison to this approach in the experimental section.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
3) The experiments are completely preliminary and not reasonable:' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
These issues would maybe be excusable if not for the totally inadequate experimental validation.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The weight sharing was also needed further investigation and experimental data on sharing different parts.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The rest experiments' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The setup for the learning to permute experiment is not as general as it would imply in the main text.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
(4) For the error-specific attack task, it would be better to provide an ablation experiment.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
*The experimental section is too limited.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
3) The simulation is not convincing.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"- at the start of section 3: what is an ""experiment""?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
this is important since all your experiments rely on that assumption.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Yet the experiments considered in the paper are limited to very few time series.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The only set of experiments are comparisons on first 500 MNIST test images.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Moreover, I don't think some of the presented experiments are necessary.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
We think that this is not enough, and more extensive experimental results would provide a better paper.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It would be nice to see a better case made for spherical convolutions within the experimental section.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
* The connections to deep learning seem arbitrary in some of the experiments.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Given the paper title, the reviewer would have expected more experiments in a multiple domain context.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- The performance gain is not substantial in experiments.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
* The baselines in the experiments could be improved.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
3. The experimental study is weak.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- Detailed experimental setups are missing.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
However, the experimental results are weak in justifying the paper's claims.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The main problems come from the experiments, which I would ask for more things.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Regarding the experimental evaluation of the model rather confusing.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Experimental results itself are fine but not complete.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The second weakness is experimental design.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
=> Environment: The experimental section of the paper can be further improved.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
6, the experimental design of Sec. 4.2 is also a bit unfair.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
So the experiments in this paper is also not convincing.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
However, my concern is about the experiments.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The paper’s primary drawback is the restrictive setting under which the experiments are performed.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Please run at least 10 experiments.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The evaluation section lacks experiments that evaluate the computational savings.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Additional experiments on at least ImageNet would have made the paper stronger.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"2.	The experiments are rather insufficient.' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
However, it seems the experiments do not seem to support this.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
In fact, the separate training seems to make this unlikely.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Thus the experiment comparison is not really fair.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
2 The experimental settings are not reasonable.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The current experimental settings are not matched with the practice environment.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Thus, the evidence of the experiments is not enough.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The experiment section needs significant improvement, especially when there is space left.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The experiments of this paper lack comparisons to certified verification' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It might be beneficial to include comparison to this approach in the experimental section.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
3) The experiments are completely preliminary and not reasonable:' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
These issues would maybe be excusable if not for the totally inadequate experimental validation.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The weight sharing was also needed further investigation and experimental data on sharing different parts.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The rest experiments' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- The setup for the learning to permute experiment is not as general as it would imply in the main text.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
(4) For the error-specific attack task, it would be better to provide an ablation experiment.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
*The experimental section is too limited.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
3) The simulation is not convincing.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Experiments are on toy domains with very few goals and sub-task dependencies.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"- at the start of section 3: what is an ""experiment""?' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
It is hard to support this motivation when no experiments are done in its favor.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.' [SEP] 'rebuttal_social"	We will gladly provide files with the trained weights and also fully trained neural networks on request.
The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
this is important since all your experiments rely on that assumption.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.' [SEP] 'rebuttal_social	We will gladly provide files with the trained weights and also fully trained neural networks on request.
Here are a few examples: The ICLR citation style needs to use sometimes \citep.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
- There is no need for such repetitive citing (esp paragraph 2 on page 2).' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
Sometimes the same paper is cited 4 times within a few lines.' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
- Missing references on page 3' [SEP] 'rebuttal_done	We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The equation (1) should hold for any \theta’, not \theta.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The equation (1) should contain \rho, not p.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
For example, it is unclear to me why some larger models are not amenable to truncation.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
From the supplementary, it seems Epsilon means the environment?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"Fourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
The maths is not clear, in particular the gradient derivation in equation (8).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
But again, it's not super clear how the paper estimates this derivative.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
1. Are e_{i,t} and lambda_{i,t} vectors?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
In the algorithm, the authors need to define the HT function in (3) and (4).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Also, it is not clear why those are called axioms since they are not use to build anything else.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"I take issue with the usage of the phrase ""skill discovery"".' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
It is in the end plugged into a continual learning algorithm which also performs domain transformation.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
The purpose of the public set is explained only in section 5.2.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
However, I don't understand the use of $\alpha$ here.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
* Equations (1, 2): z and \phi are not consistently boldfaced' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I believe that the presentation of the proposed method can be significantly improved.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
The method description was a bit confusing and unclear to me.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- the sentence under eq. (2)' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"4. I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
While the general description of the model is clear, details are lacking.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Similarly, you did not indicate what the deterministic version of your model is.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
-Some technical details  are missing.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Because of the above many discussions about discrete vs. continuous variables are missleading.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Many of the parameters here are also unclear and not properly defined/introduced.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
What is the relationship between $\theta$ and $\tilde\theta$ exactly?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The 3rd line of lemma 1: epsilon1 -> epsilon_1' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- Page 14, Eq(14), \lambda should be s' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"Some statements don't make sense, however, eg. ""HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
This makes the contribution of this paper in terms of the method' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- What is dt in Algorithm 1 description?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
In later sections they use theta and theta’ for encoder/decoder resp.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Why mention it here, if it's not being defined.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
Minor, 1/2 is missing in the last line of Eq 19.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Also perhaps better to use the curly sign for vector inequality.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
“From a high-level perspective both of these approaches” --> missing comma after “perspective”' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
In equations 1 and 2, should a, b be written in capital? Since they represent random variables.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I identify this ambiguity between BERTScore versions as an important weakness of the paper.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- There is a typo in equation 6' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
The paper seems to lack clarity on certain design/ architecture/ model decisions.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Also, I had to go through a large chunk of the paper before coming across the exact setup.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
I did not completely follow the arguments towards directed graph deconvolution operators.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I also struggled a little to understand what is the difference between forward interpolate and filtering.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
1) There is no clear rationale on why we need a new model based on Transformers for this task.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- Lambda sim and lambda s are used interchangeably. Please make it consistent.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I am not convinced that the measure theoretic perspective is always' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
4.4, law of total variation -- define' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
However, what was not clear to me is how this reduces the non-stationarity of MARL.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- What is the choice of beta in the beta-VAE training objective?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
1. What is M in Algorithm 1 ?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Not sure why Eqns. 2 and 9 need any parentheses' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Generalizing this to the multi-channel input as the next step could make the proof more accessible.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The egocentric velocity field is not described (section 5)' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
However, the  transfer learning model is unclear.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
It is unknown the used model is a new model or existing model.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I have doubts on applying the proposed method to higher dimensional inputs.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
ReLU network with 2 hidden layers, and it is unknown if it works in general.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
positive - it unlikely to be true that an undefended network is predominantly' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
adversarial examples (or counter-examples for property verification) with L_inf' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
- Your F and \tilde{f} are introduced as infinite series.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Isn't this just restating the point made in the first sentence?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
3, The notations in Eq. (1) and (2) are messy.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
It is unclear how well the proposed method works in general.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
In the start of Section 3, it is not clear why having the projection be sparse is desired.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
eqn (8): use something else to denote the function 'U'.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
This clipping will also introduce bias, this is not discussed, and will probably lower variance.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
It would be better if the authors were a little more careful in their use of terminology here.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
*The strategy proposed to introduce weak-supervision is too ad-hoc.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
- U^m in Eq 1 is undefined and un-discussed.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The term p(w) disappears on the left hand side of Eq 2.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
p2-3, Section 3.1 - I found the equations impossible to read. What' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
It is unclear how the model actually operates and uses attention during execution.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
"-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
"-	Attention should be given to the notation in formulas (3) and (4).' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
The projection function there is no longer accepts a 3D point parametrized by 3 variables.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
Instead only depth is provided.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
In addition, the subindex ‘1’ of the point ‘q’ is not explained.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.' [SEP] 'rebuttal_summary	In terms of GR, we are trying to address the two open questions mentioned above.
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".' [SEP] 'rebuttal_summary"	In terms of GR, we are trying to address the two open questions mentioned above.
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_summary"	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_summary"	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Making this algorithm not very practical.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_summary"	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
So the closure axiom of a group is violated.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Did you try to have a single network?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_summary"	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
How does the transformer based method comparing to others?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_summary"	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_summary"	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_summary"	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_summary"	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_summary"	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_summary"	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Lemma 3 is too trivial.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
o feedforward rather than recurrent network;' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
Is that also true in this domain?' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_summary"	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_summary	Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.
I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- the complexity of the proposed algorithm seems to be very high' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Beyond this simplification, I am not clear if that is actually intended by the authors.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Moreover, due to the trace based loss function, the computational cost will also be very high.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The evaluation of the proposed method is not complete.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"Furthermore PTB is not a ""challenging"" LM benchmark.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Lemma 2.4, Point 1: The proof is confusing.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
What is G_t in Theorem 2.5. It should be defined in the theorem itself.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Thus, the theoretical contribution of this paper is limited.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is better to explain the major difference and the motivation of updating the hidden states.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
For example, it is curious to see how denoising Auto encoders would perform.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Making this algorithm not very practical.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
How do we choose a proper beta, and will the algorithm be sensitive to beta?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
compared two schemes of this work, the ones with attentions are “almost” identical with ones' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
in practice. For most of newly proposed graph embedding algorithms, it is hard to convince' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
* The oracle-augmented datasteam model needs to be contextualized better.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
What is the benefit of using DL algorithms within the oracle-augmented datastream model?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Is a simple algorithm enough? What algorithms should we ideally use in practice?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
What if you used simpler online learning algorithms with formal accuracy guarantees?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Why not compare with Sparsely-Gated MoE?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Another concern is that the evaluation of domain adaptation does not have much varieties.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- there is no attempt to provide a theoretical insight into the performance of the algorithm' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Consequently why did not you compare simple projected gradient method ?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- I would like to see some more interpretation on why this method works.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I wonder how the straightforward regression term plus the smooth term will perform for the mask.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In general, I feel this section could use some tighter formalism and justifications.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In terms of actual technical contributions, I believe much less significant.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
So the closure axiom of a group is violated.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This matters, because the notion of equivariance really only makes sense for a group.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Theorem 1 does not take account for the above conditions.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Did you try to have a single network?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The drawbacks  of the work include the following: (1) There is not much technical contribution.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In this case, the paper proves that no careful selection of the learning rate is necessary.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The use of Glorot uniform initializer is somewhat subtle.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- what prior distributions p(z) and p(u) are used? What is the choice based on?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Please comment on the choice, and its impact on the behavior of the model.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
How does the transformer based method comparing to others?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. The proxy f(z) does not bear any resemblance to LP(z).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Otherwise, this choice is incomprehensible.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I would strongly recommend including the computational cost of each method in the evaluation section.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- In the case of the search space II, how many GPU days does the proposed method require?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The authors haven't come up with a recommendation for a single configuration of their approach.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
How this proxy incentives the agent to explore poorly-understood regions?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
For instance, how deep should a model be for a classification or regression task?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Why does temporal correlation reduce the non-stationarity of the MARL problem?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Why does structured exploration reduce the number of network parameters that need to be learned?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- This approach seems very limited, as there must exist a known transformation that removes the desired information.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- Can this approach learn multiple factors as opposed to just two?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Can the proposed approach perform just as well without a modified objective?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Also, what will be the performance of a standard image captioning system on the task ?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Also, the compared methods don’t really use the validation set from the complex data for training at all.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, the Pareto front of the proposed method is concentrated on a specific point.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"In this sense, the proposed method is not comparable with ""noisy"".' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
and bounded failure rate, otherwise it is not really a verification method.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
methods. There are some scalable property verification methods that can give a' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, the evaluation of the proposed adaptive kernels is rather limited.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
How big is the generalization gap for the tested models when adaptive kernel is used?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
It would be good to know how $\gamma$ varies across tasks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. The formulation uses REINFORCE, which is often known with high variance.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I think it needs to be made clearer how reconstruction error works as a measure of privacy.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Yet the metrics proposed depend on supervision in the target domain.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
3. The structure of the meta-training loop was unclear to me.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This seems like a limitation of the method if this is the case.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, the authors do not provide an in-depth discussion of this phenomena.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
* The BiLSTM they use is very small (embedding and hidden dimension 50).' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Authors should scope the paper to the specific function family these networks can approximate.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, the function of interest is limited to a small family of affine equivariant transformations.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Lemma 3 is too trivial.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
o feedforward rather than recurrent network;' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
2. The main technical contribution claim needs to be elaborated.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
They need to elaborate how their method overcomes these issues better.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Yet their approach is only able to solve the fractional version of the AdWords problem.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
For example, if rules contain quantifiers, how would this be extended?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
As a minor note, were different feature extractors compared?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
Is that also true in this domain?' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).' [SEP] 'rebuttal_refute-question"	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.' [SEP] 'rebuttal_refute-question	*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*
I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Attacking CRBMs is highly relevant and should be included as a baseline.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, the idea is very related to Yeh et al.’s work which has already published but not mentioned at all.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- For semi-supervised classification, the paper did not report the best results in other baselines.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2].' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
1. The paper should also talk about the details of ARNet and discuss the difference, as I assume they are the most related work' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It would probably help to position the VAE component more precisely w.r.t. one of the two baselines, by indicating the differences.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, ""here are some simple functions for which we would need the additional parameters that we define"" makes sense; but arguing that Hartford et al. ""fail approximating rather simple functions"" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting).' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
There should be a better discussion of related work on the topic.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
=> Baselines: The comparison provided in the paper is weak.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Moreover, in spite of the authors writing that their goal is “completely different” from [Lee at al 18a, Ma et al 18a], I found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
A proper baseline should have been compared.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Third, the comparison to baseline and “DeepSet” is not fair.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
4. Comparison with past works.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This baseline was also missing in image reconstruction.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
This seems like a very weak baseline---there are any number of other reasonable ways to encode this.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
For e.g. the results on the oscillating behavior of Dirac-GAN are described in related works (e.g. Mescheder et al. 2018), and in practice, WGAN with no regularization is not used (as well as GAN with momentum, as normally beta1=0 in practice).' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
I am also wondering if the comparison with the baselines is fair.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
In other words, although in the present results, the proposed NF-SGAN/WGAN outperforms the baseline, the reported performance of the baselines is worse than in related works on CIFAR10.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it’s being reported as 6.24.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Weakness: It would be good to see some comparison to the state of the art' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
The main issue of this paper is the fair comparisons with other works.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- Baseline missing: Random actions from expert' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- Baseline missing: Simple RNN policies that communicate hidden states.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
- comparison should be made to the linear composition method in the Arora, Liang, Ma ICLR 2017 paper' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, there is no comparison against existing work.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
However, memory overhead is still an issue compared to existing method.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
It is important to place the contributions in this paper in context of these other works.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
A number of these references are missing and no experimental comparison to these methods has been made.' [SEP] 'rebuttal_done	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
"* In related work, no reference to previous work on ""statistical"" approaches to NN' [SEP] 'rebuttal_done"	In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.
In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Finally, the experimental part is also too weak to evaluate the proposed method.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
However, the experiments feel like they are missing motivation as to why this method is being used.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The experiments on SHREC17 show all three spherical methods under-performing other approaches.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
However I find the white-box experiments lacking as almost every method has 100% success rate.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
In the current experiments there is a comparison only with CO algorithm and SGDA.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
So overall I have the feeling that the authors have not succeeded to evaluate the model’s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
I also was not convinced by the experiments which are mostly qualitative. I did not find that this set of experiments provide enough support to the proposed method.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class ‘8’). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above — if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?' [SEP] 'rebuttal_done	We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.
In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
Finally, the experimental part is also too weak to evaluate the proposed method.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
However, the experiments feel like they are missing motivation as to why this method is being used.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
The experiments on SHREC17 show all three spherical methods under-performing other approaches.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
However I find the white-box experiments lacking as almost every method has 100% success rate.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
In the current experiments there is a comparison only with CO algorithm and SGDA.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
So overall I have the feeling that the authors have not succeeded to evaluate the model’s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
I also was not convinced by the experiments which are mostly qualitative. I did not find that this set of experiments provide enough support to the proposed method.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class ‘8’). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above — if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?' [SEP] 'rebuttal_answer	Moreover, we investigate the mixing distribution learned in Appendix G.
In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Finally, the experimental part is also too weak to evaluate the proposed method.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
However, the experiments feel like they are missing motivation as to why this method is being used.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The experiments on SHREC17 show all three spherical methods under-performing other approaches.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
However I find the white-box experiments lacking as almost every method has 100% success rate.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
In the current experiments there is a comparison only with CO algorithm and SGDA.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
So overall I have the feeling that the authors have not succeeded to evaluate the model’s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
I also was not convinced by the experiments which are mostly qualitative. I did not find that this set of experiments provide enough support to the proposed method.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class ‘8’). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above — if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?' [SEP] 'rebuttal_concede-criticism	It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.
In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Finally, the experimental part is also too weak to evaluate the proposed method.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
However, the experiments feel like they are missing motivation as to why this method is being used.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
The experiments on SHREC17 show all three spherical methods under-performing other approaches.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
However I find the white-box experiments lacking as almost every method has 100% success rate.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
In the current experiments there is a comparison only with CO algorithm and SGDA.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
So overall I have the feeling that the authors have not succeeded to evaluate the model’s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
I also was not convinced by the experiments which are mostly qualitative. I did not find that this set of experiments provide enough support to the proposed method.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class ‘8’). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above — if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?' [SEP] 'rebuttal_summary	In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.
This is discussed on p4, but it's unclear to me how keeping $\sigma$ separate is benefitial for the analysis, and this is not picked up again explicitly again?' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
this analysis is not currently included in the main body of the manuscript, but rather in the appendix, which I find rather annoying.' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
The paper offers some analysis, suggesting when each of the conditions occurs, upper bounds the smallest singular value of D A (where the example dependent diagonal matrix D incorporates the ReLU activation (shouldn’t this be more clearly introduced and notated?).' [SEP] 'rebuttal_done	In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.
More precisely, for the digit datasets, the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
2. Sec. 3.3, Fig. 3: The capacity (in terms of parameters)of both Resnet-18 and VGG-16 is higher than the capcity for YFCC100M dataset for n=10K images (comes to 161K bits), while the capacity of Resnet-18, with 14.7 million parameters (assuming float32 encoding) has 14.7 * 32 bits = 470.4 million bits, thus capacity alone cannot explain why VGG converges faster than Resnet-18, since both networks exceed the capacity, and capacity does not seem to have an established formal connection to rate of memorization.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
- Does the MonoGAN exhibit stable training dynamics comparable to training WGAN on CIFAR-10, or do the training dynamics change on the single-image data set?' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
Clearly, replaying data accurately from all tasks will work well, but why is it harder to guard against the generative forgetting problem than the discriminative one?' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
Generative replay also brings the time complexity problem since it is time consuming to generate previous data.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
What is the minimum/maximum layers of a deep model? How much data is sufficient for a model to learn?' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
What's the relation between the size of a model and that of a data set? By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
* p.4 first paragraph you claim that this method responds well to data which exhibits seasonality, but none of your datasets deal with data that would exhibit seasonality.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
In particular, if the goal is to use this method to augment the data we use to train NLP systems in order to make them more robust, it seems that the time cost of the process will be prohibitive.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
These classification datasets are often so close, that I do wonder whether even simpler methods would work just as well.' [SEP] 'rebuttal_concede-criticism	As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.
I do not see much insight into the problem.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
The main problem with this paper is that it is difficult to identify its main and novel contributions.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
At the conceptual level, the idea of jointly modeling local video events is not novel, and can date back to at least ten years ago in the paper “Learning realistic human actions from movies”, where the temporal pyramid matching was combined with the bag-of-visual-words framework to capture long-term temporal structure.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
The idea of having a separate class for out-distribution is a very interesting idea but unfortunately previously explored.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
The idea that introduces labels in VAE is not novel.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
At a high level, the idea of imposing a mixture of gaussian structure in the feature space of a deep neural network classifier is not new.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
The idea to extend the use of VAE-GANs to the video prediction setting is a pretty natural one and not especially novel.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
- The idea is a simple extension of existing work.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
Although the idea seems to be interesting, the paper seems to be a bit incremental and is a simple application of existing GAN techniques.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
ii) Although the idea of generating contrastive explanations is quite interesting, it is not that novel.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
Last comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
* the idea of smoothing gradients is not new' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
- the idea is trivial and simple. I don't think there's significant novelty here: all the components are existing and combining them seems very trivial to me.' [SEP] 'rebuttal_done	We clarified this in Section 2.1 of the revised draft.
Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
"The proposed method adapt a previously presented hierarchical clustering method in the ""standard space"" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model.' [SEP] 'rebuttal_concede-criticism"	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
Further the contributions are not clear at all, since a large part of the detailed approach/equations relate to the masking which was taken from previous work.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
Furthermore, there are several similar ideas already published, so comparison against those (for example by J. Peng, N. Heess or J. Mere) either as argument or even better as experiment, would be helpful to evaluate the quality of the proposed hierarchy.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
The proposed method PowerSGD is an extension of the method in Yuan et al. (extended to handle stochastic gradient and momentum).' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
"A weighted k-means solver is also used in [2], though the ""weighted"" in [2] comes from second-order information instead of minimizing reconstruction error.' [SEP] 'rebuttal_concede-criticism"	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
Once a low-level walking controller is trained, the high-level multi-agent navigation control is not much different from simple environments, e.g. point mass control, used in the previous works.' [SEP] 'rebuttal_concede-criticism	[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.
