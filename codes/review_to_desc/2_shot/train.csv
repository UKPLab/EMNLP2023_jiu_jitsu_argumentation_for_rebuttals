reviews	descs
The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).	More variations of experiments needs to be added
Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.	More variations of experiments needs to be added
The paper is not very self contained.	Lacking clarity overall (needs better presentation)
5. The paper is imprecise and unpolished and the presentation needs improvement.	Lacking clarity overall (needs better presentation)
Due to several shortcomings of the paper, most important of which is on presentation of the paper, this manuscript requires a significant revision by the authors to reach the necessary standards for publication, moreover it would be helpful to clarify the modeling choices and consequences of these choices more clearly.	While sensible, this seems to me to be too minor a contribution to stand alone as a paper.
This could have made the paper much stronger.	While sensible, this seems to me to be too minor a contribution to stand alone as a paper.
However the theoretical contribution is poor. And the experiment uses a very classical benchmark providing simulated data.	Not enough novelty in experiments (seems similar to previous work)
Even though the proposed approach seems to have significant potential, the experimental	Not enough novelty in experiments (seems similar to previous work)
Fig 4 is very confusing.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
2. table 2: Dynamic -> Adaptive?	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
A number of these references are missing and no experimental comparison to these methods has been made.	Missing baselines
The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.	Missing baselines
Hence, I am not very sure whether the novelty of the paper is significant.	Overall not original enough (primary claim, limited evaluation, limited novelty)
Overall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty.	Overall not original enough (primary claim, limited evaluation, limited novelty)
The justification that the method of Khadka & Tumer (2018) cannot be extended to use CEM, since the RL policies do not comply with the covariance matrix is unclear to me.	incorrect claims for related work
You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.	incorrect claims for related work
I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.	Limited novelty in theoretical contribution
The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times).	Limited novelty in theoretical contribution
- experiments are performed using a synthetic setup on a single data set, so it remains unclear if the algorithm would be successful in a real life scenario	Not enough info on dataset and hyper-parameters
- Given the small size of the dataset, I would propose experimenting with non-neural approaches as well, which are also quite common in NLG.	Not enough info on dataset and hyper-parameters
My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.	Missing theoretical comparisons
And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared.	Missing theoretical comparisons
The introduction contains a few statements that may paint an incomplete or confusing picture of the current literature in adversarial attacks on neural networks:	Incorrect claims in introduction (confusing related work presented in intro, wrong claims)
"1. The title of the paper is ""visual reasoning by progressive module networks."" The title may be a little overstated since the major task is focused on visual question answering (VQA)."	Incorrect claims in introduction (confusing related work presented in intro, wrong claims)
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.	Less datasets used
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.	Less datasets used
I believe that this paper is not the first one to study question generation from logical form (cf. Guo et al, 2018 as cited), so it is unclear what is the contribution of this paper in that respect.	Limited novelty as compared to related work
In general, recent work has found that the raw number of parameters has little to do with the size of the model class or the capacity of a model for deep models, and thus work like [A] has been trying to come up with better complexity measures for models to explain generalization.	Limited novelty as compared to related work
"3.	Are the authors willing to release the code? Overall the model looks complicated and the appendix is not sufficient to reproduce the results in the paper."	Missing details for reproducibility of result
While the authors do report some interesting results, they do a poor job of motivating the proposed extensions.	Missing details for reproducibility of result
- Some parts of the paper feel long-winded and aimless.	3. The paper is not nicely written or rather easy to follow.
Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.	3. The paper is not nicely written or rather easy to follow.
Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
However, you are in a different computational model in which you now have access to an oracle.	Theoretical misunderstanding in methodology
Meanwhile the actual method used in the paper is hidden in Appendices A.1.1-A.2.	Theoretical misunderstanding in methodology
Citations used for Gradcam are wrong -- Sundarajan et al., 2016 should be changed to Selvaraju et al., 2017.	Incorrect citation
https://openreview.net/references/pdf?id=Sy2fzU9gl	Incorrect citation
More rigorous experiments and analysis is needed to make this a good ICLR paper.	The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).
One thing that puts me off is that, in Table 2, AnyBurl (the single one baseline authors considered other than the original NeuralLP) yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.	The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).
"-	While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited."	Limited contribution in problem definition
2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem  or sample selection problem)	Limited contribution in problem definition
This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.	Improper explanation of results (as in advantages, why are results better, etc)
Particularly, the authors mixed their observations up with the results of published works, making it hard to identify the contributions of this paper.	Improper explanation of results (as in advantages, why are results better, etc)
- Although the authors discussed the experiment setting in detail in supplements, I believe open-sourcing the code / software used to conduct the experiments would be greatly help with the reproducibility of the proposed method for researchers or practitioners.	Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.
- Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch?	Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.
* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.	Limited improvement over baselines
For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.	Limited improvement over baselines
- I'm not sure we can conclude much from the results on fetchSlide (and it would make sense not to use the last set of parameters but the best one encountered during training)	correctness of experiments is questionable
- There is no motivations for the use of $\lambda >1$ neither practical or theoretical since the results are only proven for $\lambda =1$ whereas the experiments are done with \lambda = 5,20 or 30.	correctness of experiments is questionable
Besides, as the base classifier is different for various baselines, it is hard to compare the methods.	Incorrect baselines used
RNN-based approaches are with better “complexity” comparing to your sum baseline and “Deepset” approach.	Incorrect baselines used
Second, the experiments are (as I understand it, but I may be wrong) in deterministic domains, which definitely does not constitute a general test of a proposed RL  method.	design choices are questionable
7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images’’ -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen’’ images which it labels as the negative class, thus the negative class is also seen by the memorization model.	design choices are questionable
Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
- The idea is a simple extension of existing work.	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
This is not so interesting, even though results are impressive.	Limited impact of results
Then to state to which of these cases the results of the paper are applicable, allow for an improvement of the variance and at what additional computational cost (considering the cost of evaluating the discrete derivatives).	Limited impact of results
While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.	Lack of analysis
This may also help to understand some of the limitations of this analysis.	Lack of analysis
If faster training of dictionary learning models was a bottleneck in practical applications, this might be of interest, but it is not.	Missing explanation of utility of proposed method
In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.	Missing explanation of utility of proposed method
- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled “Generative Ensembles for Robust Anomaly Detection” makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
While reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.	Lacking details on datasets
My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small.	Lacking details on datasets
As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.	Limited insights based on design choices
- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.	Limited insights based on design choices
The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
Indeed, without referencing the original Pointer Network and (and especially the) Transformer papers, it would not be possible to understand this paper at all.	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
It would be helpful to have a brief paragraph describing this architecture, for readers not familiar with the referenced paper.	Missing implementation details of related work used as baselines
Some details are missing, which is hardly reproduced by the other researchers.	Missing implementation details of related work used as baselines
Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?	Limited generalizability of claims on other datasets
In the real world, one would not have access to OOD data during training, how is one to pick \lambda in such cases?	Limited generalizability of claims on other datasets
However, the theoretical justification and experimental results are not.	Improper Structuring of experimental results on paper (as in on some page, before some section)
Regarding the presentation, I found odd having some experimental results (page 5) before the Section on experience even have started.	Improper Structuring of experimental results on paper (as in on some page, before some section)
Furthermore, in the beginning of Section 3.1 the authors present their idea on probabilistic hypernetoworks which “maps x to a distribution over parameters instead of specific value \theta.” How is this different from the case that we were considering so far? If we had a point estimate for \theta we would not require to take an expectation in Equation (3) in the first place.	While better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.
Though the title promises a contribution to an understanding of word embedding compositions in general, they barely expound on the broader implications of their idea in representing elements of language through vectors.	While better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.
Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.	Incorrect assumptions as compared to related work
"For example, ""The old system of private arbitration courts is off the table"" from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture."	Incorrect assumptions as compared to related work
"* In the introduction, ""the classical approach"" is mentioned but to be the latter is"	Unclear intro (eg. contributions)
The introduction can start at a lower level (such as flat/hyperbolic neural networks).	Unclear intro (eg. contributions)
I am wondering this method can be applied to other complex datasets whose latent factors are unknown.	generalizability of method on datasets created from different distributions is questionable
However, I can not agree because you can simply generate poisoned data and train the neural networks on the poisoned data regardless of the underlying approach that is targeted in generating the poisoned data.	generalizability of method on datasets created from different distributions is questionable
* p.4 first paragraph you claim that this method responds well to data which exhibits seasonality, but none of your datasets deal with data that would exhibit seasonality.	Lack of discussion of performance of method on different datasets
In particular, if the goal is to use this method to augment the data we use to train NLP systems in order to make them more robust, it seems that the time cost of the process will be prohibitive.	Lack of discussion of performance of method on different datasets
"-	How the first camera pose is initialized?"	Missing details on methodology (eg., use of notation, use on tasks, etc)
My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).	Missing details on methodology (eg., use of notation, use on tasks, etc)
2. The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction.	More comparisons needed with variations of the proposed method
I don't think this is really a fair comparison; I would have liked to have seen results for the unmodified reward function.	More comparisons needed with variations of the proposed method
Additionally, in section 6.4, the results in Figure 2 also does not look very	Improper presentation of results in tables and figures
It is also not clear why Table. 3 does not report the Bayes baseline results.	Improper presentation of results in tables and figures
The idea in this paper is novel but experiments do not seem to be enough.	Need more interesting problem definition
"The basic idea is very interesting, but I would have expect more interesting use cases as teased by the first sentence of the abstract ""find algorithms with strong worst-case guarantees for online combinatorial optimization problems""."	Need more interesting problem definition
The abstract mentioned that the proposed algorithm works as an “implicit regularization leading to better classification accuracy than the original model which completely ignores privacy”. But I don’t see clearly from the experimental results how the accuracy compares to a non-private classifier.	Incorrect explanation of resuts
This is actually a direct consequence of any minmax theorem in game theory; the authors decided to credit that result to Yao (I tend to *strongly* disagree with that point as, even if he stated this fact in CS, this result was quite standard several decades before him - anyway.).	Incorrect explanation of resuts
Although some promising	Not enough originality in results (not surprising)
I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations.	Not enough originality in results (not surprising)
Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers.	claims on the datasets is questionable
3. One concern I have with discrete representation is how robust they are wrt different dataset.	claims on the datasets is questionable
- Lambda sim and lambda s are used interchangeably. Please make it consistent.	Unclear description of method
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).	Unclear description of method
* The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
"4) There are several typos/grammar issues e.g. ""believed to occurs"", ""important parameters sections"", ""capacity that if efficiently allocated"", etc.)."	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).	generalizability of results is questionable
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.	generalizability of results is questionable
But there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.), not clear	Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc)
1. What are the key limitations of AutoLoss ? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ?	Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc)
Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.	correctness of results presented is questionable(eg., metrics, complexity, etc)
The proposed sampling distributions assumes independence between the random variables over which the authors optimize — I find it surprising that this leads to good empirical results	correctness of results presented is questionable(eg., metrics, complexity, etc)
In particular, the exact difference between the proposed method and the ES baseline is not as clear as it could be.	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
"- The parameter count of a 2-layer network with $h$ hidden units and input dimension $d$ is $O(dh)$. So perhaps it makes sense to study an asymptotic regime where $dh/n$ approaches $\gamma$, instead of both d and h growing linearly in n. While this issue is hinted at in the discussion section, I don't understand the statement ""the mechanism that provably gives rise to double descent from previous works Hastie et al. (2019); Belkin et al. (2019) might not translate to optimizing two-layer neural networks."""	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper.	Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)
Though promising results have been demonstrated, a drawback of the proposed method is that it introduces more memory overhead compared to GPipe.	Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.	Experimental study not strong enough
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.	Experimental study not strong enough
They can indeed be subsumed by generalization bounds based on VC theory.	Correctness of algorithm proposed
- Sec 4.1:	Correctness of algorithm proposed
Sometimes the same paper is cited 4 times within a few lines.	Incorrect citation styles
- There is no need for such repetitive citing (esp paragraph 2 on page 2).	Incorrect citation styles
While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance.	More experiments needed with related work
The MTurk experiment gives a qualitative picture, but it could be improved with comparisons to pairwise distances learned through alternative means using the RGB image itself (given that images would permit such a comparison).	More experiments needed with related work
- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect.	Incomplete ablation in terms of tables and figures
When I look at Figure 4abcd, it appears that the Convolution and Dilated Convolutions fit a clean signal faster (it is just not as clean.	Incomplete ablation in terms of tables and figures
This has not been considered in the analysis.	Too strong assumptions in analysis
There is a key concern about the feasibility of the numerical analysis for the first part.	Too strong assumptions in analysis
- The claim made in the first line of the abstract (applying RL algorithms to continuous control problems often leads to bang-bang control)	5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.
- Given the current form of the paper, the abstract and introduction should be modified to reflect the fact that only limited architectures and optimizers were experimented with, and the claims of the paper are not experimentally validated in general.	5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.
- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.	Missing explanation of comparsion with related work in tables and figures
The author never explains. E.g., link to NRMSE and PFC to the Table.	Missing explanation of comparsion with related work in tables and figures
The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions.	Lack of comparison with related works
Additionally, I believe the experimental tasks are new, and as a result all implementations of competing techniques are by the paper authors. This makes it difficult to have confidence in the higher reported performance of the proposed techniques.	Lack of comparison with related works
They might be more convincing if there were results on a very large corpus such as 1 billion word corpus.	In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.	In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.	Incomplete details on perfromance of the method
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.	Incomplete details on perfromance of the method
There were some experimental details that were poorly explained but in general the paper was readable.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
1. The presentation is somewhat convoluted.	1. The presentation is somewhat convoluted.
Quality: Below average	1. The presentation is somewhat convoluted.
This latter baseline is a zero-cost baseline as it is not even dependent on the method.	Improper comparison of related work in terms of implementation
- It would be also better to show the coefficient of existing methods that have no theoretical justification.	Improper comparison of related work in terms of implementation
5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.	Lack of analysis of proposed method
-3 For image-to-image translation experiments, no quantitative analysis whatsoever is offered so the reader can't really conclude anything about the effect of the proposed method in this domain.	Lack of analysis of proposed method
More experiments based on other types of data sets with clear global structures such as faces or stop signs will	Lack of realistic datasets used in experiments (small siz, synthetic)
2. Were there experiments that applies your metric to the training datasets like CelebA and FFHQ? In theory your metric should show no gap between N_R_obs and N_R_ref measured on the training dataset since that's the sampled ground truth.	Lack of realistic datasets used in experiments (small siz, synthetic)
2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?	Lack of experimental details (no of images, sampling criteria, etc)
I think all claims about running time should be corroborated by controlled experiments.	Lack of experimental details (no of images, sampling criteria, etc)
If the novelty is in applying to continual learning and new datasets, it is not clear that this is sufficient.	In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.
With lack of clear novel insights, or at least more systematic study on additional datasets of the 'winning' techniques and a sensitivity analysis, the paper does not give a valuable enough contribution to the field to merit publication.	In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.	Incremental novelty of method as compared to related work
For instance, using codes and codebooks to compress the weights has already been used in [1,2].	Incremental novelty of method as compared to related work
- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.	Need more experimental results
"-	The experimental results of section 5.2 are somewhat disappointing."	Need more experimental results
It is not clear how the noise is introduced in the graphs.	Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)
1. Where is L_da in Figure 2? In Figure 2, what’s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?	Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)
The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms.	Lack of discussion on datasets (size, motivation for use, etc)
"For example, I have trouble understanding the sentence ""So the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets."" in the introduction."	Lack of discussion on datasets (size, motivation for use, etc)
For example, “Fuzzy Choquet Integration of Deep Convolutional Neural Networks for Remote Sensing” by Derek T. Anderson et al.	Suggest missing related work
See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.	Suggest missing related work
However, the experimental comparison is not fair, the description of the model (e.g. how Choquet is integrated into the model and help to learn “intermediate meaningful results”) is not clear, some claims are not true.	Concerns regarding correctness of methods (assumption, budget, etc)
The two things that make me more skeptical, is the convergence of the proposed algorithm and the experiments.	Concerns regarding correctness of methods (assumption, budget, etc)
The above papers are not cited in this paper.	Missing citations in the paper
The cited paper 'Learning an adaptive learning rate schedule' does not appear online.	Missing citations in the paper
Besides, the results on the sentimental analysis are comparable with the compared baselines.	Results are more or less similar to baselines
"The authors stated that ""F-pooling remarkably increases accuracy and robustness w.r.t. shifts of moderns CNNs""; however, in Table 1-3, the winning margin of accuracy is actually quite small (<2%), and the consistency (<3.5% compared to the second best baseline except resnet-18 on CIFAR 100 has large improvement ~7-8%)."	Results are more or less similar to baselines
My main concern about the paper is that, currently, the experiments do not include any strong baseline (the ES currently is not a strong baseline, see comments below).	Lack of experiments
2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)?	Lack of experiments
4. Most importantly, for table4, authors are comparing to the ResNet18 ARNet instead of ResNet50? which is better than the proposed method.	"Perhaps I missed it, but I believe Dan Ciresan's paper ""Multi-Column Deep Neural Networks for Image Classification"" should be cited."
In addition, I observe that in Table 1, the proposed method does not outperform the Joint Training in SVHN with A_10.	"Perhaps I missed it, but I believe Dan Ciresan's paper ""Multi-Column Deep Neural Networks for Image Classification"" should be cited."
The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.	Missing literature review (some literature not included, misses baseline citations, etc)
The paper misses the key baseline in Bayesian optimisation using tree structure [1] which can perform the prediction under the tree-structure dependencies.	Missing literature review (some literature not included, misses baseline citations, etc)
The paper offers some analysis, suggesting when each of the conditions occurs, upper bounds the smallest singular value of D A (where the example dependent diagonal matrix D incorporates the ReLU activation (shouldn’t this be more clearly introduced and notated?).	Lack of discussion of analysis
this analysis is not currently included in the main body of the manuscript, but rather in the appendix, which I find rather annoying.	Lack of discussion of analysis
