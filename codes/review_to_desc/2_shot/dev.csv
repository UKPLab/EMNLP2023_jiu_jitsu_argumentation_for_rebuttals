reviews	descs
- (W3) Baselines for transfer learning: I felt this was another notable oversight.	Limited improvement over baselines
However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.	Limited improvement over baselines
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.	Incomplete details on perfromance of the method
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.	Incomplete details on perfromance of the method
Hence, I kindly do not think the outcome is truly a research result.	Not enough originality in results (not surprising)
Again, this follows from known results.	Not enough originality in results (not surprising)
-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves.	Improper comparison of related work in terms of implementation
As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.	Improper comparison of related work in terms of implementation
"Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being ""meta domain adaptation""."	claims on the datasets is questionable
One observation from the submission is that the token set may need to very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive (I noticed that the BERT model is trained on 128 GPUs)	claims on the datasets is questionable
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).	Unclear description of method
p2-3, Section 3.1 - I found the equations impossible to read. What	Unclear description of method
I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
"- In figure 5 (a) ""cencept"" should be ""concept"""	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
- It would be nice if different stopping criteria were analysed.	Lack of analysis
For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?	Lack of analysis
I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.	Missing baselines
1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing.	Missing baselines
I believe that a more challenging experiment should be conducted e.g. using celebA dataset.	Lack of realistic datasets used in experiments (small siz, synthetic)
- the data sets used in the experiments are very small	Lack of realistic datasets used in experiments (small siz, synthetic)
As the approach uses NF is derived specifically for unstable dynamics, it is not clear to me how adding it would affect the training if the dynamics *is stable*. In this context, I consider that for example, the work by Balduzzi et al. 2018 may be relevant as it describes that the dynamics of games (the Jacobian) has two components, one of which describes the oscillating behavior (Hamiltonian game); whereas most games are a mix of oscillating and non-oscillating dynamics.	incorrect claims for related work
The results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?	incorrect claims for related work
The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.	Limited novelty in theoretical contribution
While I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel.	Limited novelty in theoretical contribution
Also, from the three Tables in the experimental part, the improvement of F-pooling over AA-pooling (developed by the main reference of this work) does not seem to be significant or consistent.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
It is not clear how the compression ratio in table 1 is obtained.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
The paper would need to be improved substantially in order to appear at a conference like ICLR.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?	Missing theoretical comparisons
Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.	Missing theoretical comparisons
- The shown inception scores are far from state-of-the-art.	generalizability of results is questionable
* The biggest problem for me was the unconvincing results.	generalizability of results is questionable
I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.	Limited insights based on design choices
- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here.	Limited insights based on design choices
Although the idea behind this paper is fairly simple, the paper is very difficult to understand.	Unclear problem definition
- It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.	Unclear problem definition
Is it better to decay learning rates for toy data sets?	Less datasets used
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons""."	Less datasets used
It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).	Incremental novelty of method as compared to related work
In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.	Incremental novelty of method as compared to related work
The ResNet on Cifar-10 results are not convincing.	correctness of results presented is questionable(eg., metrics, complexity, etc)
While this is a reasonable assumption, it does not necessarily seem to be the case that a larger, more disconnected saliency map indicates worse classification performance.	correctness of results presented is questionable(eg., metrics, complexity, etc)
This paper looks very hastily put together, especially pages 7 and 8.	3. The paper is not nicely written or rather easy to follow.
There are a few typos throughout the paper such as:	3. The paper is not nicely written or rather easy to follow.
"* It is unclear to me whether the ""efficient method for SN in convolutional nets"" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides."	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
Otherwise, it is impossible for a reader to correctly and objectively relate your proposed approach to previous literature.	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
Experimental results itself are fine but not complete.	Experimental study not strong enough
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.	Experimental study not strong enough
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings."	Correctness of algorithm proposed
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?	Correctness of algorithm proposed
It seems to me; the author assumed data from one modality is generated by all the latent factors (see Eq. (11)), then what is the point for assuming the prior of the latent factor is factorized (see Eq. (4) and (5))?	generalizability of method on datasets created from different distributions is questionable
3.  Or simply to a well tuned \lambda, chosen on a per dataset basis? From the text it appears that \lambda is manually selected to trade off accuracy against uncertainty on OOD data.	generalizability of method on datasets created from different distributions is questionable
There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below.	More experiments needed with related work
- For the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbel-sinkhorn.	More experiments needed with related work
2. Sec. 3.3, Fig. 3: The capacity (in terms of parameters)of both Resnet-18 and VGG-16 is higher than the capcity for YFCC100M dataset for n=10K images (comes to 161K bits), while the capacity of Resnet-18, with 14.7 million parameters (assuming float32 encoding) has 14.7 * 32 bits = 470.4 million bits, thus capacity alone cannot explain why VGG converges faster than Resnet-18, since both networks exceed the capacity, and capacity does not seem to have an established formal connection to rate of memorization.	Lack of discussion of performance of method on different datasets
What's the relation between the size of a model and that of a data set? By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?	Lack of discussion of performance of method on different datasets
3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.	Lack of ablation on different datasets/ sizes
- Does training the generator and interpolation jointly improve the quality of the generator in general ? It would have been nice to run this method on more complicated dataset like CIFAR10 and see if this method increase the overall FID score.	Lack of ablation on different datasets/ sizes
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.	Missing details on methodology (eg., use of notation, use on tasks, etc)
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.	Missing details on methodology (eg., use of notation, use on tasks, etc)
The presented analysis seems to neglect the error term corresponding to the value function.	Too strong assumptions in analysis
"2.	What is the reason for using rule-based agents in all the experiments? It would have been more useful if all the analysis are done with RL agents rather than rule-based agents."	Too strong assumptions in analysis
