reviews	descs
2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?	Lack of experimental details (no of images, sampling criteria, etc)
However, you are in a different computational model in which you now have access to an oracle.	Theoretical misunderstanding in methodology
2. In experiments, the authors explored many existing methods on improving	2. In experiments, the authors explored many existing methods on improving
- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.	Need more experimental results
* p.4 first paragraph you claim that this method responds well to data which exhibits seasonality, but none of your datasets deal with data that would exhibit seasonality.	Lack of discussion of performance of method on different datasets
The two ideas (use of grids, and intra-life curiosity vs inter-life curiosity) should be independently investigated and put in context of past work.	The two ideas (use of grids, and intra-life curiosity vs inter-life curiosity) should be independently investigated and put in context of past work.
"-	How the first camera pose is initialized?"	Missing details on methodology (eg., use of notation, use on tasks, etc)
I would have liked to see results for both trivial baselines like random ranking as well as more informed baselines where we can estimate transfer potential using say k representation techniques, and then use that to help us understand how well it would do on the other representations.	I would have liked to see results for both trivial baselines like random ranking as well as more informed baselines where we can estimate transfer potential using say k representation techniques, and then use that to help us understand how well it would do on the other representations.
Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.	correctness of results presented is questionable(eg., metrics, complexity, etc)
My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).	Missing details on methodology (eg., use of notation, use on tasks, etc)
- Lambda sim and lambda s are used interchangeably. Please make it consistent.	Unclear description of method
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?	Missing details on methodology (eg., use of notation, use on tasks, etc)
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?	Missing details on methodology (eg., use of notation, use on tasks, etc)
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.	Incomplete details on perfromance of the method
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).	Unclear description of method
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).	generalizability of results is questionable
They can indeed be subsumed by generalization bounds based on VC theory.	Correctness of algorithm proposed
- Sec 4.1:	Correctness of algorithm proposed
- Some parts of the paper feel long-winded and aimless.	3. The paper is not nicely written or rather easy to follow.
Not sure why Eqns. 2 and 9 need any parentheses	Unclear description of method
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.	Incomplete details on perfromance of the method
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.	Experimental study not strong enough
The paper is not very self contained.	Lacking clarity overall (needs better presentation)
My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.	Missing theoretical comparisons
- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance	Missing details on methodology (eg., use of notation, use on tasks, etc)
Second, the experiments are (as I understand it, but I may be wrong) in deterministic domains, which definitely does not constitute a general test of a proposed RL  method.	design choices are questionable
Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.	Missing details on methodology (eg., use of notation, use on tasks, etc)
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.	Correctness of algorithm proposed
Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?	Limited generalizability of claims on other datasets
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.	Correctness of algorithm proposed
The function composition doesn't capture that.	Correctness of algorithm proposed
While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.	Lack of analysis
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.	Unclear description of method
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.	Less datasets used
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.	Correctness of algorithm proposed
7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images’’ -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen’’ images which it labels as the negative class, thus the negative class is also seen by the memorization model.	design choices are questionable
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.	Correctness of algorithm proposed
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:	Correctness of algorithm proposed
A number of these references are missing and no experimental comparison to these methods has been made.	Missing baselines
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches).	When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches).
There were some experimental details that were poorly explained but in general the paper was readable.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.	Improper explanation of results (as in advantages, why are results better, etc)
The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).	More variations of experiments needs to be added
One question which is not addressed is the reason for only one RBM layer.	Correctness of algorithm proposed
Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.	3. The paper is not nicely written or rather easy to follow.
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.	Unclear description of method
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.	generalizability of results is questionable
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.	Less datasets used
The justification that the method of Khadka & Tumer (2018) cannot be extended to use CEM, since the RL policies do not comply with the covariance matrix is unclear to me.	incorrect claims for related work
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).	Correctness of algorithm proposed
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.	Correctness of algorithm proposed
While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.	Experimental study not strong enough
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?	Unclear description of method
The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.	Missing literature review (some literature not included, misses baseline citations, etc)
I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.	Limited novelty in theoretical contribution
As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.	Limited insights based on design choices
This latter baseline is a zero-cost baseline as it is not even dependent on the method.	Improper comparison of related work in terms of implementation
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.	Correctness of algorithm proposed
The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times).	Limited novelty in theoretical contribution
While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance.	More experiments needed with related work
They might be more convincing if there were results on a very large corpus such as 1 billion word corpus.	In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.
Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.	More variations of experiments needs to be added
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?	Correctness of algorithm proposed
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.	Unclear description of method
Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.	generalizability of results is questionable
Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2.	design choices are questionable
Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?	Correctness of algorithm proposed
The MTurk experiment gives a qualitative picture, but it could be improved with comparisons to pairwise distances learned through alternative means using the RGB image itself (given that images would permit such a comparison).	More experiments needed with related work
"-	While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited."	Limited contribution in problem definition
In some RL tasks, it is not allowed to access the RAM state.	Correctness of algorithm proposed
Negative points: (1) The authors should provide more justification on equation-3.	Missing details on methodology (eg., use of notation, use on tasks, etc)
3) The simulation is not convincing.	Experimental study not strong enough
However, the theoretical justification and experimental results are not.	Improper Structuring of experimental results on paper (as in on some page, before some section)
-  From the plots of learning curves in appendix, the proposed methods doesn’t seem to show a huge boost of performance comparing to the uniform bandit. Could you show aggregated comparison between the proposed method and uniform bandit similarly to what is done in Figure 4 ?	Incorrect claims about performance in tables and figures
Second, the conclusion of Theorem 2 seems to be flawed.	Correctness of algorithm proposed
* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.	Limited improvement over baselines
This has not been considered in the analysis.	Too strong assumptions in analysis
Is the number right?	Correctness of algorithm proposed
It seems heavily dependent on GBDT.	Missing details on methodology (eg., use of notation, use on tasks, etc)
5. The paper is imprecise and unpolished and the presentation needs improvement.	Lacking clarity overall (needs better presentation)
How were the number of layers and kernels chosen?	Correctness of algorithm proposed
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.	Incomplete details on perfromance of the method
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.	Incomplete details on perfromance of the method
For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.	Limited improvement over baselines
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?	Incomplete details on perfromance of the method
While reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.	Lacking details on datasets
And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared.	Missing theoretical comparisons
The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
and bounded failure rate, otherwise it is not really a verification method.	Incomplete details on perfromance of the method
In addition, the paper would also need to show that such a model does not generalize to a validation set of images.	Missing details on methodology (eg., use of notation, use on tasks, etc)
However, I think the relative novelty of the paper does not meet ICLR standards, and it’s better suited as a whitepaper attached to an open dataset release.	Lacking clarity overall (needs better presentation)
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.	Correctness of algorithm proposed
- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
Thus, the advantage of using a LM optimization scheme is not very convincing.	Correctness of algorithm proposed
The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.	Incomplete details on perfromance of the method
Thus I believe authors must compare their method with these state-of-the-art approaches.	Missing theoretical comparisons
"3.	Are the authors willing to release the code? Overall the model looks complicated and the appendix is not sufficient to reproduce the results in the paper."	Missing details for reproducibility of result
"To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach."	Correctness of algorithm proposed
"-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed."	Correctness of algorithm proposed
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy."	Unclear description of method
- It would be also better to show the coefficient of existing methods that have no theoretical justification.	Improper comparison of related work in terms of implementation
- experiments are performed using a synthetic setup on a single data set, so it remains unclear if the algorithm would be successful in a real life scenario	Not enough info on dataset and hyper-parameters
I would strongly recommend including the computational cost of each method in the evaluation section.	Incomplete details on perfromance of the method
The abstract mentioned that the proposed algorithm works as an “implicit regularization leading to better classification accuracy than the original model which completely ignores privacy”. But I don’t see clearly from the experimental results how the accuracy compares to a non-private classifier.	Incorrect explanation of resuts
The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.	Missing baselines
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.	Correctness of algorithm proposed
Fig 4 is very confusing.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
To me these two reasoning statements are not particularly convincing. One could also say:	Correctness of algorithm proposed
2. table 2: Dynamic -> Adaptive?	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].	Incomplete details on perfromance of the method
This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.	Experimental study not strong enough
So there is no guarantee this algorithm will minimise the overall regret.	Correctness of algorithm proposed
Also, what will be the performance of a standard image captioning system on the task ?	Incomplete details on perfromance of the method
The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.	More variations of experiments needs to be added
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.	Correctness of algorithm proposed
In particular, the exact difference between the proposed method and the ES baseline is not as clear as it could be.	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
Towards this, how does the computational complexity scale wrt to the connectedness?	Missing details on methodology (eg., use of notation, use on tasks, etc)
-What’s the 3d plot supposed to represent?	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers.	claims on the datasets is questionable
* The experiments do not demonstrate that the model learns a meaningful *conditional* distribution for the missing modalities, since the provided figures show just one sample per conditioning image.	design choices are questionable
I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).	Experimental study not strong enough
Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.	Incorrect assumptions as compared to related work
Particularly, the authors mixed their observations up with the results of published works, making it hard to identify the contributions of this paper.	Improper explanation of results (as in advantages, why are results better, etc)
3) I would suggest a different name other than Neural-LP-N, as it is somewhat underselling this work. Also it makes Table 2 not that easy to read.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Overall the paper, while interesting is unacceptably messy.	3. The paper is not nicely written or rather easy to follow.
- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9).	Limited improvement over baselines
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??	Correctness of algorithm proposed
What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.	Missing details on methodology (eg., use of notation, use on tasks, etc)
- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.	Improper comparison of related work in terms of implementation
However, in its current state the work lacks sufficiently strong baselines to support the paper’s claims; thus, the merits of this approach cannot yet be properly assessed.	Improper comparison of related work in terms of implementation
How is this a reasonable assumption?	Correctness of algorithm proposed
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?	Correctness of algorithm proposed
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.	Correctness of algorithm proposed
- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect.	Incomplete ablation in terms of tables and figures
-- While the overall claim of the paper in the introduction seems to be to speed up frequency estimation using machine learned advice, results are only given for the Zipfian distribution.	-- While the overall claim of the paper in the introduction seems to be to speed up frequency estimation using machine learned advice, results are only given for the Zipfian distribution.
1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.	Experimental study not strong enough
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.	Missing details on methodology (eg., use of notation, use on tasks, etc)
- The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
e. What are the two modalities in Table 2? The author should explain.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.	generalizability of results is questionable
Hence, I am not very sure whether the novelty of the paper is significant.	Overall not original enough (primary claim, limited evaluation, limited novelty)
This may also help to understand some of the limitations of this analysis.	Lack of analysis
The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.	Limited novelty in theoretical contribution
Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)?	Missing theoretical comparisons
Specifically, the first contribution listed in the introduction makes it look like this paper introduces the idea of not learning the decoder on the dataset (the one that starts with “The network is not learned and itself incorporates all assumptions on the data.”).	Specifically, the first contribution listed in the introduction makes it look like this paper introduces the idea of not learning the decoder on the dataset (the one that starts with “The network is not learned and itself incorporates all assumptions on the data.”).
More rigorous experiments and analysis is needed to make this a good ICLR paper.	The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).	Unclear description of method
There is a key concern about the feasibility of the numerical analysis for the first part.	Too strong assumptions in analysis
- The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.	Limited novelty in theoretical contribution
Instead only depth is provided.	Unclear description of method
Although some promising	Not enough originality in results (not surprising)
It is unclear how well the proposed method works in general.	Unclear description of method
Therefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).	3. The paper is not nicely written or rather easy to follow.
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).	Correctness of algorithm proposed
10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).	Limited improvement over baselines
While the authors do report some interesting results, they do a poor job of motivating the proposed extensions.	Missing details for reproducibility of result
Sometimes the same paper is cited 4 times within a few lines.	Incorrect citation styles
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper."	Missing details on methodology (eg., use of notation, use on tasks, etc)
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.	Correctness of algorithm proposed
Besides, as the base classifier is different for various baselines, it is hard to compare the methods.	Incorrect baselines used
The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms.	Lack of discussion on datasets (size, motivation for use, etc)
While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.	Incomplete details on perfromance of the method
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.	Correctness of algorithm proposed
The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.	generalizability of results is questionable
Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent.	Limited novelty in theoretical contribution
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.	Experimental study not strong enough
- I'm not sure we can conclude much from the results on fetchSlide (and it would make sense not to use the last set of parameters but the best one encountered during training)	correctness of experiments is questionable
2. The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction.	More comparisons needed with variations of the proposed method
I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.	generalizability of results is questionable
In the real world, one would not have access to OOD data during training, how is one to pick \lambda in such cases?	Limited generalizability of claims on other datasets
"At the very least we need another ""partial"" sign in front of the ""\delta"" function in the numerator."	Correctness of algorithm proposed
The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.	Incomplete details on perfromance of the method
Did you try to have a single network?	Incomplete details on perfromance of the method
RNN-based approaches are with better “complexity” comparing to your sum baseline and “Deepset” approach.	Incorrect baselines used
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?	Incomplete details on perfromance of the method
In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.	Incomplete details on perfromance of the method
It is not clear how the noise is introduced in the graphs.	Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)
However such problems are entirely missing in the results section.	generalizability of results is questionable
It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text.	Missing theoretical comparisons
Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).	generalizability of results is questionable
i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.	Incomplete details on perfromance of the method
Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.	Incomplete details on perfromance of the method
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).	Correctness of algorithm proposed
When I look at Figure 4abcd, it appears that the Convolution and Dilated Convolutions fit a clean signal faster (it is just not as clean.	Incomplete ablation in terms of tables and figures
4. How sensitive are the results to the number of adaptive kernels in the layers.	generalizability of results is questionable
Then, can the function family the authors used in the paper approximate this function?	Correctness of algorithm proposed
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper	Unclear description of method
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:	Less datasets used
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.	Less datasets used
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.	Correctness of algorithm proposed
"-	The experimental results of section 5.2 are somewhat disappointing."	Need more experimental results
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.	Correctness of algorithm proposed
Current representation is difficult to read / parse.	3. The paper is not nicely written or rather easy to follow.
But there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.), not clear	Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc)
2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper.	Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)
My major concern is whether the results are significant enough to deserve acceptance.	generalizability of results is questionable
The input and output types of each block in Figure 1. should be clearly stated.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Finally, the CNN architecture should be compared with modern SAT solvers which have been participating to SAT competitions.	Missing theoretical comparisons
The paper misses the key baseline in Bayesian optimisation using tree structure [1] which can perform the prediction under the tree-structure dependencies.	Missing literature review (some literature not included, misses baseline citations, etc)
Dual-1 and Dual-5 are introduced without explanation.	Missing details on methodology (eg., use of notation, use on tasks, etc)
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?"	Correctness of algorithm proposed
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.	Correctness of algorithm proposed
Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?	Lack of analysis
It is unclear how the model actually operates and uses attention during execution.	Unclear description of method
- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space	Limited novelty in theoretical contribution
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.	Missing details on methodology (eg., use of notation, use on tasks, etc)
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.	Correctness of algorithm proposed
Due to several shortcomings of the paper, most important of which is on presentation of the paper, this manuscript requires a significant revision by the authors to reach the necessary standards for publication, moreover it would be helpful to clarify the modeling choices and consequences of these choices more clearly.	While sensible, this seems to me to be too minor a contribution to stand alone as a paper.
Citations used for Gradcam are wrong -- Sundarajan et al., 2016 should be changed to Selvaraju et al., 2017.	Incorrect citation
The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...	Missing details on methodology (eg., use of notation, use on tasks, etc)
Theorem 1 does not take account for the above conditions.	Incomplete details on perfromance of the method
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.	Incomplete details on perfromance of the method
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.	Correctness of algorithm proposed
- it's better to show time v.s. testing accuracy as well.	generalizability of results is questionable
As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.	Incomplete details on perfromance of the method
"For example, ""The old system of private arbitration courts is off the table"" from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture."	Incorrect assumptions as compared to related work
The drawbacks  of the work include the following: (1) There is not much technical contribution.	Incomplete details on perfromance of the method
2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.	Limited improvement over baselines
4. Most importantly, for table4, authors are comparing to the ResNet18 ARNet instead of ResNet50? which is better than the proposed method.	"Perhaps I missed it, but I believe Dan Ciresan's paper ""Multi-Column Deep Neural Networks for Image Classification"" should be cited."
Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.	Incomplete details on perfromance of the method
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.	Unclear description of method
It is in the end plugged into a continual learning algorithm which also performs domain transformation.	Unclear description of method
I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.	Limited improvement over baselines
1. Where is L_da in Figure 2? In Figure 2, what’s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?	Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.	Incomplete details on perfromance of the method
1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.	Incomplete details on perfromance of the method
There are many typos and unclear statements.	3. The paper is not nicely written or rather easy to follow.
Thus, it is hard to say whether the results are applicable in practice.	generalizability of results is questionable
Concerns: I find the claim on deep networks kind of irresponsible.	Correctness of algorithm proposed
More experiments based on other types of data sets with clear global structures such as faces or stop signs will	Lack of realistic datasets used in experiments (small siz, synthetic)
1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled “Generative Ensembles for Robust Anomaly Detection” makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.	Correctness of algorithm proposed
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.	Incomplete details on perfromance of the method
The paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.	3. The paper is not nicely written or rather easy to follow.
Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper.	Missing theoretical comparisons
"3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?"	Missing details on methodology (eg., use of notation, use on tasks, etc)
In a related note, using MF for training BMs have been proposed previously and found to not work due to various reasons:	Incorrect assumptions as compared to related work
Overall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty.	Overall not original enough (primary claim, limited evaluation, limited novelty)
The real data experiments (sections 4.2 and 4.3) are not very convincing, not only because of the very small size of N, but also because there is no comparison with the other approaches.	The real data experiments (sections 4.2 and 4.3) are not very convincing, not only because of the very small size of N, but also because there is no comparison with the other approaches.
Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?	Missing details on methodology (eg., use of notation, use on tasks, etc)
There are many typos and grammar errors	3. The paper is not nicely written or rather easy to follow.
- Experimental results provided in the paper are only qualitative -- as such, I do not find the comparisons (and improvements) over the existing approaches convincing enough.	Missing interpretation of results due to missing related work
For example, in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.	Missing theoretical comparisons
My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small.	Lacking details on datasets
1) There is no clear rationale on why we need a new model based on Transformers for this task.	Unclear description of method
- There is a typo in equation 6	Unclear description of method
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.	Unclear description of method
- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.	generalizability of results is questionable
Although the idea is interesting, I would like to see more experimental results showing the scalability of the proposed method and for evaluating defense strategies against different types of adversarial attacks.	Although the idea is interesting, I would like to see more experimental results showing the scalability of the proposed method and for evaluating defense strategies against different types of adversarial attacks.
Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.	Missing details on methodology (eg., use of notation, use on tasks, etc)
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.	Correctness of algorithm proposed
Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.	Experimental study not strong enough
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.	Unclear description of method
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?	Incomplete details on perfromance of the method
I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations.	Not enough originality in results (not surprising)
What is G_t in Theorem 2.5. It should be defined in the theorem itself.	Incomplete details on perfromance of the method
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.	Correctness of algorithm proposed
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.	Incomplete details on perfromance of the method
Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.	Missing details on methodology (eg., use of notation, use on tasks, etc)
(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well)	Lack of analysis
Please comment on the choice, and its impact on the behavior of the model.	Incomplete details on perfromance of the method
Perhaps I am misreading this plot, but it is not obvious to me that this plot supports the claims the authors are making.	Incomplete ablation in terms of tables and figures
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?	Missing details on methodology (eg., use of notation, use on tasks, etc)
- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.	Limited novelty in theoretical contribution
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.	Correctness of algorithm proposed
The proposed sampling distributions assumes independence between the random variables over which the authors optimize — I find it surprising that this leads to good empirical results	correctness of results presented is questionable(eg., metrics, complexity, etc)
-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.	Lack of analysis
something that is either deterministic, or a probabilistic result with a small	generalizability of results is questionable
In particular, if the goal is to use this method to augment the data we use to train NLP systems in order to make them more robust, it seems that the time cost of the process will be prohibitive.	Lack of discussion of performance of method on different datasets
- The idea is a simple extension of existing work.	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
It is also not clear to me how domain translation is relevant to continual learning.	Missing details on methodology (eg., use of notation, use on tasks, etc)
While I wish there were more such studies -- as I believe reproducing past results experimentally is important, and so is providing practical advice for practitioners -- this work in many parts hard to follow, and it is hard to get lot of new insight from the results, or a better understanding of GANs.	Not enough originality in results (not surprising)
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not."	Correctness of algorithm proposed
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).	Correctness of algorithm proposed
It is hard to support this motivation when no experiments are done in its favor.	Experimental study not strong enough
How would the given graph network compare to this?	Correctness of algorithm proposed
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.	Correctness of algorithm proposed
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.	Experimental study not strong enough
- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.	Limited insights based on design choices
If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.	Missing theoretical comparisons
2. Were there experiments that applies your metric to the training datasets like CelebA and FFHQ? In theory your metric should show no gap between N_R_obs and N_R_ref measured on the training dataset since that's the sampled ground truth.	Lack of realistic datasets used in experiments (small siz, synthetic)
I also was not convinced by the experiments which are mostly qualitative. I did not find that this set of experiments provide enough support to the proposed method.	More variations of experiments needs to be added
"For example, I have trouble understanding the sentence ""So the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets."" in the introduction."	Lack of discussion on datasets (size, motivation for use, etc)
It is necessary to test it on datasets with much more fine classes and much-complicated hierarchy, e.g., ImageNet, MS COCO or their subsets, which have ideal class hierarchy structures.	Lacking details on datasets
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?	Correctness of algorithm proposed
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.	Correctness of algorithm proposed
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation"	Unclear description of method
I’m also confused by the presentation of the results.	I’m also confused by the presentation of the results.
A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).	Limited improvement over baselines
2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling.	Need more experimental results
- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.	Lack of analysis
However, the evaluation of the proposed adaptive kernels is rather limited.	Incomplete details on perfromance of the method
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.	Correctness of algorithm proposed
1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019.	Missing baselines
While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.	Limited insights based on design choices
I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.	Lack of analysis
A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.	Incomplete ablation in terms of tables and figures
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.	Correctness of algorithm proposed
I don't think this is really a fair comparison; I would have liked to have seen results for the unmodified reward function.	More comparisons needed with variations of the proposed method
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).	Unclear description of method
- in section 4.3 how is the reconstruction built (Figure 3b)?	Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)
A reader’s most natural question is whether there is a large enough improvement to offset the extra computational cost, so the fact that wall-clock times are relegated to the appendix is a significant weakness.	Improper explanation of results (as in advantages, why are results better, etc)
The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions.	Lack of comparison with related works
Moreover, in spite of the authors writing that their goal is “completely different” from [Lee at al 18a, Ma et al 18a], I found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.	Missing baselines
The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.	Incomplete details on perfromance of the method
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.	Unclear description of method
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?	Incomplete details on perfromance of the method
2 The experimental settings are not reasonable.	Experimental study not strong enough
-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well.	Limited novelty in theoretical contribution
A more general function is	Correctness of algorithm proposed
My main concern about the paper is that, currently, the experiments do not include any strong baseline (the ES currently is not a strong baseline, see comments below).	Lack of experiments
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.	Correctness of algorithm proposed
It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent.	Limited novelty in theoretical contribution
The idea in this paper is novel but experiments do not seem to be enough.	Need more interesting problem definition
- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.	Correctness of algorithm proposed
"-	Figures 1 is way too abstract given the complicated set-up of the proposed architecture."	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
There are also concerns about the motivations behind parts of the technique.	Limited insights based on design choices
Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.	Incomplete details on perfromance of the method
In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.	3. The paper is not nicely written or rather easy to follow.
Finally, the experimental part is also too weak to evaluate the proposed method.	More variations of experiments needs to be added
The idea of using a constrained formulation is not novel either (constrained MDPs have been thoroughly studied since Altman (1999)).	The idea of using a constrained formulation is not novel either (constrained MDPs have been thoroughly studied since Altman (1999)).
3. One concern I have with discrete representation is how robust they are wrt different dataset.	claims on the datasets is questionable
5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.	Lack of analysis of proposed method
2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?	Lack of analysis
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.	Unclear description of method
This could have made the paper much stronger.	While sensible, this seems to me to be too minor a contribution to stand alone as a paper.
1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey.	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.	Missing details on methodology (eg., use of notation, use on tasks, etc)
However, the experimental comparison is not fair, the description of the model (e.g. how Choquet is integrated into the model and help to learn “intermediate meaningful results”) is not clear, some claims are not true.	Concerns regarding correctness of methods (assumption, budget, etc)
- It is not clear why authors did not follow the evaluation protocol of [Achlioptas’17] or [Wu’16] more closely.	Incorrect assumptions as compared to related work
It would make sense to use image captioning data to create the image lookup.	Less datasets used
While sensible, this seems to me to be too minor a contribution to stand alone as a paper.	While sensible, this seems to me to be too minor a contribution to stand alone as a paper.
I don’t see a significant difference between RAN and DNN in Figure 5. Maybe more explanation or better visualization would help.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
- doesn’t answer the one question regarding observation representation that it set out to evaluate	Correctness of algorithm proposed
If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?	Incomplete details on perfromance of the method
I would have liked to see a bit more analysis as to why some pre-training strategies work over others.	Lack of analysis
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?	Incomplete details on perfromance of the method
In contrast, the studied learning rates are asymptotic and there is a big discrepancy.	Missing details on methodology (eg., use of notation, use on tasks, etc)
2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?	Missing details on methodology (eg., use of notation, use on tasks, etc)
In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?	Missing details on methodology (eg., use of notation, use on tasks, etc)
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.	Incremental novelty of method as compared to related work
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text."	Unclear description of method
-Some technical details  are missing.	Unclear description of method
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.	Incomplete details on perfromance of the method
- it wasn't clear how the sparsity percentage on page 3 was defined?	Missing details on methodology (eg., use of notation, use on tasks, etc)
You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.	incorrect claims for related work
Specifically, prior work considered non-missing data during training, while we can't always guarantee that all the modalities are available.	Incorrect assumptions as compared to related work
So this work has to be supported with more detailed experimental results to express the potential of this approach fully.	Need more experimental results
- The analysis in [1] handles the case of noisy updates, whereas the analysis given here only works for exact updates.	Too strong assumptions in analysis
However the theoretical contribution is poor. And the experiment uses a very classical benchmark providing simulated data.	Not enough novelty in experiments (seems similar to previous work)
While I like the premise of the paper, I feel that it needs more work.	Lacking clarity overall (needs better presentation)
Please explain the logic for this architectural choice.	Correctness of algorithm proposed
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.	Incomplete details on perfromance of the method
More explanation of canonicalization is needed.	Correctness of algorithm proposed
A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper.	Not enough originality in results (not surprising)
Clarity: The clarity is below average.	3. The paper is not nicely written or rather easy to follow.
Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.	Incomplete details on perfromance of the method
All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.	Incomplete details on perfromance of the method
- Contribution overall may be a bit limited	While sensible, this seems to me to be too minor a contribution to stand alone as a paper.
Making this algorithm not very practical.	Incomplete details on perfromance of the method
Eq (2) cannot have Delta Chi on the two sides.	Correctness of algorithm proposed
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.	Correctness of algorithm proposed
However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.	Limited novelty in theoretical contribution
Attacking CRBMs is highly relevant and should be included as a baseline.	Missing baselines
6. Validation data / test sets: Throughout this work, it is unclear what / how validation is performed.	Lack of discussion on datasets (size, motivation for use, etc)
The paper offers some analysis, suggesting when each of the conditions occurs, upper bounds the smallest singular value of D A (where the example dependent diagonal matrix D incorporates the ReLU activation (shouldn’t this be more clearly introduced and notated?).	Lack of discussion of analysis
3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used.	Need more experimental results
5. One of the anonymous comments on OpenReview is very interesting: samples from a CIFAR model look nothing like SVHN.	claims on the datasets is questionable
Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.	Incomplete details on perfromance of the method
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.	Experimental study not strong enough
"In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM"")."	Correctness of algorithm proposed
The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.	Incomplete details on perfromance of the method
4.4, law of total variation -- define	Unclear description of method
"Also, a ""1x"" label seems to be missing in for the full softmax, so that the reference is clearly specified."	Incorrect assumptions as compared to related work
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.	Incomplete details on perfromance of the method
"The basic idea is very interesting, but I would have expect more interesting use cases as teased by the first sentence of the abstract ""find algorithms with strong worst-case guarantees for online combinatorial optimization problems""."	Need more interesting problem definition
Additionally, I believe the experimental tasks are new, and as a result all implementations of competing techniques are by the paper authors. This makes it difficult to have confidence in the higher reported performance of the proposed techniques.	Lack of comparison with related works
Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
I believe it will not be great, but I think for completeness, you should add such a baseline.	Limited improvement over baselines
Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.	generalizability of results is questionable
While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e.	Missing theoretical comparisons
For example, it is unclear to me why some larger models are not amenable to truncation.	Unclear description of method
There is not sufficient detail to reproduce the models based on the paper alone.	Missing details on methodology (eg., use of notation, use on tasks, etc)
On the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the Fig. is very confusing.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
However, the experiments feel like they are missing motivation as to why this method is being used.	More variations of experiments needs to be added
The only issue with this paper is its degree of novelty, which is narrow.	Overall not original enough (primary claim, limited evaluation, limited novelty)
1. The approach is not well justified either by theory or practice.	Correctness of algorithm proposed
As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.	Incomplete details on perfromance of the method
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.	Unclear description of method
However, the selected experiments on the cubic regression toy data (Section 5.1) and the out-of-distribution classification (Section 5.2) are clear examples of system’s noise, i.e. epistemic uncertainty.	Lack of realistic datasets used in experiments (small siz, synthetic)
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.	Correctness of algorithm proposed
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?	Correctness of algorithm proposed
However, the authors do not provide an in-depth discussion of this phenomena.	Incomplete details on perfromance of the method
* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model.	design choices are questionable
Weaknesses: Paper could have been written better. I had hard time understanding it.	3. The paper is not nicely written or rather easy to follow.
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?	Correctness of algorithm proposed
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.	Correctness of algorithm proposed
The introduction contains a few statements that may paint an incomplete or confusing picture of the current literature in adversarial attacks on neural networks:	Incorrect claims in introduction (confusing related work presented in intro, wrong claims)
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.	Unclear description of method
This hyperparameter itself benefits from (requires?) some scheduling.	Correctness of algorithm proposed
It is not clear to me that the classifier difference metric is well-defined.	Missing details on methodology (eg., use of notation, use on tasks, etc)
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).	Incomplete details on perfromance of the method
Furthermore, in the beginning of Section 3.1 the authors present their idea on probabilistic hypernetoworks which “maps x to a distribution over parameters instead of specific value \theta.” How is this different from the case that we were considering so far? If we had a point estimate for \theta we would not require to take an expectation in Equation (3) in the first place.	While better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.
For instance, using codes and codebooks to compress the weights has already been used in [1,2].	Incremental novelty of method as compared to related work
If faster training of dictionary learning models was a bottleneck in practical applications, this might be of interest, but it is not.	Missing explanation of utility of proposed method
2) The main contributions of this paper are not quite clear to me.	3. The paper is not nicely written or rather easy to follow.
- Although the authors discussed the experiment setting in detail in supplements, I believe open-sourcing the code / software used to conduct the experiments would be greatly help with the reproducibility of the proposed method for researchers or practitioners.	Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.
2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem  or sample selection problem)	Limited contribution in problem definition
Overall, the method looks incremental and experimental results are mixed on small datasets so I vote for rejection.	Overall, the method looks incremental and experimental results are mixed on small datasets so I vote for rejection.
More importantly, the results presented are quite meager.	generalizability of results is questionable
It is highly recommended to provide the pseudocode of the proposed method.	Missing details on methodology (eg., use of notation, use on tasks, etc)
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.	Incomplete details on perfromance of the method
- there is no attempt to provide a theoretical insight into the performance of the algorithm	Incomplete details on perfromance of the method
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?"	Correctness of algorithm proposed
The imagenet experiment lacks details.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
One drawback is that it is highly specific to language models.	Limited insights based on design choices
This is also lacking from the description of the experimental protocol, which does not address the data-splits (how many classes were used for each) and size of the unlabelled test set.	Limited generalizability of claims on other datasets
In terms of writing, the paper is a bit confusing in terms of motivations and notations.	3. The paper is not nicely written or rather easy to follow.
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps	Unclear description of method
This is a very good point, however the paper do not compare or contrast with existing methods.	Improper comparison of related work in terms of implementation
Three datasets cannot make the experiments convincing.	Lack of realistic datasets used in experiments (small siz, synthetic)
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?	Correctness of algorithm proposed
- Why does structured exploration reduce the number of network parameters that need to be learned?	Incomplete details on perfromance of the method
So at the end, I am a bit puzzled. I really like the idea, but I have the feeling that this technique should have been developed for more complicated setting. Or maybe it is actually not working on more difficult combinatorial problem (and this is hidden in the paper).	Need more interesting problem definition
The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.	Missing literature review (some literature not included, misses baseline citations, etc)
a) The uncertainties produced by CDN in Figure 2 seems strange.	Incomplete ablation in terms of tables and figures
"It should be better motivated why one should use the duality gap as an upper bound for the ""F-distance""."	Missing details on methodology (eg., use of notation, use on tasks, etc)
One of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search.	Missing theoretical comparisons
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.	Incomplete details on perfromance of the method
"and rendering the third claim from the introduction (""We propose an new algorithm with the new metric which demonstrates better results than state-of-the-art algorithms."") completely untrue."	"and rendering the third claim from the introduction (""We propose an new algorithm with the new metric which demonstrates better results than state-of-the-art algorithms."") completely untrue."
3. The structure of the meta-training loop was unclear to me.	Incomplete details on perfromance of the method
* The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
3. This apart, I think that the experiment section is pretty hard to read, given all the metrics and methodology is in the Appendix.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
"Also, Figure 6 is referenced in the text in the context of binary multiplication (""[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6""), but presents results for addition and factorization only."	Incomplete ablation in terms of tables and figures
"2. In the caption of figure 2, there should be a space after `"":""."	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
The convergence analysis is on Z, not on parameters x and hyper-parameters theta.	Too strong assumptions in analysis
1. The paper should also talk about the details of ARNet and discuss the difference, as I assume they are the most related work	Missing baselines
-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization	correctness of results presented is questionable(eg., metrics, complexity, etc)
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
"4) There are several typos/grammar issues e.g. ""believed to occurs"", ""important parameters sections"", ""capacity that if efficiently allocated"", etc.)."	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?"	Incomplete details on perfromance of the method
If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.	generalizability of results is questionable
Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.	generalizability of results is questionable
- Why are there missing BLEU scores and the number of parameters in Table 1?	Incomplete ablation in terms of tables and figures
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.	Correctness of algorithm proposed
(1) using codes and codebooks to compress weights; and	Limited novelty in theoretical contribution
However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.	Missing theoretical comparisons
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.	Unclear description of method
The plot (Figure2 (d)) is very blurry and people cannot really tell local structure from it.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
I think all claims about running time should be corroborated by controlled experiments.	Lack of experimental details (no of images, sampling criteria, etc)
6. On CIFAR10 the results seem to be worse that other methods.	Limited improvement over baselines
Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.	Incomplete details on perfromance of the method
There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.	Incomplete details on perfromance of the method
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).	Correctness of algorithm proposed
Moreover, as observed by the authors this analysis currently rely on strong assumptions that might make it rather unrealistic.	Too strong assumptions in analysis
Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training.	Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training.
Besides, the results on the sentimental analysis are comparable with the compared baselines.	Results are more or less similar to baselines
"""Table 4 shows that our first results are promising, even though they are not as good as the state of the art."" The state of the art on LibriSpeech is not Mohamed at al. 2019. See e.g. Irie et al. Interspeech 2019 for better result"	correctness of results presented is questionable(eg., metrics, complexity, etc)
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.	Experimental study not strong enough
Some figures, like Figure 3 and 4, are hard to read.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).	Incomplete details on perfromance of the method
The testbeds all existed previously and this is mostly the effort of pulling then together.	Incremental novelty of method as compared to related work
- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)	Missing baselines
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.	Unclear description of method
Some of the experiments (eg. comparisons involving ShakeShake and ScheduledDropPath, Section 5.2) could also be moved to the appendix in order to make room for a description of LEMONADE in the main paper.	Some of the experiments (eg. comparisons involving ShakeShake and ScheduledDropPath, Section 5.2) could also be moved to the appendix in order to make room for a description of LEMONADE in the main paper.
What if you used simpler online learning algorithms with formal accuracy guarantees?	Incomplete details on perfromance of the method
Overall, I am not sure what we could gain from this research direction.	While sensible, this seems to me to be too minor a contribution to stand alone as a paper.
The comparisons are also absent in experiments.	More experiments needed with related work
There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.	Limited improvement over baselines
Cons:  unclear transfer learning model, insufficient experiments.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail	Lack of analysis
But there are no comparisons between the proposed training method and previous related works.	Incorrect baselines used
- In Table 4, it is hard to compare DeepTwist with the other methods because activation quantization is not used.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?	Correctness of algorithm proposed
3) The experiments are completely preliminary and not reasonable:	Experimental study not strong enough
Indeed, without referencing the original Pointer Network and (and especially the) Transformer papers, it would not be possible to understand this paper at all.	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.	Correctness of algorithm proposed
Thus, the theoretical contribution of this paper is limited.	Incomplete details on perfromance of the method
Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?)	Lack of analysis
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.	Correctness of algorithm proposed
The IDF scores would be stronger if they were computed on a bigger in-domain corpus than the gold test set.	Lacking details on datasets
Both the results on the development set and on the test set should be reported for the validity of the experiments.	Need more experimental results
1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data.	claims on the datasets is questionable
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.	Unclear description of method
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.	Missing details on methodology (eg., use of notation, use on tasks, etc)
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?	Incomplete details on perfromance of the method
In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.	More variations of experiments needs to be added
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).	Correctness of algorithm proposed
This is actually a direct consequence of any minmax theorem in game theory; the authors decided to credit that result to Yao (I tend to *strongly* disagree with that point as, even if he stated this fact in CS, this result was quite standard several decades before him - anyway.).	Incorrect explanation of resuts
I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning.	design choices are questionable
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.	Unclear description of method
"--What does a ""heavy classifier"" imply concretely?"	Missing details on methodology (eg., use of notation, use on tasks, etc)
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?	Correctness of algorithm proposed
The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper.	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.	Unclear description of method
-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?	Missing details on methodology (eg., use of notation, use on tasks, etc)
Therefore, it is a little misleading to still call it Bayesian active learning.	Correctness of algorithm proposed
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.	Correctness of algorithm proposed
- figures 2 & 3 should be a lot larger in order to be readable	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?"	Correctness of algorithm proposed
The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.	generalizability of results is questionable
Though promising results have been demonstrated, a drawback of the proposed method is that it introduces more memory overhead compared to GPipe.	Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)
Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).	Incomplete details on perfromance of the method
Spectrum pooling has been used in the community of computer vision and machine learning.	Incremental novelty of method as compared to related work
The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.	Incomplete details on perfromance of the method
The experiments are not very convincing or illustrative of the theoretical results in my opinion.	design choices are questionable
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.	Correctness of algorithm proposed
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.	Correctness of algorithm proposed
These synthetic setting can be used for sanity check, but cannot be the main part of the experiments.	design choices are questionable
Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?	Missing details on methodology (eg., use of notation, use on tasks, etc)
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.	Correctness of algorithm proposed
While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.	correctness of results presented is questionable(eg., metrics, complexity, etc)
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.	Unclear description of method
There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.	Limited novelty in theoretical contribution
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.	Correctness of algorithm proposed
* \sigma is not given in Figure 3(a)	Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)
The idea that introduces labels in VAE is not novel.	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
The above papers are not cited in this paper.	Missing citations in the paper
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)	Unclear description of method
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.	Experimental study not strong enough
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.	Correctness of algorithm proposed
However, I have a few concerns about the results.	correctness of results presented is questionable(eg., metrics, complexity, etc)
- Page 14, Eq(14), \lambda should be s	Unclear description of method
- Limited analysis of model/architecture design choices	Reproducibility of result
I also struggled a little to understand what is the difference between forward interpolate and filtering.	Unclear description of method
In particular, the qualitative results are too limited and no quantitative evaluations is provided.	generalizability of results is questionable
Nevertheless, I believe that it still has to address some points in order to be better suited for publication:	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
"* In the introduction, ""the classical approach"" is mentioned but to be the latter is"	Unclear intro (eg. contributions)
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.	Correctness of algorithm proposed
- Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.	Less datasets used
- Grammatical errors and odd formulations all over the place	3. The paper is not nicely written or rather easy to follow.
Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.	Incomplete details on perfromance of the method
"2.	The experiments are rather insufficient."	Experimental study not strong enough
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.	Missing details on methodology (eg., use of notation, use on tasks, etc)
(a) a comparison to other methods (outside the current framework) for sound separation	Improper comparison of related work in terms of implementation
"9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to."	Unclear description of method
First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?	Missing details on methodology (eg., use of notation, use on tasks, etc)
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.	Correctness of algorithm proposed
- the more interesting problem, RL + auxiliary loss isn’t evaluated in detail	Need more interesting problem definition
Was crossvalidation used to select the topology?	Missing details on methodology (eg., use of notation, use on tasks, etc)
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.	Correctness of algorithm proposed
3) The overall performance of the proposed SST in the experiments is not convincing and not promising.	design choices are questionable
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.	Correctness of algorithm proposed
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?	Correctness of algorithm proposed
"The performance of the proposed method is worse than the previous work but they claimed ""state-of-the-art"" results."	incorrect claims for related work
Fourth, there are some grammar mistakes and typos.	3. The paper is not nicely written or rather easy to follow.
"* In related work, no reference to previous work on ""statistical"" approaches to NN"	Missing baselines
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.	Correctness of algorithm proposed
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets	Incomplete details on perfromance of the method
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?	Correctness of algorithm proposed
The role of \sigma seems very redundant given \omega.	Correctness of algorithm proposed
What is the purpose then for introducing the matrix variate Gaussian?	Missing details on methodology (eg., use of notation, use on tasks, etc)
Third, the comparison to baseline and “DeepSet” is not fair.	Missing baselines
First, I consider the tabular features as multi-feature data and less to be the multimodal data.	claims on the datasets is questionable
The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.	Incomplete details on perfromance of the method
Similarly, I'd have expected baselines that included those models in the evaluation section showing the differences in performance between the newly proposed Transformer model for trees and previously used methods.	Improper comparison of related work in terms of implementation
However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.	Experimental study not strong enough
Further the contributions are not clear at all, since a large part of the detailed approach/equations relate to the masking which was taken from previous work.	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.	Missing details on methodology (eg., use of notation, use on tasks, etc)
I vote to reject the paper at this stage, mainly because of the following three points:	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
- The paper is over the hard page limit for ICLR so needs to be edit to reduce the length.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?	Missing details on methodology (eg., use of notation, use on tasks, etc)
- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.	Lacking details on datasets
Given the paper title, the reviewer would have expected more experiments in a multiple domain context.	Experimental study not strong enough
It looks like all the results are given on the test set. Did you not do any tuning on the validation data?	claims on the datasets is questionable
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.	Incomplete details on perfromance of the method
The proposed method is also heuristic and lacks promising guarantee.	Correctness of algorithm proposed
Also, since the synthetic image pairs are not multimodal in nature, it is unclear to me for what the messages are conveyed in Figure 3 and 4.	Incomplete ablation in terms of tables and figures
"* Some turns of phrase like ""recently gained a flourishing interest"", ""there is still a wide gap in quality of results"", ""which implies a variety of underlying factors"", ... are vague / do not make much sense and should probably be reformulated to enhance readability."	Improper explanation of results (as in advantages, why are results better, etc)
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.	Correctness of algorithm proposed
"1. The title of the paper is ""visual reasoning by progressive module networks."" The title may be a little overstated since the major task is focused on visual question answering (VQA)."	Incorrect claims in introduction (confusing related work presented in intro, wrong claims)
I feel the baseline in domain adaptation area is a bit limited.	Limited improvement over baselines
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.	Correctness of algorithm proposed
The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.	Incomplete details on perfromance of the method
The definition of “recovering true factor exactly” need to be given.	Correctness of algorithm proposed
The remaining components of the proposed method are not very new.	Limited novelty in theoretical contribution
Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.	Incomplete details on perfromance of the method
However it's not clear how is this range used in practice ? Do you sample uniformly $\alpha$ in this range to train the linear interpolation ?	Lack of experimental details (no of images, sampling criteria, etc)
Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.	Incomplete details on perfromance of the method
There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven.	Need more experimental results
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?	Incomplete details on perfromance of the method
2. I don’t understand Figure 4.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
1. The presentation is somewhat convoluted.	1. The presentation is somewhat convoluted.
However, memory overhead is still an issue compared to existing method.	Missing baselines
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.	Incomplete details on perfromance of the method
It is difficult to judge the performance of the proposed model based on so small data set.	Less datasets used
Regarding the experimental evaluation of the model rather confusing.	Experimental study not strong enough
Is Harmonic Convolution applicable to complex STFT coefficients as well?	Missing details on methodology (eg., use of notation, use on tasks, etc)
But there are some minus ones in the random projection?	Correctness of algorithm proposed
"-2 Diversity of additional ""agents"" not analyzed (more below)."	Lack of analysis
Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.	Incomplete details on perfromance of the method
"2.	The experimental data set is too small, with only 635 problems."	Lack of realistic datasets used in experiments (small siz, synthetic)
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.	Correctness of algorithm proposed
Hence the theoretical sample complexities contributed are not comparable to those of MIME.	Missing theoretical comparisons
On the other hand, the obtained results are very weak: only one layered version of the paper is analysed and the theorem applies only to networks with less than some threshold of parameters.	Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)
The novelty of this method is minimal.	Limited novelty in theoretical contribution
It would be helpful to have a brief paragraph describing this architecture, for readers not familiar with the referenced paper.	Missing implementation details of related work used as baselines
However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:	Incremental novelty of method as compared to related work
If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.	generalizability of results is questionable
- The term p(w) disappears on the left hand side of Eq 2.	Unclear description of method
- My main concern about the analysis is that it shows why several methods (e.g., momentum, multiple update steps) are *not* helpful for stabilising GANs, but does not tell why training with these methods, as well as others such as gradient penalty, *do converge* in practice with properly chosen hyper-parameters?	- My main concern about the analysis is that it shows why several methods (e.g., momentum, multiple update steps) are *not* helpful for stabilising GANs, but does not tell why training with these methods, as well as others such as gradient penalty, *do converge* in practice with properly chosen hyper-parameters?
"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?"	Less datasets used
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.	Correctness of algorithm proposed
In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.	Improper comparison of related work in terms of implementation
- The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper.	Limited improvement over baselines
Third, I don’t get what is plotted on different subplots.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?	Incomplete details on perfromance of the method
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.	Incomplete details on perfromance of the method
However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison.	Missing baselines
However, the idea is very related to Yeh et al.’s work which has already published but not mentioned at all.	Missing baselines
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.	Unclear description of method
“From a high-level perspective both of these approaches” --> missing comma after “perspective”	Unclear description of method
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.	Correctness of algorithm proposed
The explanation in Fig 2 on why this is the case seem to me not so clear. Are you trying to show that the Wave-U-Net does not work since there is no 1/f^2 law for clean audio signals?	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Otherwise, this choice is incomprehensible.	Incomplete details on perfromance of the method
The two things that make me more skeptical, is the convergence of the proposed algorithm and the experiments.	Concerns regarding correctness of methods (assumption, budget, etc)
From the supplementary, it seems Epsilon means the environment?	Unclear description of method
First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.	Limited novelty in theoretical contribution
They need to elaborate how their method overcomes these issues better.	Incomplete details on perfromance of the method
I do not see the GAN style approach taken by the paper, ensures this.	Correctness of algorithm proposed
Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.	Incomplete details on perfromance of the method
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.	Correctness of algorithm proposed
1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs	Experimental study not strong enough
I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:	Incomplete details on perfromance of the method
- I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Although I personally enjoyed reading the results that from control theory perspective are inline with GAN literature, the paper does not provide novel surprising results.	Not enough originality in results (not surprising)
I do not understand how the model is trained to solve multiple tasks.	Missing details on methodology (eg., use of notation, use on tasks, etc)
How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?	Missing details on methodology (eg., use of notation, use on tasks, etc)
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?	Correctness of algorithm proposed
First, the labeled data portion is fixed and is relatively high	Less datasets used
1. I had hard time to understand latent canonicalization.	Correctness of algorithm proposed
- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?	Incomplete details on perfromance of the method
It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
The proposed method is very simple and frames the problem basically as a supervised learning problem.	Correctness of algorithm proposed
1 The implementation steps of the proposed method (MoVE) are not clear.	Missing details on methodology (eg., use of notation, use on tasks, etc)
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.	Incomplete details on perfromance of the method
- The hyperparameter selection regime (and the experiments used to find them) is not described	Lack of experimental details (no of images, sampling criteria, etc)
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.	Correctness of algorithm proposed
Though Kaleidoscope include these as special cases, it is not clear whether when given the same resources (either memory or computational cost), Kaleidoscope would outperform them.	Missing theoretical comparisons
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.	Incomplete details on perfromance of the method
- in section 4.3, there is no guarantee that the intersection between the training set and test set is empty.	claims on the datasets is questionable
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.	Correctness of algorithm proposed
For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.	Missing details on methodology (eg., use of notation, use on tasks, etc)
At the conceptual level, the idea of jointly modeling local video events is not novel, and can date back to at least ten years ago in the paper “Learning realistic human actions from movies”, where the temporal pyramid matching was combined with the bag-of-visual-words framework to capture long-term temporal structure.	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
The proposed method is not appropriately compared with the other methods in experiments.	Lack of comparison with related works
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?"	Incomplete details on perfromance of the method
An example is presented in Figure 3 but is not expanded upon in the main text.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).	Correctness of algorithm proposed
As a minor note, were different feature extractors compared?	Incomplete details on perfromance of the method
It is also not clear how the loss function proposed differs from that of the CDVAE, etc.	Limited novelty in theoretical contribution
2) The writing is poor and hard to follow.	3. The paper is not nicely written or rather easy to follow.
(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.	Limited insights based on design choices
In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.	Lack of analysis
The algorithm assumptions are strong.	Correctness of algorithm proposed
The reasons for the use of the energy-based formulation are not clear to me.	Missing details on methodology (eg., use of notation, use on tasks, etc)
The rest experiments	Experimental study not strong enough
There is no experiment clearly shows convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches (Xie & Ermon (2019); Kool et al. (2019); PlÂšotz & Roth (2018) ).	Incorrect baselines used
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.	Unclear description of method
For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?	Experimental study not strong enough
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.	Correctness of algorithm proposed
Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].	Limited improvement over baselines
"2)	It is not clear what the “replicates” refer to in the experiments."	Lack of experimental details (no of images, sampling criteria, etc)
- Lack of a strong explanation for the results or a solution to the problem	Improper explanation of results (as in advantages, why are results better, etc)
Quality: Below average	1. The presentation is somewhat convoluted.
I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).	Experimental study not strong enough
- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class ‘8’). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.	More variations of experiments needs to be added
The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.	Incomplete details on perfromance of the method
4. [Experiments.] The author presented a multimodal representation learning framework for partially-observable multimodal data, while the experiments cannot corraborrate the claim.	design choices are questionable
Hence it is unclear how large the running time improvement is compared to a well-tuned baseline.	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.	Limited improvement over baselines
They would have a very low weight difference score though they are ideal representations for each other.	Improper explanation of results (as in advantages, why are results better, etc)
- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.	Incorrect baselines used
I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.	Limited improvement over baselines
"- The overall method seems to be not very principled, and requires a lot of ""tweaks and tunes"", with additional losses and regularizers, to work."	Correctness of algorithm proposed
Is there any explanation for this?	Correctness of algorithm proposed
Assessment: Overall, this is a borderline paper, as the task is interesting and novel, but the presentation is lacking in technical detail and there is a lack of novelty on the modeling side.	Overall not original enough (primary claim, limited evaluation, limited novelty)
- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?	Experimental study not strong enough
In later sections they use theta and theta’ for encoder/decoder resp.	Unclear description of method
The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty.	Missing theoretical comparisons
- In the case of the search space II, how many GPU days does the proposed method require?	Incomplete details on perfromance of the method
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.	Unclear description of method
- Paper is often hard to follow, and contains a significant number of typos.	3. The paper is not nicely written or rather easy to follow.
Second, the writing can be greatly improved.	3. The paper is not nicely written or rather easy to follow.
"8. ""Beyond fixed schedules, automatically adjusting the training of G and D remains untacked"" -- this is not 100% true."	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.	Incomplete details on perfromance of the method
Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:	generalizability of results is questionable
- What is the difference between the result of Theorem 4.3 and the result from (Lacoste-Julien 2016)?	- What is the difference between the result of Theorem 4.3 and the result from (Lacoste-Julien 2016)?
The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together.	3. The paper is not nicely written or rather easy to follow.
Furthermore, there are several similar ideas already published, so comparison against those (for example by J. Peng, N. Heess or J. Mere) either as argument or even better as experiment, would be helpful to evaluate the quality of the proposed hierarchy.	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
The notations are overall confusing and not explained well.	3. The paper is not nicely written or rather easy to follow.
1. What are the key limitations of AutoLoss ? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ?	Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc)
For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?	Incomplete ablation in terms of tables and figures
- It is not clear how the initialisation (10) is implemented.	Missing details on methodology (eg., use of notation, use on tasks, etc)
This claim is simply not true, and more care is needed in reporting the results here in the wider context of the literature.	correctness of results presented is questionable(eg., metrics, complexity, etc)
Second, SST itself is only comparable with or even worse than the state-of-art methods.	Improper comparison of related work in terms of implementation
- Figures 1-4 are difficult to interpreted on a printed version.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
- for GN optimization, lambda should be set to 0 - not a constant value.	Correctness of algorithm proposed
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.	Incomplete details on perfromance of the method
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.	Unclear description of method
Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.	Missing baselines
Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy.	Limited insights based on design choices
However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).	Experimental study not strong enough
If the test set is not shuffled (by emphasis on first I assume not) these images are from training NIST (cleaner) set and may not include samples of all digits.	claims on the datasets is questionable
Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.	Incomplete details on perfromance of the method
https://openreview.net/references/pdf?id=Sy2fzU9gl	Incorrect citation
- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?	Improper comparison of related work in terms of implementation
The selected baselines are not sufficient.	Limited improvement over baselines
Some baseline DA methods [A, B] and datasets [C, D] are not considered.	Some baseline DA methods [A, B] and datasets [C, D] are not considered.
These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).	Incremental novelty of method as compared to related work
I found the paper confusing at times.	3. The paper is not nicely written or rather easy to follow.
* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
* The baselines in the experiments could be improved.	Experimental study not strong enough
2. The learning procedure is confusing.	Missing details on methodology (eg., use of notation, use on tasks, etc)
The paper used very restricted Gaussian distributions for the formulation.	Missing details on methodology (eg., use of notation, use on tasks, etc)
One thing that puts me off is that, in Table 2, AnyBurl (the single one baseline authors considered other than the original NeuralLP) yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.	The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).
The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,	Incomplete details on perfromance of the method
However, the experiments conducted generally show that SAVP offers only a trade-off between the visual quality of GANs and the coverage of VAEs, and does not show a clear advantage over current VAE models (Denton & Fergus, 2018) that with simpler architectures obtain similar results.	Missing theoretical comparisons
- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)	Missing details on methodology (eg., use of notation, use on tasks, etc)
In terms of method, the guidance for learning Siamese networks are designed heuristically (e.g. edges, colors, etc.) which limits its applicability over various datasets; I think that designing more principled approach to build such guidances from data should be one of the key contributions of the paper.	In terms of method, the guidance for learning Siamese networks are designed heuristically (e.g. edges, colors, etc.) which limits its applicability over various datasets; I think that designing more principled approach to build such guidances from data should be one of the key contributions of the paper.
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)	Unclear description of method
What is the L1 norm applied on?	Missing details on methodology (eg., use of notation, use on tasks, etc)
For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.	Improper comparison of related work in terms of implementation
- Given the small size of the dataset, I would propose experimenting with non-neural approaches as well, which are also quite common in NLG.	Not enough info on dataset and hyper-parameters
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.	Correctness of algorithm proposed
3. Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?	claims on the datasets is questionable
A proper baseline should have been compared.	Missing baselines
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments."	Unclear description of method
The method provides impressive compression ratios (in the order of x20-30) but at the cost of a lower performance.	While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited.
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.	Unclear description of method
- The claim made in the first line of the abstract (applying RL algorithms to continuous control problems often leads to bang-bang control)	5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.
- the method is not applicable to episodes of different length	Missing details on methodology (eg., use of notation, use on tasks, etc)
It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.	Incomplete details on perfromance of the method
The paper is relatively well-written, although the description of the neural models can be improved.	3. The paper is not nicely written or rather easy to follow.
"Yeh, Raymond A., et al. ""Image Restoration with Deep Generative Models."" 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018."	Incorrect citation
My main concern comes from the novelty of this paper.	Overall not original enough (primary claim, limited evaluation, limited novelty)
Overall, the idea is simple, the explanation is clear and experimentation is extensive. I would like to see more commentary on why this method might have long-term impact (or not).	Overall, the idea is simple, the explanation is clear and experimentation is extensive. I would like to see more commentary on why this method might have long-term impact (or not).
\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.	Lack of analysis
- No large corpus results.	generalizability of results is questionable
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.	Correctness of algorithm proposed
However, I don’t know how effective this is in practice.	design choices are questionable
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.	Incomplete details on perfromance of the method
The experiment section needs significant improvement, especially when there is space left.	Experimental study not strong enough
But the only negative aspect is the basis competitor algorithms are very simple in nature without any form of learning and that are very old.	Missing theoretical comparisons
The privacy definition employed in this work is problematic.	Limited insights based on design choices
"The paper also lacks experimental results, and the main conclusion from these results seems to be ""MNIST is not suitable for benchmarking of adversarial attacks""."	correctness of results presented is questionable(eg., metrics, complexity, etc)
-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.	Lack of analysis
* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause	Limited improvement over baselines
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.	Unclear description of method
Results on more scenes will make the performance more convincing.	generalizability of results is questionable
What is the minimum/maximum layers of a deep model? How much data is sufficient for a model to learn?	Lack of discussion of performance of method on different datasets
Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).	Incorrect baselines used
Specifically, how can mutual information in this context be formally linked to generalization/overfitting?	Missing details on methodology (eg., use of notation, use on tasks, etc)
However, I found that the contribution of this paper is fairly small.	Lacking clarity overall (needs better presentation)
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled."	Unclear description of method
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?	Correctness of algorithm proposed
So overall I have the feeling that the authors have not succeeded to evaluate the model’s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.	More variations of experiments needs to be added
methods. There are some scalable property verification methods that can give a	Incomplete details on perfromance of the method
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).	generalizability of results is questionable
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.	Unclear description of method
-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred	Incorrect baselines used
* Figure 5 should appear after Figure 4.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
2. Even though Figure 2b shows that SVHN test likelihoods are higher than CIFAR test likelihoods, the overlap in the histograms of CIFAR-train and CIFAR-test is much higher than the overlap in CIFAR-train and SVHN-test.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
However, this only affects the method of drawing the samples from a fixed known distribution and should have no more effect on the results than say a choice of a pseudo-random number generator.	Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)
It is unclear whether the data augmentation techniques is applied only at training time or also at test time.	Missing details on methodology (eg., use of notation, use on tasks, etc)
- the sentence under eq. (2)	Unclear description of method
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.	Unclear description of method
However I find the white-box experiments lacking as almost every method has 100% success rate.	More variations of experiments needs to be added
Overall, this is a reasonable paper but experimental section needs much more attention.	design choices are questionable
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).	Correctness of algorithm proposed
Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets.	claims on the datasets is questionable
I’m not sure whether it is due to parameter choice or due to weak D/G networks used in the simulation.	design choices are questionable
However, I don't understand the use of $\alpha$ here.	Unclear description of method
- Baseline missing: Simple RNN policies that communicate hidden states.	Missing baselines
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.	Correctness of algorithm proposed
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.	Unclear description of method
The model is not well motivated and the optimization algorithm is also not well described.	Limited insights based on design choices
- The evaluation of the proposed method is not complete.	Incomplete details on perfromance of the method
1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?	Less datasets used
This paper is very well written, although very dense and not easy to follows, as many methods are referenced and assume that the reviewer is highly familiar with the related works.	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.	Experimental study not strong enough
However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory.	Limited novelty in theoretical contribution
This again greatly concerned me as I am not certain how stable these metrics are.	Correctness of algorithm proposed
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.	Unclear description of method
"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting."	Correctness of algorithm proposed
"If that's true, then this distribution would be very bad for training a value function, which is supposed to involve an expectation over ""nature""'s choices in the MDP."	design choices are questionable
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.	Unclear description of method
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.	Correctness of algorithm proposed
"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is."	Less datasets used
Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.	Missing details for reproducibility of result
Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.	More variations of experiments needs to be added
3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4).	design choices are questionable
If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).	Limited improvement over baselines
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.	Correctness of algorithm proposed
Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator.	Lacking details on datasets
This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.	Incomplete details on perfromance of the method
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?	Correctness of algorithm proposed
Actually in the experiments the authors never use an increasing batch size.	design choices are questionable
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.	Experimental study not strong enough
This is not so interesting, even though results are impressive.	Limited impact of results
In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.	Experimental study not strong enough
However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.	Incomplete details on perfromance of the method
For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.	Missing baselines
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.	Correctness of algorithm proposed
In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.	Improper comparison of related work in terms of implementation
The proposed method PowerSGD is an extension of the method in Yuan et al. (extended to handle stochastic gradient and momentum).	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
Many of the parameters here are also unclear and not properly defined/introduced.	Unclear description of method
4) In the experiments, more comparisons with methods in [1] or [2] should be conducted given they are all parallelizing the backpropagation algorithm and achieve speedup in the training.	Lack of comparison with related works
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).	Correctness of algorithm proposed
Specifically, the author mentioned Tsai et al. assumed factorized latent variables from the multimodal data, while Tsai et al. actually assumed the generation of multimodal data consists of disentangled modality-specific and multimodal factors.	Incorrect assumptions as compared to related work
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.	Correctness of algorithm proposed
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.	Unclear description of method
* The text is quite hard to read.	3. The paper is not nicely written or rather easy to follow.
Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.	Incomplete details on perfromance of the method
- The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.	Limited novelty in theoretical contribution
It is thus unclear if the approach is robust against different hyperparameter settings.	Correctness of algorithm proposed
3. The paper is not nicely written or rather easy to follow.	3. The paper is not nicely written or rather easy to follow.
I do not see much insight into the problem.	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
However, all experiments only show results in the MCAR setting, so the claim is not experimentally validated.	design choices are questionable
Also, please place the related work earlier on in the paper.	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).	Missing details on methodology (eg., use of notation, use on tasks, etc)
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?	Correctness of algorithm proposed
However, since quantitative exploration of large real-world datasets may be challenging and expensive to collect, the synthetic experiments could have been more detailed.	However, since quantitative exploration of large real-world datasets may be challenging and expensive to collect, the synthetic experiments could have been more detailed.
Minor comment: An interesting line of work is that of [3] which could be included in the discussion.	Limited improvement over baselines
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.	Correctness of algorithm proposed
2. Figure 3: it is confusing to call the cumulative distribution of the maximum classification score as the CDF of the model (y-axis fig. 3 left) as CDF means something else generally in such contexts, as the CDF of a predictor.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:	Incomplete details on perfromance of the method
Another concern is that the evaluation of domain adaptation does not have much varieties.	Incomplete details on perfromance of the method
Hence, the effectiveness and advantage of the proposed methods are not clear.	Limited insights based on design choices
compared two schemes of this work, the ones with attentions are “almost” identical with ones	Incomplete details on perfromance of the method
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e."	Unclear description of method
Yet their approach is only able to solve the fractional version of the AdWords problem.	Incomplete details on perfromance of the method
My assumption is the visual feature already contains the label information for image captioning.	Correctness of algorithm proposed
This is not to say that this way of doing things is wrong, but rather that it is misleading in the context of prior work.	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
This seems like a limitation of the method if this is the case.	Incomplete details on perfromance of the method
Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
2. First paragraph in related work is very unrelated to the current subject, please remove.	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
There are many typos (see below).	3. The paper is not nicely written or rather easy to follow.
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.	Correctness of algorithm proposed
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.	Correctness of algorithm proposed
- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.	generalizability of results is questionable
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.	Correctness of algorithm proposed
I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models.	Missing theoretical comparisons
10. Reading the baselines before the experiments is very confusing.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.	Correctness of algorithm proposed
Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem.	Missing baselines
But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).	Incremental novelty of method as compared to related work
- Some of the figures your report are compelling but it is a bit unclear to the reader if the results are general (e.g., the examples could have been hand-picked). Are there any quantitative measures you could provide (in addition to Tables 1 and 2 which don't measure the quality of the approach)?	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
3. Scenario discussed in Sec. 4 seems somewhat impractical.	Missing details on methodology (eg., use of notation, use on tasks, etc)
(3) A large body of graph neural network literature is omitted.	Missing literature review (some literature not included, misses baseline citations, etc)
- The premises of the analyses are not very convincing, limiting the significance of the paper.	Lack of analysis
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.	Correctness of algorithm proposed
"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big)."	Correctness of algorithm proposed
There are several weaknesses in this paper.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.	Incomplete details on perfromance of the method
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.	Incomplete details on perfromance of the method
2) Related work, although extensive in terms of the number of references, do not help to place this work in the literature.	Missing literature review (some literature not included, misses baseline citations, etc)
The main drawback of the paper is that it seems to be more engineering-focused, and doesn’t provide much insight into semi-supervised learning.	Limited insights based on design choices
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.	Experimental study not strong enough
- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.	Limited insights based on design choices
For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.	Incomplete details on perfromance of the method
Thus we may only apply the proposed model on a few tasks with exactly known F.	Limited insights based on design choices
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.	Correctness of algorithm proposed
* The oracle-augmented datasteam model needs to be contextualized better.	Incomplete details on perfromance of the method
Clearly, replaying data accurately from all tasks will work well, but why is it harder to guard against the generative forgetting problem than the discriminative one?	Lack of discussion of performance of method on different datasets
The work is rather incremental from current state-of-the-art methods.	Incremental novelty of method as compared to related work
- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).	Limited improvement over baselines
However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).	Incomplete ablation in terms of tables and figures
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.	Unclear description of method
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture."	Incomplete details on perfromance of the method
The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs.	Limited novelty in theoretical contribution
I believe that the paper should currently be rejected, but I encourage the authors to revise the paper.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
"- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001"	Missing details on methodology (eg., use of notation, use on tasks, etc)
The setup where labeled data (c) also seems a bit unnatural (this also seems to be confirmed by the fact that the authors had to build datasets for the problem).	claims on the datasets is questionable
My concern here is that beta might be affecting the result more than the proposed training algorithm.	Correctness of algorithm proposed
- The authors haven't come up with a recommendation for a single configuration of their approach.	Incomplete details on perfromance of the method
It would be better if the authors were a little more careful in their use of terminology here.	Unclear description of method
If so it would be better to define the operator in a more general notation.	Missing details on methodology (eg., use of notation, use on tasks, etc)
For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.	Incremental novelty of method as compared to related work
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)"	Correctness of algorithm proposed
It was kind of surprising that the authors did not cite this paper given the results are pretty much the same.	Not enough originality in results (not surprising)
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?"	Unclear description of method
ii) In table 2, I don’t really see any promising results compared to baselines. There are	The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).
Once a low-level walking controller is trained, the high-level multi-agent navigation control is not much different from simple environments, e.g. point mass control, used in the previous works.	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
However, there are some key issues with the paper that are not clear.	3. The paper is not nicely written or rather easy to follow.
So the experiments in this paper is also not convincing.	Experimental study not strong enough
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?	Unclear description of method
- Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch?	Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.
However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.	Lack of realistic datasets used in experiments (small siz, synthetic)
The plots in figure 4 are too small.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.	More variations of experiments needs to be added
The main problems come from the experiments, which I would ask for more things.	Experimental study not strong enough
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.	Correctness of algorithm proposed
Because of the above many discussions about discrete vs. continuous variables are missleading.	Unclear description of method
The paper is also littered with typos and vague statements (many enumerated below under *small issues*).	3. The paper is not nicely written or rather easy to follow.
The paper can benefit from a proofreading.	3. The paper is not nicely written or rather easy to follow.
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.	Unclear description of method
In its current state, I am not sure that it adds a lot to the manuscript.	Lacking clarity overall (needs better presentation)
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.	generalizability of results is questionable
1a. Comparison to other exploration methods.	Missing theoretical comparisons
Also, the claim in Fig. 1 that the transition from ‘’high capacity’’ to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand, and should be toned down. (*)	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)	Incomplete details on perfromance of the method
Moreover, if I understand correctly the WGAN analysis does not take into account that G and D are non-linear, and it is unclear if these can be done.	Too strong assumptions in analysis
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.	Missing details on methodology (eg., use of notation, use on tasks, etc)
adversarial examples (or counter-examples for property verification) with L_inf	Unclear description of method
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?	Incomplete details on perfromance of the method
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?	Incomplete details on perfromance of the method
In addition, the results seem very weak.	generalizability of results is questionable
"The authors stated that ""F-pooling remarkably increases accuracy and robustness w.r.t. shifts of moderns CNNs""; however, in Table 1-3, the winning margin of accuracy is actually quite small (<2%), and the consistency (<3.5% compared to the second best baseline except resnet-18 on CIFAR 100 has large improvement ~7-8%)."	Results are more or less similar to baselines
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.	Correctness of algorithm proposed
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution."	Experimental study not strong enough
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.	In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?	Unclear description of method
In summary, I find there is no novelty involved apart from combining the already existing SOTA model in disentangled feature learning (beta-VAE) and image generation (StackGAN).	Limited novelty in theoretical contribution
Especially if the lower-level component of the hierarchy is pre-trained in a non-MARL setup, it can just be seen as part of the environment from the point of view of the MARL training, offerring limited new insight into MARL.	Missing theoretical comparisons
In addition, I observe that in Table 1, the proposed method does not outperform the Joint Training in SVHN with A_10.	"Perhaps I missed it, but I believe Dan Ciresan's paper ""Multi-Column Deep Neural Networks for Image Classification"" should be cited."
1. The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage. After all, the paper says BERT scores 88% before the dataset is attacked.	claims on the datasets is questionable
Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.	Incomplete details on perfromance of the method
Lemma 3 is too trivial.	Incomplete details on perfromance of the method
Additionally, in section 6.4, the results in Figure 2 also does not look very	Improper presentation of results in tables and figures
This would be an effective baseline to compare. (Correct me if I am wrong here.)	Limited improvement over baselines
In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.	Correctness of algorithm proposed
However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.	Missing theoretical comparisons
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?"	Unclear description of method
"For example, the authors should at least say that the ""D"" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned."	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
2. The main technical contribution claim needs to be elaborated.	Incomplete details on perfromance of the method
With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places.	With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places.
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice."	Experimental study not strong enough
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?"	Experimental study not strong enough
Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method.	Incorrect baselines used
section 6.3, the authors show an experiments in this case, but only on a dense	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.	Incomplete details on perfromance of the method
Since the paper focuses explicitly on providing contrastive explanations for choosing a class A over another class B -- experiments on datasets which do not have real-images seem insufficient.	Lack of realistic datasets used in experiments (small siz, synthetic)
4. The biggest flaw that I see in this method is the practicality of it's use.	Correctness of algorithm proposed
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.	Incomplete details on perfromance of the method
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.	Unclear description of method
So, I have some doubts about the experimental results.	More experiments needed with related work
- In Table 2 and 3, how are the degree and block information leveraged into the model?	Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)
There should be a better discussion of related work on the topic.	Missing baselines
The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.	Missing details on methodology (eg., use of notation, use on tasks, etc)
"I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD."	Correctness of algorithm proposed
In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.	Limited improvement over baselines
Some details are missing, which is hardly reproduced by the other researchers.	Missing implementation details of related work used as baselines
4. How large is the training set of (T, P) pairs? I don't think this is mentioned in the paper.	Lacking details on datasets
Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.	Correctness of algorithm proposed
Overall, while I find the proposed approach simple -- the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same.	Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)
Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).	Less datasets used
Overall, the paper requires significant improvement.	Lacking clarity overall (needs better presentation)
It is unclear how important this particular objective is to the results.	correctness of results presented is questionable(eg., metrics, complexity, etc)
Therefore, the paper cannot be qualified for ``meta domain adaptation’’ and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair.	Limited novelty in theoretical contribution
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading."	Unclear description of method
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,"	Unclear description of method
There are tons of algorithms for image inpainting, denoising, and super-resolution, but the proposed method was not compared with them.	Missing theoretical comparisons
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.	Correctness of algorithm proposed
- Experimental results are provided only on MNIST and Fashion-MNIST.	Need more experimental results
It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets.	Lacking details on datasets
In this case, the paper proves that no careful selection of the learning rate is necessary.	Incomplete details on perfromance of the method
However, the derivations about \phi are missing.	Missing details on methodology (eg., use of notation, use on tasks, etc)
If so, what was the methodology.	Missing details on methodology (eg., use of notation, use on tasks, etc)
What is the point that you are trying to make? Also, note that some of the algorithms that you are citing there have indeed applied beyond architecture search, eg. Bayesian optimization is used for gait optimization in robotics, and Genetic algorithms have been used for automatic robot design.	Incorrect assumptions as compared to related work
However, it’s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events.	Missing theoretical comparisons
For example, “Fuzzy Choquet Integration of Deep Convolutional Neural Networks for Remote Sensing” by Derek T. Anderson et al.	Suggest missing related work
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.	Correctness of algorithm proposed
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.	Incomplete details on perfromance of the method
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.	Unclear description of method
• Not all of the arrows in Figure 1 are pointing to the right lines.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.	Lack of experimental details (no of images, sampling criteria, etc)
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.	Missing details on methodology (eg., use of notation, use on tasks, etc)
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?	Incomplete details on perfromance of the method
Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.	Incomplete details on perfromance of the method
The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN.	More experiments needed with related work
1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.	Limited novelty in theoretical contribution
I wonder how the straightforward regression term plus the smooth term will perform for the mask.	Incomplete details on perfromance of the method
The only set of experiments are comparisons on first 500 MNIST test images.	Experimental study not strong enough
Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.	Incomplete details on perfromance of the method
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.	Correctness of algorithm proposed
There might be other constructions that are more efficient and less restrictive.	Correctness of algorithm proposed
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)	Unclear description of method
- Section 1.1 presents results with too many details without introducing the problem.	Improper explanation of results (as in advantages, why are results better, etc)
Originality: The work is moderately original.	Overall not original enough (primary claim, limited evaluation, limited novelty)
In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.	Missing details on methodology (eg., use of notation, use on tasks, etc)
If the novelty is in applying to continual learning and new datasets, it is not clear that this is sufficient.	In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.
Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.	Incomplete details on perfromance of the method
However, the function of interest is limited to a small family of affine equivariant transformations.	Incomplete details on perfromance of the method
4. Can the authors show concrete examples on how the attacks are generated?	Lack of experimental details (no of images, sampling criteria, etc)
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D	Correctness of algorithm proposed
We think that this is not enough, and more extensive experimental results would provide a better paper.	Experimental study not strong enough
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.	Unclear description of method
Last comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?	Missing details on methodology (eg., use of notation, use on tasks, etc)
- There is no motivations for the use of $\lambda >1$ neither practical or theoretical since the results are only proven for $\lambda =1$ whereas the experiments are done with \lambda = 5,20 or 30.	correctness of experiments is questionable
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?	Less datasets used
2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)?	Lack of experiments
I am not convinced that the measure theoretic perspective is always	Unclear description of method
In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.	Missing details on methodology (eg., use of notation, use on tasks, etc)
Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.	Incomplete details on perfromance of the method
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.	Correctness of algorithm proposed
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.	Correctness of algorithm proposed
- The problem setting description is neither formal nor intuitive which made it very hard for me to understand exactly the problem you are trying to solve.	Unclear problem definition
And finally, the discussion of this figure makes claims about the behaviour of the model that seems to be too strong to be based on a single image experiment.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
In terms of actual technical contributions, I believe much less significant.	Incomplete details on perfromance of the method
While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.	Incomplete details on perfromance of the method
5. In fact, I don't know why \omega needs to output p. It's never mentioned in the experiment section.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
3. How can the proposed method be generalized to non-image data?	Correctness of algorithm proposed
A good paper overall, but the experiments were relatively weak (common for most ICLR submissions) and the novelty was somewhat limited.	Lacking clarity overall (needs better presentation)
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?	Unclear description of method
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.	Correctness of algorithm proposed
4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.	Limited novelty in theoretical contribution
"Taking a random example (there are others by simple searching), in the ECCV paper ""DFT-based Transformation Invariant Pooling Layer for Visual Classification, Ryu et al., 2018"" The DFT magnitude pooling is almost the same as the authors' propositions, where the ""Fourier coefficients are cropped by cutting off high-frequency components""."	Incremental novelty of method as compared to related work
As I have mentioned above, actually a lot of methods have been developed to address general image restoration tasks.	Missing theoretical comparisons
I have doubts on applying the proposed method to higher dimensional inputs.	Unclear description of method
Even though the proposed approach seems to have significant potential, the experimental	Not enough novelty in experiments (seems similar to previous work)
The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.	Limited improvement over baselines
For example, Narayanaswamy et al. [1] also propose to utilize labels to VAE.	Incremental novelty of method as compared to related work
"- In figure 3 (c) ""number |T of input"" should be  ""number |T| of input"""	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
From the perspective of a purely technical contribution, there are fewer exciting results.	generalizability of results is questionable
Was the 5x10x20x10 topology used for MNIST the only topology tried?	Correctness of algorithm proposed
* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS."	Incomplete details on perfromance of the method
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}	Unclear description of method
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.	Correctness of algorithm proposed
I enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop.	Lacking clarity overall (needs better presentation)
"For example, there are two ""the"" in the end of the third paragraph in Related Work."	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
- In Appendix D, the Figures could be slightly clarified by using a colored heatmap to color the curve, with colors corresponding to the threshold values.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.	generalizability of results is questionable
The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.	Limited novelty in theoretical contribution
Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.	Limited improvement over baselines
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.	Unclear description of method
- (W2) Related to the last line: I did not see any experiments / analysis showing how stable these different numbers are across different runs of the representation technique. Nor did I see any error bars in the experiments.	The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).
The results are not strong. And, unfortunately, the model contribution currently is too modest.	Not enough originality in results (not surprising)
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?	Incomplete details on perfromance of the method
Then to state to which of these cases the results of the paper are applicable, allow for an improvement of the variance and at what additional computational cost (considering the cost of evaluating the discrete derivatives).	Limited impact of results
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.	Correctness of algorithm proposed
"4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the ""model complexity"" introduced upto numerical constants."	Lack of experiments
How this proxy incentives the agent to explore poorly-understood regions?	Incomplete details on perfromance of the method
In the experiment there is no details on how you set the hyperparameters of CW and EAD.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?	Missing details on methodology (eg., use of notation, use on tasks, etc)
Meanwhile the actual method used in the paper is hidden in Appendices A.1.1-A.2.	Theoretical misunderstanding in methodology
1) the proof techniques are very standard	Limited novelty in theoretical contribution
"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information."	Limited novelty in theoretical contribution
"Particularly the ""fusion"" module remains extremely unclear."	Missing details on methodology (eg., use of notation, use on tasks, etc)
Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.	More variations of experiments needs to be added
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?	Correctness of algorithm proposed
However, the paper contains only little novelty and does not provide sufficiently new scientific insights.	Overall not original enough (primary claim, limited evaluation, limited novelty)
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).	Correctness of algorithm proposed
The proposed  CLF weight difference method has some concerning aspects as well.	Correctness of algorithm proposed
* The connections to deep learning seem arbitrary in some of the experiments.	Experimental study not strong enough
That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.	Less datasets used
- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.	Incomplete details on perfromance of the method
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.	generalizability of results is questionable
Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.	Lack of analysis
2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.	Limited novelty in theoretical contribution
- U^m in Eq 1 is undefined and un-discussed.	Unclear description of method
The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.	Correctness of algorithm proposed
The introduction can start at a lower level (such as flat/hyperbolic neural networks).	Unclear intro (eg. contributions)
- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.	Lack of analysis
- comparison should be made to the linear composition method in the Arora, Liang, Ma ICLR 2017 paper	Missing baselines
Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.	Incomplete details on perfromance of the method
For example, in Table 2, the F-pooling only wins at either accuracy (marginally) or consistency, but not both.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
- I’d also like to see more extensive comparisons between FICM and ICM across different datasets, for example, Super Mario Bros. and the Atari games, instead of only comparing FICM against ICM on ViZDoom.	- I’d also like to see more extensive comparisons between FICM and ICM across different datasets, for example, Super Mario Bros. and the Atari games, instead of only comparing FICM against ICM on ViZDoom.
The paper does not provide very significant evidence that this method is useful.	Correctness of algorithm proposed
I identify this ambiguity between BERTScore versions as an important weakness of the paper.	Unclear description of method
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.	Unclear description of method
this analysis is not currently included in the main body of the manuscript, but rather in the appendix, which I find rather annoying.	Lack of discussion of analysis
e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)	correctness of results presented is questionable(eg., metrics, complexity, etc)
The same holds for the type of data, since the paper only shows results for image classification benchmarks.	In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).	Unclear description of method
If this is what these graphs show, consider using a different visualization to make it clearer that you're improving the final performance, not just the training process.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).	Experimental study not strong enough
1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.	Experimental study not strong enough
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.	Unclear description of method
- the complexity of the proposed algorithm seems to be very high	Incomplete details on perfromance of the method
5. lack of related work:  4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR 2019.	Missing literature review (some literature not included, misses baseline citations, etc)
"- In figure 8 ""Each column corresponds to ..."" should be ""Each row corresponds to ...""."	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?	Correctness of algorithm proposed
If that's the goal, however, a more detailed error analysis would need to be included.	Lack of analysis
4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,	Limited improvement over baselines
- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.	Limited improvement over baselines
1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.	Missing details on methodology (eg., use of notation, use on tasks, etc)
3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures.	Need more experimental results
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?	Missing details on methodology (eg., use of notation, use on tasks, etc)
- The notion of divergence D(P|G) is not made concrete in section 3 and 3.1, which makes the notation of rather little use.	Theoretical misunderstanding in methodology
Therefore, it is not clear how the proposed framework is helping the model compression techniques.	Correctness of algorithm proposed
The authors claim that some amount of noise can be tolerated, but do not quantify how much.	correctness of results presented is questionable(eg., metrics, complexity, etc)
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting	Correctness of algorithm proposed
The details of the approach is not entirely clear and no theoritcal results are provided to support the approach.	The details of the approach is not entirely clear and no theoritcal results are provided to support the approach.
3) In the experiments, the accuracy values are too low for me.	design choices are questionable
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.	Correctness of algorithm proposed
- I would like to see some more interpretation on why this method works.	Incomplete details on perfromance of the method
The idea to extend the use of VAE-GANs to the video prediction setting is a pretty natural one and not especially novel.	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
In this case, A: PointNet, B: DeepSet	claims on the datasets is questionable
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018).	[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018).
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.	Correctness of algorithm proposed
"3. The authors argue in their rebuttal that ""the grid"" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn't the key aspect of their algorithm."	"3. The authors argue in their rebuttal that ""the grid"" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn't the key aspect of their algorithm."
This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?	Incomplete details on perfromance of the method
- The egocentric velocity field is not described (section 5)	Unclear description of method
More discussions on these questions can be very helpful to further understand the proposed method.	Missing details on methodology (eg., use of notation, use on tasks, etc)
- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?	Lack of experimental details (no of images, sampling criteria, etc)
To be honest, the theoretical contribution of the paper is limited.	Missing details on methodology (eg., use of notation, use on tasks, etc)
Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).	Missing details on methodology (eg., use of notation, use on tasks, etc)
- It does not seem necessary to predict cumulative mixture policies (ASN network).	Correctness of algorithm proposed
[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this.	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).	Improper comparison of related work in terms of implementation
3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which	Incomplete details on perfromance of the method
2. Though seemingly very important to the architecture, the purpose of constructing the super-graph g^{sup} in the training of C^{CAT} seems to be unclear to me.	design choices are questionable
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?	Unclear description of method
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.	Correctness of algorithm proposed
The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense.	claims on the datasets is questionable
However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi).	Missing details on methodology (eg., use of notation, use on tasks, etc)
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?	Unclear description of method
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.	Experimental study not strong enough
My overall concern is that the experiment result doesn’t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn’t really fit in the “transfer” learning scenario.	In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.	Correctness of algorithm proposed
"-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion."	Unclear description of method
Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?	Incomplete details on perfromance of the method
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t	Correctness of algorithm proposed
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.	Correctness of algorithm proposed
If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].	Incomplete details on perfromance of the method
"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?"	Correctness of algorithm proposed
It would have been useful to compare the general models here with some specific math problem-focused ones as well.	Missing theoretical comparisons
- Consequently why did not you compare simple projected gradient method ?	Incomplete details on perfromance of the method
I think this is a very interesting direction, but the present paper is somewhat unclear.	3. The paper is not nicely written or rather easy to follow.
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ..."""	Unclear description of method
It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.	Incomplete details on perfromance of the method
In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.	Limited insights based on design choices
Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.	Incomplete details on perfromance of the method
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?	Less datasets used
The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across	Missing theoretical comparisons
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.	Correctness of algorithm proposed
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.	Unclear description of method
* I found it difficult to follow the theoretical motivation for performing the work.	Limited insights based on design choices
The MNIST+SVHN dataset setup is described in detail, yet there is no summary of the experimental results, which are presented in the appendix.	The MNIST+SVHN dataset setup is described in detail, yet there is no summary of the experimental results, which are presented in the appendix.
The contributions of the method could also be underlined more clearly in the abstract and introduction.	Unclear intro (eg. contributions)
"* The descriptor distributions in Figure 3 don't look like an ""almost exact match"" to me (as claimed in the text)."	Incomplete ablation in terms of tables and figures
Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality.	Incomplete ablation in terms of tables and figures
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.	Correctness of algorithm proposed
Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.	Incomplete details on perfromance of the method
The experiments of this paper lack comparisons to certified verification	Experimental study not strong enough
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results."	Experimental study not strong enough
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?	Correctness of algorithm proposed
Compressability is evaluated, but that was already present in the previous work.	Incremental novelty of method as compared to related work
- the exposition could be improved, in particular the description of the plots is not very clear, I'm still not sure exactly what they show	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.	Incomplete details on perfromance of the method
In the algorithm, the authors need to define the HT function in (3) and (4).	Unclear description of method
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.	Correctness of algorithm proposed
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.	Correctness of algorithm proposed
Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.	Missing details on methodology (eg., use of notation, use on tasks, etc)
- what prior distributions p(z) and p(u) are used? What is the choice based on?	Incomplete details on perfromance of the method
2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.	Missing details on methodology (eg., use of notation, use on tasks, etc)
I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example	3. The paper is not nicely written or rather easy to follow.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?	Missing details on methodology (eg., use of notation, use on tasks, etc)
Ultimately this paper is interesting but falls well below the standards of exposition that I expect from a theory paper and doesn’t go very far at connecting the analysis back to the claimed motivation of investigating practical invariances. If the authors significantly improve the quality of the draft, I’ll be happy to revisit it and re-evaluate my score.	Lacking clarity overall (needs better presentation)
Additional experiments on at least ImageNet would have made the paper stronger.	Experimental study not strong enough
Overall, I think this paper is below the borderline of acceptance due to insufficient comparison with Sparsely-Gated MoE.	Lacking clarity overall (needs better presentation)
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?"	Correctness of algorithm proposed
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).	Correctness of algorithm proposed
It would probably help to position the VAE component more precisely w.r.t. one of the two baselines, by indicating the differences.	Missing baselines
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?	Unclear description of method
- The performance gain is not substantial in experiments.	Experimental study not strong enough
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?	Correctness of algorithm proposed
I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.	Experimental study not strong enough
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.	Incomplete details on perfromance of the method
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?	Incomplete details on perfromance of the method
Regarding the presentation, I found odd having some experimental results (page 5) before the Section on experience even have started.	Improper Structuring of experimental results on paper (as in on some page, before some section)
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.	Experimental study not strong enough
1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].	Improper comparison of related work in terms of implementation
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).	Unclear description of method
The main criticism I have is that I found the paper harder to read.	3. The paper is not nicely written or rather easy to follow.
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.	Correctness of algorithm proposed
Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].	generalizability of results is questionable
* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?	Incomplete ablation in terms of tables and figures
The paper would gain in clarity	Lacking clarity overall (needs better presentation)
It is important to place the contributions in this paper in context of these other works.	Missing baselines
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.	Correctness of algorithm proposed
In summary, the quality of the paper is poor and the originality of the work is low.	Overall not original enough (primary claim, limited evaluation, limited novelty)
I did not completely follow the arguments towards directed graph deconvolution operators.	Unclear description of method
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does."	Incomplete details on perfromance of the method
- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.	Incorrect baselines used
Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).	Improper comparison of related work in terms of implementation
In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].	Incremental novelty of method as compared to related work
How do we choose a proper beta, and will the algorithm be sensitive to beta?	Incomplete details on perfromance of the method
- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.	Missing explanation of comparsion with related work in tables and figures
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.	Correctness of algorithm proposed
I believe the proposed techniques have some flaws which hurt the eventual method.	Correctness of algorithm proposed
Given the fact that it applies the same MLP independently to individual set members, it is obvious that it is not universal equivariant (for example, consider a function that performs a fixed permutation to its input), and I fail to see why the paper goes into the trouble of having theorems and experiments just to demonstrate this point. If there were any other objectives beyond this in the experiments could you please clarify?	design choices are questionable
The proposed approach should compared to other stronger methods such as graph convolution neural network/message passing neural networks/structure2vec.	Missing theoretical comparisons
- offers minimal (but some) evidence that curiosity-based reward works in more realistic settings	Theoretical misunderstanding in methodology
Regarding the experiments on adversarial examples, I am not convinced of their relevance at all.	design choices are questionable
It would be good to know how $\gamma$ varies across tasks.	Incomplete details on perfromance of the method
4. Fig. 3 (right): It is not clear	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
The experiments were only done on simple image datasets.	Limited generalizability of claims on other datasets
It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.	Incomplete details on perfromance of the method
It reads very much like a STOC theory paper, and a lot of the key ML details that would be relevant to audience at this conference seem to have been shoved under the rug in a way.	Lacking clarity overall (needs better presentation)
However, there is no comparison with ENAS and DARTS in experiments.	More experiments needed with related work
First, it is somewhat misleading about its contributions: it's not obvious from abstract/introduction that the whole model is the same as DIP except for the proposed architecture.	Unclear intro (eg. contributions)
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.	Incomplete details on perfromance of the method
The model-based method achieves better validation error than the other baselines that use actual data.	Missing theoretical comparisons
The authors spend two paragraphs in the introduction trying to draw a distinction but I am still not convinced.	Incorrect claims in introduction (confusing related work presented in intro, wrong claims)
- Trick is specific to LM.	Missing details on methodology (eg., use of notation, use on tasks, etc)
Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.	Limited insights based on design choices
There is no value in being very confident if you are wrong and vice-versa, so unless there is an accompanying plot/table reporting the accuracy I see not much value from this plot alone.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
- Incremental modeling contribution	Limited novelty in theoretical contribution
In Table 2 I saw some optimizers end up with much lower test accuracy.	Incomplete ablation in terms of tables and figures
Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples).	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
- What is dt in Algorithm 1 description?	Unclear description of method
Also, it is not clear why those are called axioms since they are not use to build anything else.	Unclear description of method
See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.	Suggest missing related work
Furthermore, no comparisons were provided to any baselines/alternative methods.	Incorrect baselines used
- In Table 1, for ImageNet, Shadow Attach does not always generate adversarial examples that have on average larger certified radii than the natural parallel, at least for sigma=0.5 and 1.0. Could the authors explain the reason?	Incomplete ablation in terms of tables and figures
Also, the fact that the accuracy on the Kaggle non-doctored test set is low is simply because the test set is not coming from the same distribution of the training set.	claims on the datasets is questionable
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.	Correctness of algorithm proposed
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.	Correctness of algorithm proposed
With regards to the fMRI experiments, good baselines are missing: DEMINE is compared to Pearson correlation.	Missing theoretical comparisons
#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.	More variations of experiments needs to be added
Even though the authors answer positively to each of their four questions in the experiments section	design choices are questionable
- The setup for the learning to permute experiment is not as general as it would imply in the main text.	Experimental study not strong enough
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.	Unclear description of method
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.	Correctness of algorithm proposed
- no qualitative analysis on how modulation is actually use by the systems.	Lack of analysis
- limited amount of new insight, which is limiting as new and better understanding of GANs and practical guidelines are arguably the main contribution of a work of this type	Limited insights based on design choices
- There is no need for such repetitive citing (esp paragraph 2 on page 2).	Incorrect citation styles
Since quantitative real-world results are challenging to obtain, improved presentation of the qualitative results would be helpful as well.	Improper explanation of results (as in advantages, why are results better, etc)
However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.	generalizability of results is questionable
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.	Incomplete details on perfromance of the method
Without such a guarantee, the proposed method is not very useful because we	Correctness of algorithm proposed
Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.	Missing details on methodology (eg., use of notation, use on tasks, etc)
But the problem settings are not clear to me.	Need more interesting problem definition
Beyond this simplification, I am not clear if that is actually intended by the authors.	Incomplete details on perfromance of the method
If I were to go through the computation of then why not just train a smaller version of that representation technique instead and **directly** see how well it can encode data in k dimensions via that technique / for that task?	If I were to go through the computation of then why not just train a smaller version of that representation technique instead and **directly** see how well it can encode data in k dimensions via that technique / for that task?
Experiments are on toy domains with very few goals and sub-task dependencies.	Experimental study not strong enough
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.	Correctness of algorithm proposed
This illustrates that the conclusion of Theorem 2 may be wrong.	Correctness of algorithm proposed
Can the proposed approach perform just as well without a modified objective?	Incomplete details on perfromance of the method
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum."""	Unclear description of method
* the idea of smoothing gradients is not new	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.	correctness of results presented is questionable(eg., metrics, complexity, etc)
This also becomes apparent in the experiments section, where rotational data augmentation is found to be necessary.	Limited generalizability of claims on other datasets
The paper seems to lack clarity on certain design/ architecture/ model decisions.	Unclear description of method
Overall, I feel that the result is interesting but it depends on a strong assumption and doesn't capture all interesting cases.	correctness of results presented is questionable(eg., metrics, complexity, etc)
- Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016	Suggest missing related work
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.	Correctness of algorithm proposed
2. The authors should provide ablation study and analysis of their CTAugment.	Lack of analysis
Q2: Can the authors structure the experimental results with different sections? Currently it is just a single section which is difficult to read.	Improper Structuring of experimental results on paper (as in on some page, before some section)
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?	Missing details on methodology (eg., use of notation, use on tasks, etc)
Third, the datasets used in this paper are rather limited.	Less datasets used
It is also not clear how this theoretical result can shed insight on the empirical study of neural networks.	correctness of results presented is questionable(eg., metrics, complexity, etc)
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).	Unclear description of method
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.	Correctness of algorithm proposed
With lack of clear novel insights, or at least more systematic study on additional datasets of the 'winning' techniques and a sensitivity analysis, the paper does not give a valuable enough contribution to the field to merit publication.	In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics."	Unclear description of method
Authors should analyze the stability of their method in details.	Correctness of algorithm proposed
But the paper only provides empirical results on sentimental analysis and digit recognition.	generalizability of results is questionable
Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.	Lack of analysis
From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.	Limited novelty in theoretical contribution
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.	Correctness of algorithm proposed
- Given the current form of the paper, the abstract and introduction should be modified to reflect the fact that only limited architectures and optimizers were experimented with, and the claims of the paper are not experimentally validated in general.	5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.
1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).	Incremental novelty of method as compared to related work
It would be nice to see a better case made for spherical convolutions within the experimental section.	Experimental study not strong enough
3) The experiments lack comparisons to several important baselines from self-supervised learning community, and methods using soft labels for training (as mentioned in 2) above).	More experiments needed with related work
"It is also unclear how the calculation of relative entropy ""D"" was performed in figure 3."	Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)
I am wondering this method can be applied to other complex datasets whose latent factors are unknown.	generalizability of method on datasets created from different distributions is questionable
The results on real datasets are similar to the regular GCN.	In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.
"-	In Figure 2.b I’m surprised by the difference obtained in the feature maps for images which seems very similar (only the lighting seems to be different). Is it three consecutive frames?"	Incomplete ablation in terms of tables and figures
It would be nice to position the ideas from the paper w.r.t. this line of research too.	Need more interesting problem definition
However, there is no comparison against existing work.	Missing baselines
- It is unclear why the results on WikiSQL is presented in Appendix. Combining the results on both datasets in the experiments section would be more convincing.	- It is unclear why the results on WikiSQL is presented in Appendix. Combining the results on both datasets in the experiments section would be more convincing.
There are a few grammatical/spelling errors that need ironing out.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.	Incomplete details on perfromance of the method
- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).	Missing details on methodology (eg., use of notation, use on tasks, etc)
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.	Unclear description of method
Optimizing compression rates should be done on the training set with a separate development set.	claims on the datasets is questionable
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.	Unclear description of method
Otherwise, it is hard to tell how significant these correlations are. Since the end goal is to determine transferability of tasks and not the methods, it does seem like there are simpler baselines that you could compare against.	Missing theoretical comparisons
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.	Correctness of algorithm proposed
The paper positions itself generally as dealing with arbitrary transformations T, but really is	Correctness of algorithm proposed
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.	Correctness of algorithm proposed
As the authors admit, the main result is not especially surprising.	Not enough originality in results (not surprising)
I vote for rejection for four major weaknesses explained as follows.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
2) The presentation is not professional, hard to follow and the submission overall looks very rushed:	3. The paper is not nicely written or rather easy to follow.
=> The results shown in Figure-4 (Section-4.2) seems unclear to me.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
- Since the proposed method starts from Laplace transform, it would be helpful to further discuss the connection between other methods that regularises the eigenvalues of the Jacobian (such as spectral-normalisation), which work in the frequency domain from a different perspective.	Missing theoretical comparisons
In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.	Incomplete details on perfromance of the method
Finally yet importantly, though a large number of works have been proposed to try to solve this problem especially the catastrophic forgetting, most of these works are heuristic and lack mathematical proof, and thus have no guarantee on new tasks or scenarios.	Incorrect assumptions as compared to related work
Furthermore, it may be informative to show the saliency maps in Figure 5 not only for cases in which the learner classified the image correctly in both time steps, but also cases in which the learner classified the image correctly the first time and incorrectly the second time.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
Therefore, I don’t find interesting to report how DDGC improve upon “no baseline”, because known methods do even better.	incorrect claims for related work
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.	Unclear description of method
I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?	Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)
This poses a challenge in evaluating this paper.	3. The paper is not nicely written or rather easy to follow.
The paper has few really minor grammatical errors and typos. Please fix those before uploading the final draft.	Lacking clarity overall (needs better presentation)
Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.	Limited novelty in theoretical contribution
x is already present within the indicator, no need to add yet	Correctness of algorithm proposed
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.	Correctness of algorithm proposed
1) The motivation is unclear and overall structure of the paper is confusing.	While sensible, this seems to me to be too minor a contribution to stand alone as a paper.
which is much smaller than the number of time series usually involved say in gene regulatory network data	Less datasets used
These issues would maybe be excusable if not for the totally inadequate experimental validation.	Experimental study not strong enough
- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?	Incomplete details on perfromance of the method
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.	Incomplete details on perfromance of the method
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.	Unclear description of method
Significance: It is hard to assess given the current submission.	While sensible, this seems to me to be too minor a contribution to stand alone as a paper.
In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not.	Missing theoretical comparisons
My main concerns are about the evaluation and comparison of standard neural models.	Missing theoretical comparisons
Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.	generalizability of results is questionable
In terms of modeling, since the input into the prior network has finite possible discrete values, we do not need a fully connected network to generate $\hat{\mu}_c$ and $\hat{\sigma}_c$. Instead, we can directly optimize $\hat{\mu}_c$ and $\hat{\sigma}_c$ for each $c$ as parameters.	Theoretical misunderstanding in methodology
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?	Less datasets used
"While the authors say ""attributing a deep network’s prediction to its input is well-studied"" they don't compare directly against these methods."	Missing theoretical comparisons
5. Clarity: The first half of this paper was easy to follow and clear. The experimental section had a couple of areas that left me confused. In particular:	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
"* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a ""domain confusion loss""), contrary to what is claimed in the introduction."	Incorrect assumptions as compared to related work
However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.	generalizability of results is questionable
- Providing the same computational budget seem rather arbitrary at the moment, and it heavily depends from implementation. How many evaluations do you perform for each method? why not having the same budget of experiments?	Concerns regarding correctness of methods (assumption, budget, etc)
Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)	More variations of experiments needs to be added
1. Are e_{i,t} and lambda_{i,t} vectors?	Unclear description of method
As for GAN, due to the inexact update, it is not really solving the min-max problem.	Correctness of algorithm proposed
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions."	Unclear description of method
this is important since all your experiments rely on that assumption.	Experimental study not strong enough
This very much limits the utility of the method.	Limited insights based on design choices
So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.	Missing details on methodology (eg., use of notation, use on tasks, etc)
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)	Incomplete details on perfromance of the method
Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.	Correctness of algorithm proposed
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?	Correctness of algorithm proposed
For example, [1] could be used to reduce the variance of gradient w.r.t. \phi.	Limited improvement over baselines
Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.	Incomplete details on perfromance of the method
- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP).	Limited improvement over baselines
Authors should scope the paper to the specific function family these networks can approximate.	Incomplete details on perfromance of the method
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.	Incomplete details on perfromance of the method
*The strategy proposed to introduce weak-supervision is too ad-hoc.	Unclear description of method
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.	Correctness of algorithm proposed
Overall, this paper is good, but is not novel or important enough for acceptance.	Overall not original enough (primary claim, limited evaluation, limited novelty)
"Also, the sentence, ""We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy."", is confusing. If I use data parallelization, the gain should be also around 2."	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity."	Correctness of algorithm proposed
The authors also rely mostly on the FID metric, but do not show if and how there is improvement upon visual inspection of the generated images (i.e. is resolution improved, is fraction of images that look clearly 'unnatural' reduced etc.)	More comparisons needed with variations of the proposed method
In the experiments, both the setting and the experimental results show that the proposed W_s will be very close to W_U. As a result, the improvement caused by the proposed method is incremental compared with its variants.	In the experiments, both the setting and the experimental results show that the proposed W_s will be very close to W_U. As a result, the improvement caused by the proposed method is incremental compared with its variants.
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.	Correctness of algorithm proposed
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.	Correctness of algorithm proposed
2. Although the experiments are detailed and interesting they support poor theoretical developments and use a very classical benchmark	Not enough novelty in experiments (seems similar to previous work)
My main concern about this paper is why this algorithm has a better performance than CW attack?	Missing theoretical comparisons
The experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
I don’t think it is of any practical use to show that a new algorithm (such at DDGD) provide some defence compared to no defence.	Limited insights based on design choices
The question is, why one would exlude the mixture-of-softmax approach here?	Correctness of algorithm proposed
However, it seems the experiments do not seem to support this.	Experimental study not strong enough
In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.	generalizability of results is questionable
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?	Unclear description of method
This might be because the Bayes and MAT attacks are too simplistic.	Correctness of algorithm proposed
"For example, for baseline 1, it is very hard to understand why would we want to use such an unusual baseline, and why it is called a ""random baseline""."	"For example, for baseline 1, it is very hard to understand why would we want to use such an unusual baseline, and why it is called a ""random baseline""."
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)	Incomplete details on perfromance of the method
In conclusion, I suggest a reject of this paper due to the lacks of comprehensive study and evaluation.	Lacking clarity overall (needs better presentation)
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?	Unclear description of method
- This paper is a slightly difficult read - not because of the	3. The paper is not nicely written or rather easy to follow.
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?	Correctness of algorithm proposed
Also, I find the experiments done in section 3 and 4 are similar to previous works and even the conclusions are similar.	Not enough novelty in experiments (seems similar to previous work)
The descent lemma used by the author is not valid for the stochastic result.	Correctness of algorithm proposed
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.	Limited improvement over baselines
A bigger source of uncertainty is likely due to there being a limited amount of data to fit the coefficients to.	claims on the datasets is questionable
-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.	Experimental study not strong enough
However, the results are not enough to be accepted to ICLR having a very high standard.	generalizability of results is questionable
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?	Unclear description of method
6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?	Improper comparison of related work in terms of implementation
However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).	Less datasets used
2) Applying the model of Hartford et al’18 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation.	Incorrect baselines used
9. It is not clear that baseline 1 and 2 correspond to which baselines in later experiments.	9. It is not clear that baseline 1 and 2 correspond to which baselines in later experiments.
It is thus very hard to know if this new approach brings any improvement to previous work.	Incremental novelty of method as compared to related work
Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.	Experimental study not strong enough
After all, the former approach gets a lot more knowledge about the target function built into it.	Limited novelty in theoretical contribution
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.	Unclear description of method
- The paper mentions the MSVV algorithm twice but no reference or explanation is provided.	Missing theoretical comparisons
It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.	generalizability of results is questionable
"The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to ""directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems."" Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems."	Need more interesting problem definition
2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?	Missing details on methodology (eg., use of notation, use on tasks, etc)
Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear.	3. The paper is not nicely written or rather easy to follow.
The authors should clearly explain how to update \phi when optimizing Eq 12.	Missing details on methodology (eg., use of notation, use on tasks, etc)
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.	Unclear description of method
The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it.	Missing theoretical comparisons
"- The parameter count of a 2-layer network with $h$ hidden units and input dimension $d$ is $O(dh)$. So perhaps it makes sense to study an asymptotic regime where $dh/n$ approaches $\gamma$, instead of both d and h growing linearly in n. While this issue is hinted at in the discussion section, I don't understand the statement ""the mechanism that provably gives rise to double descent from previous works Hastie et al. (2019); Belkin et al. (2019) might not translate to optimizing two-layer neural networks."""	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
"3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?"	Incomplete details on perfromance of the method
2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors:	Not enough originality in results (not surprising)
5, Although there is a notable difference, one may properly mention previous work Yamamoto et al. (2019), which uses GAN as an auxiliary loss within ClariNet and obtains high-fidelity speech ( https://r9y9.github.io/demos/projects/interspeech2019/ ).	Missing literature review (some literature not included, misses baseline citations, etc)
I trust that the authors did in fact achieve these results but I cannot figure out how or why.	Missing details for reproducibility of result
6, the experimental design of Sec. 4.2 is also a bit unfair.	Experimental study not strong enough
- It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images.	Too strong assumptions in analysis
Perhaps the results would be a little more convincing if additional common word embeddings were also tested.	generalizability of results is questionable
have no idea how confident the sampling based result is.	correctness of results presented is questionable(eg., metrics, complexity, etc)
This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.	Incomplete details on perfromance of the method
The cited paper 'Learning an adaptive learning rate schedule' does not appear online.	Missing citations in the paper
I like the area of research the authors are looking into and I think it's an important application. However, the paper doesn't answer key questions about both the application and the models:	Limited insights based on design choices
As mentioned earlier, the actual conclusion in Section 4.2 is that minimizing the KL-divergence between the parametric policy and the optimal policy by SGD will converge to the optimal policy, which is straightforward and is not what AlphaGo Zero does.	Incorrect explanation of resuts
- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.	Incomplete details on perfromance of the method
This seems like a very weak baseline---there are any number of other reasonable ways to encode this.	Missing baselines
In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf	Limited improvement over baselines
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.	Correctness of algorithm proposed
While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model.	Missing theoretical comparisons
13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.	Missing details on methodology (eg., use of notation, use on tasks, etc)
6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').	Incomplete details on perfromance of the method
What is the benefit of using DL algorithms within the oracle-augmented datastream model?	Incomplete details on perfromance of the method
In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.	Incomplete details on perfromance of the method
If the authors don't discuss a motivation then how will a reader know how to apply the tool?	Limited insights based on design choices
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.	Correctness of algorithm proposed
I also wonder if the video data will be released, which could be important for the following comparisons.	Lacking details on datasets
Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques.	Missing theoretical comparisons
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.	Unclear description of method
This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.	Incomplete details on perfromance of the method
-3 For image-to-image translation experiments, no quantitative analysis whatsoever is offered so the reader can't really conclude anything about the effect of the proposed method in this domain.	Lack of analysis of proposed method
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.	Correctness of algorithm proposed
However in the approach proposed here, the negative examples are missing.	Correctness of algorithm proposed
Honestly, this paper is very difficult to follow.	3. The paper is not nicely written or rather easy to follow.
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.	Unclear description of method
1. The classification of base class into super classes seems questionable to me.	Correctness of algorithm proposed
- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.	Need more experimental results
- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.	Less datasets used
The negation example is nice but this doesn't seem to display the potential power of the method to understand a neural network.	design choices are questionable
Table 8 rises some concerns.	Incomplete ablation in terms of tables and figures
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.	Correctness of algorithm proposed
A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past.	Incremental novelty of method as compared to related work
The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018), although not perfect either, were better in this regard.	Results are more or less similar to baselines
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.	Less datasets used
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?	Correctness of algorithm proposed
I recommend adjust the language to be more consistent throughout.	3. The paper is not nicely written or rather easy to follow.
(2) The function of the discriminator is not very clear, especially for the classification error test.	Missing details on methodology (eg., use of notation, use on tasks, etc)
First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel.	Limited novelty in theoretical contribution
Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?	Missing details on methodology (eg., use of notation, use on tasks, etc)
It seems to me just a combination of several mature techniques.	Limited novelty in theoretical contribution
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true."	Correctness of algorithm proposed
Overall, the paper is a little confusing.	3. The paper is not nicely written or rather easy to follow.
Do they here refer to the gradients with respect to the weights ONLY?	Missing details on methodology (eg., use of notation, use on tasks, etc)
How do we take a limit of M -> ∞ ? Does k also go ∞?	Missing details on methodology (eg., use of notation, use on tasks, etc)
Does such approximation guarantee the policy improvement?	Correctness of algorithm proposed
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.	generalizability of results is questionable
The writing is understandable for the most part, but the paper seems to lack focus - there is no clear take home message.	While sensible, this seems to me to be too minor a contribution to stand alone as a paper.
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.	Incomplete details on perfromance of the method
Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).	Experimental study not strong enough
This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.	Incremental novelty of method as compared to related work
Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.	Lack of experimental details (no of images, sampling criteria, etc)
It is also not clear to me why CIFAR datasets involve two domains and how these domains are relevant in each of the tasks.	Lacking details on datasets
I find the background on ELBO and GANs unnecessary occluding the clarity at this point.	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
"5. The term ""PowerSGD"" seems to have been used by other papers."	Missing literature review (some literature not included, misses baseline citations, etc)
However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).	Improper comparison of related work in terms of implementation
Authors should clarify the justification behind experimenting only on 'first 500 test images'.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.	Missing theoretical comparisons
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily."	Unclear description of method
So, the motivating assertion “[...] state-of-the-art solvers do not yet scale to large, difficult formulas, such as ones with hundreds of variables and thousands of clauses” in the introduction of the paper, is not totally correct.	Incorrect claims in introduction (confusing related work presented in intro, wrong claims)
"The column ""FLOPS"" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases."	correctness of results presented is questionable(eg., metrics, complexity, etc)
3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?	Missing details on methodology (eg., use of notation, use on tasks, etc)
* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.	Incomplete details on perfromance of the method
The graphs were difficult to parse.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
The experiments currently demonstrate that optimizing both controller and hardware is better than optimizing just the controller, which is not surprising and is a phenomenon which has been previously studied in the literature.	Not enough novelty in experiments (seems similar to previous work)
The reviewer votes for rejection as the method has limited novelty.	Limited novelty in theoretical contribution
"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?"	Correctness of algorithm proposed
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.	Unclear description of method
To change to a firm accept, I think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area, which can easily be implemented in the camera ready stage of paper preparation.	3. The paper is not nicely written or rather easy to follow.
- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
However, the experimental results are weak in justifying the paper's claims.	Experimental study not strong enough
In terms of presentation, the notation and statement of theorems are precise, however, the presentation is rather dry, and I think the paper can be significantly more accessible.	3. The paper is not nicely written or rather easy to follow.
The connections of the proposed approach with existing literature should be better explained.	Improper comparison of related work in terms of implementation
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.	Correctness of algorithm proposed
"The title claims way more than what is actually delivered in the paper, despite the fact that the authors have put in an ""On"" in the beginning of the title."	Incorrect claims in introduction (confusing related work presented in intro, wrong claims)
- Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.	Incomplete ablation in terms of tables and figures
6. The rationale of the two tower design (why not combine two) is not clearly explained.	Missing details on methodology (eg., use of notation, use on tasks, etc)
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.	Unclear description of method
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).	Correctness of algorithm proposed
Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?	Incomplete details on perfromance of the method
On the other hand, it is hard to say what the _main_ contribution is, which in turn makes it difficult to evaluate whether the experimental evaluation is sufficient:	design choices are questionable
There has been a large body of work which shows that weaker attacks like membership attacks can be equally damaging, ii) Privacy is a worst-case guarantee.	Incorrect assumptions as compared to related work
That said, I would like to see more analysis on the behavior of the proposed method under various interesting cases not tested yet.	Lack of analysis of proposed method
"- Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)"	3. The paper is not nicely written or rather easy to follow.
Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?	Incomplete details on perfromance of the method
Thus the experiment comparison is not really fair.	Experimental study not strong enough
I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.	More variations of experiments needs to be added
This issues makes reviewing this paper very difficult.	3. The paper is not nicely written or rather easy to follow.
3. The graph neural networks used in the model are not described in the paper, only a reference to Paliwal et al (2019) is given.	Missing implementation details of related work used as baselines
In summary, I'm inclined to reject this paper given the current version.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.	Incomplete details on perfromance of the method
Weakness: It would be good to see some comparison to the state of the art	Missing baselines
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.	Correctness of algorithm proposed
Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.	Experimental study not strong enough
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]"	Unclear description of method
As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).	Incomplete details on perfromance of the method
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right	Missing details on methodology (eg., use of notation, use on tasks, etc)
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.	Unclear description of method
I consider this assumption unrealistic.	Correctness of algorithm proposed
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?	Correctness of algorithm proposed
This matters, because the notion of equivariance really only makes sense for a group.	Incomplete details on perfromance of the method
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.	Unclear description of method
1. What is M in Algorithm 1 ?	Unclear description of method
Please run at least 10 experiments.	Experimental study not strong enough
before Eq (3): the 'photometric ..' -> a 'photometric ..'	Correctness of algorithm proposed
Overall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.	Correctness of algorithm proposed
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?	correctness of results presented is questionable(eg., metrics, complexity, etc)
When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.	Incomplete details on perfromance of the method
Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
However, I can not agree because you can simply generate poisoned data and train the neural networks on the poisoned data regardless of the underlying approach that is targeted in generating the poisoned data.	generalizability of method on datasets created from different distributions is questionable
The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.	Limited improvement over baselines
4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation?	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.	Incomplete details on perfromance of the method
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.	Unclear description of method
o feedforward rather than recurrent network;	Incomplete details on perfromance of the method
As a result, constraining the model will alter the mutual information and I think the effect of this should be remarked on.	Theoretical misunderstanding in methodology
However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.	Missing details on methodology (eg., use of notation, use on tasks, etc)
The paper has an interesting potential but seems a bit limited in its present form.	Lacking clarity overall (needs better presentation)
As such the paper is not convincing.	While sensible, this seems to me to be too minor a contribution to stand alone as a paper.
The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.	Incomplete details on perfromance of the method
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.	Unclear description of method
Have the authors considered to use categorical or binary variables?	Missing theoretical comparisons
2. In Figure 3, the baseline got different perplexity between 3(a) and 3(b).	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Furthermore, in experiments, the paper does not provide any quantitatively convincing results to suggest the generator in use is a good one.	correctness of experiments is questionable
In Figure 5, a legend indicating the relationship between color intensity and distance would be helpful.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.	Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)
It would be nice if the authors pointed to a git repository with their code an experiments.	Lack of experimental details (no of images, sampling criteria, etc)
More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.	Limited improvement over baselines
I would suggest comparing with CW attack under different sets of hyper-parameters.	Missing theoretical comparisons
This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.	Missing details on methodology (eg., use of notation, use on tasks, etc)
- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?	Incomplete details on perfromance of the method
How do performance and model size trade off?	Correctness of algorithm proposed
Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.	Missing details on methodology (eg., use of notation, use on tasks, etc)
However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.	Lack of realistic datasets used in experiments (small siz, synthetic)
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?	Incomplete details on perfromance of the method
In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.	Incomplete details on perfromance of the method
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.	generalizability of results is questionable
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?"	Correctness of algorithm proposed
The samples from MNIST in Figure 3 are indeed very blurry, supporting this.	Incomplete ablation in terms of tables and figures
positive - it unlikely to be true that an undefended network is predominantly	Unclear description of method
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.	Correctness of algorithm proposed
The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.	Less datasets used
"2) It would be great if the paper can clearly define the experiments: ""waypoint"", ""oncoming"", ""mall"", and ""bottleneck""."	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part	Unclear description of method
Finally, could you give a more accurate citation (chapter-page number) for the single-channel version of Theorem 2.?	Theoretical misunderstanding in methodology
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper."	Unclear description of method
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.	Correctness of algorithm proposed
Why mention it here, if it's not being defined.	Unclear description of method
In addition, the subindex ‘1’ of the point ‘q’ is not explained.	Unclear description of method
Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.	Limited improvement over baselines
Why should we use embedding to compare the similarity?	Correctness of algorithm proposed
All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.	Improper comparison of related work in terms of implementation
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).	Correctness of algorithm proposed
The maths is not clear, in particular the gradient derivation in equation (8).	Unclear description of method
This sort of two-stage generation is also potentially interesting, I was wondering if the authors had ideas to generalize this idea.	This sort of two-stage generation is also potentially interesting, I was wondering if the authors had ideas to generalize this idea.
o use of the Penn Treebank dataset only;	Less datasets used
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?	Experimental study not strong enough
Lemma 2.4, Point 1: The proof is confusing.	Incomplete details on perfromance of the method
It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.	Missing details on methodology (eg., use of notation, use on tasks, etc)
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).	Unclear description of method
The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.	More variations of experiments needs to be added
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.	Incomplete details on perfromance of the method
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?	Correctness of algorithm proposed
- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.	More variations of experiments needs to be added
* The paper states multiple times that VAEAC [Ivanov et al., 2019] cannot handle partially missing data, but I don’t think this is true, since their missing features imputation experiment uses the setup of 50% truly missing features.	Incorrect assumptions as compared to related work
The proofs are quite dense and I was unable to verify them carefully.	Missing details on methodology (eg., use of notation, use on tasks, etc)
That is, authors assumed that they have a degradation function F and all the inference process is just based on this known function.	Limited insights based on design choices
This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.	Incomplete details on perfromance of the method
Negative aspects: One major concern I have with the paper is the notion of privacy considered.	Correctness of algorithm proposed
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?	Incomplete details on perfromance of the method
This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.	Incomplete details on perfromance of the method
While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.	Incremental novelty of method as compared to related work
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.	Correctness of algorithm proposed
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15."	Unclear description of method
Moreover, due to the trace based loss function, the computational cost will also be very high.	Incomplete details on perfromance of the method
"In the last paragraph in Related Work, ""provide"" should be ""provides""."	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
I am also wondering if the comparison with the baselines is fair.	Missing baselines
https://arxiv.org/abs/1802.04942	Incorrect citation
Is a simple algorithm enough? What algorithms should we ideally use in practice?	Incomplete details on perfromance of the method
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.	Less datasets used
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.	Unclear description of method
The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.	Incomplete details on perfromance of the method
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other	Unclear description of method
The experiments on SHREC17 show all three spherical methods under-performing other approaches.	More variations of experiments needs to be added
caption of Fig 1: extractS	Incomplete ablation in terms of tables and figures
- very incremental improvements on PTB over a very simple baseline (far from SotA)	Missing implementation details of related work used as baselines
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.	Correctness of algorithm proposed
1. Experimental settings are clear, however, what makes me confused is that the construction for p_{\bar{d}} is straightforward for simple distribution like 2D points dataset, however, it might be intractable for complex high dimensional data such as images.	Limited generalizability of claims on other datasets
It is not even clear that the final compression of the baselines would not be better.	Incorrect assumptions as compared to related work
1. the difficult to train the network	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.	Unclear description of method
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.	Unclear description of method
This baseline was also missing in image reconstruction.	Missing baselines
(b) a significant clarification of Figure 4.	Incomplete ablation in terms of tables and figures
Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018).	Incremental novelty of method as compared to related work
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?	Incomplete details on perfromance of the method
The current experimental settings are not matched with the practice environment.	Experimental study not strong enough
3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper.	3. The paper is not nicely written or rather easy to follow.
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.	Unclear description of method
However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts.	Incremental novelty of method as compared to related work
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.	Unclear description of method
- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.	Incomplete details on perfromance of the method
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.	Missing details on methodology (eg., use of notation, use on tasks, etc)
Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?	Incomplete details on perfromance of the method
- This approach seems very limited, as there must exist a known transformation that removes the desired information.	Incomplete details on perfromance of the method
#3 is so generic that a large part of the previous literature on the topic fall under this category -- not new.	Incorrect assumptions as compared to related work
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.	Correctness of algorithm proposed
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.	Correctness of algorithm proposed
Without the comparison it’s not clear how much improvement this approach provides compared to existing work that perform stale updates.	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
Besides, in the experiments, the proposed method is not compared to other transfer learning methods.	Lack of comparison with related works
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.	Correctness of algorithm proposed
Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).	generalizability of results is questionable
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.	Experimental study not strong enough
Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?	Missing details on methodology (eg., use of notation, use on tasks, etc)
However, in this way, when more and more tasks come, the generator will become larger and larger.	Correctness of algorithm proposed
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.	Correctness of algorithm proposed
The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.	Limited novelty in theoretical contribution
How big is the generalization gap for the tested models when adaptive kernel is used?	Incomplete details on perfromance of the method
Missing references - authors may want to consider citing https://arxiv.org/abs/1906.06187 as well in Sec. 2 - it seems very related to this work.	Missing citations in the paper
The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods.	Limited novelty in theoretical contribution
little improvements over the baselines or even significantly worse. More importantly,	Limited improvement over baselines
For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4).	claims on the datasets is questionable
In general, the space that was used to explain the Transformer baselines---which are essentially straightforward ways to adapt transformers to this task---could have been used to give more detail on the dataset.	Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc)
* There is still no comparison with competing nonparametric tests on the fMRI data.	Less datasets used
3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.	Limited insights based on design choices
I believe that this paper is not the first one to study question generation from logical form (cf. Guo et al, 2018 as cited), so it is unclear what is the contribution of this paper in that respect.	Limited novelty as compared to related work
- Detailed experimental setups are missing.	Experimental study not strong enough
=> Baselines: The comparison provided in the paper is weak.	Missing baselines
As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.	Incomplete details on perfromance of the method
Though the title promises a contribution to an understanding of word embedding compositions in general, they barely expound on the broader implications of their idea in representing elements of language through vectors.	While better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?	More variations of experiments needs to be added
However image captioning datasets are not mentioned.	Less datasets used
On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it’s being reported as 6.24.	Missing baselines
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there)."	Incomplete details on perfromance of the method
- Does the MonoGAN exhibit stable training dynamics comparable to training WGAN on CIFAR-10, or do the training dynamics change on the single-image data set?	Lack of discussion of performance of method on different datasets
Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?	Missing details on methodology (eg., use of notation, use on tasks, etc)
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.	Incomplete details on perfromance of the method
"I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful."	Correctness of algorithm proposed
Beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scaling-up of [Pathak, et al., ICML17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like ‘curiosity honeypots’ is interesting).	Incremental novelty of method as compared to related work
Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.	Improper comparison of related work in terms of implementation
that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for ICLR.	that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for ICLR.
However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:	Missing baselines
From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification).	Limited novelty in theoretical contribution
It is hard to understand the results without discussing it.	Improper explanation of results (as in advantages, why are results better, etc)
It is also not clear why Table. 3 does not report the Bayes baseline results.	Improper presentation of results in tables and figures
3, The notations in Eq. (1) and (2) are messy.	Unclear description of method
More precisely, for the digit datasets, the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset.	Lack of discussion of performance of method on different datasets
The author never explains. E.g., link to NRMSE and PFC to the Table.	Missing explanation of comparsion with related work in tables and figures
In general, recent work has found that the raw number of parameters has little to do with the size of the model class or the capacity of a model for deep models, and thus work like [A] has been trying to come up with better complexity measures for models to explain generalization.	Limited novelty as compared to related work
How do “we choose a specific number of assignments based on prediction probabilities”?	Missing details on methodology (eg., use of notation, use on tasks, etc)
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).	Incremental novelty of method as compared to related work
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.	Correctness of algorithm proposed
It also adds capacity and it is not at all made clear whether the comparison is fair since no analysis on number of parameters are shown.	More experiments needed with related work
This needs more elaboration. Is this way of training results expected? What is the lesson learned?	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
Is there a reason why the authors do not introduce their objective by following the variational framework?	Missing details on methodology (eg., use of notation, use on tasks, etc)
"Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases)."	Correctness of algorithm proposed
However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain.	Limited insights based on design choices
"(1) My main problem with this paper is that the novel objective proposed by the authors in Eq. 7 is equivalent to the objective of WAEs appearing in Eq. 4 of [1] (up to a heuristic of applying logarithm to the divergence measure, which is not justified but meant to ""improve the balance between two terms"", see footnote 2), where the authors use the newly introduced Cramer-Wold divergence as a choice of the penalty term in Eq. 4 of [1]."	Limited novelty in theoretical contribution
In Figure 6 there seem to be unnecessary discrepancies between the y-axis and colorbar of subplots (a) and (b), and keeping those more consistent would improve readability.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?	Correctness of algorithm proposed
- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.	Improper comparison of related work in terms of implementation
hence, there is no connection between the theoretical results on MDL generalization bound and the proposed method MULANN.	hence, there is no connection between the theoretical results on MDL generalization bound and the proposed method MULANN.
- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.	Lack of realistic datasets used in experiments (small siz, synthetic)
The result does not at all apply to all of them.	Limited impact of results
In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.	Missing explanation of utility of proposed method
- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.	Missing details on methodology (eg., use of notation, use on tasks, etc)
(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.	Incomplete details on perfromance of the method
It might be beneficial to include comparison to this approach in the experimental section.	Experimental study not strong enough
I believe that the presentation of the proposed method can be significantly improved.	Unclear description of method
1. While I see the value in designing an automatic exploration mechanism, the complexity of the underlying approach makes the contribution of the bandit-based algorithm difficult to discern from the large number of other bells and whistles in the experiments.	-The experimental section do not clarify the benefits of the proposed approach.
However, the real-world experiments are not necessarily the easiest to read.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
Even with the hybrid method, the accuracy still drops.	Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)
However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.	Incomplete details on perfromance of the method
The improvement from the baselines is also limited.	Limited improvement over baselines
Given that BERT is most robust against their manipulation, it would be good to see a more powerful recurrent model for comparison.	Missing theoretical comparisons
- presumably, the sample complexity is ridiculous	Correctness of algorithm proposed
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem	Correctness of algorithm proposed
- In Section 3.1 : “Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.” For me, this is not particularly clear.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
With the current table, one has to compare cells in the top half of the table to those in the bottom half of the table, which is quite difficult to do.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.	Incomplete details on perfromance of the method
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).	Correctness of algorithm proposed
The difference with the other reference model (SVG) is less clear.	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.	Correctness of algorithm proposed
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.	Incomplete details on perfromance of the method
In fact, the separate training seems to make this unlikely.	Experimental study not strong enough
