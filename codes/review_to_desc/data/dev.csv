reviews	descs
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.	Missing details on methodology (eg., use of notation, use on tasks, etc)
It's hard for me to judge of the experimental results of section 5.3, given that there are no other	Missing interpretation of results due to missing related work
There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below.	More experiments needed with related work
* Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model.	Incorrect claims in introduction (confusing related work presented in intro, wrong claims)
Also, from the three Tables in the experimental part, the improvement of F-pooling over AA-pooling (developed by the main reference of this work) does not seem to be significant or consistent.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).	Unclear description of method
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.	Missing details on methodology (eg., use of notation, use on tasks, etc)
- The shown inception scores are far from state-of-the-art.	generalizability of results is questionable
The ResNet on Cifar-10 results are not convincing.	correctness of results presented is questionable(eg., metrics, complexity, etc)
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.	Incomplete details on perfromance of the method
p2-3, Section 3.1 - I found the equations impossible to read. What	Unclear description of method
1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?	Missing theoretical comparisons
This makes the contribution of this paper in terms of the method	Unclear description of method
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings."	Correctness of algorithm proposed
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.	Incomplete details on perfromance of the method
I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.	Limited insights based on design choices
Although the idea behind this paper is fairly simple, the paper is very difficult to understand.	Unclear problem definition
I believe that a more challenging experiment should be conducted e.g. using celebA dataset.	Lack of realistic datasets used in experiments (small siz, synthetic)
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?	Correctness of algorithm proposed
I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Is that also true in this domain?	Incomplete details on perfromance of the method
In particular, it is unclear what the assumption on the size of the unlabelled test set is.	Lack of discussion on datasets (size, motivation for use, etc)
For example, one question is how often a single partial tree has multiple possible completions in the data.	Missing details on methodology (eg., use of notation, use on tasks, etc)
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.	Correctness of algorithm proposed
- little methodological innovation or analytical explanations	- little methodological innovation or analytical explanations
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?	Missing details on methodology (eg., use of notation, use on tasks, etc)
The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.	Limited novelty in theoretical contribution
It is not clear how the compression ratio in table 1 is obtained.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?	Unclear description of method
-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves.	Improper comparison of related work in terms of implementation
Is it better to decay learning rates for toy data sets?	Less datasets used
Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.	Need more experimental results
While I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel.	Limited novelty in theoretical contribution
https://arxiv.org/abs/1802.05822	Incorrect citation
Similarly, you did not indicate what the deterministic version of your model is.	Unclear description of method
Hence, I kindly do not think the outcome is truly a research result.	Not enough originality in results (not surprising)
As the approach uses NF is derived specifically for unstable dynamics, it is not clear to me how adding it would affect the training if the dynamics *is stable*. In this context, I consider that for example, the work by Balduzzi et al. 2018 may be relevant as it describes that the dynamics of games (the Jacobian) has two components, one of which describes the oscillating behavior (Hamiltonian game); whereas most games are a mix of oscillating and non-oscillating dynamics.	incorrect claims for related work
This paper looks very hastily put together, especially pages 7 and 8.	3. The paper is not nicely written or rather easy to follow.
2. Sec. 3.3, Fig. 3: The capacity (in terms of parameters)of both Resnet-18 and VGG-16 is higher than the capcity for YFCC100M dataset for n=10K images (comes to 161K bits), while the capacity of Resnet-18, with 14.7 million parameters (assuming float32 encoding) has 14.7 * 32 bits = 470.4 million bits, thus capacity alone cannot explain why VGG converges faster than Resnet-18, since both networks exceed the capacity, and capacity does not seem to have an established formal connection to rate of memorization.	Lack of discussion of performance of method on different datasets
- can you motivate why you are not using perplexity in section 3.2?	Correctness of algorithm proposed
"* It is unclear to me whether the ""efficient method for SN in convolutional nets"" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides."	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.	Improper comparison of related work in terms of implementation
- It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.	Unclear problem definition
Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap.	design choices are questionable
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.	Correctness of algorithm proposed
Experimental results itself are fine but not complete.	Experimental study not strong enough
"-In the introduction, ""it is in general impossible to find an embedding in R^d such that ..."", why do we have to make v and v'(and u, and u') far from each other?"	"-In the introduction, ""it is in general impossible to find an embedding in R^d such that ..."", why do we have to make v and v'(and u, and u') far from each other?"
What's the relation between the size of a model and that of a data set? By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?	Lack of discussion of performance of method on different datasets
As for the experimental results, the paper only provides results on the sentimental analysis results and digit datasets, which are small benchmarks.	In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.
My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.	Incomplete details on perfromance of the method
- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here.	Limited insights based on design choices
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations"""	Unclear description of method
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.	Correctness of algorithm proposed
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons""."	Less datasets used
- It would be nice if different stopping criteria were analysed.	Lack of analysis
There are a few typos throughout the paper such as:	3. The paper is not nicely written or rather easy to follow.
"- In figure 5 (a) ""cencept"" should be ""concept"""	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
Otherwise, it is impossible for a reader to correctly and objectively relate your proposed approach to previous literature.	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
It seems to me; the author assumed data from one modality is generated by all the latent factors (see Eq. (11)), then what is the point for assuming the prior of the latent factor is factorized (see Eq. (4) and (5))?	generalizability of method on datasets created from different distributions is questionable
"I take issue with the usage of the phrase ""skill discovery""."	Unclear description of method
For example, it is curious to see how denoising Auto encoders would perform.	Incomplete details on perfromance of the method
Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.	Missing theoretical comparisons
3.  Or simply to a well tuned \lambda, chosen on a per dataset basis? From the text it appears that \lambda is manually selected to trade off accuracy against uncertainty on OOD data.	generalizability of method on datasets created from different distributions is questionable
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.	Incomplete details on perfromance of the method
- (W3) Baselines for transfer learning: I felt this was another notable oversight.	Limited improvement over baselines
3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,	3. The paper is not nicely written or rather easy to follow.
The theoretical contribution is very limited.	Limited novelty in theoretical contribution
While this is a reasonable assumption, it does not necessarily seem to be the case that a larger, more disconnected saliency map indicates worse classification performance.	correctness of results presented is questionable(eg., metrics, complexity, etc)
I agree that local SO(2) invariance is too limiting. But it is not true that rotating filters is not effective in planar/volumetric CNNs, as shown by many recent papers on equivariant networks.	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
However, it is not obvious that how to move from line 3 to line 4 at Eq 15.	Missing details on methodology (eg., use of notation, use on tasks, etc)
The presented analysis seems to neglect the error term corresponding to the value function.	Too strong assumptions in analysis
Only some heuristic results are obtained for them without rigorous theory.	correctness of results presented is questionable(eg., metrics, complexity, etc)
- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful	Limited insights based on design choices
Also perhaps better to use the curly sign for vector inequality.	Unclear description of method
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.	Less datasets used
"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"")."	Correctness of algorithm proposed
The paper would need to be improved substantially in order to appear at a conference like ICLR.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.	Correctness of algorithm proposed
7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.	Less datasets used
However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.	Limited improvement over baselines
Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.	Limited novelty in theoretical contribution
"Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being ""meta domain adaptation""."	claims on the datasets is questionable
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.	Unclear description of method
While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.	Limited novelty in theoretical contribution
Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.	Incomplete details on perfromance of the method
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.	Correctness of algorithm proposed
"2.	What is the reason for using rule-based agents in all the experiments? It would have been more useful if all the analysis are done with RL agents rather than rule-based agents."	Too strong assumptions in analysis
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.	Incomplete details on perfromance of the method
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.	Correctness of algorithm proposed
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.	Correctness of algorithm proposed
Again, this follows from known results.	Not enough originality in results (not surprising)
First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).	Limited novelty in theoretical contribution
* The biggest problem for me was the unconvincing results.	generalizability of results is questionable
4. From Figure 6 and Figure 8-11, it looks like the bandit is more or less on par with fixed exploration policies.	Missing explanation of comparsion with related work in tables and figures
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.	Experimental study not strong enough
Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.	Experimental study not strong enough
However, the results are a bit misleading in their reporting of the std error.	generalizability of results is questionable
3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.	Lack of ablation on different datasets/ sizes
- For the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbel-sinkhorn.	More experiments needed with related work
- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number.	correctness of results presented is questionable(eg., metrics, complexity, etc)
Originality: Given the existing body of literature, I found the technical novelty of this paper rather weak. However, it seems the experiments are thoroughly conducted.	Not enough novelty in experiments (seems similar to previous work)
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications	Unclear description of method
However, it looks to me that the authors need to better explain the motivation of DiVA, the differences of DiVA from existing supervised VAE, and the experimental settings, before the acceptance of this paper.	Missing explanation of utility of proposed method
It is not obvious that shuffling image patches at a particular scale would lead to complete loss of global information, but the paper does show results on SVHN and CIFAR10 for which global information is sufficiently disentangled.	* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?
In addition, there is not much theoretical justification for it, it seems like a one-off trick.	Limited insights based on design choices
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).	Correctness of algorithm proposed
"- at the start of section 3: what is an ""experiment""?"	Experimental study not strong enough
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?	Missing details on methodology (eg., use of notation, use on tasks, etc)
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching""."	Unclear description of method
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.	Unclear description of method
This is discussed on p4, but it's unclear to me how keeping $\sigma$ separate is benefitial for the analysis, and this is not picked up again explicitly again?	Lack of discussion of analysis
How does the proposed method perform in more complicated tasks such as	Missing details on methodology (eg., use of notation, use on tasks, etc)
- the data sets used in the experiments are very small	Lack of realistic datasets used in experiments (small siz, synthetic)
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)	Correctness of algorithm proposed
eqn (8): use something else to denote the function 'U'.	Unclear description of method
- In Table 2, it is not clear to me what is the point for comparing errors of the natural images (which measures the misclassification rate of a natural image) and that of the adversarial images (which measures successful attacks rate), and why this comparison helps to support the claim that the attack results in a stronger certificates.	Incomplete ablation in terms of tables and figures
Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.	Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.	Missing details on methodology (eg., use of notation, use on tasks, etc)
One observation from the submission is that the token set may need to very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive (I noticed that the BERT model is trained on 128 GPUs)	claims on the datasets is questionable
(2) The method is not well motivated.	Limited insights based on design choices
It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).	Incremental novelty of method as compared to related work
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing."	Unclear description of method
While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.	Incomplete details on perfromance of the method
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.	Unclear description of method
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.	Experimental study not strong enough
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.	Unclear description of method
I am confused by Figure 4, and in general with the relative rank metrics.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
3. The experimental study is weak.	Experimental study not strong enough
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.	Experimental study not strong enough
Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.	generalizability of results is questionable
For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?	Lack of analysis
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what	Correctness of algorithm proposed
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.	Correctness of algorithm proposed
Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation	Limited insights based on design choices
*The experimental section is too limited.	Experimental study not strong enough
However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.	generalizability of results is questionable
First, this paper is not easy to follow.	3. The paper is not nicely written or rather easy to follow.
The results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?	incorrect claims for related work
I think it needs to be made clearer how reconstruction error works as a measure of privacy.	Incomplete details on perfromance of the method
Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
The theory tells the same as in case 2 above but with an additional price of optimizing a different function.	Theoretical misunderstanding in methodology
- The 3rd line of lemma 1: epsilon1 -> epsilon_1	Unclear description of method
Moreover, I  don't think that the data sets in experiments are good enough to cover the importance and the nature of the problem.	Not enough info on dataset and hyper-parameters
Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets.	Lacking details on datasets
"The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM"" doesn’t seem to be a standard approach."	Correctness of algorithm proposed
Many of the results have been already presented in	Not enough originality in results (not surprising)
However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.	generalizability of results is questionable
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.	Correctness of algorithm proposed
I also had a hard time going through the paper - there aren't many details.	3. The paper is not nicely written or rather easy to follow.
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.	Experimental study not strong enough
"- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification."	Missing details on methodology (eg., use of notation, use on tasks, etc)
The weight sharing was also needed further investigation and experimental data on sharing different parts.	Experimental study not strong enough
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.	Correctness of algorithm proposed
- From Figure 9, we see the certificate radii of the natural have at least two peaks. Though on average the certificate radii of the adversarial attacks is higher than that of the natural images, it is smaller than the right peak. Could the authors elaborate more of the results?	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
Clarifications of these points, and more in general the philosophy behind the architectural choices made, would make this paper a much clearer accept.	Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.	Unclear description of method
I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.	Missing baselines
However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.	Missing details on methodology (eg., use of notation, use on tasks, etc)
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.	Incomplete details on perfromance of the method
However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).	Experimental study not strong enough
The paper motivates the problem that we need to pick out an exploration sequence that optimizes learning progress, but then approximates it as simply measuring the return.	Need more interesting problem definition
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds."	Experimental study not strong enough
"- Pioneering work is not necessarily equivalent to ""using all the GPUs"""	Limited improvement over baselines
This was a fun, albeit incremental paper.	Overall not original enough (primary claim, limited evaluation, limited novelty)
First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.	Less datasets used
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.	Correctness of algorithm proposed
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.	Correctness of algorithm proposed
I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.	Incomplete details on perfromance of the method
3. In the introduction, the Authors point that prior methods have trouble dealing with textureless, reflective or transparent approaches, but it's not clear form the paper where it addresses these cases, and if yes, what is the mechanism for that.	3. In the introduction, the Authors point that prior methods have trouble dealing with textureless, reflective or transparent approaches, but it's not clear form the paper where it addresses these cases, and if yes, what is the mechanism for that.
In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.	Incremental novelty of method as compared to related work
1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing.	Missing baselines
Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.	Missing details on methodology (eg., use of notation, use on tasks, etc)
c. Figure 1 is over-complicated.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
"I do not understand the ""deep integration of MARL and HRL"" that is claimed in the Introduction."	Unclear intro (eg. contributions)
I have some concerns on this paper:	1. The presentation is somewhat convoluted.
3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]	Incremental novelty of method as compared to related work
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:	Unclear description of method
Yet, in Fig.1 some difference is observed between the methods, why is that so?	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
- Does training the generator and interpolation jointly improve the quality of the generator in general ? It would have been nice to run this method on more complicated dataset like CIFAR10 and see if this method increase the overall FID score.	Lack of ablation on different datasets/ sizes
Here are a few examples: The ICLR citation style needs to use sometimes \citep.	Incorrect citation styles
To evaluate the impact of the proposed changes in this paper, one would have to perform extended evaluations and ablations for the submission.	Too strong assumptions in analysis
You probably have to limit the operation to a half-sphere (there's some ideas for this in Gu et al).	Incorrect assumptions as compared to related work
How does the transformer based method comparing to others?	Incomplete details on perfromance of the method
- The writing looks very rushed, and should be improved.	3. The paper is not nicely written or rather easy to follow.
1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has	Limited novelty in theoretical contribution
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.	Incomplete details on perfromance of the method
1) I wonder if the proposed method work for most GAN models, more experiments evaluated on more recent GAN-based  models should be added to verify the superiority claimed in this paper, e.g., TP-GAN [Huang et al., ICCV 2017], PIM [Zhao et al., CVPR 2018], DR-GAN [Tran et al., CVPR 2017], DA-GAN [Zhao et al., NIPS 2017], MH-Parser [Li et al., 2017], 3D-PIM [Zhao et al., IJCAI 2018], SimGAN [Shrivastava et al., CVPR 2016], AIM [Zhao et al., AAAI 2019].	Missing implementation details of related work used as baselines
