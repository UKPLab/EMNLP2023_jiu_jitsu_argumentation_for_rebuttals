reviews	descs
- (W3) Baselines for transfer learning: I felt this was another notable oversight.	Limited improvement over baselines
1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.	Incomplete details on perfromance of the method
Hence, I kindly do not think the outcome is truly a research result.	Not enough originality in results (not surprising)
-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves.	Improper comparison of related work in terms of implementation
"Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being ""meta domain adaptation""."	claims on the datasets is questionable
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).	Unclear description of method
I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
- It would be nice if different stopping criteria were analysed.	Lack of analysis
I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.	Missing baselines
I believe that a more challenging experiment should be conducted e.g. using celebA dataset.	Lack of realistic datasets used in experiments (small siz, synthetic)
As the approach uses NF is derived specifically for unstable dynamics, it is not clear to me how adding it would affect the training if the dynamics *is stable*. In this context, I consider that for example, the work by Balduzzi et al. 2018 may be relevant as it describes that the dynamics of games (the Jacobian) has two components, one of which describes the oscillating behavior (Hamiltonian game); whereas most games are a mix of oscillating and non-oscillating dynamics.	incorrect claims for related work
The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.	Limited novelty in theoretical contribution
Also, from the three Tables in the experimental part, the improvement of F-pooling over AA-pooling (developed by the main reference of this work) does not seem to be significant or consistent.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
The paper would need to be improved substantially in order to appear at a conference like ICLR.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?	Missing theoretical comparisons
- The shown inception scores are far from state-of-the-art.	generalizability of results is questionable
I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.	Limited insights based on design choices
Although the idea behind this paper is fairly simple, the paper is very difficult to understand.	Unclear problem definition
Is it better to decay learning rates for toy data sets?	Less datasets used
It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).	Incremental novelty of method as compared to related work
The ResNet on Cifar-10 results are not convincing.	correctness of results presented is questionable(eg., metrics, complexity, etc)
This paper looks very hastily put together, especially pages 7 and 8.	3. The paper is not nicely written or rather easy to follow.
"* It is unclear to me whether the ""efficient method for SN in convolutional nets"" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides."	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
Experimental results itself are fine but not complete.	Experimental study not strong enough
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings."	Correctness of algorithm proposed
It seems to me; the author assumed data from one modality is generated by all the latent factors (see Eq. (11)), then what is the point for assuming the prior of the latent factor is factorized (see Eq. (4) and (5))?	generalizability of method on datasets created from different distributions is questionable
There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below.	More experiments needed with related work
2. Sec. 3.3, Fig. 3: The capacity (in terms of parameters)of both Resnet-18 and VGG-16 is higher than the capcity for YFCC100M dataset for n=10K images (comes to 161K bits), while the capacity of Resnet-18, with 14.7 million parameters (assuming float32 encoding) has 14.7 * 32 bits = 470.4 million bits, thus capacity alone cannot explain why VGG converges faster than Resnet-18, since both networks exceed the capacity, and capacity does not seem to have an established formal connection to rate of memorization.	Lack of discussion of performance of method on different datasets
3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.	Lack of ablation on different datasets/ sizes
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.	Missing details on methodology (eg., use of notation, use on tasks, etc)
The presented analysis seems to neglect the error term corresponding to the value function.	Too strong assumptions in analysis
