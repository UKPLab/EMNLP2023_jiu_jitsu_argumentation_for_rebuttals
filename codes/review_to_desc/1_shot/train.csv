reviews	descs
The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).	More variations of experiments needs to be added
The paper is not very self contained.	Lacking clarity overall (needs better presentation)
Due to several shortcomings of the paper, most important of which is on presentation of the paper, this manuscript requires a significant revision by the authors to reach the necessary standards for publication, moreover it would be helpful to clarify the modeling choices and consequences of these choices more clearly.	While sensible, this seems to me to be too minor a contribution to stand alone as a paper.
However the theoretical contribution is poor. And the experiment uses a very classical benchmark providing simulated data.	Not enough novelty in experiments (seems similar to previous work)
Fig 4 is very confusing.	Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)
A number of these references are missing and no experimental comparison to these methods has been made.	Missing baselines
Hence, I am not very sure whether the novelty of the paper is significant.	Overall not original enough (primary claim, limited evaluation, limited novelty)
The justification that the method of Khadka & Tumer (2018) cannot be extended to use CEM, since the RL policies do not comply with the covariance matrix is unclear to me.	incorrect claims for related work
I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.	Limited novelty in theoretical contribution
- experiments are performed using a synthetic setup on a single data set, so it remains unclear if the algorithm would be successful in a real life scenario	Not enough info on dataset and hyper-parameters
My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.	Missing theoretical comparisons
The introduction contains a few statements that may paint an incomplete or confusing picture of the current literature in adversarial attacks on neural networks:	Incorrect claims in introduction (confusing related work presented in intro, wrong claims)
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.	Less datasets used
I believe that this paper is not the first one to study question generation from logical form (cf. Guo et al, 2018 as cited), so it is unclear what is the contribution of this paper in that respect.	Limited novelty as compared to related work
"3.	Are the authors willing to release the code? Overall the model looks complicated and the appendix is not sufficient to reproduce the results in the paper."	Missing details for reproducibility of result
- Some parts of the paper feel long-winded and aimless.	3. The paper is not nicely written or rather easy to follow.
Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).	Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)
However, you are in a different computational model in which you now have access to an oracle.	Theoretical misunderstanding in methodology
Citations used for Gradcam are wrong -- Sundarajan et al., 2016 should be changed to Selvaraju et al., 2017.	Incorrect citation
More rigorous experiments and analysis is needed to make this a good ICLR paper.	The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).
"-	While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited."	Limited contribution in problem definition
This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.	Improper explanation of results (as in advantages, why are results better, etc)
- Although the authors discussed the experiment setting in detail in supplements, I believe open-sourcing the code / software used to conduct the experiments would be greatly help with the reproducibility of the proposed method for researchers or practitioners.	Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.
* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.	Limited improvement over baselines
- I'm not sure we can conclude much from the results on fetchSlide (and it would make sense not to use the last set of parameters but the best one encountered during training)	correctness of experiments is questionable
Besides, as the base classifier is different for various baselines, it is hard to compare the methods.	Incorrect baselines used
Second, the experiments are (as I understand it, but I may be wrong) in deterministic domains, which definitely does not constitute a general test of a proposed RL  method.	design choices are questionable
Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.	The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
This is not so interesting, even though results are impressive.	Limited impact of results
While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.	Lack of analysis
If faster training of dictionary learning models was a bottleneck in practical applications, this might be of interest, but it is not.	Missing explanation of utility of proposed method
- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange.	Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)
While reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.	Lacking details on datasets
As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.	Limited insights based on design choices
The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.	Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)
It would be helpful to have a brief paragraph describing this architecture, for readers not familiar with the referenced paper.	Missing implementation details of related work used as baselines
Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?	Limited generalizability of claims on other datasets
However, the theoretical justification and experimental results are not.	Improper Structuring of experimental results on paper (as in on some page, before some section)
Furthermore, in the beginning of Section 3.1 the authors present their idea on probabilistic hypernetoworks which “maps x to a distribution over parameters instead of specific value \theta.” How is this different from the case that we were considering so far? If we had a point estimate for \theta we would not require to take an expectation in Equation (3) in the first place.	While better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.
Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.	Incorrect assumptions as compared to related work
"* In the introduction, ""the classical approach"" is mentioned but to be the latter is"	Unclear intro (eg. contributions)
I am wondering this method can be applied to other complex datasets whose latent factors are unknown.	generalizability of method on datasets created from different distributions is questionable
* p.4 first paragraph you claim that this method responds well to data which exhibits seasonality, but none of your datasets deal with data that would exhibit seasonality.	Lack of discussion of performance of method on different datasets
"-	How the first camera pose is initialized?"	Missing details on methodology (eg., use of notation, use on tasks, etc)
2. The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction.	More comparisons needed with variations of the proposed method
Additionally, in section 6.4, the results in Figure 2 also does not look very	Improper presentation of results in tables and figures
The idea in this paper is novel but experiments do not seem to be enough.	Need more interesting problem definition
The abstract mentioned that the proposed algorithm works as an “implicit regularization leading to better classification accuracy than the original model which completely ignores privacy”. But I don’t see clearly from the experimental results how the accuracy compares to a non-private classifier.	Incorrect explanation of resuts
Although some promising	Not enough originality in results (not surprising)
Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers.	claims on the datasets is questionable
- Lambda sim and lambda s are used interchangeably. Please make it consistent.	Unclear description of method
* The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission.	Reasons for rejection (not fit for conference, contains several weaknesses, etc.)
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).	generalizability of results is questionable
But there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.), not clear	Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc)
Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.	correctness of results presented is questionable(eg., metrics, complexity, etc)
In particular, the exact difference between the proposed method and the ES baseline is not as clear as it could be.	Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)
2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper.	Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)
Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.	Experimental study not strong enough
They can indeed be subsumed by generalization bounds based on VC theory.	Correctness of algorithm proposed
Sometimes the same paper is cited 4 times within a few lines.	Incorrect citation styles
While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance.	More experiments needed with related work
- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect.	Incomplete ablation in terms of tables and figures
This has not been considered in the analysis.	Too strong assumptions in analysis
- The claim made in the first line of the abstract (applying RL algorithms to continuous control problems often leads to bang-bang control)	5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.
- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.	Missing explanation of comparsion with related work in tables and figures
The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions.	Lack of comparison with related works
They might be more convincing if there were results on a very large corpus such as 1 billion word corpus.	In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.	Incomplete details on perfromance of the method
There were some experimental details that were poorly explained but in general the paper was readable.	Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)
1. The presentation is somewhat convoluted.	1. The presentation is somewhat convoluted.
This latter baseline is a zero-cost baseline as it is not even dependent on the method.	Improper comparison of related work in terms of implementation
5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.	Lack of analysis of proposed method
More experiments based on other types of data sets with clear global structures such as faces or stop signs will	Lack of realistic datasets used in experiments (small siz, synthetic)
2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?	Lack of experimental details (no of images, sampling criteria, etc)
If the novelty is in applying to continual learning and new datasets, it is not clear that this is sufficient.	In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.	Incremental novelty of method as compared to related work
- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.	Need more experimental results
It is not clear how the noise is introduced in the graphs.	Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)
The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms.	Lack of discussion on datasets (size, motivation for use, etc)
For example, “Fuzzy Choquet Integration of Deep Convolutional Neural Networks for Remote Sensing” by Derek T. Anderson et al.	Suggest missing related work
However, the experimental comparison is not fair, the description of the model (e.g. how Choquet is integrated into the model and help to learn “intermediate meaningful results”) is not clear, some claims are not true.	Concerns regarding correctness of methods (assumption, budget, etc)
The above papers are not cited in this paper.	Missing citations in the paper
Besides, the results on the sentimental analysis are comparable with the compared baselines.	Results are more or less similar to baselines
My main concern about the paper is that, currently, the experiments do not include any strong baseline (the ES currently is not a strong baseline, see comments below).	Lack of experiments
4. Most importantly, for table4, authors are comparing to the ResNet18 ARNet instead of ResNet50? which is better than the proposed method.	"Perhaps I missed it, but I believe Dan Ciresan's paper ""Multi-Column Deep Neural Networks for Image Classification"" should be cited."
The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.	Missing literature review (some literature not included, misses baseline citations, etc)
The paper offers some analysis, suggesting when each of the conditions occurs, upper bounds the smallest singular value of D A (where the example dependent diagonal matrix D incorporates the ReLU activation (shouldn’t this be more clearly introduced and notated?).	Lack of discussion of analysis
