descs	scores
Limited generalizability of claims on other datasets [SEP] rebuttal_done [SEP] In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.	0.3508773619358619
Limited generalizability of claims on other datasets [SEP] rebuttal_done [SEP] We found that this heuristic works very well in our experiments (the results have updated to reflect on this heuristic).	0.649122638064138
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.	0.1334172456785001
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] Still, we pursued two directions.	0.1712191370351586
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] We tried this path, but soon realized we would not be done in time (especially with a hyperparameter search).	0.2439874587384383
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.	0.4513761585479028
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_reject-criticism [SEP] The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.	0.3508773619358619
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_reject-criticism [SEP] In that case, many cutting edge ideas will by necessity be excluded from the discussion, as will many research groups.	0.649122638064138
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_future [SEP] For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.	0.1975795937322861
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_future [SEP] We agree on this remark, the idea of scaling scores is right and would improve the interpretability of our benchmarks.	0.2815511087403978
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_future [SEP] It is definitely a good idea to analyse the correlation between changes in classification accuracy and in FSM values, thank you. We will rigorously investigate this in future work.	0.5208692975273159
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_done [SEP] We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).	0.0991274512750463
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_done [SEP] Furthermore, we present an experimental investigation of these different objectives (Sec. 6.4).	0.1201921274650668
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_done [SEP] Thanks for pointing this out, we fixed this in the updated version of the paper we just posted.	0.1542466413058904
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_done [SEP] -> Added a comment to “Practical Implications” in the revision	0.219801450657675
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_done [SEP] 10. Thank you for pointing these typos out, we have addressed it in the revision.	0.4066323292963215
generalizability of method on datasets created from different distributions is questionable [SEP] rebuttal_done [SEP] We added a new Section 4 in the revised version of the paper discussing these differences.	0.3508773619358619
generalizability of method on datasets created from different distributions is questionable [SEP] rebuttal_done [SEP] We found that this heuristic works very well in our experiments (the results have updated to reflect on this heuristic).	0.649122638064138
claims on the datasets is questionable [SEP] rebuttal_concede-criticism [SEP] We apologize for unclear description of experimental settings.	0.1334172456785001
claims on the datasets is questionable [SEP] rebuttal_concede-criticism [SEP] We agree that an ablation study on robustness of the embeddings across different datasets would be very interesting.	0.1712191370351586
claims on the datasets is questionable [SEP] rebuttal_concede-criticism [SEP] See our response to Shengyang Sun’s comment below.	0.2439874587384383
claims on the datasets is questionable [SEP] rebuttal_concede-criticism [SEP] (a) Thanks for catching this. Indeed this was due to a bug in the toy regression experiment which we have fixed now.	0.4513761585479028
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.	0.3508773619358619
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] * It looks like all the results are given on the test set. Did you not do any tuning on the validation data?	0.649122638064138
"Incorrect assumptions as compared to related work [SEP] rebuttal_answer [SEP] Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."	0.3508773619358619
Incorrect assumptions as compared to related work [SEP] rebuttal_answer [SEP] We understand the limitations of MacKay's analysis, which was presented to give a rough theoretical comparison point to our empirical evaluation.	0.649122638064138
"Incorrect assumptions as compared to related work [SEP] rebuttal_structuring [SEP] Concerning the point "" It is not even clear that the final compression of the baselines would not be better."	0.3508773619358619
Incorrect assumptions as compared to related work [SEP] rebuttal_structuring [SEP] Please check Table 3 in the revision.	0.649122638064138
design choices are questionable [SEP] rebuttal_concede-criticism [SEP] We apologize for unclear description of experimental settings.	0.3508773619358619
design choices are questionable [SEP] rebuttal_concede-criticism [SEP] If the reviewers feel strongly, we can move it to appendix, however we feel it helps to provide a complete picture.	0.649122638064138
design choices are questionable [SEP] rebuttal_structuring [SEP] Please refer to Table 3 and Appendix C.4 for updated comparison results.	0.1334172456785002
design choices are questionable [SEP] rebuttal_structuring [SEP] 1) Intuition about the improvement	0.1712191370351586
design choices are questionable [SEP] rebuttal_structuring [SEP] Next, we address the question regarding the gradient update types.	0.2439874587384383
design choices are questionable [SEP] rebuttal_structuring [SEP] See Figure.7 in updated Appendix C.2.	0.4513761585479028
design choices are questionable [SEP] rebuttal_done [SEP] We have addressed your concern about the baseline models and learning rate schedules in our updated paper.	0.3508773619358619
design choices are questionable [SEP] rebuttal_done [SEP] We added an illustrative example in the introduction to give an intuitive understanding of invariance, stability and their relationship.	0.649122638064138
correctness of experiments is questionable [SEP] rebuttal_structuring [SEP] Comment: The paper does not provide any quantitatively convincing results	0.3508773619358619
correctness of experiments is questionable [SEP] rebuttal_structuring [SEP] Comment: One of the problems highlighted in the paper regarding existing	0.649122638064138
Incomplete ablation in terms of tables and figures [SEP] rebuttal_concede-criticism [SEP] We apologize for unclear description of experimental settings.	0.1975795937322861
"Incomplete ablation in terms of tables and figures [SEP] rebuttal_concede-criticism [SEP] About the significance of score differences, we agree that it needs more details and comparisons, it was also noted by ""AnonReviewer1"" and we should make alternative tests to scale or give a few more references to the benchmark."	0.2815511087403978
Incomplete ablation in terms of tables and figures [SEP] rebuttal_concede-criticism [SEP] (a) Thanks for catching this. Indeed this was due to a bug in the toy regression experiment which we have fixed now.	0.5208692975273159
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] We discuss this point at the bottom of page 4 after equation 6 and will further clarify.	0.1334172456785001
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] We have presented results for addition and factorization in the main body of the paper, but refer to readers of the paper to the appendix where we have included a larger set of results.	0.1712191370351586
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] We have updated these tables so that the numbers are computed from the same model, rather than separate training runs.	0.2439874587384383
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] 2. In appendix B.2 of the paper, we have added the convergence plot for all methods on the CIFAR data sets.	0.4513761585479028
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] -Thanks! This was indeed an error, which we’ve corrected in the updated draft.	0.0781068773141067
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] To avoid confusing, we use different colors for them and explained that in the figure.	0.0913848418340474
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] We added this clarification to Section 3.4.	0.1108039684446293
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] We have fixed all the typos as suggested in the revised version.	0.1421984327951225
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] Please see the (updated) last paragraph of Section 5, which explains this comparison in detail.	0.2026331680715508
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] We rewrote the caption for Fig. 4.	0.3748727115405428
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We focus on simpler domains to provide proof-of-concept results as the first step on this direction.	0.04080177759611
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] Thanks for pointing out this.	0.0446552480567715
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] A1: Thanks for pointing this out.	0.0493998377992478
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.	0.0553983625149924
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We apologize for the confusions in Section 3.1.	0.0632464454201133
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] 1. You are right.	0.0739983405612589
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] (Bounds of KL divergence) Thank you for this good comment.	0.0897230222209655
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] Having said that, we acknowledge that more agility to, at least, discover that early on would be beneficial.	0.1151446465888704
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We acknowledge that the text of the paper can be improved to explain better why splits with lower #rhs/lhs are generally harder than those with higher #rhs/lhs, and we thank R1 for pointing this	0.1640813531677918
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] See our response to Shengyang Sun’s comment below.	0.3035509660738781
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.	0.3508773619358619
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] In addition to our brief overview, you could also find a very neatly detailed summarization of our method in reviewer 2’s comments (paragraphs 1-4).	0.649122638064138
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.	0.0362423289050409
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We want to however draw the reviewer’s attention to the ablation study and figure 5, if they were accidentally missed in the first reading.	0.0393229085776574
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Through this process, their respective strengths, weaknesses as well as biases can be most easily revealed (see figures in the appendix).	0.0430367252494965
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Instead, we focus on a single idea, and show that it can be applied very broadly.	0.047609371449088
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We tried this path, but soon realized we would not be done in time (especially with a hyperparameter search).	0.0533905157550535
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Still, we pursued two directions.	0.0609541973803305
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, we didn’t present the micro-benchmark results in this paper due to the space limit.	0.0713164650418791
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] (2) The logic has been well-utilized and verified in the image-translation domain. Again please see the details in the answer to the last question.	0.0864713115823742
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Instead, we feel the more appropriate baseline is performing tasks from a natural language description, as many prior zero-shot works have, and so we have moved this result to the main text, as noted above.	0.1109716774548606
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The very strong rejection of Gaussian noise and the observations in Fig. 4 point in this direction.	0.1581348739371265
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Indeed we have generally found CE to help the baselines in this setting.	0.2925496246670925
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] *Please also see reply to reviewer #2 on a similar question of evaluating against other methods*	0.3508773619358619
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] >> We respectfully disagree.	0.649122638064138
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] [A3] We were very impressed with the mathematical detail of the reviewer’s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.	0.0991274512750463
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.	0.1201921274650668
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] We hope clarifies our unintentionally imprecise original approach.	0.1542466413058904
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] Response: Thanks for pointing it out.	0.2198014506576749
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] Regarding comment 2: Thanks for the comment.	0.4066323292963214
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.	0.0362423289050409
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] - ""I don't see a discussion about the downsides of the method"""	0.0393229085776574
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ** Clarification on Mathematics in Section 3.1 **	0.0430367252494965
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Please refer to Section 3.1 and Section 3.4 for more details.	0.047609371449088
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ** Addressing Technical Comments:	0.0533905157550535
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Please kindly refer to Appendix A for more detailed results.	0.0609541973803305
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] You can check our updated paper with clarification and new experimental results.	0.0713164650418791
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q4: Clarification of Figure-4 (Section-4.2)	0.0864713115823742
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Next, we address the question regarding the gradient update types.	0.1109716774548605
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 1. Proof or citation for the flaws of FID	0.1581348739371265
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Let’s consider this small graph below	0.2925496246670925
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.	0.0628744420533733
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We are sorry for not stating this clearly in the theorem and we have revisited the present of the theorem. We will fix this issue in the next revision.	0.0661821281315749
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We will update the manuscript with these additional results and discussion and post it shortly.	0.0717816072467407
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We will revise accordingly. Instead of ``immediate justification``, we believe this work does provide a first-step, formal framework towards a better theoretical understanding.	0.074608677334513
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We are currently setting up experiments for detailed analysis of the respective advantages and disadvantages and will report about these results in a future paper.	0.0966678911420495
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.	0.1240571500284387
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We will present the full experiments of semantic segmentation in the future revision.	0.1767816030392985
"Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] The ""Structural Knowledge"" is in TabNN by default. We will clarify this in the paper."	0.3270465010240109
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general.	0.0224028604746036
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The simplifying assumptions, mentioned in the paper, allow us to i) formulate the tasks concretely and ii) curate the dataset and evaluate models viably.	0.0224028604746036
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.	0.0224028604746036
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We also hope our work can motivate the design of new tools/techniques tailored for this direction.	0.0249419030622368
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.	0.0265727492540584
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In that case, many cutting edge ideas will by necessity be excluded from the discussion, as will many research groups.	0.0312316932518316
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We have now made this very clear in both Remarks 3.3 and 3.5.	0.0326665180146478
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We respectfully disagree with the referee’s conclusions, and will elaborate on the above statements in the following.	0.0338864148537023
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Providing a controlled experiment is always challenging, though the elements mentioned by the reviewer were specifically selected as to reduce various confounds.	0.0396375932366517
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We agree that setup we selected is destined to fail, but it was done on purpose to illustrate the presence of spurious solutions.	0.0438731454791568
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We would like to ask the reviewer what is believed to be missing from this explanation on the subsampling part of our proposed method.	0.0466257724080095
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We show that in spite of the assumptions, our dataset presents significant challenges for current models.	0.047903625565274
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The results are comparable but the added term in setting II shows benefits on some datasets.	0.0600173966365479
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Next, we wonder how would the reviewer correct the confounds mentioned with regard to the clique experiment.	0.0727709831126622
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] First, we’d like to emphasize that the clique problem studied in the paper is far from being a toy problem.	0.09338919142869
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] 1- We briefly mentioned the way problem embedding with similarity metric is used in the recommendation system in this work, but here is more explanation on that.	0.133079106433532
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In the MNIST experiments we already included Gumbel top-k sampling, but we will also add this for the other experiments in the revised manuscript.	0.2461953258391875
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.	0.3508773619358619
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] In general, we agree with the reviewer’s point that hyperparameter searching can be important.	0.649122638064138
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] => We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.	0.0781068773141067
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] We are definitely interested in studying how our approach can be applied to more complex tasks as future work.	0.0913848418340474
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] We leave more comprehensive studies on diversity to future work.	0.1108039684446293
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.	0.1421984327951226
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.	0.2026331680715508
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] It is definitely a good idea to analyse the correlation between changes in classification accuracy and in FSM values, thank you. We will rigorously investigate this in future work.	0.3748727115405429
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.	0.010017689785661
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] (we give related discussion in section 3.1 and add much more experimental results in Figure 5, further details please see the revised paper.)	0.0103015097726253
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have further clarified the explanation given in the text and included another ablation experiment  (row 4 of Table 5) to confirm its usefulness.	0.0113736095365891
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.	0.0117467078017389
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).	0.0117496185898352
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] This is further discussed in the newly added section 6.4.	0.0118864231835581
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.	0.0129343183566167
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We clarified this in Section 2 of the revised draft.	0.0137322011474794
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We updated section 2.3 to reflect this more clearly.	0.0139389224081312
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] Thank you for this suggestion, we have now clarified this connection in Section 3.	0.0140007753794213
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] Good point! We have updated this in the revised document, and we think it enhanced clarity.	0.0163095922579771
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have also added the comment at the beginning of Section 3 of the paper.	0.0163251325105083
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] (refer to section 3.3 in the revised paper.)	0.017438492339321
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have reorganized this section, as shown in our updated paper.	0.0184303988419473
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We discuss our usage of prior knowledge in greater detail in the section titled “Prior Knowledge” in our response to Reviewer 2.	0.0186837762980108
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] In page 4 in the revised version (footnote 3), we have clarified this and notified its potential for future work.	0.0193998793661481
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have added this clarification in Section 1.1.	0.0194747977481187
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We’ve updated the paper to mention this.	0.0196646258117628
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We thank the referee for bringing the article [V] to our attention and we now have acknowledged the prior work properly in our introduction.	0.020858568684736
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] Furthermore, we present an experimental investigation of these different objectives (Sec. 6.4).	0.0278256122850972
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] 10. Thank you for pointing these typos out, we have addressed it in the revision.	0.029292688059769
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We added an illustrative example in the introduction to give an intuitive understanding of invariance, stability and their relationship.	0.0320950590672526
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] 8. Thank you for pointing out several typos. We have fixed all of them in the revision.	0.0345198435663721
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have fixed all the typos as suggested in the revised version.	0.0362090431724517
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We also included the suggested related work (Balduzzi et al. 2018) in Section 5.	0.0422099406641721
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] In the revision, we have added more explanations on the selection of n and k in Section 2.4.	0.0442518064076293
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] 2. Following your suggestions, we add a study on the diversity of agents (presented in Appendix A of the updated paper).	0.0457949366929263
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] But in the revised version we have recapped the definition of Chi when introducing our method at the beginning of Section 4.	0.0568927853889977
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] On reviewer’s comments, we have updated A.4., and moved the note about eps_t = O^*(1/log(n)) to the Appendix A.	0.0716503074397223
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have added a respective proof in Appendix, Section A, where also the precise mathematical construction of the general form of Cramer-Wold metric is presented.	0.1021022184809594
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] Secondly, we agree with the reviewer that comparing with the Gauss-Newton algorithm will be interesting and have updated such a comparison in Appendix B in the revised version according to the reviewer’s suggestions:	0.1888887189544625
Too strong assumptions in analysis [SEP] rebuttal_done [SEP] Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.	0.1975795937322861
Too strong assumptions in analysis [SEP] rebuttal_done [SEP] -> For more on this we refer to the newly added Section “Scope” in the revision.	0.2815511087403978
Too strong assumptions in analysis [SEP] rebuttal_done [SEP] In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.	0.5208692975273159
Missing details for reproducibility of result [SEP] rebuttal_structuring [SEP] 3. Are the authors willing to release the code?	0.3508773619358619
Missing details for reproducibility of result [SEP] rebuttal_structuring [SEP] We have added an explanation of what we mean by “learning dynamics of deep learning” in the last paragraph of the first page.	0.649122638064138
Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc) [SEP] rebuttal_concede-criticism [SEP] We agree much detail on embeddings can be condensed or moved to Appendix.	0.3508773619358619
Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc) [SEP] rebuttal_concede-criticism [SEP] We included embedding details in the paper because a reviewer from the venue we previously submitted to requested these details to be in the paper.	0.649122638064138
Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc) [SEP] rebuttal_done [SEP] We haved add the above discussion to the latest version as Appendix A.9.	0.3508773619358619
Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc) [SEP] rebuttal_done [SEP] We revised the notations in the paper to make formulation clearer.	0.649122638064138
Lacking details on datasets [SEP] rebuttal_concede-criticism [SEP] We agree much detail on embeddings can be condensed or moved to Appendix.	0.3508773619358619
Lacking details on datasets [SEP] rebuttal_concede-criticism [SEP] We included embedding details in the paper because a reviewer from the venue we previously submitted to requested these details to be in the paper.	0.649122638064138
Lacking details on datasets [SEP] rebuttal_reject-request [SEP] Due to the space restriction, however, we cannot present them all in the paper.	0.3508773619358619
Lacking details on datasets [SEP] rebuttal_reject-request [SEP] Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.	0.649122638064138
Lacking details on datasets [SEP] rebuttal_done [SEP] We believe that, together with other architectural details present in the paper, it makes our work reproducible.	0.1975795937322861
Lacking details on datasets [SEP] rebuttal_done [SEP] We revised the notations in the paper to make formulation clearer.	0.2815511087403978
Lacking details on datasets [SEP] rebuttal_done [SEP] We have added a pseudo-code description of TTS-GAN training algorithm to the updated submission.	0.5208692975273159
Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification. [SEP] rebuttal_done [SEP] We have already uploaded our source codes as well as the demonstration videos to the following sites.	0.3508773619358619
Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification. [SEP] rebuttal_done [SEP] Our experimental results and statements presented in the manuscript are fully reproducible and verifiable.	0.649122638064138
Missing implementation details of related work used as baselines [SEP] rebuttal_done [SEP] In addition to implementation details, the appendix has a rather detailed table of the architecture parameters.	0.3508773619358619
Missing implementation details of related work used as baselines [SEP] rebuttal_done [SEP] In addition, we have included a brief description of the graph neural network architecture used in Paliwal et al (2019).	0.649122638064138
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_structuring [SEP] It would be nice if the authors pointed to a git repository with their code an experiments.	0.3508773619358619
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_structuring [SEP] Q4: Can the authors show concrete examples on how the attacks are generated? The details are especially unclear on LOAN.	0.649122638064138
Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc) [SEP] rebuttal_done [SEP] Thanks for this; we have updated the draft to make the presentation clearer.	0.3508773619358619
Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc) [SEP] rebuttal_done [SEP] We have modified the caption of Fig. 2 to be more specific.	0.649122638064138
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.	0.0991274512750463
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).	0.1201921274650668
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] A1: Thanks for pointing this out.	0.1542466413058904
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] We agree much detail on embeddings can be condensed or moved to Appendix.	0.219801450657675
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] We included embedding details in the paper because a reviewer from the venue we previously submitted to requested these details to be in the paper.	0.4066323292963215
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Thus, we suggested applying the domain translation to address this issue.	0.3508773619358619
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] #Add the noisy examples to the list of training images for this class	0.649122638064138
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.	0.0521505911336592
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We therefore have made many improvements and adaptations by either referring to existing literature, or depending on the specific tasks.	0.0546957240735772
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Furthermore, we have put more details about model architecture as in Appendix A Figure 4 and Figure 6.	0.0648476695459493
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] This analysis is illustrated in figure 3 of our paper.	0.0765062691395816
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Thus, we adopt the same hyperparameters and architectures as the prior work, and as a result our work is fairly easy to reproduce.	0.0806761842076921
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We stressed this setup in Section 3.2.	0.0848138649137229
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Thus we decided to feature the per-iteration comparison in the main paper, as it is the cleanest one.	0.1158430217138636
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We want to imply that our proposal is not incompatible with NCE, but we did not yet explore it so, to make the paper more self-contained it is probably best to leave this out.	0.1650763038379422
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;	0.3053903714340113
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_social [SEP] We will gladly provide files with the trained weights and also fully trained neural networks on request.	0.0375
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_social [SEP] Also note that all code is provided, and we invite the reviewer to replicate our experiments.	0.3208333333333333
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_social [SEP] - We thank the reviewer for suggesting additional comparisons with specific more complex models, although we feel that calling these methods 'stronger' requires some clarification or support.	0.3208333333333333
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_social [SEP] >>> Thanks for pointing out the details.	0.3208333333333333
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.	0.0267338060626354
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.	0.0284818311536879
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 4. Details in the experiments to clear up the settings.	0.0304993388060461
"Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] ""Some technical details are missing."	0.0328561475354567
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] * Missing implementation steps and optimization details:	0.0356489555633615
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q1: The learning procedure is confusing. It is highly recommended to provide the pseudocode of the proposed method.	0.0390158259326491
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] You may check our updated paper with clarification and new experimental results.	0.0507253173033506
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] See general responses #1 and #3.	0.0533937794594287
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.	0.0579113422605733
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Please kindly refer to first paragraph in Section 3.3.	0.060192090740965
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] A: Answer about Evaluation metrics:	0.0779886597181348
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q3. Figure 1 is too abstract:	0.1000850649982337
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q4: Can the authors show concrete examples on how the attacks are generated? The details are especially unclear on LOAN.	0.1426205672860165
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] >> As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.	0.2638472731794601
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] We will make all code and models trained in this paper available for reproducibility.	0.0991274512750463
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] All code and model weights used in this paper will be made available.	0.1201921274650668
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] We are sorry for not stating this clearly in the theorem and we have revisited the present of the theorem. We will fix this issue in the next revision.	0.1542466413058904
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] R1: We will provide the pseudocode in the future versions of the paper:	0.2198014506576749
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] We will discuss this process and test with different retention ratios in the final version.	0.4066323292963215
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_future [SEP] We leave it as a future work to study where the clear boundary is.	0.3508773619358619
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_future [SEP] We will investigate the benefits of more flexible mixing distributions in future work.	0.649122638064138
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] As suggested, we have utilized the appendices to give detailed information about the experimental setup.	0.0269416719955118
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We added a new Section 4 in the revised version of the paper discussing these differences.	0.0287032674104204
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] In addition to implementation details, the appendix has a rather detailed table of the architecture parameters.	0.030736446636345
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have updated the baseline details in the Appendix B.3.	0.0331115744044355
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have modified the text in Sec. 4.2, 4.3, and 4.4 to make the details more clear.	0.0359261056018614
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have updated the text in the manuscript (under section 2) to make this more clear.	0.0393191837144805
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We updated section 2.2 to relate to the references you mentioned.	0.0434969143020717
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We clarified this at the end of Section 4.3 in the revised version.	0.0487787597365404
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] (refer to section 3.3 in the revised paper.)	0.0556891685447877
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We haved add the above discussion to the latest version as Appendix A.9.	0.065156410560763
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We revised the notations in the paper to make formulation clearer.	0.0790022074061486
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have added a section in the appendix to include dilated convolution in the paper's formulation.	0.1013861400996487
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have added the detailed running time for each component in Table 3 in Appendix A of the revised version.	0.1444749447318347
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have also provided more details about LOAN dataset and how we attack in Appendix A.1.	0.2672772048551498
Reasons for rejection (not fit for conference, contains several weaknesses, etc.) [SEP] rebuttal_structuring [SEP] Q1: “There are a few grammatical/spelling errors that need ironing out.”	0.3508773619358619
Reasons for rejection (not fit for conference, contains several weaknesses, etc.) [SEP] rebuttal_structuring [SEP] Regarding your comments on a System Demonstration submission to ACL,	0.649122638064138
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] In addition, this work provides a detailed comparison of the effect of the losses on the various metrics.	0.1975795937322861
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] We agree that plausibility is indeed important, and that's what our human subject studies try to capture.	0.2815511087403978
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] We noticed the same in earlier iterations of our model.	0.5208692975273159
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_concede-criticism [SEP] We missed this previous work.	0.3508773619358619
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_concede-criticism [SEP] 5. We were not aware of this at the time of submission.	0.649122638064138
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_summary [SEP] Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.	0.3508773619358619
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_summary [SEP] This basically makes the methods and tasks quite different.	0.649122638064138
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] 5. Thank you for pointing out this related work. We refer to it in the updated version of the submission.	0.0991274512750463
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] Thank you for providing this related work and we now cite this paper in the second version.	0.1201921274650668
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] We would like to thank reviewer 3 for suggesting the related works that we have missed. These were incorporated in the revision.	0.1542466413058904
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] We have also incorporated the recent works [1,2] to appear soon at Neurips’19 (their text became available very recently and after the ICLR submission deadline).	0.2198014506576749
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] A5: We have added the result of MnasNet [2] in Table 2.	0.4066323292963214
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.	0.1975795937322861
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Higher-order and nonlinear covariance tests may make very appealing comparisons and we are looking into it.	0.2815511087403978
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] After examining the KTH results further, we realized that our results are likely weaker than they should have been, because we did not use the same preprocessing as prior work.	0.5208692975273159
Incorrect baselines used [SEP] rebuttal_by-cr [SEP] We plan to make our code public to aid research in the area.	0.1975795937322861
Incorrect baselines used [SEP] rebuttal_by-cr [SEP] Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.	0.2815511087403978
Incorrect baselines used [SEP] rebuttal_by-cr [SEP] We can include the literature review for training on imbalanced data sets as well as comparison of other methods with negative pre-training in terms of memory use and training complexity in the final version.	0.5208692975273159
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.	0.0991274512750463
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] We respectfully disagree with the referee’s conclusions, and will elaborate on the above statements in the following.	0.1201921274650668
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] We would like to ask the reviewer what is believed to be missing from this explanation on the subsampling part of our proposed method.	0.1542466413058904
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] In the meanwhile, we got the interesting idea of negative pre-training on training with imbalanced training data and tested our hypothesis and included in the paper.	0.2198014506576749
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] In the MNIST experiments we already included Gumbel top-k sampling, but we will also add this for the other experiments in the revised manuscript.	0.4066323292963214
Incorrect baselines used [SEP] rebuttal_done [SEP] In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.	0.1975795937322861
Incorrect baselines used [SEP] rebuttal_done [SEP] We have moved this latter comparison to the main text at the suggestion of Reviewer 1.	0.2815511087403978
Incorrect baselines used [SEP] rebuttal_done [SEP] We redo the visualization in Figure 6 to make the gains provided by SN clearer.	0.5208692975273159
Missing baselines [SEP] rebuttal_structuring [SEP] [R3: Weakness: It would be good to see some comparison to the state of the art ]	0.1334172456785001
Missing baselines [SEP] rebuttal_structuring [SEP] Q2: About “comparison with the baselines”:	0.1712191370351586
Missing baselines [SEP] rebuttal_structuring [SEP] Q1. Comparison with [1, 2, 3, 4].	0.2439874587384383
Missing baselines [SEP] rebuttal_structuring [SEP] Q1: The idea is very related to Yeh et al.’s work which is not mentioned at all.	0.4513761585479028
Missing baselines [SEP] rebuttal_by-cr [SEP] Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.	0.3508773619358619
Missing baselines [SEP] rebuttal_by-cr [SEP] The updated paper will discuss related work in more depth, including the suggested [A] and [B].	0.649122638064138
Missing baselines [SEP] rebuttal_reject-criticism [SEP] Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.	0.1975795937322861
Missing baselines [SEP] rebuttal_reject-criticism [SEP] A1: The entire first paragraph of our related work section is focused on Yeh et al.’s work.	0.2815511087403978
Missing baselines [SEP] rebuttal_reject-criticism [SEP] Note that proposing a generator architecture is not the goal of this paper.	0.5208692975273159
Missing baselines [SEP] rebuttal_done [SEP] In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.	0.0781068773141067
Missing baselines [SEP] rebuttal_done [SEP] In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.	0.0913848418340474
Missing baselines [SEP] rebuttal_done [SEP] We clarified this in Section 2.1 of the revised draft.	0.1108039684446293
Missing baselines [SEP] rebuttal_done [SEP] We thank the referee for bringing the article [V] to our attention and we now have acknowledged the prior work properly in our introduction.	0.1421984327951225
Missing baselines [SEP] rebuttal_done [SEP] We also updated our “Probabilistic Interpretation” section with analysis on how the contrastive loss helps us to learn a disentangled representation.	0.2026331680715508
Missing baselines [SEP] rebuttal_done [SEP] 6. We added the analysis for the 'mode jumping' problem to Section 6.2.	0.3748727115405428
More experiments needed with related work [SEP] rebuttal_by-cr [SEP] 4. We will include more related works to our paper.	0.1975795937322861
More experiments needed with related work [SEP] rebuttal_by-cr [SEP] We can include the literature review for training on imbalanced data sets as well as comparison of other methods with negative pre-training in terms of memory use and training complexity in the final version.	0.2815511087403978
More experiments needed with related work [SEP] rebuttal_by-cr [SEP] Discussion about VEEGAN and Lucas et al. will be added to our next revision.	0.5208692975273159
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.	0.1975795937322861
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.	0.2815511087403978
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] In the meanwhile, we got the interesting idea of negative pre-training on training with imbalanced training data and tested our hypothesis and included in the paper.	0.5208692975273159
More experiments needed with related work [SEP] rebuttal_done [SEP] We clarified this in Section 2.1 of the revised draft.	0.3508773619358619
More experiments needed with related work [SEP] rebuttal_done [SEP] Regarding the permutation learning experiment, in response to the feedback, we have revised the main text to clarify the setup.	0.649122638064138
Missing theoretical comparisons [SEP] rebuttal_concede-criticism [SEP] Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).	0.3508773619358619
Missing theoretical comparisons [SEP] rebuttal_concede-criticism [SEP] We recognize that the focus on parameter reduction was perhaps counter productive to making the goal or this work clear.	0.649122638064138
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We have cited this work in our related works section, and mentioned its impact.	0.0991274512750463
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We see this methodology, as illustrated by our experiments for one model, as the central part of our contribution.	0.1201921274650668
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Instead, we feel the more appropriate baseline is performing tasks from a natural language description, as many prior zero-shot works have, and so we have moved this result to the main text, as noted above.	0.1542466413058904
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] The paper may not make this point obvious enough probably because the curves are too thick to reveal the difference.	0.219801450657675
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] While we were writing the paper we in fact considered presenting Cakewalk as the reviewer suggests.	0.4066323292963215
Missing theoretical comparisons [SEP] rebuttal_social [SEP] We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.	0.1975795937322861
Missing theoretical comparisons [SEP] rebuttal_social [SEP] - We thank the reviewer for suggesting additional comparisons with specific more complex models, although we feel that calling these methods 'stronger' requires some clarification or support.	0.2815511087403978
Missing theoretical comparisons [SEP] rebuttal_social [SEP] Also note that all code is provided, and we invite the reviewer to replicate our experiments.	0.5208692975273159
Missing theoretical comparisons [SEP] rebuttal_by-cr [SEP] For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.	0.1975795937322861
Missing theoretical comparisons [SEP] rebuttal_by-cr [SEP] * We will refer to these works in the revised version to better orient reader.	0.2815511087403978
Missing theoretical comparisons [SEP] rebuttal_by-cr [SEP] Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.	0.5208692975273159
Missing theoretical comparisons [SEP] rebuttal_done [SEP] Complementary experiments have thus been performed, and tables 1, 2 updated.	0.04080177759611
Missing theoretical comparisons [SEP] rebuttal_done [SEP] Taking your comments and comments from other reviewers into account, we have made improvements to the experimental section.	0.0446552480567715
Missing theoretical comparisons [SEP] rebuttal_done [SEP] We’ve included these in an expanded discussion of related work with discussion on how they relate to the current dataset.	0.0493998377992478
"Missing theoretical comparisons [SEP] rebuttal_done [SEP] This point is now addressed in our ""Related Work"" section."	0.0553983625149924
Missing theoretical comparisons [SEP] rebuttal_done [SEP] **We have added this comment in Section 3.1**	0.0632464454201133
Missing theoretical comparisons [SEP] rebuttal_done [SEP] results in the updated paper.	0.0739983405612589
"Missing theoretical comparisons [SEP] rebuttal_done [SEP] *We have also clarified this within the ""Our Contributions"" Section**"	0.0897230222209655
Missing theoretical comparisons [SEP] rebuttal_done [SEP] We added the related discussion in Appendix E in the revision.	0.1151446465888704
Missing theoretical comparisons [SEP] rebuttal_done [SEP] We’ve updated the paper to mention this.	0.1640813531677918
Missing theoretical comparisons [SEP] rebuttal_done [SEP] We tried to cover related work thoroughly in section 2.	0.3035509660738781
Limited generalizability of claims on other datasets [SEP] rebuttal_answer [SEP] In responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.	0.0
I would have liked to see results for both trivial baselines like random ranking as well as more informed baselines where we can estimate transfer potential using say k representation techniques, and then use that to help us understand how well it would do on the other representations. [SEP] rebuttal_answer [SEP] Upon your suggestion, we would also add this random baseline in table 2 as well.	0.0
design choices are questionable [SEP] rebuttal_answer [SEP] Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.	0.0
Too strong assumptions in analysis [SEP] rebuttal_answer [SEP] We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.	0.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_by-cr [SEP] - Table 3 is indeed confusing, this is a good point. We will correct it.	0.0
1. The presentation is somewhat convoluted. [SEP] rebuttal_by-cr [SEP] The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.	0.0
incorrect claims for related work [SEP] rebuttal_concede-criticism [SEP] Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.	0.0
Incorrect assumptions as compared to related work [SEP] rebuttal_concede-criticism [SEP] However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.	0.0
Incorrect claims in introduction (confusing related work presented in intro, wrong claims) [SEP] rebuttal_done [SEP] results in the updated paper.	0.0
incorrect claims for related work [SEP] rebuttal_done [SEP] We also included the suggested related work (Balduzzi et al. 2018) in Section 5.	0.0
Incorrect assumptions as compared to related work [SEP] rebuttal_done [SEP] We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.	0.0
claims on the datasets is questionable [SEP] rebuttal_done [SEP] Fig. 3 and other evaluations have been updated for the new test set.	0.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_followup [SEP] Could you please elaborate on the comment ’the current design […] simply sums them up’?	0.0
incorrect claims for related work [SEP] rebuttal_future [SEP] Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.	0.0
claims on the datasets is questionable [SEP] rebuttal_future [SEP] We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.	0.0
design choices are questionable [SEP] rebuttal_refute-question [SEP] Though not explained in Section 4.	0.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_social [SEP] Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).	0.0
incorrect claims for related work [SEP] rebuttal_structuring [SEP] Q4. Updated abstract and performance evaluation.	0.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] Q4. The top row of Figure 2b is confusing:	0.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_structuring [SEP] -Q: Actionable consequences from paper:	0.0
Too strong assumptions in analysis [SEP] rebuttal_structuring [SEP] So, bounds here cannot be used to explain empirical observations in Section 5.	0.0
generalizability of method on datasets created from different distributions is questionable [SEP] rebuttal_summary [SEP] As the reviewer can observe the scale of the experimental evaluation is significantly different.	0.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] Note the compression rates are the same as the data in Table 3 in the original manuscript.	0.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.	0.0
Missing implementation details of related work used as baselines [SEP] rebuttal_answer [SEP] As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.	0.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;	0.0
Missing details for reproducibility of result [SEP] rebuttal_answer [SEP] We based our title on that paper since it extends some of its results to nonlinear neural networks.	0.0
Missing implementation details of related work used as baselines [SEP] rebuttal_by-cr [SEP] Q1: We are evaluating the proposed metrics on more recent GAN-based models you suggested and will update the results once the results become available.	0.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_by-cr [SEP] We’ll include more details about hyperparameters and hyperparameter selection in any future revision.	0.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_done [SEP] We have also provided more details about LOAN dataset and how we attack in Appendix A.1.	0.0
Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc) [SEP] rebuttal_future [SEP] We leave it as a future work to study where the clear boundary is.	0.0
Lacking details on datasets [SEP] rebuttal_reject-criticism [SEP] Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.	0.0
Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc) [SEP] rebuttal_summary [SEP] There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.	0.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_reject-request [SEP] Footnote 2 warned the reader about this, as we know it is unusual.	0.0
Limited generalizability of claims on other datasets [SEP] rebuttal_done [SEP] We found that this heuristic works very well in our experiments (the results have updated to reflect on this heuristic).	1.0
Limited generalizability of claims on other datasets [SEP] rebuttal_done [SEP] Therefore, in the new revision, we will add the constraints to the definition of alt-az rotation and make it one-to-one corresponds to the set points on $S^2$. See A1.	1.0
Limited generalizability of claims on other datasets [SEP] rebuttal_done [SEP] We have added a section in the updated manuscript to compare the objective of CDNs with that used in VIB and VI for Bayesian neural networks (see new Section 4).	1.0
Limited generalizability of claims on other datasets [SEP] rebuttal_done [SEP] We have made this procedure clear in the revised manuscript.	1.0
Limited generalizability of claims on other datasets [SEP] rebuttal_done [SEP] Furthermore, while we  always used 1 sample during training in the original submission (which indeed makes the CDN an instance of VIB) we now added experiments using 10 samples (see Section 6.4) in an experimental analysis of the different objectives.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] Specifically, the noise will add a term to the coefficient update in Lemma 2, and will effect the threshold, tau.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] On one hand, few-shot learning assumes that training examples in each class are quite small (only 1 or 5).	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] It is the trend of the decline in inference accuracy with pipelining is what we study.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] Nevertheless, the proposed algorithm can tolerate i.i.d. sub-Gaussian noise, including Gaussian noise and bounded noise, as long as the ``noise’’ is dominated by the ``signal’’.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] We spent a considerable amount of time trying to fulfill the reviewer’s request to match state of the art (SOTA) on PTB.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] These structures allow for guarantees as they are by design bijective, while vanilla architectures show a breadth of possible effects as shown in our analysis.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] This trend exists with both our hyperparameters and those at, for example, https://github.com/akamaster/pytorch_resnet_cifar10.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] As a result, our networks thus ran considerably slower, by more than 10x (not because our method is intrinsically slower, but just for lack of engineering optimizations on our bespoke Python implementations; we confirmed this by observing that a similar “hand-built” reimplementation of simple, non-plastic LSTMs ran similarly slower, while producing results identical to Merity et al.).	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] In this situation, Eq (3) and the closed-form version can be efficiently solved, since the dimension of S is only 80x80 or 100x100.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] This leads to the conclusion that if one wants invertibility or even just compactness reliably over the whole space, vanilla architectures using ReLU are not a good tool for the task.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] In a similar context to the one studied in the paper, such distributions have been studied by Rubinstein in his paper which discusses CE as an algorithm for combinatorial optimization, and in the classical bandit papers Exp3 is applied independently to several dimensions to study game theoretic problems.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] However, while they could simply leverage existing PyTorch implementations of LSTMs (written in extremely fast C++), we had to re-implement LSTMs “by hand” (i.e. as a series of connected layers) in PyTorch to introduce plasticity and neuromodulation.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] In this way, over time the probability for sampling such nodes becomes higher, and the chance of sampling all of them together increases.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] These experiments are thus unfortunately still running.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] The use of these set of hyperparameters, obtains an inference accuracy of 91.65% (better than the accuracy stated in the original ResNet paper) for ResNet-20 non-pipelined baseline and 91.21% for pipelined version.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] In early experiments, we trained our pure VAE model on the stochastic shape movement dataset from Babaeizadeh et al. (2018), and our pure VAE was able to model the dataset without any blur and with perfect separation of the possible futures.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] Under the noisy case, the recovered dictionary and coefficients will converge to a neighborhood of the true factors, where the neighborhood is defined by the properties of the additive noise.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] However, that is typically not the case of synthetic datasets.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] We then tried to weave neuromodulation and differentiable plasticity into the architecture and code base of Merity et al., ICLR 2018 (also tied for SOTA).	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] We found that the CDN objective leads to superior results, especially in the adversarial examples experiment.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] The simple explanation is that in some problems the conditional expectation of the objective given that some x_i=j is much better than for other values x_i=k. In such cases, for each dimension the algorithm will tend to sample values which are useful to many possible solutions.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] 1. Noise Tolerance — NOODL also has similar tolerance to noise as Arora et. al. 2015 and can be used in noisy settings as well.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] First, we tried to reimplement an architecture similar to  Melis et al. 2017.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] However, they did not publish their code, hyperparameters, or weights, requiring re-implementing and re-training from scratch.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] On miniImagenet, we performed iterative optimization and got 53.05/68.75 for 1-shot/5-shot experiments with only 10 steps.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] >>> We want to answer this question from two aspects.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] Still, we pursued two directions.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] In the clique problem for example, if some node i is part of a large clique, then sampling x_i=1 is likely to result in a good objective as there are many nodes that are connected to i, and the chance of not sampling any of them decreases with the clique size.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] One consequence of our paper is that it is close to impossible (each layer need at least to double the number of neuron) to enforce invertibility and it is similarly hard to enforce compactness in ReLU layers.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] This is slightly worse than closed-form version (53.75/69.43), because of the inaccurate computation and unstable gradients caused by multiple step iterations.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] For the dictionary, the noise will result in additional terms in Lemma 9 (which ensures that the updated dictionary maintains the closeness property).	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] On the other hand, there is plenty of prior work on the scalability and efficiency of label propagation, such as [2], [3], [4], which can extend our work to large-scale data.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] Lastly, we note that these kind of factorized distributions have a long history of being useful  in machine learning.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] Hence, our analysis can be seen as an argument for additional structure like dimension splitting in reversible networks (see e.g. Jacobsen et al. (2018)).	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] We tried this path, but soon realized we would not be done in time (especially with a hyperparameter search).	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] In other words, the noise terms will lead to additional terms which will need to be controlled for the convergence analysis.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_answer [SEP] In different contexts, such distributions have also been used as naive mean field approximations in variational inference.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_reject-criticism [SEP] In that case, many cutting edge ideas will by necessity be excluded from the discussion, as will many research groups.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_reject-criticism [SEP] -- It is true that evaluating the FSM is not necessarily the same as the classification results, which is precisely the reason why we show both in our results.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_reject-criticism [SEP] We do **not** want to claim that our results are anywhere near SOTA.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_reject-criticism [SEP] Moreover, insisting on papers to be SOTA to be accepted also likely encourages p-hacking and shoddy science to game the results (even if unintentionally), reducing the quality of science our community tries to build on.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_reject-criticism [SEP] That said, we still believe the results in the current paper demonstrate the benefits of our techniques on a sizable model, and thus it would benefit the community to allow people to know about, and build upon, these new methods and results.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_reject-criticism [SEP] We are not aware of any reports of an accuracy of ResNet-20 at 92% (perhaps this is approximate).	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_reject-criticism [SEP] Additionally, philosophically, If SOTA results are the bar for all papers to be accepted into conferences like ICLR, then those venues will be the exclusive domain of those with either the computation or time (i.e. large-scale resources) to dedicate to such results.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_reject-criticism [SEP] The purpose of the present paper is to introduce a novel technique and show that it can produce an advantage in realistic settings, which we believe our PTB task confirms.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_reject-criticism [SEP] Our claim is that, all other things being equal (especially the number of parameters), a neuromodulated plastic LSTM outperformed a standard LSTM on this particular benchmark task.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_future [SEP] We agree on this remark, the idea of scaling scores is right and would improve the interpretability of our benchmarks.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_future [SEP] It is definitely a good idea to analyse the correlation between changes in classification accuracy and in FSM values, thank you. We will rigorously investigate this in future work.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_future [SEP] A precise characterization of the relationship between the level of noise the size of convergence neighborhood requires careful analysis, which we defer to future effort.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_future [SEP] For that purpose, we should define a set of reference scores as you recommended to.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_done [SEP] Thanks for pointing this out, we fixed this in the updated version of the paper we just posted.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_done [SEP] 10. Thank you for pointing these typos out, we have addressed it in the revision.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_done [SEP] Furthermore, we present an experimental investigation of these different objectives (Sec. 6.4).	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_done [SEP] The purpose of adding adversarial losses to a pure VAE is to improve on blurry predictions where the latent variables alone cannot capture the uncertainty of the data.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_done [SEP] -> Added a comment to “Practical Implications” in the revision	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_done [SEP] We have added a new section (Sec. 4) to discuss the differences between the objective used for CDN, when performing variational inference for BNNs, and in the variational information bottleneck (VIB) framework.	1.0
generalizability of method on datasets created from different distributions is questionable [SEP] rebuttal_done [SEP] Moreover, we investigated the impact of the different objectives empirically and found that the CDN-based objective led to significantly better results, as shown in the newly added Section 6.4 in the revised manuscript.	1.0
generalizability of method on datasets created from different distributions is questionable [SEP] rebuttal_done [SEP] We found that this heuristic works very well in our experiments (the results have updated to reflect on this heuristic).	1.0
generalizability of method on datasets created from different distributions is questionable [SEP] rebuttal_done [SEP] We have made this procedure clear in the revised manuscript.	1.0
claims on the datasets is questionable [SEP] rebuttal_concede-criticism [SEP] This is a very good point.	1.0
claims on the datasets is questionable [SEP] rebuttal_concede-criticism [SEP] See our response to Shengyang Sun’s comment below.	1.0
claims on the datasets is questionable [SEP] rebuttal_concede-criticism [SEP] Thanks for pointing this out.	1.0
claims on the datasets is questionable [SEP] rebuttal_concede-criticism [SEP] We see think this phenomenon has to do with concentration of measure and typical sets, but we do not yet have a rigorous explanation.	1.0
claims on the datasets is questionable [SEP] rebuttal_concede-criticism [SEP] (a) Thanks for catching this. Indeed this was due to a bug in the toy regression experiment which we have fixed now.	1.0
claims on the datasets is questionable [SEP] rebuttal_concede-criticism [SEP] We agree that an ablation study on robustness of the embeddings across different datasets would be very interesting.	1.0
claims on the datasets is questionable [SEP] rebuttal_concede-criticism [SEP] We agree that optimizing compression rates should not use the test set before the best compression scheme is selected.	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] (1) Factorized Latent Variables:	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] [...]	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] The test set should not used before the best compression scheme is selected.	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] >> 3. One concern I have with discrete representation is how robust they are wrt different dataset.	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] >> 1.	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] * It looks like all the results are given on the test set. Did you not do any tuning on the validation data?	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] One observation from the submission is that the token set may need to be very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive [...] I think some more motivation or exploration (what kind of information did BERT learn) is needed to understand why that is the case.	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] In addition to what is explained by Maxwell, I add this clarification:	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] 4.  “Samples from a CIFAR model look nothing like SVHN. This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.”	1.0
"claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] - ""in section 4.3, there is no guarantee that the intersection between the training set and the test set is empty."""	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] (2) Multimodal Experiments:	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] Q: “ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4)”	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] Optimizing compression rates should be done on the training set with a separate development set.	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] Results are reported in Table 10 and Table 11 (Appendix C.5).	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] - permutation in the likelihood (2) does not make sense:	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] Adversarially filtering using BERT and GPT gives deep learning models a disadvantage:	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] ---------------------	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] ----------------------------------------------------------------------------------	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] Q2: Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] ------------------------------  ---------------------------	1.0
claims on the datasets is questionable [SEP] rebuttal_structuring [SEP] The accuracy results are as shown in the following table.	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_answer [SEP] This shows a clear distinction between different methods and an important result: when $\ell_2$ normalizing atoms, dictionary learning may converge to a result for which the ratio of activity between the most activated and the least activated is of the order 2.	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_answer [SEP] Although we proposed Past Decode Regularization (PDR) with language modeling in mind to exploit the symmetry between the input and output vocabulary (and the corresponding word embedding and softmax layer), any model/task that has this symmetry can potentially use a PDR term.	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_answer [SEP] In relation to section 5, we aim at seeing how much a network can remember if it is explicitly trained to remember a given set of images.	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_answer [SEP] However, SPAMS is a great inspiration for our framework.	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_answer [SEP] Our objective in this section was to provide a empirical lower-bound on the capacity by designing a setup where we can vary the quantity of information contained in a dataset (in our case, N choose n), and evaluate empirically the effect of data augmentation.	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_answer [SEP] In the present paper, we chose the “reconstructive error” as the quantification of privacy because it is the most intuitive one to measure the risk of disclosing sensitive background information in the raw data for the given perturbed data (Encoder output).	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_answer [SEP] We understand the limitations of MacKay's analysis, which was presented to give a rough theoretical comparison point to our empirical evaluation.	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_structuring [SEP] (1) Factorized Latent Variables:	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_structuring [SEP] *VAEAC trained partially:	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_structuring [SEP] (2) Comparison with VAEAC:	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_structuring [SEP] Please check Table 3 in the revision.	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_structuring [SEP] “The paper makes use of a result from the David MacKay textbook which defines the capacity of a single layer network to memorize the labelling of $n$ inputs in $d$-dimensional space. [...] It would be great if the paper also made some attempt to consider these connections. Or at least comment on how these factors could be incorporated into a more sophisticated analysis of the capacity of a network.”	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_structuring [SEP] 0.455(0.003) on Yeast; 1.312(0.021) on Glass;0.1376(0.0002) on MNIST+MNIST; 0.1198(0.0001) on MNIST+SVHN;	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_structuring [SEP] *VSAE:	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_structuring [SEP] 0.878(0.006) on Yeast; 1.846(0.037) on Glass;0.1402(0.0001) on MNIST+MNIST; 0.2126(0.0031) on MNIST+SVHN.	1.0
design choices are questionable [SEP] rebuttal_concede-criticism [SEP] >> We agree it is trivial (and indeed the proof is a one-liner).	1.0
design choices are questionable [SEP] rebuttal_concede-criticism [SEP] If the reviewers feel strongly, we can move it to appendix, however we feel it helps to provide a complete picture.	1.0
design choices are questionable [SEP] rebuttal_concede-criticism [SEP] We included it in the experiments as a naive baseline and to show that adding a single transmission layer indeed provides a significant improvement.	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] (5) Conditional imputation:	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] 1) Intuition about the improvement	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] “7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images’’ -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen’’ images which it labels as the negative class, thus the negative class is also seen by the memorization model.	1.0
"design choices are questionable [SEP] rebuttal_structuring [SEP] Remark 3. ""SST itself is only comparable with or even worse than the state-of-art methods."""	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] Q5. Evaluation on adversarial attacks.	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] (2) Conditioning on Ground-Truth Mask:	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] - Q: Illustrative experiments:	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] (2) Multimodal Experiments:	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] 2) Limitation of NADPEx	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] See Figure.7 in updated Appendix C.2.	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] Next, we address the question regarding the gradient update types.	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] Results are reported in Table 10 and Table 11 (Appendix C.5).	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] ”	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] As mentioned by the reviewer, we conduct experiments on non-MCAR masking following state-of-the-art non-MCAR model MIWAE [2].	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] R1Q2: “Though seemingly very important to the architecture, the purpose of constructing the super-graph $g^{sup}$ in the training of $C^{CAT}$ seems to be unclear to me.”	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] (3) Experiments under synthetic non-MCAR masking:	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] Additionally, we would also like to draw attention to the supergraph usage summary provided by reviewer 2 in paragraph 3 of their comments.	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] (*)	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] “The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model. ... if there were any other objectives beyond this in the experiments could you please clarify? “	1.0
design choices are questionable [SEP] rebuttal_structuring [SEP] Indeed, as can be seen in tables 1,2 and 4, SGA almost never leads to a locally optimal solution.	1.0
design choices are questionable [SEP] rebuttal_done [SEP] In the revised draft, we also consider optimization-based adaptive attacks against our method under the black-box setup (see Table 5) and the white-box setup (see Table 10).	1.0
design choices are questionable [SEP] rebuttal_done [SEP] * Used manual learning rate decay schedules for the CIFAR-10/100 baselines.	1.0
design choices are questionable [SEP] rebuttal_done [SEP] We added an illustrative example in the introduction to give an intuitive understanding of invariance, stability and their relationship.	1.0
design choices are questionable [SEP] rebuttal_done [SEP] Upon request, we have included more extensive experiments following [1] on MNIST/FashionMNIST, and [2] on CMU-MOSI/ICT-MMMO.	1.0
correctness of experiments is questionable [SEP] rebuttal_structuring [SEP] Comment: One of the problems highlighted in the paper regarding existing	1.0
correctness of experiments is questionable [SEP] rebuttal_structuring [SEP] decisions of an existing deep network (also somewhat of a black-box) which	1.0
correctness of experiments is questionable [SEP] rebuttal_structuring [SEP] explanation modalities is the use of another black-box to explain the	1.0
correctness of experiments is questionable [SEP] rebuttal_structuring [SEP] to suggest the generator in use is a good one.	1.0
correctness of experiments is questionable [SEP] rebuttal_structuring [SEP] .	1.0
correctness of experiments is questionable [SEP] rebuttal_structuring [SEP] the authors claim their model does not suffer from.	1.0
correctness of experiments is questionable [SEP] rebuttal_structuring [SEP] 3	1.0
"Incomplete ablation in terms of tables and figures [SEP] rebuttal_concede-criticism [SEP] About the significance of score differences, we agree that it needs more details and comparisons, it was also noted by ""AnonReviewer1"" and we should make alternative tests to scale or give a few more references to the benchmark."	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_concede-criticism [SEP] We apologize for the confusion caused.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_concede-criticism [SEP] (a) Thanks for catching this. Indeed this was due to a bug in the toy regression experiment which we have fixed now.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_concede-criticism [SEP] We thank the reviewer for pointing out that we should have been more thorough in explaining that while genre is a clear example of such a spurious association, it is far from the only one captured in Figure 4.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] In fact, a recent challenge held at ECCV 2018 in such a problem [2] evaluates all algorithms on both of these axes, as neither adequately captures performance.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] The small difference in results between HOF-1 in Table 1 and HOF-1 ($k=1$) in Table 4 as well as HOF-3 in Table 1 and HOF-3 ($k=1$) in Table 4 is a matter of different initialization on a later training run.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] The randomized smoothing paper reports certified radii and also accuracy (1-error) under various perturbation bounds.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] , Notably all of these features fall out from the highly-weighted features when our classifier is trained on counterfactually-augmented data.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] On the other hand, when we use a varying number of compositions (1,2,...,k) at training time, we find that the results do generalize to higher values of k. After several additional compositions (such as k+3, k+4) however, the results start worsening similar to the trend in the fixed k evaluation.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] In ImageNet, where the dimensionality is high (224X224X3) and for larger sigmas, to have a relatively dense and representative sampling, we need to sample a lot more perturbations during adversarial example crafting.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] So latents are canonicalized in both possible orderings.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] any video generator (including the one from Denton & Fergus (2018)) could be used with our losses.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] Because those missing numbers (N/A) are not reported in the corresponding literature.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] In some cases the training performance can show some oscillations.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] However this lower learning rate is not selected because it does not provide the best validation performance (this is consistent with our discussion on the step size in section 6).	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] However, the CROWN-IBP paper and the improved randomized smoothing paper based on adversarial training of smoothed classifiers (SmoothAdv) only report *error rates* using a fixed distance to the decision boundary.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] In general, we believe multi-modal data is more general than conventional image-text or video-text pairs.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] By unifying tabular data also as multi-modal (with each attribute as one modality), we show that VSAE provides us a principled way for imputation, capable of generalizing to more data families.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] During attack/crafting, we need to make an adversarial example that gets misclassified even after perturbations drawn from a Gaussian distribution centered at zero with scale sigma.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] In the original submission, we tried to produce tables that look like the tables in papers that we compare	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] Discussing a set of reference scores should also come with a better explanation of these.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] Specifically, we conducted experiments on two types of data:	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] Specifically, for Wave-U-Net, the green curve indicates the fitting result compared against the noisy target, and the red curve is the result evaluated against the clean signal.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] It's also worth noting that with a simpler feed-forward posterior and a unit Gaussian prior, our VAE ablation and SVG achieve similar performance on various metrics.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] A recent result [1] proves that there is a fundamental tradeoff between accuracy and realism, for all problems with inherent ambiguity.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] .	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] This is done because, unlike the Randomized Smoothing method, the radii are not directly calculated in the CROWN-IBP method and cannot be accessed directly;  CROWN-IBP takes a fixed radius chosen by the user, and either produces or fails to produce a certificate for that radius.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] We emphasize that this is the result of cross-validating the initial learning rate based on the validation set performance: sometimes a better behaved convergence would be obtained on the training set with a lower learning rate.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] 2. In appendix B.2 of the paper, we have added the convergence plot for all methods on the CIFAR data sets.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] Indeed, many other words, including “will”, “my”, “has”, “especially”, “life”, “works”, “both”, “it”, “its”, “lives”, “gives”, “own”, “jesus”, “cannot”, “even”, “instead”, “minutes”, “your”, “effort”, “script”, “seems”, and “something”, appear to be spuriously associated with sentiment and are captured by the original-only and revised-only classifiers as highly-weighted features.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] This results in having a sparse sample when the standard deviation is higher.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] We highly recommend listening to examples at https://anyms-sbms.github.io to feel the difference.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] Although the SVG generator is simpler than ours, ours is just a simple variation from Ebert et al. (2017).	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] Harmonic Convolution produces much better results, which is ~3.5 dB higher.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] Instead, we provide a systematic analysis of the effect of the loss function on this task (which could be applied to any generator).	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] However, in our experiments, we could only sample up to 400 instances per example (the maximum batch-size that could fit on our machine with 4 GPUs is 400).	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] Since proposing a strong generator architecture is not the goal of this paper,	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] We have presented results for addition and factorization in the main body of the paper, but refer to readers of the paper to the appendix where we have included a larger set of results.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] for both HOF models in Table 1	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] During evaluation, while the augmentations are drawn from a similar distribution, the realized random variables are not identical to those used for crafting the adversarial perturbation.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] If we keep the number of compositions fixed while training and test with a larger value of k, we observe that the performance degrades significantly.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] For Convolution and Dilated Convolution networks, they do fit faster but saturates with low-quality output.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] Shown at the top row of Figure 2b are not three consecutive frames.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] It is more stable to train, it does not require the extra ‘cost’ of an auxiliary network training and it can generalize to many-to-many transfer without requiring as many adversarial networks.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] Therefore, Wave-U-Net fits the noisy target fast but does not produce the clean version of the signal during fitting.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] We gave references to the papers that introduced such metrics.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] $k=1$	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] to	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] They are the R, G, B channels of a single frame.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] This is in contrast to randomized smoothing, which outputs different radii for different images (a larger radius means a stronger certificate).	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] One can potentially improve these results by using larger batch-sizes (i.e., sampling more) or a more powerful GPU or even a TPU, however we do not have the resources for such experiments at this time.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] We have updated these tables so that the numbers are computed from the same model, rather than separate training runs.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_answer [SEP] The naming of units as “log” “FA1”, “FA2”, etc. are meant to represent the size of the base unit that was merged to create this larger unit, “log” referring to logical units (AND, XOR, etc.), “FA1” being 1 bit full adder, “FA2” being a 2 bit full adder, etc. we have made this clear in the figure caption.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] While this is not a difference of zero, we don’t see how you could say this “clearly suggests” that the means are “very different.”  In the latest draft, this figure---now Fig 5 (a)---has an x-axis that spans from 0-255.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] We have edited the manuscript to clarify this point.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] We rewrote the caption for Fig. 4.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] We added this clarification to Section 3.4.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] To avoid confusing, we use different colors for them and explained that in the figure.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] Upon request, we have included more extensive experiments following [1] on MNIST/FashionMNIST, and [2] on CMU-MOSI/ICT-MMMO.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] We have fixed all the typos as suggested in the revised version.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] We updated Section 4.4 to indicate that it is to be expected that, although our SAVP model improves on diversity and realism, it also performs worse in accuracy compared to pure VAE models (both our own ablation and SVG).	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] Please see the (updated) last paragraph of Section 5, which explains this comparison in detail.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_done [SEP] Hopefully the overlap in the means in now conspicuous.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We see think this phenomenon has to do with concentration of measure and typical sets, but we do not yet have a rigorous explanation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] It's true that there are many activation functions that the result doesn't apply to, and in fact isn't true for.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We apologize for the lack of clarity.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We acknowledge that the text of the paper can be improved to explain better why splits with lower #rhs/lhs are generally harder than those with higher #rhs/lhs, and we thank R1 for pointing this	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] You are correct.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] out	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We agree that it is very interesting to design a model with partially-observed or even unobserved mask.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] A: It is true that CTAugment at any point in time will only perform augmentations that the model correctly predicts.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] As the author correctly states, such a distribution will not always be useful, and one can design a problem for which this distribution will lead to a poor local optimum.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] This is a very good point.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] See our response to Shengyang Sun’s comment below.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We thank the reviewer for pointing out the baseline of no message passing in GNN, which we named as ESS-BodyShare.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] (Bounds of KL divergence) Thank you for this good comment.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] (2) Thank you for this valuable suggestion!	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] .	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] 3) It is true that the extent to which randomized weights describe trained networks is unclear.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We agree that the current formulation of composition is not equivalent to a generative model.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We apologize for the confusions in Section 3.1.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We agree that it is essential to justify how the reconstruction error works as a measure of privacy in this paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] As the reviewer says, in both this work and Khadka & Tumer, the RL updates lead to policies that may differ a lot from the search distribution and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] A1: Thanks for pointing this out.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] Thanks a lot for pointing out the smoothness conditions for reparameterization; we have carefully revised our paper to remove the misleading statements and to make it clearer when our method (and also the reparameterization trick, Rep) is applicable.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We agree with the reviewer that k is a critical parameter in MAD.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] A1: Thank you for pointing this out.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] After second thoughts, this is absolutely right.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] 1. About equation 8, indeed there is a typo and should be a ""partial"" sign in front of the ""\delta"" function in the numerator."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] -Synthetic tree: contains |V| - 1 edges.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] They would have a very low weight difference score though they are ideal representations for each other	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We agree with the reviewer that images falling into Case III can be used to distinguish the associated two classifiers using the proposed semantic tree distance.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] Thanks for pointing out this.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We acknowledge this problem and agree with you about a possible misinterpretation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] A3: Yes, we agree that our original definition of alt-az convolution is not well defined on both north and south poles.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] Having said that, we acknowledge that more agility to, at least, discover that early on would be beneficial.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] As the reviewer accurately and carefully noticed, we have not formally proved that cw-distance is a true distance, and that the definition is introduced partially: first for two clouds of points, then a distribution and a cloud.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] 1. You are right that Frank-Wolfe would be advantageous over PGD when the constraints are more complicated and adversarial attack may not be such a case.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] Specifically, we agree with the 1) and 2) of your analysis.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We totally agree that in its most general form, there should be any number of observations and models should be required to generate explanatory hypotheses in natural language (as in the alpha-NLG task).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] 1. You are right.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] > You are right. For the very same reason, we take the inverse of the difference of normalized absolute classifier weights (Section 4.2).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] Very good Catch	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We agree that the coreference pooling in the graph update seems repetitive at first glance.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] More importantly, a normalization strategy on top of our attention map would help enhance its invariance properties, potentially leading to a more robust treatment of the locations of important features.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] That said, we were indeed not able to demonstrate end-to-end gains in vision.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We also agree with the reviewer that some of our statements might be too strong.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] We acknowledge that f departs from LP in a number of ways.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_concede-criticism [SEP] But we agree with you that intuitively, more diversity among agents leads to greater improvements.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] E2             E3	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] All above models have been pre-trained on ImageNet classification task first.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 6.5                               13.0	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] In fact, MAD makes the best use of model discrepancies (even if models are biased in very similar but not identical ways) to rank the model performance.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] In summary, to reliably compare the relative performance of computational models, all evaluation methodologies (including MAD) rely on the assumption that the models to be compared should be diverse to a certain extent, and the proposed MAD makes this assumption more explicit.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] En -> De	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 1:0	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] In fact, CW applies a similar binary search idea to achieve zero-confidence attack, and that is why its computation cost is high.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Note that we also fine-tune the network to make sure the features fit different lambda.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] However, we will be better off with zero-confidence attacks if we want to	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] E1	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] We present full results in Figure 4 (Appendix A), where the BLEU scores with Dual-5 model are:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Both attacks are equivalent if we only want to compute the attack success rate under a given perturbation level.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] The above discussion is not saying that it is impossible to convert PGD to a zero-confidence attack efficiently, but it at least provides a perspective on why zero-confidence attack is challenging, and why the complexity reduction as well as accuracy improvement of MarginAttack is valuable.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] We combine DSO-NAS with Deeplab v3 and search for the architecture of feature extractor with block sharing.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] It’s notable that the architecture searched on semantic segmentation task with additional operations achieve better performance in our preliminary experiment, indicating that our DSO-NAS is capable to incorporate additional operations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] .	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] But we are working on models which incorporate an unconditioned processing option (eg. training while zeroing the one-hot conditioning or adding an entry in the input embedding of FiLM which is the unconditional state) to be trained on a dataset that mixes conditional and non conditional audio (eg. adding instrument solo sections which in parts have a clear pitch track and in others none).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] If we understand your primary concern correctly, the concern comes from the bound of KL divergence in Equation 5.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 72.7                            6.7                               13.2	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Hereby, \tilde{\theta} is a noisy version of the parameters \theta.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 1) more meaningful and closer to what one would expect (Fig 4 & Fig 6 (right))	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 6.03E+00                        3.36E-01	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Walker   |	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Therefore we introduce a model that forms a Markov chain \theta -> \tilde{\theta} -> D, as shown in Figure 1a.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] The following link is a figure that explains the difference between zero-confidence attack and fix-perturbation attack.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Crucially, the data D is defined to be dependent only on the noise-corrupted version \tilde{\theta}. By choosing a convenient noise process and prior for \theta we can easily control I(\tilde{\theta}, \theta).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] However, this is hard to calculate for typical deep models.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Of course, we can also measure the margin of each example using a fix-perturbation attack, for example PGD, by binary searching over the perturbation levels.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Consider, for example, the CIFAR-10 dataset.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] We experimented with more sophisticated models, and it did not bring any improvement (see shadow models in appendix E, e.g. the performance before the softmax layer is 58.2 for Resnet101 and 60.8 for our method).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] From the current studies, we show that our algorithm is able to achieve substantial improvement with a reasonable level of diversity.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 2) better calibrated (Fig 6 (left)).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 2. We compared with various constant lambda values to see how the performance varies along with lambda.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 1. We retrained the whole pipeline with Gauss-Newton, to make sure the features are learned specifically for Gauss-Newton.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 29.52	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] In short, we verified that the results we present are valid over a various number of parameters of the network, like the learning rates (figure 2) but also sparsity and the size of the dictionary (see Response To AnonReviewer3 @ https://openreview.net/forum?id=SyMras0cFQ&noteId=BylQtQPHRX ).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 40:1	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] We evaluate the above three settings on IWSLT2014 De<->En dataset.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Then, we can conclude that Equation 2 becomes the lower bound for Equation 1.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] This gives us an upper bound on the mutual information I(D, \theta) between data and parameters, according to the DPI.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] As specified in page 2, “Here we propose a new measure ...” - our point in this regard is to propose another (different) manner via which catastrophic forgetting can be estimated, which is not the same as the classification accuracy.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 6.85E+00                       3.56E-01	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] However, the computation cost will significantly increase.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Only the encoder shares weights between main and secondary task.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 2) Probe and study the decision boundary of a classifier	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 0 :1	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] As a matter of fact, based on our experiments, we find that state-of-the-art ImageNet classifiers do have their own biases. (See figure 8 in the appendix.)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] The stethoscope module has its own trainable parameters and a separate loss function.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] We have further clarified that in Section 6.2 in the experiments by stressing that the obtained FSM results “along with the classification results” denote the significance of the whole framework in addressing catastrophic forgetting.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Since for our model, most margins fall within 10, so let’s assume the binary search range is 10 (for adversarially trained models this number will be much higher).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 29.58       29.28	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] https://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure1.pdf	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] The diversity of the above three settings would intuitively be (E2)>(E1)>(E3)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 4157.9 |   2185.1 (52.5% of NGE)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] --------------------------------------------------------	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] $$\displaystyle\sum_{\mathrm{z}}\hat{q}\mathrm{(z|c)}\ \mathrm{log}\ \frac{\hat{q}\mathrm{(z|c)}}{\hat{p}\mathrm{(c|z)}} = \displaystyle\sum_{\mathrm{z}}\hat{q}\mathrm{(z|c)}\bigg(\mathrm{log}\ \frac{\hat{q}\mathrm{(z|c)}}{\hat{p}\mathrm{(z|c)}} + \mathrm{log}\ \frac{\hat{p}\mathrm{(z)}}{\hat{p}\mathrm{(c)}} \bigg)$$	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 72.1	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] ---------------------------------------------------------------------------------------------------------------------------	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] (78.3% of NGE)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] To prove correctness of our formulation, we can rewrite the pointed term in Equation 5 by using simple bayes rule as follows:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] The goal is that (as we know and agree they are two different measures that might agree or disagree in their judgments on catastrophic forgetting) both can be used to inspect the degree of catastrophic forgetting.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] We design three group of agents with different levels of diversity: (E1) Agents with the same network structure trained by independent runs, i.e., what we use in Section 3.3; (E2) Agents with different architectures and independent runs; (E3) Homogeneous agents of different iteration, i.e., the checkpoints obtained at different (but close) iterations from the same run.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 35.56       34.97	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] We emphasize that the last line of Table 3 corresponds to the most difficult setup, where the network has been trained with a strong data-augmentation, and we only use the intermediate layers of the network (which amounts to less than 62% of the parameters for e.g. Resnet101), this is why the performance is significantly impacted.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] NGE     | ESS-BodyShare	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] As can be seen, the zero-confidence attack finds the closest point on the decision boundary; while fix perturbation-attack finds adversarial samples within a fix perturbation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] We claimed that the Equation 1 can be maximized indirectly by maximizing Equation 2 which is a lower bound of Equation 1.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] |	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Because the $\hat{p}\mathrm{(c)}$ is constant, and $\hat{p}\mathrm{(z)}$ is not included in our optimization, we just optimize $\displaystyle\sum_{\mathrm{z}}\hat{q}\mathrm{(z|c)}\mathrm{log}[\hat{q}\mathrm{(z|c)} / \hat{p}\mathrm{(z|c)}]$. Since the $\hat{q}\mathrm{(z|c)}$ and $\hat{p}\mathrm{(z|c)}$ are both normalized distributions, the $D_{KL}[\hat{q}\mathrm{(z|c)} || \hat{p}\mathrm{(z|c)}]$ is always positive.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 9.19E+00                       3.67E-01	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] DSO-NAS-seg(more operations)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] If we want to achieve a accuracy of 0.1, then we need at least 7 binary search steps.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] fish         |  70.21   |  54.97	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] In addition to our brief overview, you could also find a very neatly detailed summarization of our method in reviewer 2’s comments (paragraphs 1-4).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 20:1	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Our results (See Table 1 (bottom) in the revision) indeed show that NF can further improve to reach new state-of-the-art results.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] This is a desirable property for Riemannian vector averaging.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] De -> En	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 1) Compute the margin of each individual example; and	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] In other words, the computation complexity increases by 7 times.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 35.44	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 6.06E+00                       3.41E-01	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Ratio                   D2F (Distance to Face)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] As in Sandin, 2017 paper we have shown similar results in OMP.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] For instance, SkipThought vectors [1] use bi-GRU based encoder-decoder model to reconstruct the surrounding sentences.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Response: We are interested in limiting the mutual information I(\theta, D) between our learned parameters \theta and the dataset D.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 5.77E+00                       3.47E-01	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] DSO-NAS-cls	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] 10:1	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_summary [SEP] From the above results, we can see that diversity among agents indeed plays an important role in our method.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] (2) We use ""FCN-score"" evaluation on the cityscape dataset following [7]."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The direct interaction with 3D latent space becomes even more interesting when we pipeline our model with fast-spectrogram inversion.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The generalization is relatively straightforward and was not too surprising given the APG theory.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We use the linear layers to set the latent space dimensionality, when processing various length audio sequences, each encoding amounts to about 120ms context and we resynthesize with overlap-ad that mirrors the short-term input analysis ; this process was used when transferring on the instrument solos (a task that was beyond the training setting).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The x-axis was scaled according to the number of updates.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We propose a method that uses a form of goal-conditioned RL to learn task agnostic low-level policies that can simplify the share control structure across robots.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Intra-graph Co-ref: Inter-graph co-ref isn’t enough since the MRC module makes its span predictions independently.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We focus on confidence as a proxy to the loss, and we assume that the loss is the quantity that should be the most different between training and testing, as the optimization phase explicitly minimizes the loss on the training set.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, beta does affect the quality of the generative samples (blurriness).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Except for the learning rate, all the hyper-parameters were chosen according to the values suggested by the authors of AdaGrad, and Adam.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We then tried to weave neuromodulation and differentiable plasticity into the architecture and code base of Merity et al., ICLR 2018 (also tied for SOTA).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The hierarchically Gaussian mixture distributions are learned for different levels, and here assuming a common level proportion for all data forcefully limits the expressive power of the model.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] ’	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] .	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Therefore, the error drops exponentially with iterations t. In other words, as t —> infinity A_i —> A^*_i for i in [1,m] and x_j —> x^*_j for j in [1,m], where x_j is in R^m.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] It is indeed possible to adapt other network types to the task of predicting conditional posteriors.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We can justify PDR theoretically as an inductive bias on the language model.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In order to compare the DFW algorithm to the strongest possible baselines, we choose the baselines to use the CE loss in the CIFAR experiments.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] And for any $k$ is greater than $M c_\eta$, we have the uniform bound (w.r.t. $k$) as stated in the theorem 4.3.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This is also the reason behind the adaptive window used in this work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The reason we chose the latter approach is for modeling more flexible prior.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 2. Overall BigGAN focusses on scalability to demonstrate that one can train an impressive model for conditional generation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Instead, we feel the more appropriate baseline is performing tasks from a natural language description, as many prior zero-shot works have, and so we have moved this result to the main text, as noted above.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 8. It means it is invariant to an affine transformation of the constraint set, i.e., if we choose to re-parameterize of the constraint with some linear or affine transformation M, the original and the new optimization problem will looks the same to the Frank-Wolfe algorithm.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] But our proposal is not the topology is the new type of filters, so many topologies can be improved using this type of filters, for instance an Adaptive ResNet.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In Table 4 of the revised version (Appendix B), our method outperforms the Gauss-Newton algorithm in the last column.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] These are further updated via the intra-graph coreference step.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 4. Yes, we could just remove the distortion column in our result.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This limitation hampers learning across tasks and generalization to new unseen tasks.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, most of these solutions are designed for the single decision tree.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Regarding equation I_{DIM}, it is supposed to contain two g_{\omega} and no g_{\psi} as we use one network for encoding both the sentence and n-grams.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] k^2 is essentially the number of parameters of the model, and n is the output dimension.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The PL condition also allows us to easily prove our stochastic HGD results.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The meta-learning framework, where batches are sampled as “episodes” with N-way K-shot setting, does not perform as well in our few-shot setting on graphs for the following reasons:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Therefore, when building our supergraph, we end up with k-NN graphs of “variable size” per super-class, compared to fixed size (K nodes) k-NN graphs that we would have got using episodic learning.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Then, the parameters learned by $C^{GAT}$ get updated and further “fine-tuned” for better performance on the novel samples.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The abstracted game is generated by domain knowledge, such as clustering similar hand strength cards into the  same buckets.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Thus we can expect the gradients to be large as well.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Also note that one can strengthen the aforementioned procedure by inserting more additional continuous internal RVs into the inference model to enlarge its (marginal) description power.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] And the approximation of using mean policy is discussed in [3], with a sound deduction.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The inter-graph coreference yields new, intermediate representations for the nodes in G_t.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Simply choosing commonly recommended values for the parameters turn out to work well for the problems we looked at.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The Gaussian mixture distributions exist separately for each level, and we assume the generative process that the mixing coefficient for the level would be different for each data.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Using the partial statistical information may produce the sub-optimal split points and results in worse models.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Now, due to eq. 3.3, the two embeddings of leaf will get two different residual updates (one would be corresponding to Water and other would be because of CO_2).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Almost all of the theoretical works in nonconvex optimization and deep learning require a small step size (e.g. works of natural tangent kernel, works of showing the global convergence for a two layer neural net).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The learning rate was chosen as 1/K, with K=100 being the number of examples used to estimate the CDF.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In these situations, the MAD competition ranking, which is obtained by evaluating on corner examples searched from web-scale unlabeled dataset, is more convincing than something like 1% accuracy advantage on the validation set.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We did not follow this direction due to two motivations: first, our aim is to contribute a new quantization scheme for audio data that is trained to predict the context in a self-supervised way.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] (Schaul et al., 2015b).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The “prefixes” that our model reads at each time step comprise all sentences up to and including the current sentence s_t.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The stability bounds of B+M provide upper bounds on ||Phi(x') - Phi(x)|| (where x' is a deformation of x) based on quantities related to the corresponding diffeomorphism, i.e. the maximum norm and the maximum jacobian norm.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We don’t aim to maximize the output variation at any instant, but instead ensure that by the end of training the model is invariant to large perturbations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Therefore, we define them from either mathematical reasoning (say learning rate is non-negative, $\beta_1, \beta_2$ in Adam are between (0, 1) and close to 1) or using the calibration step, where we determine those distributions by fitting on the configurations that yielded reasonably good results.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Please consider that the data-instance we handled is a high-dimensional data of a document/an image rather than a word/a pixel.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] SO(3)  is a group which can be parametrized by a 3-sphere .	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The number of required triplets is theoretically lower bounded by nd log n, and this is also being confirmed by our experiments (our algorithm, as well as our competitors, break down when they get fewer triplets).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We use top-k accuracy, instead of perplexity, because approximating top-k is required for most inference tasks in practice (see [1]).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The distribution of f(z) does change as a function of the parameter change and thus as a function of time.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We just have the predicted outputs, e.g. cardinality, states and premutation and the order which we want to show the states will not change the value of the states.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] - As we explained in Thm.1, \eta_{\mathcal{H}} is a constant measuring how well the model family \mathcal{H} can fit the true models from both domains.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] To just give one example, LSTM cell state can increment by a fixed amount at every time step and can count the number of tokens reliably (i.e the string length) [1][4].	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Moreover, early experiments showed that using the outputs of intermediate layers provide no improvement for membership inference (on preliminary CIFAR-10 experiments, we obtained respectively 67.7 accuracy with the output layer and 66.5 when using all layers).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Gradient updates such as those of AdaGrad and Adam on the other hand will lessen the impact of such deviations as the importance of each case is inversely proportional to the number of previous observations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Therefore, in a setting with passive triplet answers, and without extra information, it is impossible to overcome this problem.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In this way, the imaginary part is zero.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We truncated the vocabulary by keeping approximately 100k words with the highest frequency and used the same validation and test sets as (Yang et al. 2017).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Hence, our assumption is fairly realistic in the neural net setting.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Nevertheless, we want to note that the step size $\eta = O(\epsilon^5)$ in our paper is of the same order as the closely related work (Daneshmand et al. 2018) of escaping saddle points.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Both minibatches contain points on the same circle and 1 point off the circle that is unique to each minibatch.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Some of these findings can be attributed to the additive property of the cell state of the LSTMs c_t = f_t c_{t-1} + h_t {c’}_{t}, which is free from matrix rotations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Here both water and CO_2 exist in the same location, leaf.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] - The discriminator uses the same amount of (s,a,s') experience as VIME consumes because the discriminator is fixed after pre-training.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Finally, we choose to always employ the multi-class hinge loss for DFW because it gives an optimal step-size in closed form for the dual, which is a key strength of the formulation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Because of the different updates, the two representations of the same entity might diverge.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] To remedy this, we re-use the coreference matrix “U” we create in eq. (2), which should already have a high attention score corresponding to the two leaf locations.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In a unidirectional language model, we cannot look into the future tokens and hence we use the output distribution as a proxy for the ""true second word"" and decode the distribution of the first word."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] For image-to-image translation tasks, we further add two quantitative measures: (1) We use the Fréchet Inception Distance (FID) [6], which measures the distance between generated images and real images to evaluate the painting to photos translation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Thus the PDR term can be thought of as biasing the language model to retain more information about the distribution of the first word given the second word in a bigram.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] While the CFS is not invariant to rotations, it is a surprising, and empirically noteworthy, finding that all 4 different ways of producing representations consistently encode a dozen tasks very succinctly.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The sampling mechanism should have a property of oversampling trajectories with larger errors/‘surprise’.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The result of this theorem holds uniformly for any $k$ (not a fixed $k$).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The outputs of modules can serve as the inputs of other modules.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] One intuitive explanation for why an algorithm that maintains a ‘memory’ of previous gradient updates like AdaGrad or Adam is required	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We study seq2seq classification tasks since they have been widely used in real world applications for RNNs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] CO_2 --> leaf	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] One of the key findings of our work is that task-specific information is captured succinctly for a majority of 13 different NLP tasks across 4 different choices of encoder architectures.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In a sense, one can see the size of the window as a proxy for the effective time horizon at which things can be seen as stationary in the learning.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 1) The VAE is hypothesized to produce blurry images when the inference/generative models are not sufficiently expressive for the data modeling task, and in particular due to the typical choice of MSE loss (i.e. Gaussian error model), thus blurring sharp edges in complex natural image data [1,2,3].	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Yet it is also well-known that Frank-Wolfe has quite different optimization behavior compared with PGD even though they have the same order of convergence rate.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] How is precision and NDCG calculated	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Intuitively, if some node i is part of a large clique, then sampling x_i=1 is likely to result in a good objective as there are many nodes that are connected to i, and the chance of not sampling any of them decreases with the clique size.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We spent a considerable amount of time trying to fulfill the reviewer’s request to match state of the art (SOTA) on PTB.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] So it is better to use circular padding in convolutional layers.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 3. The limitation of imaginary part is easy to overcome: set the resolution of F-pooling’s output to an odd number or padding it to an odd number when the resolution is an even number.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 1. Skip-Thought Vectors (https://arxiv.org/pdf/1506.06726.pdf)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] But, multiple entities can actually exist in the same location.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Second, we wanted to show that good performance can be achieved with discretized audio on actual speech tasks.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] R)It is possible to change all the layers to use Adaptive convolutions, we replaced only one to measure the unitary contribution.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In it, we pass through a linear batchnorm network 2 minibatches.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] R1A1(a): Our model has two major components, $C^{sup}$ (for super-class prediction) and $C^{GAT}$ (for graph label prediction).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In contrast, our absolute fitness naturally has this effect when paired with a non-stationary bandit.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We want to emphasize that MAD is especially useful on image classification tasks where most cutting-edge classifiers achieve very close performance.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In our preliminary experiments we evaluated alternative strategies, such as (a) only considering the current sentence s_t, and (b) considering the entire paragraph at every time step.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 3. We currently do not have a full explanation for the large adversarial resistance, but noise resistance must play a part in it.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] These experiments are thus unfortunately still running.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This LMO shares the same intuition as FGSM, which also tries to linearize the neural network loss function to find the adversarial examples.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We only take one of the terms from the full objective function and mix it with MLM.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We don’t want these representations to diverge from each other because of information propagation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Now the graph G_{t-1} might contain some location nodes which are predicted again at time step ‘t’ (e.g., in Figure 2, leaf node already existed in G_{t-1}).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, when we have access to a handful of labelled target data, we can certainly estimate this term and perform model selection (e.g., choosing neural network models \mathcal{H}) better, meaning that we can find better values for alphas, and so achieve even better adaptation performance.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Specifically, Lemma 4 establishes an upper bound for empirical Rademacher complexity of general Lipschitz loss functions (the last line in Appendix A.4).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We combine it with MLM which is designed for learning (contextual) word representations, since our overall goal is to create better representations for both the sentence and each word in the sentence.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Thus, 2 sentence input tasks and 1 sentence input tasks cannot be compared using the metric.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Any density estimation method that can approximate the trajectory density can provide a more efficient proposal distribution q(τ) than the uniform distribution p(τ).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This is why the Levenberg-Marquardt algorithm is the standard choice for conventional bundle adjustment.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] - We added a motivation for mixing two different terms in the objective function.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, often, an assumption can be made that the robots in the environment share similar morphology.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In fact, we intended to model the level proportion as shown in the third part of our generative process on page 4.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We call Eq. 3 and Eq.4 adversarial, as explained in out intuition, they need not be opposite all the time.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We choose to include it because we do not want others to think that we actually trade a lot of distortions (to make problem easy) for speedup in runtime.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] , The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Phrase similarity results: the tensor component T(v_a,v_b,.) does yield improvement over all other weighted additive methods in 5 out of 6 cases, as shown in Table 3.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] @Inverse square root: We are fully aware of the distinction.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In those cases full access to factors of variation is available as this is used to generate the data.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Still, we pursued two directions.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In the conditional case, we apply both additive and multiplicative interaction between the label and z, instead of concatenation as in BigGAN.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Instead of replacing an old node with an entirely new node at ‘t’, we take a recurrent approach and do a gated update that preserves some information stored in the node in previous time steps while adding new information unique to time step ‘t’.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In most HRL methods, the lower level can be viewed as part of the environment, yet this restructuring of the environment enables faster and more capable learning.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] @Informal main theorem: By “informal” we truly mean that we are suppressing the smoothness constants (L, M) for readability and space constraints. We are simply adopting the widespread practice of deferring the non-asymptotic mathematical statement to the appendix.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Then, during the fine-tuning phase on graphs with novel class labels, the feature extractor’s (GIN) parameters are fixed and $C^{sup}$ is used to infer the super-class label of the novel class labeled graphs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In addition, [5, 6] both apply RNNs to real-world healthcare datasets (MIMIC-III, PhysioNet, and ICU data) for mortality prediction and other multiple classifications tasks.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Clone a good initialization, and then continuously update the two neural networks using our method.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Regarding missing values:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Also, we would like to remark that this seemingly strange things is in fact the ‘artifact’ caused by the using of Langevin dynamics at beginning to obtain the $M$ initial samples when we designed the practical implementation of the proposed methods.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In our experiments, we use n=10,000 for MNIST, UbiSound and Har with batch size=128, and adopt n=20,000 for CIFAR-10 and ImageNet with batch size=256 and batch size=512, respectively.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We believe that procedure could be useful for the inference of models like the multi-layer sigmoid belief networks.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This constant then acts as a regularization parameter, just like the size of additive perturbations in the case of adversarial perturbations, and can be tuned by cross-validation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] First, it does not contain learner-subjective information, but this is partly mitigated through the joint use of with prioritised replay that over-samples high error experience.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] There are some works addressed this problem, like Hoeffding trees, which stores the statistical histograms into leaf nodes.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Moreover, a single location might have the same surface form but be from different parts of the paragraph (e.g. “leaf” in the 1st and the 5th sentence of the para in figure 2).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We want to however draw the reviewer’s attention to the ablation study and figure 5, if they were accidentally missed in the first reading.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Re: (W4) Metrics for ranking of transfer don't make sense (and some are missing).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In such framework, there are many modules, each of which may correspond to one sub-task with a global optimization goal.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] $$\lim_{k,M \to\infty, \eta \to 0^+} \mathbb{D}_{\text{BL}} (\rho_k, \rho^*)=0$$	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] There are, of course, many other ways to introduce more diversity, including using different optimization strategies, or training with different subsets as you suggested.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We saw worse approximation (the condition number dependence kicks in) and worse GPU performance (parallel computation time scales with polynomial degree).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In such spurious regimes models adapt to the specific lhs-rhs combinations from the training and can not generalize to unseen lhs-rhs combinations (i.e. generalizing from questions about “A” in relation with “B” to “A” in relation to “D” (as in #rhs/lhs=1) is more difficult than generalizing from questions about “A” in relation to “B” and “C” to the same “A” in relation to “D” (as in #rhs/lhs=2).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] See our general response for what happens when we replace zero paddings with circular padding.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This is by no means optimal, nor do we claim it is, but it seems to be a reliable enough proxy to outperform candidates that do assume stationarity (as portrayed by the comparison in Figure 16).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] There is no ‘optimal’ constant lambda for all data and iterations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] (We experimentally observe this fact: for example, rotation is initially only invariant up to +/- 13 degrees but throughout training becomes invariant to +/- 30 degrees.)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] To name a few, in speech recognition, [1] hybridizes hidden Markov model with RNNs to label unsegmented sequence data; In computer vision, [2, 3] demonstrate scene labeling with LSTM and RNNs, achieving higher accuracy than baseline methods; In healthcare, [4] proposes a model, Doctor AI, to perform multiple label prediction (one for each disease or medication category).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Our current experiments use zero paddings.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Although with the same term, the ""multi-agent"" or ""agent"" in this paper has no relationship with multi-agent reinforcement learning."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] They designed data-specific mixing coefficients of Gaussian mixture models, for improving more flexibility like ours.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Water - -> leaf	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 1) We have very limited total number of training classes (in order of 10s), when compared to the image domain (order of 100s and 1000s).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Re: (W8) The proposed  CLF weight difference method has some concerning aspects as well. For example say we had two task with exact opposite labels.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 1. BigGAN performs conditional generation, whilst we primarily focus on unconditional generation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, while they could simply leverage existing PyTorch implementations of LSTMs (written in extremely fast C++), we had to re-implement LSTMs “by hand” (i.e. as a series of connected layers) in PyTorch to introduce plasticity and neuromodulation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] For simple classes of deformations these can be computed precisely in terms of the parameters of the deformation, e.g. for translations, rotations, scaling or simple parametric warps.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] R)We started with the original topology replacing convolutional kernels by the Adaptive kernels, then we reduced kernel by kernel, retraining the model each time to match the accuracy (with small drop).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We fail to see the explicit relationship between our work and the papers you have referenced.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We found that the CDN objective leads to superior results, especially in the adversarial examples experiment.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This is in line with the findings of Dalvi et al., 2018, where the Pro-Global model (which uses prefixes) performs better than the Pro-Local model (which operates on single sentences).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] To provide more than just this quantitative insight, we’ll expand here on how KG-MRC handles coreference to better motivate the modeling choices:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Specifically [1] only proposes that there is an optimal batch size that is dependent on the momentum parameter.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] R)We tested many, and we think that we can continue reducing the model, but our purpose is not to present a topology, our purpose was to show the advantages of Adaptive convolutions, having a model 66X smaller, 2X less MAC operations and trained 2X faster give us the clue that many researchers can explore on their own topologies and get benefits of it.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Under our non-grouped data setting, for example, two following approaches are possible: 1) as the reviewer mentioned, globally define a level proportion once, take multiple level samplings for each data, and 2) as our modeling, locally define the data-specific level proportion, followed by sampling the level (this is actually auxiliary variable for specifying the Gaussian distribution).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In reality, they refer to the same location entity “leaf”.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In experiment 1.2 we use beta=1.0 which is the same as using the original VAE objective.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] [A2] Also, I would like to explain the reviewer’s comment as the formulae.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Now if we apply vanilla gradient updates this can skew the sampling distribution in random directions.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] where the number of features between the tasks are of the same size.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 1) The intuition for batchnorm can be put in a more general setting.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We have added a new figure to the Appendix to further support this intuition.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In this sense, it is a quite natural attempt to revisit FGSM under the Frank-Wolfe framework.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] If we use zero paddings as in most situations, the beneficial of F-pooling is reduced.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Moreover, the word shift in this paper means circular shift.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The permutation takes into the account when there is loss and a GT to compare as GT  annotations are permutated to be assigned to the outputs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The observed bigrams in a language are not random and the distribution of the second word given the first word in a bigram is not uniform.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, this is the case for most recent ML methods based on neural nets.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Their findings are also in sync with our empirical finding.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The Alt-az rotation, according to our definition, is not a group.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] As for the conditional independency, it is actually removed after marginalizing out additional continuous RVs (which could be non-reparameterizable RVs like Gamma).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Our vivid demonstration of this issue in the maze environment has already inspired some recent papers to look into, in particular, by incentivizing episodic reachability (in the interest of preserving anonymity, we don't include references to these, but we will include them in the final version of the paper).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We perform coreference disambiguation between location nodes of G_t and G_{t-1} via Eq. 1 (call this inter-graph coreference) and between the location nodes in the same graph Gt (call this intra-graph coreference) via Eq. 2.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] During the training phase of our classification, $C^{sup}$, which is a MLP layer, learns the super-class labels of the samples based on GIN’s extracted feature vectors (which represent base class labeled graphs).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In the appendix, we showed that the correlation between two different batches tend to a constant value independent of the input batches.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Consider for example the case when the execution is at the start, and the sampling distribution still has maximum entropy.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The construction of graph G_t from G_{t-1} uses co-reference disambiguation of nodes to prevent node duplication and to enforce temporal dependencies.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] During inference, we don’t have loss and GT.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] As such deviations will inevitably occur whenever we rely on polynomially sized samples to represent a combinatorial solution space, without such corrections a gradient based adaptive sampling algorithm will almost surely fail.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 2. The details of derivative estimation can be found in Section 3.1 (especially equation 9 and 10 in our updated version.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] with	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This is true.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] And we agree that the resulting model is highly affected by the setting of hyper-parameters n and k. In particular, we have compared the settings of k=1, k=2, k=3, and k=4 for each task and finally select the best overall value k=3.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] , Notably all of these features fall out from the highly-weighted features when our classifier is trained on counterfactually-augmented data.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] While, $C^{GAT}$ takes as input the “graph of graphs” (supergraph) which models the latent inter-class as well as intra-class information and is constructed in every training batch, along with base-class labels, to learn the associated class distribution.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We find that removing any of them leads to a decrease in performance (Rows 2, 3, 4 of Table 5).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] - Note that iterative solvers like conjugate gradient do not immediately apply here, as we are solving a linear system in M^{1/2}, not M.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The current form of our approach is not designed for learning from human demonstrations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Q2: No the mean policy is not used due to the likelihood ratio trick.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Those distributions should be prescribed by the designers of the optimizer.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Inter-graph Co-ref: One way to think about this is that we construct a new graph G_t at every time step.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 2. The complete connectivity graph for our Boltzmann machine, as presented in Fig 2, can be interpreted as having two hidden layers.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] - Yes, the numbers are from their paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Multi-agent dual learning framework can achieve better quantitative results than the baselines.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Furthermore, in [1], the authors use a similar strategy in their “baseline++” method and produce good results.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Also, for preventing the overfitting, we placed the common prior, Dirichlet(\alpha), on the data-specific level proportion, which can be considered as one of the regularization terms.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In a similar context to the one studied in the paper, such distributions have been studied by Rubinstein in his paper which discusses CE as an algorithm for combinatorial optimization, and in the classical bandit papers Exp3 is applied independently to several dimensions to study game theoretic problems.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We think that it is a gap, for which people in the community haven't have any good remedies yet.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] A: Generally, Eq.10 is an idea of behavior cloning algorithm.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We obtained a valid/test perplexity of 44.0/42.5 for the model with PDR and 44.3/43.1 for the model without PDR, showing a gain of 0.6 points in the test perplexity.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] As for the number of epoch n, it depends on the usual practices of developers for an acceptable converged result.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] is that they protect against sampling biases.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The ranked list (in the decreasing utility of transfer learning gain) is then considered the ‘gold’ set.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This is because the value of lambda should be adapted to different data and optimization iterations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The difference between TabNN (S) and TabNN (R), as shown in Table 3, implies that that the structural knowledge from GBDT yields a large contribution to the performance of TabNN.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] For example, we adopt different options of model architectures, nine weight updating schemes on when and what order to update Encoder, Decoder and Classifier,  and several settings of the important hyper-parameters (e.g., “n” and “k”) to select the empirically optimized one.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, we find circular padding slower the training speed in PyTorch.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] First, we tried to reimplement an architecture similar to  Melis et al. 2017.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, they did not publish their code, hyperparameters, or weights, requiring re-implementing and re-training from scratch.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This is because the objective function to be optimized is non-convex, and the vanilla Gauss-Newton method might get stuck at saddle point or local minimum.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] (2) The logic has been well-utilized and verified in the image-translation domain. Again please see the details in the answer to the last question.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We obtained distinct ""agents"" f_i and g_i through multiple independent runs with different random seeds and different input orders of the training samples."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In the updated version of the paper, we show that our algorithm with an oracle converges to stationary point globally with a fast rate, which provides insight into why APO works well.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In fact, we have already conducted exhaustive micro-benchmark experiments to determine the current design of RAN.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This is something that we verify empirically in our experiments at the end of training by checking the values of spectral norms as a proxy of the upper bound, and looking at the gap with the lower bound.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Furthermore, this reasoning explains why AdaGrad is superior to Adam: AdaGrad corrects against sampling biases that entail all the examples that have been encountered, while Adam does this only within some exponentially moving time window.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In general, NGE has significant improvement both quantitatively and qualitatively.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The proposition is only interesting if k^2 log(n_0)  / n <= 1/20 even without this assumption (due to the right hand side of the lower bound) therefore this assumption is not restrictive.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In the case of the SNLI data set, we allow the baseline to use either CE or SVM because using the hinge loss can increase their performance.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] For example, in some extreme cases, if a word only has a very small probability which makes it unpredictable at all (i.e. couldn’t be retrieved by top-k for any reasonably small k)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Although there are ensemble versions of them, most of them are based on bagging (like Random Forest), which is proven not as good as GBDT.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Therefore, it is interesting and important to examine the performance of Frank-Wolfe algorithm for adversarial attack, given the fact that PGD has been shown to be a very effective for adversarial attack.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In the case of cryo-EM, the high noise in the images is typically assumed to be Gaussian and therefore using the MSE loss has a denoising effect.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] By replacing the loss function in Lemma 1, we can derive generalization bounds for various tasks other than classification.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We provide a thorough empirical evaluation across critical design decisions in GANs and demonstrate that it is a robust and practically useful contribution.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] 2) In each of our batches, we randomly sample a fixed-size of training samples belonging to the set of N labels chosen.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Our aim was to show that in the case where the human-engineered topology needs to be preserved, it is better to co-evolve the attributes and controllers with NGE rather than only training the controllers (controllers are trained from scratch for both NGE and baselines).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Q3: alt-az convolution is not well defined on the south pole	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] For NN's learning, we can use Stochastic Gradient Descent (SGD) or mini-batch SGD to naturally learn from streaming data, since the NN model could be updated per data sample or per mini-batch of samples.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We agree that the full distribution of the softmax layer provides more information, but there is no straightforward way to extend the Kolmogorov-Smirnov distance to multi-dimensional distributions, beyond the two- and three-dimensional cases.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In this setting therefore, tuning the learning rate or any other hyper-parameter for that matter will compromise the validity of our results.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We envisage an optimizer not merely as update equations, but as the conjunction of the update equations, the hyperparameters, and distributions of those hyperparameters.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] For experiment 1.1, different beta produce similar disentanglement results, we use beta=20 to produce the figures as it created nicest looking samples.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Precision@K, Reciprocal Rank and NDCG are among the popular information retrieval metrics to compare a recommended list against a gold ranked list.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Similarly, the distribution of the first word given the second word will be far from uniform.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] But when we reduce one parameter from it, it is not a group anymore mathematically; the composition of two alt-az rotations becomes a general rotation in SO(3).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, we didn’t present the micro-benchmark results in this paper due to the space limit.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Our ultimate goal is to find a safer and more efficient way for on-policy exploration on physical robots.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] While we present the training algorithm naively in an online version for clearness in Algorithm 1, in practice mini-batching can be done efficiently, due to the availability of batched linear algebra operations, at least in the framework we use (PyTorch), e.g. torch.bmm, broadcasting semantics, etc.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Our reasoning is that lower #rhs/lhs are harder because the training admits more spurious solutions in them.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This is in line with some early work that observed that certain characteristic properties like length [1][2], sentiment [3], presence/absence of brackets [4] are encoded in a single dimension in the space.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] A similar reasoning applies for the k-medoids problem.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We establish bounds for classification because it is typical in learning theory and is easy to compare among existing literature.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] No matter how close two input batches are, the output batches will have the same “distance” from each other -- small movements in the input space leads to large movements in the output space.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The loss function on top of the network output is usually defined as a simple convex function; for instance, in regression, a common choice of loss function is the quadratic loss (i.e, the squared distance between the network output and the true label), which is strongly convex.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This would have been sufficient if every entity had a unique location.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In our case, a batchnorm network can be understood as a function that sends a batch of inputs to a batch of outputs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This was a drop-in replacement for the reparameterization estimator and slightly outperformed a Gumble-softmax actor.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] , it could still have a huge impact in terms of perplexity, but has a much smaller impact on top-k accuracy, which seems more reasonable given the goal of top-k retrieval.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Specifically if the current location of entity “e_t” is predicted as “\lambda_t”, the graph update steps ensures that both the entity and location representation gets the same update (via eq 3.2 and 3.3).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In environment where global information is needed (for example, walker with multiple rigid body contact), the performance is jeopardized. But in an easier environment, message passing is less needed.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Therefore, we contrasted the performance of DEBAL against the plain ensemble method and showed empirically that DEBAL gives rise to better measures of uncertainty.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] - We actually *did* try polynomial approximations to M^{-1/2} as an alternative to our proposed small-SVD step.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The very strong rejection of Gaussian noise and the observations in Fig. 4 point in this direction.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Discussing a set of reference scores should also come with a better explanation of these.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Through this process, their respective strengths, weaknesses as well as biases can be most easily revealed (see figures in the appendix).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] There are similar works, which previously published [1-3].	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, it is not really necessary to use Langevin dynamics to get $M$ initial samples, as we can simply using some other initialization distribution and get the $M$ initial samples from that distribution (and by this setting, our dynamics is simply the second phases in Eq (3)).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Our ablation experiment also demonstrates the similar advantage of using both losses for graph translation than only using L1 loss.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] That temporally consistent exploration is fairly important for physical robots is one of our motivations for this whole project.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We argue that to estimate the integral of the loss function L(τ) of the RL agent efficiently, we need to draw samples τ from the buffer in regions which have a high probability, p(τ), but also where L|(τ)| is large.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Figure 3 presents the accuracy boost using the best task till now in the produced recommendations for the candidate tasks using different methods.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] - Krylov subspace iterative solvers suffer from a condition number dependence, incurring a hard tradeoff between iteration complexity and \eps. [1]	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] As we explain in the paper, classifier weight difference metric is only applicable in cases	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We had compared the benefit brought by the 'Structural Knowledge' in the experiment.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In general, as NF is essentially a penalty term that regularizes $D$ to the zero-function, we can expect it to be effective for most dynamics [*3].	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The novel perspective in this paper is that we consider the PL condition on a different objective, namely the squared gradient norm, rather than on the game objective $g$. This perspective allows us to prove our new bounds, although we still require some nontrivial linear algebra.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] As also pointed out by Review #2, the selected top-k images provide the strongest examples to let classifiers compete with one another.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The window over which we integrate evidence is chosen to make the best recommendation; thus every time we deviate too much from the sample distribution captured within it, we consider this as a sign of non-stationarity and shrink the window.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In fact, even without assuming that the loss function is strongly convex and that the output manifold is dense, we are still able to show a fast convergence rate.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The graph has bipartite connectivity between the visible units and the first 128 hidden units and bipartite connectivity between the first 128 hidden units and the second 128 hidden units.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, it is not effective for tree-based model to support this as its learning needs the global statistical information.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, there are ways to modify our approach towards that direction: i) learning probing policy with model-based RL; ii) incorporating inductive bias from humans (e.g., the learner knows a specific set of possible goals of the demonstrator and probes the demonstrator to test which goal it has).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We note that these kind of factorized distributions have a long history of being useful in machine learning.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Re: (W1 & W2) Adversely affected by rotations	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] BigGAN splits the latent vector z and concatenates it with the label embedding, whereas we transform z using a small MLP per layer, which is arguably more powerful.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] If a function f: X -> Y tends to spread out small clusters in the input space almost evenly in the output space, then one can expect that its gradients will be large typically.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We found that operating on prefixes performed best.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] But let’s say that the MRC model picked the “leaf” span from sentence 1 (of the text in Fig 2) for “Water” and from sentence 4 for CO_2.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In addition, the hand-designed learning rate schedule of SGD and the l2 regularization were originally tuned for CE.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Since, p(τ) is a uniform distribution, i.e., the agent replays trajectories at random, we only need to draw samples which has large errors L|(τ)|.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] - That may be true to some extent.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] VIME cannot be pre-trained, otherwise, it won’t detect novel states.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] One possible explanation for this could be their use of high regularization for a single model instead of ensembling.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We uses beta=40 for all clustering experiments which had been searched from beta=\{1, 10, 20, 30, 40, 50, 60\} for the best digit identity clustering results.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] One of the drawbacks is that our method needs GPUs, while the more traditional algorithms run on CPUs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Moreover, the point that the reviewer pointed out is on \eta_{nl}, i.e., ‘the reason for designing \eta as \eta_{nl}, why \eta is data-specific variable?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In our current subjective assessment environment, we choose to stop labeling images in Case III because it is difficult for humans to select one among 200 classes, especially when they are unfamiliar with the class ontology.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Due to the combinatorial nature of the solution space, the examples that have been sampled thus far create a distorted representation of the solution space.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The result can be highly efficient, meaning the agent needs less samples than sampling from the uniform distribution p(τ).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Indeed, we should remind that our adaptive homeostasis allows to be implemented by modifying the norm of each atom of the dictionary (as was done in the original work by Olshausen).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Regarding the assumptions: The proposition uses the assumption that k^2 log(n_0)  / n <= 1/32.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Many multi-agent problems have been studies using simple robot models (point-mass, etc), where more complex and realistic models have used the problem because significantly more challenging.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Thanks to reviewer3, we incorporated this information into the revision.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Another potential mechanism by which the episodic return can be indicative of future learning is because an improved policy tends to be preceded by some higher-return episodes -- in general, there is a lag between best-seen performance and reliably reproducing it.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In those scenarios, we conjecture that we need increase k to a reasonably larger number, thus at the cost of efficiency.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] As a result, our networks thus ran considerably slower, by more than 10x (not because our method is intrinsically slower, but just for lack of engineering optimizations on our bespoke Python implementations; we confirmed this by observing that a similar “hand-built” reimplementation of simple, non-plastic LSTMs ran similarly slower, while producing results identical to Merity et al.).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The networks we used in this work are fairly simple: dense layers with standard ReLu activation, and we use standard Adam optimizer.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The operations in Eq. 2 resolve this by performing self-attention (i.e., the predicted locations of all entities are compared to each other).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] VIME can only be trained along with the policy.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We further compare our recommendations of candidate tasks generated using CFS and classifier weight difference methods against the gold ranked list.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In a nutshell, this is necessary because, after the recurrent and residual graph updates (Eqs 3.1 - 3.3) that propagate information across edges, we may end up with different representations for location nodes corresponding to the same location.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We suspect this further allows our GAT to learn and generalize better to unseen graphs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In this way, over time the probability for sampling such nodes becomes higher, and the chance of sampling all of them together increases.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In the large extensive game, the initial strategy is obtained from an abstracted game which has a manageable number of information sets.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] (Q2) Equivariance property of the Alt-az convolution	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] You are right in that the term ""agent"" in our context refers to ""mapping"" or ""network""."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The graph update step ensures information propagation between entities and location representations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We choose simple priors for their ease of estimation, though given enough computation, arbitrarily complex priors can be computed and used.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We found that VMG’s results (obtained from their original code https://github.com/AMLab-Amsterdam/SEVDL_MGP) are not as good as that for the CDN, as shown in Figure 8 in the appendix.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This is crucial: For example, if we take Adam with LR between $10^1$ and $10^5$ and claim that Adam is less tunable than others, the evaluation is inherently faulty, as it doesn't capture where the mode of the distribution of LRs for which Adam is expected to work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In Figure 6 of the revised version (Appendix B), our method also consistently performs better than different constant lambda values.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Instead, we focus on a single idea, and show that it can be applied very broadly.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Here, the constant 1/32 is not optimal.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We had similar doubts about the benefit of adding MC-Dropout to an ensemble.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] When bounding these away from zero by a certain constant, ||Phi(x') - Phi(x)|| is then included in a centered ball of the RKHS with a radius growing with this constant.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] > To compute the gold set, we first train a neural network for each of the candidate tasks and then use the pre-trained sentence encoder (part of the network) from the candidate task to fine-tune on the target task.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Note that we assume strong convexity of the loss as a function of the output units, not as a function of the weights.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] A3: As stated above in our response to Q1, we added the new results of applying negative feedback to SN-GAN, which is a state-of-the-art variant of GANs with empirically stable performance.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] NN can naturally support this, as its learning algorithm is the back-propagation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The random zero-inputs make the timing of the cues unpredictable, forcing the network to be driven specifically by the stimuli - as opposed to learning a pre-programmed strategy at each given time step.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We also point out that, as our system is complicated, in taking the limit of $M\to\infty$, we need to ensure that the number of iteration we run is larger than $Mc_\eta$. To be specific, the asymptotic convergence would be	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The reason is that the rank-based variant is more robust because it is not affected by outliers nor by density magnitudes.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Please refer to [Jaggi (2013)], [Lacoste-Julien (2016)] for more details.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] A RNN based language model models the first dependence (and more long range ones) and our proposed PDR tries to model the second one.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Thus, as a controlled experiment for this hypothesis, we first fixed the set of all hyper-parameters for all methods, and then proceeded to apply them to various problems.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] - To mitigate the influence of very unusual stochastic transitions, we use the ranking instead of the density directly.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] A:(1) The logic of using both of them has been explained in the answer to the last question.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The prior we suggested is this: \sum_{\zeta, l} nCRP(\zeta_n) * \eta_{nl} * Normal(-) ( please refer to the Figure 3(a).).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] These metrics are meaningful in our case, for instance, Reciprocal Rank tells us how many tasks we need to consider as per our recommendation before we hit the highest performing candidate task.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The motivation for this modeling choice was empirical.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] However, it is true that most commonly used weight initialization schemes are random.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] One might take inspiration from our framework try to use MAML for zero-shot task performance by transforming task representations would require adopting our meta-mapping framework, as well as a number of ideas of our architecture (where do the task representations come from, and how are they used?), and so its not clear to us that this is an appropriate baseline, rather than simply another implementation of our technique.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Note that we tuned the PDR loss coefficient very coarsely and tuning it further could lead to higher gains.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Indeed we have generally found CE to help the baselines in this setting.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We are not focusing on a setting where the source domain has no labels.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In different contexts, such distributions have also been used as naive mean field approximations in variational inference.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Perplexity captures the normalized log-likelihood of all possible words, while top-k accuracy is a better measure for inference speedup for top-k retrieval.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We also note that Deep InfoMax for learning image representations mixes multiple terms in their objective function.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In the present paper, we chose the “reconstructive error” as the quantification of privacy because it is the most intuitive one to measure the risk of disclosing sensitive background information in the raw data for the given perturbed data (Encoder output).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In particular, when using the ||f||_M penalty, lower and upper bounds seem to be controlled together in our experiments (Figure 2), making the bound useful, in contrast to PGD, for which spectral norms grow uncontrolled when the lower bound decreases.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Specifically, the proposed GT-GAN that uses both loses outperformed the S-Generator that only uses L1 loss on all three datasets by 10% in accuracy on average as shown in Table 2,3 and 4.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] where the joint limit of k and M requires that $k\eta\to\infty$; $\exp(C\alpha^{2}k\eta)\eta^{2}=o(1)$; $(k\eta)/(Mc)=q(1+o(1))$ with $q>1$. Here if $q \leq 1$, we degenerate to Langevin. But when $q>1$ (intuitively that means, when $M$ is large, the number of iterations we run is larger), our dynamics is different from Langevin, which is what we do in the practice.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Specifically,  we use a 2-layer LSTM with hidden dimension 1024 and a word embedding dimension of 1024.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] (There are, however, initialization schemes that are not random and that are not described by our theory).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We focus on improving modeling machine agents, and applying the improved agent models for multi-agent tasks.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Therefore, to train such a framework in an end-to-end way, the module should be able to propagate the errors from its outputs to its inputs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] The CDP framework finds the samples that have large errors based on the ‘surprise’ of the trajectory.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In this case we could get that some x_i=j will occur few times, while some other x_k will not receive the value j at all.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] As previously noted, this empirically also results in single cells of the LSTM being interpretable.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] One Adaptive layer can replace two traditional layers.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] For problem domains where there are few sufficiently accurate models, we may still apply the underlying principle behind MAD to create adaptive test sets such that the strengths and weaknesses of the models are most easily revealed.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] At first we validated that our models could perform well in term of training/test spectrogram reconstructions with only 3 latent dimensions, some reasons that we found interesting to enforce this are more related to a possible music/creative application of the model: less synthesis/control parameters for the user (and controls which may then be more expressive), direct visualization of the latent space which is turned into a 3D synthesis space from which users may draw and decode sound paths or create other interaction schemes, a denser latent space that may be better suited for random sampling/interpolations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] For example, He initialization [1] and Xavier initialization [2] strategies are both special cases of the setup considered here.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Furthermore, its heavy-tail property also guarantees that samples will be diverse	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Often, for grouped-data, the level proportion (or topic proportion) is modeled as a group-specific variable.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Q1: “Alt-az” rotation is not a group.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Indeed, this phenomenon is studied in detail in the AdaGrad paper (though without assuming a data distribution), and sparse data like ours (one can say our data points are N indicator vectors of length M) is the first motivating example in their paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Indeed, many other words, including “will”, “my”, “has”, “especially”, “life”, “works”, “both”, “it”, “its”, “lives”, “gives”, “own”, “jesus”, “cannot”, “even”, “instead”, “minutes”, “your”, “effort”, “script”, “seems”, and “something”, appear to be spuriously associated with sentiment and are captured by the original-only and revised-only classifiers as highly-weighted features.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This means that, at time step t, the model could predict the same span/location for multiple entities and add all these duplicates to the graph.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In fact, from our work, we found that Frank-Wolfe based methods are generally more efficient than PGD method.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] (1) If one uses the same matrix-variate normal distribution that we use for p(\theta | x) as approximate posterior p(\theta) of a BNN in conjunction with the ELBO objective, one arrives at a BNN proposed by Louizos and Welling (2016) [1], i.e. the Variational Matrix Gaussian (VMG).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In the present paper, we focus on demonstrating that high-quality posteriors can actually be learned using bi-directional training as facilitated by INNs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] When two classifiers perform at a reasonable level and achieve very close accuracy numbers (e.g., VGG16BN and ResNet34 on ImageNet validation set), MAD provides the most efficient way of differentiating the two models by maximizing their discrepancies over a large-scale image set.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] On the other hand, our analysis applies in other tasks as long as a suitable Lipschitz loss function is chosen.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] For convolution layers, we can simply use a different type of mixing distribution, e.g. a fully-factorized multivariate normal instead of matrix-variate normal.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We therefore view our theory as a theory of neural networks at initialization.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We chose the first one because it is where the feature extraction is performed, in addition fully connected layers can use this technique.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] From this perspective (and other reasons mentioned in the discussion section), MAD should be viewed as complementary to, rather than a replacement for, the conventional accuracy comparison for image classification.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This can be of disadvantage if non-machine learning experts want to use our method.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] These prescriptions are absent for the optimizers considered in the paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Thus we perform a similar operation to the intra-graph update.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We find that with this 1 line change, Dreamer solves discrete action tasks of the Atari suite and a 3D DMLab environment.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] While the circle in each minibatch will remain an ellipse as they are propagated through the network, the angle between the planes spanned by them increasingly becomes chaotic with depth.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We agree that better performance could be obtained by running the initial model for more epochs, but our goal is to stay close to the standard training of Imagenet models, i.e. 90 epochs with an initial learning rate of 0.1, divided by 10 every 30 epochs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] When we apply MAD to compare imageNet classifiers, we find that the MAD ranking stabilizes very quickly when around k>15.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] From another perspective, Frank-Wolfe solves the problem by calling Linear Minimization Oracle (LMO) over the constraint set at each iteration.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] There is a recently published survey paper that can back our claim. It is [Ali Borji, ""Pros and Cons of GAN Evaluation Measures"" (Arxiv 18)]"	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Our DIM is primarily designed to improve sentence and span representations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We tried this path, but soon realized we would not be done in time (especially with a hyperparameter search).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We gave references to the papers that introduced such metrics.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Second, the fitness is based on absolute returns not differences in returns as suggested by Equation 1; this makes no difference to the relative orderings of z (and the resulting probabilities induced by the bandit), but it has the benefit that the non-stationarity takes a different form: a difference-based metric will appear stationary if the policy performance keeps increasing at a steady rate, but such a policy must be changing significantly to achieve that progress, and therefore the selection mechanism should keep revisiting other modulations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Under the newly proposed Gaussian mixture models from the above papers, the cluster assignment of data is sampled once from the data-specific mixing coefficient, where there is no theoretical problem as a fully Bayesian formalization.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This is precisely the kind of non-stationarity that our adaptive mechanism has to deal	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] This is merely a convenient choice to make the task more challenging.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] As our stated goal is to present an algorithm which can be blindly applied with some fixed set of hyper-parameters to any possible objective, one of the goals of the experiments is to show that in such a setting some methods will work, while others will fail.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] In our experiments, we found that the disentanglement of global and local information is very robust to different values of beta.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] We would like to also emphasize that despite the small size of labeled images, MAD successfully tracks the steady progress in image classification, as verified by a reasonable Spearman rank-order correlation coefficient (SRCC) of 0.89 between the accuracy rank on ImageNet validation set and the MAD rank on our test set.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Besides, we do not require $k$ bigger than $M c_\eta$ in the definition of $\tilde{\rho}_k^M$. When $k$ is no more than $M c_\eta$, $\tilde{\rho}_k^M$ and $\rho_k^M$ are stochastic processes with same distribution and thus the Wasserstein distance between them is 0.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_answer [SEP] Estimating this term requires *labelled* target samples, which is usually unavailable in domain adaptation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] Again drawing the analogy of playing Go, the objective is mostly on training an agent that can make competitive moves rather than very fast moves, and there is no known “general-purpose” strategy to accomplish this.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] We could therefore compare to MAML for our basic-meta-learning results, but those are simply a sanity check.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] Thus we don't compare to the running time of offline solvers, but to the worst-case competitive ratio of the optimal online algorithms.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] from	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] We emphasize that we are not trying to approximate the density function, only approximate the difference and characterize its sign.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] Unlike ensemble learning, only one agent (f_0 for forward direction, or g_0 for backward direction) is used during inference.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] >> We respectfully disagree.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] 5) The 1/sqrt(2) should cancel out on both sides of the guarantee in Theorem 5.2 (and eg. in equation 68).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] In case of training, its a base-class and in the case of fine-tuning its a novel class	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] MAML as such is not a method of zero-shot task performance, it requires examples to learn	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] Just to clarify, we do compare 4 different sentence encoders [1][2][3][4] which display a fair amount of variety in ways which sentence representations can be computed.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] In the case of $C^{GAT}$, the graph is accompanied by a regular class label and in case of $C^{sup}$, the graph is accompanied by a superclass label.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] Note that this is not similar to the case of solving an offline combinatorial problem via integer programming or other solvers, since our problems are online, i.e., the instance is not known beforehand, so there is no comparison to such “general-purpose” solvers.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] Reiterating the main result of this paper: *Every* continuous equivariant function defined solely on a set of feature vectors can be approximated with PointnetST over a compact domain.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] The crux of the proposed model is the selective proposal distribution.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] .	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] Our main contribution is to propose a meta-mapping framework for zero-shot task performance, and parsimonious method for performing these meta-mappings.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] The point of this work is only to see if ML can find optimal algorithms, and not about doing it faster than the known theoretical algorithms.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] Lastly, SIF [4] is a tf-idf based weighted average of individual GloVe word representations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] Our model involves two additional pairs of agents (f_1 and g_1, f_2 and g_2) during training.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] ParaNMT [2] and InferSent [3] use different LSTM based architectures to perform back-translation and textual entailment respectively.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] The standard CycleGAN model (baseline) already leverages both primal and dual mappings, which is equivalent to our “Dual-1” model in NMT experiments, i.e., the dual method with only one pair of agents f_0 and g_0.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] We also compare to a variety of baselines, including chance and optimal performance, untransformed representations, and the most correlated task experienced (in the cards domain).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] We did provide some details on the first version (in the appendix).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_refute-question [SEP] R1A1(b): In every batch of graphs during both training and fine-tuning phase, each graph is associated with its corresponding graph label.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] Thanks for pointing this out.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] Regarding comment 2: Thanks for the comment.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] Regarding comment 1: Thanks for recognizing the merit of our idea.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] For the first and second comments, we appreciate the detailed example you proposed.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] [A1] Thank you for the very constructive comments.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] Thanks for pointing out the connection to this concurrent submission.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] Response:  Thank you.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] We hope clarifies our unintentionally imprecise original approach.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] We would appreciate further suggestions.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] We will discuss the connections in the related work section.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_social [SEP] Response: Thanks for pointing it out.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] “In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Please kindly refer to Appendix A for more detailed results.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q3: Comparison to more baseline, for example models with no message passing.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q4: Clarification of Figure-4 (Section-4.2)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 2) Hard to learn from streaming data.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Response:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 2. The contradiction between the two statements	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ====	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q: The convergence results appear to rely on strong convexity of the loss.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] The results are reported in Table 6 and Table 7 respectively.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] The main differences are as follows:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Using \tilde x in the E- and M-steps.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Specifically for ESS-BodyShare baseline:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 4) How were the number of layers and kernels chosen?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] * In-depth evaluation of MoVE and comparison of with/without conditioning:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] -- Comment on scale / speed for large instances of combinatorial optimization:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] .	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] - How to generalize to unseen items	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] .Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] =====	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] We refer the reviewer to the general response for further information.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 7. Table 4, accuracies are from Zhang et al. 2018	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] R1Q1(b): Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ** Clarification on Mathematics in Section 3.1 **	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] For your questions:	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 1. I feel the approach to implicitly assume that the classifiers to be compared are already ""reasonably accurate""; since if not, both classifiers might be easily falsified by certain trivial examples, making the ""disagreed examples"" not as meaningful. If that is true, I would suggest the authors to make this hidden assumption clearer in the paper"	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] [R1: Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] In other terms, how this proxy help to tradeoff between exploration and exploitation ?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] A: We wish to emphasize that:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] This makes a comparison with MAML even more desirable. Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] - Relationship to z-conditioning strategy in BigGAN.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Concern 2: Size of unlabelled test set, data-split information	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Architecture	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 3) How do performance and model size trade off?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] The reviewer also raises doubts about the fact that the method of Khadka & Tumer (2018) cannot be extended to use CEM.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] It is not well explained in the paper how this proxy correlates with the Learning progress criteria.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] FLOPS(B)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Let us describe these two shortages with more details here.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] We also show in the paper the application to a one-layer convolution network and our preliminary results show that we can extend this to a hierarchical network.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q2: It does not seem necessary to predict cumulative mixture policies (ASN network)?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] > in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q1. “The search space of the proposed method, such as the number of operations in the convolution block, is limited.”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q: How can the proposed method be generalized to non-image data?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Reviewer 3 points out the strong state-of-the-art performance of our approach as a strength and mentions prior knowledge (our use of RAM state information) as a minor weakness.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] The proxy seems to encourage selecting modulations that lead to generate most rewarding trajectories.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] mIOU                     Params(M)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] This is especially the case in a few places involving coreference:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] * A fully convolutional model would process arbitrary length of audio:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q3. The state vector Chi is not defined for the proposed method.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 3. One minor comment: for images in ""Case III"", the authors considered them ""contribute little to performance comparison between the two classifiers"" and therefore did not source labels for them."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 1. Proof or citation for the flaws of FID	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 2. Heavy feature engineering and ad-hoc practical steps	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] R2: ""Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method. However, as I said previously, this is probably a discussion worth having given the popularity and visibility of game-based testbeds"""	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] In terms of graph classification, the task distribution is supported on the joint distributions (G, Y).”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] a. Prior distributions of hyperparameters:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] * Not suited to transfer from audio without label:	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ""The purpose of generative models is not to interpolate per se; the interpolation is really a sanity check that the model is capturing the underlying distribution rather than just memorizing training examples."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Also, we try different ratios as we replied to R2 above.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] (1) Reconstruction from prior during training:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] > Inapplicability to discrete controls:  One restriction of re-parameterized gradients is that the technique is not applicable to discrete random variables.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] However, to state the result of Theorem 4.3, $k$ should be bigger than $M c_\eta$ from the dentition of $\tilde{\rho}_k^M$, as shown under the equation (4).	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] But I think there are lots of useful things you can do without that capability, e.g. do 3D point cloud completion, go image -> structure, etc. I think this function composition angle should be deemphasized in the title/abstract, but I think the paper stands  reasonably on its own without that."""	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] A more economical approach is to use BERT-trained model as initialization for acoustic model training, which is the classical way how RBMs pre-training were used in ASR.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D should be: D <- \theta -> \tilde{\theta} (if it is a generative model) or D -> \theta -> \tilde{\theta} (if a discriminative model).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 1) Hard to be integrated into complex end-to-end frameworks.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] “…unlike as advertised, the paper does not address	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q6.Typos:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] - Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] === Regarding to the small step size ===	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] … “	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] (4) Require mask during training:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Also, I felt that the proposed approach in Section 5 is very similar to MAML intuitively.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Please refer to Section 3.1 and Section 3.4 for more details.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q2. Relation to Tandem approach.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] a. Prior distributions of hyperparameters	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] R: - FSM vs. Classification performance	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] To give you more detail:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Question 1:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] POINTS 4 AND 5	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Re: no exploration of encoder architectures is performed	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q3: How does negative feedback (NF) influence the training of stable dynamics and further evaluation:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q: Does the hyperparameter lambda itself benefit from some scheduling?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] “A more general function is $P(X)_i=Ax_i+\sum_{j\in N(x_i,X)} B(x_j,x_i) x_j + c$, where $N(x_n,X)$ is the set of index of neighbors within the set… Then, can the function family the authors used in the paper approximate this function?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Please explain the logic for this architectural choice.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Is it worth using a neural graph?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] The function composition doesn't capture that.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ** Addressing Technical Comments:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] How is this a reasonable assumption?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] There might be other constructions that are more efficient and less restrictive.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] controlling the amount of deformations	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ** Study on diversity of agents **	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] R: - ""important features for the new task should be in similar locations ..."""	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Next, we address the question regarding the gradient update types.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Concern 1: Concerns with title “Meta Domain Adaptation”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Let’s consider this small graph below	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 6. No guarantee to work for any task or scenario.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] In the meta-learning language, the author attempts to learn a good representation of graphs	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] * Incomplete definition of the metrics:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] x is already present within the indicator, no need to add yet	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] R1Q1(a): “The classification of base class into super classes seems questionable to me.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Indeed, as can be seen in tables 1,2 and 4, SGA almost never leads to a locally optimal solution.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Instead, here you seem to suggest using L1 and GAN to do basically the same thing, or with significant overlap anyways.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 1.  “Why investigate a component specific to just flow-based models (the volume term)? It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 1. We are not training Restricted Boltzmann Machines (RBMs), but Boltzmann machines where the hidden units can be fully connected.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] b) All algorithms should optimize both G and theta for a fair comparison.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 5. Generalizability argument	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q2: a) Optimizing both the controller and the hardware has been previously studied in the literature.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q2: Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] No.“	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q4. Should the paper be called Bundle Adjustment?:	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] - ""I don't see a discussion about the downsides of the method"""	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] You can check our updated paper with clarification and new experimental results.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] This restriction should be noted in the paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] This might be because the Bayes and MAT attacks are too simplistic.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 4.  “Samples from a CIFAR model look nothing like SVHN. This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] > There is no mention about variance of policy gradient estimates.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] * Term usage of ""multi-agents"" **"	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Vacuous bounds in the regime \beta >1.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 2)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] * Insufficient justification of the 3D latent space:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Please refer to the updated PDF of the paper to see these changes.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ** Image Translation Evaluation **	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] The reviewer mentioned: “ Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax”:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Why does the graph update require coreference pooling again?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] > The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Comment: typographical errors...	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] -- “hyperparameter searching”:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] based on different graph classification tasks generated by a task distribution	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] >> 2.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] [...]	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Re. limited form of Abductive Reasoning:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] I think the authors should just acknowledge that you can't soundly *sample* from their generative model the way e.g. VAE or GAN allows (their function composition is not a sampling method).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 4) State-dependent lower bound	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] > The proposed proxy is simply the empirical episodic return.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] - predicting unordered sets	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 4. Benefits brought by the ""Structural Knowledge"""	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 2. Scalability?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] (lambda)	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] > ""The advantage of INN is not crystal clear to me versus other generative methods such as GAN and VAE."""	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ---------------------------------------------------------------------------------------------------------------------------	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] This step does indeed repeat Eq. 2.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] - Q: Illustrative experiments:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q1: the feasibility of using neural networks to learn cumulative quantities:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] “The paper positions itself generally as dealing with arbitrary transformations T, but really is about angular transformations (e.g. Definition 3.1).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Please see our high-level clarification on top.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] *	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 1. Regarding exact recovery guarantees — NOODL converges geometrically to the true factors.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Still, such approximations would make learning more challenging, especially with long-horizon backpropagation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Comment: Learning such a model of the input space is an overhead in itself.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q1. The use of the word “guarantees” is imprecise:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 4. CIDEr score of captioning	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] “The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model. ... if there were any other objectives beyond this in the experiments could you please clarify? “	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] - Can you motivate why you are not using perplexity in section 3.2?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 1) Upper bound	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ** tightness of the lower bounds	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ======	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] [Q] A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 3. Two shortages of tree-based models	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Re: ""Why were zero-sequences necessary in Experiment 1? [...] Perhaps the authors could clarify on what a confounding ""time-locked scheduling strategy"" would look like in this task?"""	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] - the dependence on \pi drops out when getting a MAP estimate of outputs:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] (Redundant weights)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] - Methods, where items have no representation, are questionable	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] How this proxy incentives the agent to explore poorly-understood regions?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 4. 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] We are not sure what you meant by “How does their ensemble method compare to just their single-agent dual method?	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q: ""...I find these assumptions too strong for the task of learning disentangled representation."""	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Second, we address the issue of using a sampling distribution that assumes independence between the different dimensions.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] Q: Third, and slightly related to the previous point, why do you need a conditional GAN discriminator, if you already model similarity by L1?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 5) Was the 5x10x20x10 topology used for MNIST the only topology tried?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 1. Scalability?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] -- “Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, i.e. problem instances are not generated by use of a machine learning model, which is one of the main claims the authors are making.”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] ""(1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective."""	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_structuring [SEP] 2. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We are currently setting up experiments for detailed analysis of the respective advantages and disadvantages and will report about these results in a future paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We agree and this was also pointed by 'AnonReviewer2', we are working on new incremental benchmarks, more detailed on both one-to-one and many-to-many models.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We will present the full experiments of semantic segmentation in the future revision.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] >	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We will revise the writing to make it more rigorous.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We will update the paper to be more explicit in explaining these considerations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We are happy to include a discussion about this in the revised paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We will also revise the title, perhaps to ``applying AGZ`` so as to make the connection to MDP more clear in our paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] However, if our paper is accepted, and you feel that the comparison to MAML for basic meta-learning is useful, we will run MAML on our tasks before the camera-ready submission.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We agree that a formal exposition introducing an NLP/deep learning audience to the basics of interventions and counterfactuals and expressing a toy DAG to explain the spurious associations between the review sentiment and the manifestation in text of other attributes of the review, including but not limited to the genre, actors, budget, etc. We are actively working on preparing this exposition and while it is not yet in the draft we plan to have it prepared in advance of the camera-ready version.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We are in the process of extending this framework to other sparse coding algorithms (LARS and lasso_lars) as plugged in from sklearn without any modification (in theory) to these algorithms.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We will revise accordingly. Instead of ``immediate justification``, we believe this work does provide a first-step, formal framework towards a better theoretical understanding.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We will update the manuscript with these additional results and discussion and post it shortly.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] The ""Structural Knowledge"" is in TabNN by default. We will clarify this in the paper."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] Hence we trained models without conditioning mechanism and, as answered to 'AnonReviewer2', we are planning experiments on models which are conditional but integrating an unconditioned state to be trained in parallel of the note-conditional state.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We will make this assumption explicit in the revised manuscript.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_by-cr [SEP] We are sorry for not stating this clearly in the theorem and we have revisited the present of the theorem. We will fix this issue in the next revision.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] -- It is true that evaluating the FSM is not necessarily the same as the classification results, which is precisely the reason why we show both in our results.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] A mask simply indicates which  modalities are observed and which are not.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Actually, Equation 7 consists of three terms.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] To the best of our knowledge, the traditional methods require re-optimizing theta from scratch for each different topology, which is computationally demanding and breaks the joint-optimization.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] So we believe it is justified to call this method feature-metric BA.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We have now made this very clear in both Remarks 3.3 and 3.5.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The window is for the purpose of forgetting gradients from the distant past, motivated by (1) our theory, (2) the small-scale synthetic experiments, and (3) the extreme ubiquity of Adam and RMSprop, which do the same.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Each 3D scene point is still constrained by a bundle of camera view rays, though the error function has been changed.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] @Full-matrix terminology: The use of “full-matrix” to distinguish from “diagonal-matrix” is standard, and taken directly from [2].	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] After weight pruning is performed and zero weights are removed, we usually obtain a sparse matrix to represent non-zero weights.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Response: Overhead calculations of some form are almost impossible to	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Although the property doesn’t hold if one performs multiple alt-az rotations to the input spherical image, it is still valuable because we assume the different SO(3) orientation of an input 3D shape is from a composite of an azimuthal rotation and an alt-az rotation, the azimuthal rotation is treated by data augmentation and the single alt-az rotation is treated by the network equivariance and invariance.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Our claim is that, all other things being equal (especially the number of parameters), a neuromodulated plastic LSTM outperformed a standard LSTM on this particular benchmark task.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Based on them, we cannot concur with the statement “if k is relatively small the method seems very sensitive to selected examples”.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] per domain.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In contrast, using discriminative conditional distributions can keep the dimension of the feature map as [W x H x dim] regardless of the number of classes.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Some audio data do not have note qualities, which are out of the current training setting.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] For instance winograd is designed to use a batch of images to convolve with a kernel, here an image convolves with a “batch of kernels”.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The purpose of the counterexample is only to show that there exists some spurious solutions to GANs with general DeepSets-style discriminator for point clouds.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] For example, if the f_\phi, the discriminator of GAN, is k-Lipschitz, the lower bound estimate should be divided by k.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] By “optimizing both G and theta”, we meant to indicate that the learned controllers can be transferred to the next generation even if the topologies are changed (instead of throwing away old controllers).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We are glad that you also agree that setting makes sense (“... the combination … is fair”).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] As opposed to task-incremental setup and shown in previous work, e.g. [3,4,5], models in class incremental setup (with single-head architecture) require a replay of previously seen categories when learning new ones.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] That is, seemingly “obvious” algorithms can fail in the absence of a rigorous mathematical proof.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] (2) our goal is not to find disentangled representations; Our goal is transferability so that we can learn on real data with minimal supervision.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] As pointed out in our work, restricting storage of real samples represents a more realistic setup, since in real-world applications such an “episodic memory” with real samples is often impossible due to memory and privacy restrictions.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We show that in spite of the assumptions, our dataset presents significant challenges for current models.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We found empirically that GAN with simple DeepSet-like discriminator most of the times fails to learn to generate point clouds even after converging, however, it does sometimes results in reasonable generations (although worse than proposed PC-GAN).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] 2. Re: iterative methods: the preconditioner is a -1/2 power of the Gram matrix, not the inverse.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Actually, if we have a large enough buffer to save all the sampled nodes, it’s easy to inference the mixture policy accordingly.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] >>> As mentioned in the main response, the proposed TPN is not a mere combination of CNN representation learning and label propagation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We also clarify that it is not the case that “we have no interests in extending ML techniques” in general.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] 1. GGT does not take the view of a low-rank *approximation*. This is a central point of the paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We want to emphasize that we do use the standard definition of mutual information.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Therefore, the bottleneck implied by Eq. 5 is purely a property of the generative model and not influenced by the approximate inference distribution q.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We respectfully disagree with the referee’s conclusions, and will elaborate on the above statements in the following.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] A problem is recommended that is within the capacity of students close to their boundary to help them learn, and at the same time recommendation is done so that all the concepts necessary for students are practiced by them.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] For example, a feature map that has shape of [W x H x dim] becomes [W x H x (dim + the number of classes)].	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] A: While ReMixMatch comprises many components (some of which are new), we believe our ablation study justifies the reason why each component exists. If there are additional ablation experiments that you think would be helpful for us to run, please let us know.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Gumbel (1954) showed that this reparametrization allows sampling from the original categorical distribution.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The purpose of the present paper is to introduce a novel technique and show that it can produce an advantage in realistic settings, which we believe our PTB task confirms.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] @Tweaks: We don’t believe that any of the tweaks should be so controversial.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We have observed interesting patterns, e.g. similar problems are more likely to be solved correctly at the same time or wrong at the same time.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This way, we have an evaluation of the performance of students on unseen problems.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] More comparisons between using RNN and DeepSets for other tasks on set data can refer to Zaheer et al., (2017).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This way, we have an idea of what problems should be recommended to them and which problems should not by having an evaluation of their ability to solve unseen problems and recommend problems in the boundary of their capacity, not way beyond, and to recommend problems in a way that covers all concepts necessary for students to learn.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The effort put for rule-based concept extractor is negligible compared to effort needed for annotation of all problems with their corresponding concepts.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The Gumbel-max reparametrization perturbs the logits of the categorical distribution with Gumbel noise after which, by means of the argmax, the highest value is selected.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] If the audio carries a note information, it can be easily/automatically extracted in the form of pitch tracks as we did for transferring on instrument solos.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] But we further improve the photometric error to featuremetric error.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We would like to ask the reviewer what is believed to be missing from this explanation on the subsampling part of our proposed method.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Besides, our method *directly* optimizes the upper bound without resorting to heuristics, unlike prior methods.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] To avoid such catastrophic forgetting, we used the neural network parameters from previous iterations as initialization, which gives an online learning/adaptation to the update.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] There are lots of existing sparse matrix computation libraries to support SpMV (sparse matrix-vector multiplication) and so on.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This requires developing (i) a precise mathematical model, (ii) a quantitative performance bound within the model.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We believe that in our case, generalization is realizable.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] A: As you say, any information nodes I_i would be sampled proportionally to \pi^{\sigma^t}_i(I_i), which is the same probability as in the definition of the mixture policy (Eq.4).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Moreover, insisting on papers to be SOTA to be accepted also likely encourages p-hacking and shoddy science to game the results (even if unintentionally), reducing the quality of science our community tries to build on.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Therefore, we just numerically did a coarse grid search and find the best mixture ratio.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] First, we want to mention that DeepTwist is proposed not only for weight pruning, but also for other compression techniques, such as quantization and low-rank approximation, as we discussed in Section 4.2 and 4.3	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] So in this extreme case, both MAD and accuracy fail to compare those two models.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] For the extreme case that two models are exactly the same (i.e, they are biased in identical ways and make identical prediction errors), both MAD and traditional accuracy-based methods will draw the same conclusion - the two models have the same performance.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In our work, function composition primarily serves the purpose of demonstrating that the model learns a meaningful subspace of objects (rather than memorizing the training set, as you mentioned).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In that case, many cutting edge ideas will by necessity be excluded from the discussion, as will many research groups.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] E.g., on ImageNet with 1000 classes, it can not rule out the case when then generator simply repeating the same image for each different class.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] A: In each iteration, only a small subset of information sets are sampled, which may lead to the neural networks forgetting values for those unobserved information sets.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Items having no representation is a caveat of the data available rather than that of the method.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] If a matrix is highly sparse, then we would reduce memory footprint and amount of computations (for example, we can skip zero weights during computation) significantly.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In the example given in the last paragraph on page 3 for example, math expressions are used to extract the concept n-choose-k.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] More importantly, our contribution is the combination of conventional multi-view geometry (i.e. joint optimization of depth and camera poses) and end-to-end deep learning (I.e. depth basis generator learning and feature learning).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Multiple recent works, e.g. [Engel et al., 2017,Delaunoy and Pollefeys, 2014], have generalized it to “photometric BA” where scene points and camera poses are optimized together by minimizing the photometric error.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] However, these differences are less likely to be revealed using a fixed and small test set (i.e., they will probably have the same accuracy numbers as models to be compared are very similar and are biased in similar ways).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We both annotated all problems manually and used rule-based concept extractor for annotation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] TabNN is a fully end-to-end learning approach with no need of an extra feature engineering step.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] There can exist multiple orders to be true, but not all.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In Table 1 and Table 2, the proposed TPN achieved much higher accuracy than the mere combination model (referred to as ""Label Propagation"")."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Note however that a global maximizer for the objective suggested by the reviewer can be easily found just by random sampling: sampling such a maximizer has the same probably as sampling an odd integer - half.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We also do think that the problem setting we have proposed is an important problem that deserves attention, and has not been studied in the meta-learning paradigm.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We also disagree with R3 that the problem is either unordered sets or there exist only one order to be correct.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Note that we do no approximation on the windowed Gram matrix, the fact that it is low rank is a feature.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] As also mentioned by Reviewer #2, the proposed MAD implicitly assumes that classifiers in the competition are reasonably accurate.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] By just having the most sophisticated concept extractor, the concept continuity cannot be retrieved.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Regarding the referee’s conclusion that the manuscript lacks comparison to the approaches of (Xie & Ermon (2019); Kool et al. (2019); Plötz & Roth (2018): We would like to point out that these three references all together put forward the Gumbel top-k method.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] So, we do not consider the argument to be unrealistic as we often observe the degeneracy.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Second, the search space without block share is even much larger than existing NAS methods.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In this paper, we have not discussed particular sparse matrix implementation methods which are not our focus in this paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] To that end, we unfold the iterations of a proximal gradient scheme (Mardani et al., NeurIPS, 2018), allowing for explicit embedding of the acquisition model (and therewith the learned sampling) in the reconstruction network.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] avoid.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Minimizing an upper bound is arguably the only viable option *with theoretical generalization guarantee*. It is a common practice in the domain adaptation community (Mansour et al., 2009a, 2009b; Ben-David et al. 2007, 2010; Cortes and Mohri, 2011), and it is essentially the key idea of PAC learning and generalization analysis (Györfi et al., 2006; Schölkopf et al., 2002; Vapnik, 2013).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Firstly, we want to clarify that our contribution is beyond improving the Gauss-Newton optimization to Levenberg-Marquardt.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Since the objective is the only source of information for such algorithms, an all-or-none kind of objective would not be very useful.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] If $\theta=0 or \theta=pi, and “\phi \neq 0$, this rotation belongs to the azimuthal rotation in SO(2) group.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] -- Continual learning typically assumes a degree of similarity among the tasks.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] By better we mean	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] However, solutions during optimization might not always correspond to such good solutions and can also correspond to the demonstrated spurious solutions.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Note that this shares similarities with the reparameterization trick used for sampling from trained gaussian distributions in a vanilla variational autoencoder.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5].	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We also hope our work can motivate the design of new tools/techniques tailored for this direction.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In particular, we use the turn-based game model to capture the self-play aspect.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] After some tuning, its test performance was still around 3% worse than the same architecture without adaptive computation time.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In the former method, we observed 100% accuracy in the similarity detection test and observed 96.88% accuracy in the latter method.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Having concept embedding, concept continuity can be reached as is discussed in the last paragraph on page 7 and some other examples are given in table 2.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Whether this is an overly large computational burdon will depend	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] As such, the augmentation boundary will grow progressively as the training process converges.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] However, we select augmentations where the probability the model output will change is less than 1.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] is not having access to samples of previous classes in the “strict” incremental setup and using generated samples instead.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] R)We believe as future work our algorithm can be combined with Winograd techniques for optimization.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Regarding the theoretical basis used for the design of the task network; we took a theoretically principled approach by exploiting a model-driven network architecture for the CIFAR10 reconstruction problem.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In such a regime, we can attribute any performance gains to the algorithms themselves, and not to any prior knowledge that is reflected by the structure of some complex sampling distribution.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] 1- There are two reasons that concept and problem embedding are performed in this work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] However, excluding the main experiment in supplementary material, we did enforce the problem to be orderless by removing O2 and the permutation loss.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The original label propagation constructs a fixed graph (Eq (1)) to explore the correlation between examples.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Over the years, this dataset has become a standard benchmark for clique finding algorithms, and results on it are regularly published.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The proposed framework is very flexible as we don’t need to enforce the problem to be necessarily orderless  (although it can be).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Considering concept continuity is an important matter in education.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This architecture has an adaptive number of “thinking” steps at every timestep dependent on the input, learnt via gradient descent.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] For example, in Appendix F (page 19, 2nd paragraph), we discussed a scenario where this intuition is incorrect under a careless measure.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Eq. 2 is only introduced to provide additional motivation for our approach as it allows to characterize overfitting in variational inference.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] If we extend to a more complex dataset such as ImageNet, it will become highly redundant.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This contribution is achieved by our differentiable LM optimization that allows end-to-end training.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In contrast, tree-based model is hard to solve these two problems due to its learning algorithm is based on global statistical information.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] That said, we still believe the results in the current paper demonstrate the benefits of our techniques on a sizable model, and thus it would benefit the community to allow people to know about, and build upon, these new methods and results.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] More specifically, we project the performance of students on problems they solved onto the problems that they have not solved.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We believe that a rigorous mathematical analysis is crucial to provide a solid foundation for understanding AGZ and similar algorithms.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In terms of the sampling distribution, as our focus is on the update step, we decided to use the simplest possible sampling distribution we can think of.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Indeed, such a result has been shown in other works, e.g. [1].	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] While the cross-entropy loss seems intuitive, using it as a quantitative performance measure requires careful thought.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] However, the main purpose of Tandem is not for handling noisy labels.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This is realized by utilizing the ""pseudo"" sampling described before (and in the paper)."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] On running the decoding LSTM for a few steps before outputting the answer: we found that it was one of the few (relatively simple) architectural changes to the standard recurrent encoder/decoder setup that significantly helped performance (thus the performance on the standard architecture can be taken to be slightly worse than the numbers reported in the paper for the architecture with “thinking steps”), but we also realize that it is not a widespread architectural change. (Possibly the need for this is less in standard machine translation tasks.) Since your review, we have also ran experiments using the published architecture introduced in “Adaptive Computation Time for Recurrent Neural Networks” (Graves).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Since only the third term is proposed additional regularization, we applied weighting parameter lambda to the third term only.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Our work tackles the problem of class incremental learning.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Dreamer uses reparamterization gradients that already have low variance (Kingma & Welling 2013, Rezende et al. 2014); although see Miller et al. (2017).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] For that we have been training unconditioned one-to-one models or solely instrument conditional many-to-many models that do not require any note information.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The most similar problem is not necessarily recommended to a student.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The exponential term stems from the layer wise covering argument rather than the range of the output.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] So the message here is that we need additional constraints for GANs with simple DeepSet-like discriminator to exclude such bad solutions and lead to a more stable training.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] (1) we only require access to meta-labels on the source set	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Our initial aim was not to compare stochastic ensembles with deterministic or single MC-dropout but to correct for the mode collapse issue in estimating posteriors with MC-dropout.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Furthermore, due to the generalization ability of the neural networks, even samples from a small number of information sets are used to update the new neural networks, we find that the newly updated neural networks can produce very good value for the cumulative regret and the strategy mixture.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Recent state-of-the art work on a relaxation of this trick, termed Gumbel-softmax sampling (Jang et al., 2017) or the concrete distribution (Maddison et al., 2016), allows us to apply this relaxed reparametrization inside a neural network as it enables gradient calculation, which is needed for error backpropagation in the training procedure of the network.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] However, the rule-based concept extractor needs much less manual effort than manual problem annotation and is capable to provide us with relatively high level of accuracy we need in our application.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Third, we can trivially extend our DSO-NAS to accommodate more operations such as dilated conv like our ongoing experiments on PASCAL VOC semantic segmentation task, we extend our search space to accommodate 3x3 and 5x5 separable convolution with dilated = 2.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Furthermore, using generated samples accommodates for better performance than simply storing instances only in case of tasks of relatively low complexity such as MNIST.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This leads to better generalization ability for test tasks.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We agree that setup we selected is destined to fail, but it was done on purpose to illustrate the presence of spurious solutions.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] 2- Prob2Vec only uses expert knowledge for rule-based concept extractor, but does not use selected informative words.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] 3. To avoid confusion of the proposed method to utilize techniques of DGR[3] in order to prevent forgetting in the G, we kindly ask the reviewer to refer to our response (2) to the Reviewer 1.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This can be inferred by learning p_m(\pi | x_i, w) from samples derived during training by Eq. 5.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] 3. We are not simply shifting the forgetting problem into G.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Second, we had already done the comparison ""against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties"" but we had initially omitted to include this supplementary data (that takes the form of a single jupyter notebook which allows to reproduce all results)."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The reason we would like to learn  p_m(\pi | x_i, w) is to infer the nature of the problem.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] NGE approximately doubles the performance of previous approach (Sims, 1994) as shown in Q1.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Therefore, TabNN is a better general solution for tabular data.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This means we cannot infer any specific ordering from GT.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This means that for a single alt-az rotation of input spherical image; the output of a convolution layer will rotate in the same way.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Note that our method is scalable as long as problems are in the same domain as the rule-based concept extractor is automated for a single domain, but for the case that problems span many different domains, it is the natural complexity of the data set that requires a more sophisticated rule-based concept extractor.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] An evaluation on real students is presented in part 2 of the comment titled “Response to questions about Prob2Vec” on this page, and we observed that similar problems are more likely to be solved correctly at the same time or wrong at the same time.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Additionally, philosophically, If SOTA results are the bar for all papers to be accepted into conferences like ICLR, then those venues will be the exclusive domain of those with either the computation or time (i.e. large-scale resources) to dedicate to such results.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Our work takes an important step in this direction by modeling AGZ’s self-play and its supervised learning algorithm accurately.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The term ‘Bundle Adjustment’ is originally used to refer to the joint optimization of 3D scene points and camera poses by minimizing the reprojection error.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Another method called reservoir sampling was used in NSFP to address a similar problem.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In our experiments, the binary mask is always fully-observed as is the nature of partially-observed data.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In the proposed work we adopt the generative replay not in order to avoid storing previous samples, but in order to prevent forgetting in the discriminator (which is used as a final classification model).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Accuracy-based evaluation methods arrive at this conclusion by comparing model predictions with ground truth labels and outputting the same accuracy numbers.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Finally, as we strive to make our assumptions hold theoretically, we agree that adding theoretical Bayesian support to our method is of great importance if we are to further improve the understanding of Bayesian deep learning.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Next, we wonder how would the reviewer correct the confounds mentioned with regard to the clique experiment.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Even if this is unconvincing, the performance gap upon removing this tweak is minor, and our empirical results hold without this tweak.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Note that by just having the concepts of problems that are not in numerical form, performance projection may not be feasible and there is a need for using other methods like embedding.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] There have been extensive studies of efficient hardware implementation after weight pruning, and we want you to refer to the paper “EIE: efficient inference engine on compressed deep neural network” or “Deep compression: compressing deep neural networks with pruning, trained quatization and Huffman coding.”	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Note that none of the tested methods were given enough samples even to recover the graph itself, as most graphs have more than 100 nodes, and we’ve allowed only for 100 |V| samples in each execution.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We borrow this idea to our method, however, the achieved mixture policy cannot converge to a low exploitability.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] However, we still require to solve Eq. 5 to find the best permutation based on f1 only, which is equivalent to use Hungarian to solve the assignments.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] As we point out in Appendix A.2, in the theory for Adam/AMSgrad [3,4], \beta_1 *degrades* the moment estimation, yet everyone uses momentum in practice.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] However, k is unknown in practice.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Please note we did not aim to devise a method that is able to reject all adversaries.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Notably, Cakewalk approaches the performance of the best clique finding algorithms that directly search a graph, and which are tailored to this specific task.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] To clarify, in our experiments, we outperform previous non-demonstration state-of-the-art approaches that use a comparable amount of prior knowledge.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Clearly, the performance of the classifier trained on the generated samples highly depends on the complexity of the task and quality of the generated samples.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In our implementation we perform a set of convolutions with the input image where FFT can be applied too.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The keyword Bundle comes from the fact that a bundle of camera view rays pass through each of the 3D scene points.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] There is no reason why those two techniques can be merged.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] By using the complete data, the setting II describes the complete ELBO corresponding to the partially observed multimodal data (in consideration).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The interpolation with SGD is just another take on this.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Instead, the objective is designed in a manner that provides information even for partial solutions, thus allowing the tested algorithms to gradually improve the objective.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Our method is along this line.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Specifically, using the result of Lemma 7, we have that eps_0 undergoes a contraction at every step, therefore, eps_t <= eps_0.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] All the algorithms are evaluated on the DIMACS clique dataset which was published as part of the second DIMACS challenge which specifically focused on combinatorial optimization.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] If tasks are completely different from each other, then most continual learning frameworks will somehow struggle.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The results are comparable but the added term in setting II shows benefits on some datasets.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We do **not** want to claim that our results are anywhere near SOTA.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Critically, these two sources of data need not be identical!	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] - The exponential smoothing of the first moment estimator is a subtler point.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] As explained in Sec. 5.2, this can be attributed to a potentially higher diversity with steady quality of the generated samples.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] 1- We briefly mentioned the way problem embedding with similarity metric is used in the recommendation system in this work, but here is more explanation on that.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] And as stated in the paper, the design of TabNN follows two principles: \emph{to explicitly leverages expressive feature combinations} and \emph{to reduce model complexity}. We cannot agree there are ad-hoc parts in the proposed model. Could you explain this with more details?	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In the MNIST experiments we already included Gumbel top-k sampling, but we will also add this for the other experiments in the revised manuscript.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We are simply offering a heuristic that we have observed to help training unconditionally, just like momentum in Adam.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In short, NN does not suffer from these two problems due to its mini-batch back-propagation learning process.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Overall, we think that we have made an important contribution to Meta-Learning literature, by identifying its limitation for few-shot learning under domain shift, and proposed a solution to tackle this problem.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The reason for using G	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Indeed, we believe that for the future success of our approach on more open problems in online algorithms, it very much relies on the advances of ML in terms of neural network structure, optimization algorithms and training techniques.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In this respect, this dataset is an important benchmark for clique algorithms very much like CIFAR10 and CIFAR100 are for image classification methods.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The assumption is what is available as GT is a set.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] On a high level, if a student performs well on problems, we assume he/she performs well on similar problems as well, so we recommend a dissimilar problem and vice versa.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] As you pointed out, our method is somewhat related to Tandem approach [1] in that both post-process a generative model on top of hidden features extracted by DNNs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We evaluated the model under two training settings: (I) optimize the final ELBO without conditional log-likelihood for unobserved modalities x_u; and (II) optimize the final ELBO with  conditional log-likelihood of unobserved modalities.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] For example, the standard Split MNIST benchmark is in line with this “locations of important features” assumption.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Furthermore, problem embedding is used by the recommender system to project the performance of students on the problems they solved onto other problems that they have not solved.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Regarding the theoretical correctness of deep probabilistic subsampling, in section 3.2 we explain how we incorporate a well-known reparametrization trick, termed the Gumbel-max trick (Gumbel,1954), to sample from a categorical probability distribution.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The search space of our method is different from exiting NAS methods in that the number of input of certain operation is not limited.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] A good generator and discriminator would definitely be a solution as well.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Furthermore, in most realistic cases for education purposes, problems span a single domain not multiple ones.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] ""Pseudo"" sampling for unobserved modalities during training provides a way to facilitate model training process."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We both use math expressions and text to label problems with appropriate concepts.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This is equivalent to assume p_m(\pi | x_i, w) is uniform (order does not matter) in Eq.2 and you can see O2 and its loss will be eliminated from Eq. 5 and 6.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We also note that the proposed reconstructive adversarial network (RAN), is not an extension of GAN but only borrows GAN’s thoughts on adversarial training several neural networks, for the data privacy-uniquely problem.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Also, training with mitosis achieves similar performance as training without it shown in Appendix B, Figure (a).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In contrast, MAD arrives at the same conclusion without any human labeling since the set S for subjective testing is empty.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Mitosis training can be considered as executing Algorithm 1 for multiple times with an increasing number of experts and inherited initialization from last round by changing W^e and W^g.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Rather, we attempted to show that a CNN with less over-generalization is able to reject some of the adversaries while correctly classifies many of the remainder, particularly non-transferable attacks.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Note, that input dependent samples are also needed in the variational training of VAEs (where the number of hidden variables is of course much smaller than the number of weight parameters in our setting).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] (1) While indeed we need more samples of weight matrices than e.g. for applying VI for BNNs for due to the input dependency, we do not believe this makes our method unscalable to real world scenarios.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Actually, the third possible solution could employ the checkpoint of each current strategy, and mixture this current strategy accordingly.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The guarantee derived in section 2.2 ties this quantity back to the mutual information from Eq. 5.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Nonetheless, for the clique problem such a distribution can be effective.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Otherwise, the selected counterexamples may be less meaningful.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] @Full-matrix vs. full-rank: Note that we do not consider the windowed Gram matrix to be an “approximation” of the “full” gram matrix.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] 1. We did stated both in the Theorem statements and Remark 3.4 that the a large batch size $B_t=T$ is used for the convergence proof.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] As long as two (or multiple) models differ (even in slightly different ways), MAD provides the highly efficient way of spotting such differences by exploring a large-scale unlabelled dataset.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This rate matches the currently best known rate of convergence for SGD (see, e.g. Ge et al., COLT'15).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Note that the use of the Gumbel top-k method for compressive sampling is also new, and in fact constitutes a specific case (constrained version with shared weights across distributions) of the proposed deep probabilistic subsampling (DPS) framework.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] While in our work, we adaptively construct the graph structure for each episode (training task) with a learnable graph construction module (Figure 4, Appendix A).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We use the word ""loss of diversity"" since IS's measuring of diversity is limited."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] - The \eps parameters are present in *every* adaptive optimizer, for stability.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We have empirically shown that adding ensembles to this, greatly improves the MC-dropout technique and outperforms the deterministic ensembles as well.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] 3. The 20:1 mixture used in practice do not directly correspond to s in theory, because the distances we compute are not scaled.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] on the problem, although we believe the GAN or VAE need only be trained once	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] For our analysis we fix eps_t = O^*(1/log(n)), which follows from the assumption on eps_0= O^*(1/log(n)) and Lemma 7.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The main research question we try to address is whether algorithms that only rely on function evaluations can recover locally optimal solutions.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] First, we’d like to emphasize that the clique problem studied in the paper is far from being a toy problem.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The math expressions are not ignored in our proposed Prob2Vec method.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Thus, this effect can be observed neither in the SVHN nor the CIFAR10 benchmarks.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Learning baselines for variance reduction is common for Reinforce estimators as used in A3C and PPO (Mnih et al. 2016, Schulman et al. 2017) but not for reparameterization estimators as used in Dreamer, SVG, and SAC (Heess et al. 2015, Haarnoja et al. 2018).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] A1: First, the size of search space is not determined by the number of operations but the number of connections.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Other architectures like RNN might work, but they are not permutation invariant, which is a desirable property for set data like point clouds.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The simplifying assumptions, mentioned in the paper, allow us to i) formulate the tasks concretely and ii) curate the dataset and evaluate models viably.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] 2. On eps_t and A.4. —  Indeed, we don’t need to assume that eps_t is bounded.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We develop a quantitative bound in terms of cross-entropy loss in supervised learning, which is the “metric” of choice in AGZ.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Graph neural network formulation is KEY here, enabling it to perform this efficient policy transfer.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] More specifically we investigated the use of this for both the recurrent encoder and decoder (replacing the single fixed number of “thinking” steps at the start of the decoder).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Furthermore, if we consider fully-convolutional architecture (without fully-connected layers), redundancy becomes a serious problem.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In addition, we added a thorough comparison of the DPS to LOUPE (Bahadir et al, 2019), a recently proposed data-driven method for subsampling.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] In particular, the Tandem approaches utilize the EM algorithm that should be highly influenced by outliers, while our method is specialized to be robust against them.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] The network can be trained with extra batches of triplets which involves the new items.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Moreover, the quotient SO(3)/SO(2) is isomorphic to $S^2$ and to avoid the ill-definition on the two poles (the two degenerate points), we will add a constraint to the alt-az rotation, i.e. $\phi=0, if \theta=0 or \theta=\pi$. This is because, when the altitude rotation is zero or PI, the azimuth rotation is meaningless in a alt-az rotation and is therefore fixed as zero.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] This means the effective rate of convergence is $O(1/\sqrt{T})$ as pointed out by the reviewer.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] We note that only NGE among all the baselines has the ability to do that.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Even though we have not yet proved the above, we have empirically showed that the benefit of DEBAL over plain ensemble methods consists of a better representation of uncertainty, that is paramount in active learning.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] @Update overhead: We argue that per-iteration performance is a worthwhile objective in itself, which is less significant in some scenarios (e.g. costly function evaluation, like in RL, or expensive backprops, like in RNNs).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] While setting I is solely based on the observed modalities, the setting II incorporates the unobserved modalities along with the observed ones.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_reject-criticism [SEP] Providing a controlled experiment is always challenging, though the elements mentioned by the reviewer were specifically selected as to reduce various confounds.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] Our main idea is to show as many configurations as possible to the learner by learning a good probing policy.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] While using a schedule for lambda can potentially further improve performance, a simple grid search over fixed lambda values already leads to strong performance, and has the advantage that it is easy to use in practice.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] The maximization of the derivatives in the objective directly implies increased sensitivity to perturbations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] We think we can still have the equivariance property but only for single alt-az rotation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] For your comments wrt discrete random variables (RVs), unfortunately, we haven’t found a principled way to back-propagate gradient through discrete internal RVs (like in multi-layer sigmoid belief networks).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] To obtain a better generalization, we may need to use a better imitation learning approach to replace the current one (behavioral cloning), and possibly using multiple starting configurations.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] But we think that it is somewhat orthogonal to our main contribution.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] First, there is no single standard definition of data privacy-preserving problems and corresponding adversary attacks.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] All of these can potentially bring further improvements to our framework, yet are not the focus of this work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] Our point is made in the context of volume term which is only one of the terms in the change-of-variable objective.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] To evaluate RAN, one has to pick some quantifications.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] But if the RL actor shows good enough performance, this does not prevent from computing a new covariance matrix which includes it.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] However, we solve Equation (2) with this proxy algorithm for two reasons.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] Since the probing always starts from a single setting, there is indeed a limit in terms of how different the new settings could be.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] In general, we agree with the reviewer’s point that hyperparameter searching can be important.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] First, using the full \tilde x matrix allows to factor the computation of the pseudo-inverse of \tilde x and thus allows for a much faster algorithm, see answer to Reviewer 2 and the details of the M-step in the paper (as well as footnote 2).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] The log volume term in the change-of-variable objective is maximizing the very quantity (the Jacobian’s diagonal terms) that the cited work on derivative-based regularization penalties has sought to minimize.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] Our key contribution in this paper is the RAN framework and the training algorithm, which can accommodate different choices of privacy attackers and privacy quantification.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] We agree with Reviewer 3 that “the error arising from quantizing v into c is only affected by a subset of rows of \tilde x”.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] > We are not sure if we understand this completely.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] Second, early (and slow) experiments suggested that the gains were not significant when using the right subsets of \tilde x in this particular context.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] While we agree that the tasks in this paper are not real world problems, we think, as a first step towards this direction, the evaluations in this paper have provided some promising proof-of-concept results. Applying the approach to more realistic and more complex tasks could be a good future research direction.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] As pointed out by the reviewer and is true for many machine learning method, there is no guarantee that the proposed method will work for any task or scenario.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] Notice the definition of alt-az convolution do not use any composite rotation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] We are not saying that the model will totally disregard the latent density and attempt to scale the input to very large or infinite values.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] In our updated paper we show that APO with a fixed lambda achieves comparable performance to manual learning rate decay schedules.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] The corresponding ellipsoid in the search space may be very large, leading to a widespread next generation, but the process should tend to converge again towards a population of actors where evolutionary and RL actors are closer to each other.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] And a fundamental problem in it is the natural tradeoff between privacy and utility, which is affected by different data privacy-preserving methods.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] E.g., in Maze Navigation, it is impossible for the learner to change the room layout drastically in the time limit, so the learned policy won’t make sense in a very different room layout (e.g., 8 rooms instead of 4 rooms).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] Second, finding the right measurement for privacy is an open problem in itself.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_mitigate-criticism [SEP] The objective of our approach is to discover more diverse settings/configurations and consequently improve whatever imitation learning approach we actually use.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] In this paper we have made the approximation that the state distribution is stationary and the discount is large enough to assume that the value is more or less constant.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] A theoretical analysis will be an interesting future work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] In the new revision, we will use the term alt-az rotation in “quotient SO(3)/SO(2)”  instead of alt-az rotation group.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] While this holds for locomotion tasks, this does not apply in e.g. the swingup phase of the cartpole task and as a result the penalty is completely ignored during this phase.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] In our experiments, we were able to achieve resolutions up to the ground truth resolution or matching published structures with our architecture and training settings, though we agree with the reviewer that exploring alternative generative models is a promising future direction.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] We are definitely interested in studying how our approach can be applied to more complex tasks as future work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] However, it is beyond the scope of this work and we will consider it in future work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] Defining a state-dependent bound is indeed not trivial and requires knowledge of what is feasible in the system, and as such we leave this up to future work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] A result of these second thoughts is that one could definitely build an ERL algorithm where the evolutionary part is replaced by CEM.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] Third, in the future, we will evaluate RAN using other quantifications of privacy as well in a definitely defined application.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] In the next step we will look for simulator environments with more authentic actuators to see how NADPEx could help solve that.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] Finally, as for the further evaluation (such as multiple seed runs and 2nd-momentum estimates), we agree it is interesting, but it is very demanding in computational resources, and we leave it for a systematic future investigation.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] We hope our work will lead to this future line of research.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] For example, we could measure the privacy by the hidden failure, i.e., the ratio between the background patterns that were discovered based on RAN’s Encoder output, and the sensitive patterns founded from the raw data, in the object recognition application.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] It is definitely a good idea to analyse the correlation between changes in classification accuracy and in FSM values, thank you. We will rigorously investigate this in future work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] We leave more comprehensive studies on diversity to future work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] This seems to be a good direction for future work, but we also think that the current research has provided promising results in simpler domains, and hopefully incites more research where human demonstrators are also involved by introducing this problem to the community.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] Yes, this is an interesting avenue for future work!	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_future [SEP] For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We thank the referee for bringing the article [V] to our attention and we now have acknowledged the prior work properly in our introduction.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] Secondly, we agree with the reviewer that comparing with the Gauss-Newton algorithm will be interesting and have updated such a comparison in Appendix B in the revised version according to the reviewer’s suggestions:	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] But in the revised version we have recapped the definition of Chi when introducing our method at the beginning of Section 4.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] R)     A new experiment added to the paper shows the accuracy degradation vs model compression	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have added this clarification in Section 1.1.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have fixed the typos.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] These include updating the draft to include (i) a detailed analysis of edits performed on SNLI, (ii) results on various datasets using an ELMo based classifier; (iii) concerning your question about larger Bi-LSTMs, we had tried a large Bi-LSTM but it overfit badly.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] (refer to section 3.3 in the revised paper.)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We detail this in Appendix F.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have also added the comment at the beginning of Section 3 of the paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We also added Table 4, which repeats the phrase-similarity task for verb-object pairs, and shows that the tensor component leads to improvement in most cases.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We added an illustrative example in the introduction to give an intuitive understanding of invariance, stability and their relationship.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have added a respective proof in Appendix, Section A, where also the precise mathematical construction of the general form of Cramer-Wold metric is presented.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We corrected this mistake.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We discuss our usage of prior knowledge in greater detail in the section titled “Prior Knowledge” in our response to Reviewer 2.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have reorganized this section, as shown in our updated paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We applied Dreamer to environments with discrete actions using the DiCE estimator (Foerster et al. 2018) locally for the da/dμ and da/dσ derivatives.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] it should be (N)x(N) instead of (N+1)x(N+1). (Fixed on the paper)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] This is further discussed in the newly added section 6.4.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] In the revision, we have added more explanations on the selection of n and k in Section 2.4.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] Thank you for this suggestion, we have now clarified this connection in Section 3.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We corrected the paper according to this new insight.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] (b) We have revised the baselines so that they either use \lambda = 1 or the settings that the original authors recommended.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] 10. Thank you for pointing these typos out, we have addressed it in the revision.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] Good point! We have updated this in the revised document, and we think it enhanced clarity.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] (Added to the paper)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] Therefore, in the new revision, we will add the constraints to the definition of alt-az rotation and make it one-to-one corresponds to the set points on $S^2$. See A1.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We also included the suggested related work (Balduzzi et al. 2018) in Section 5.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] In fact, we have mostly changed the name from “Meta Domain Adaptation” to “Meta Learning with Domain Adaptation”, and the rest of the paper is almost identical, which we believe addresses the concerns of false advertising.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] On reviewer’s comments, we have updated A.4., and moved the note about eps_t = O^*(1/log(n)) to the Appendix A.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have now included it in an anonymized format.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] (we give related discussion in section 3.1 and add much more experimental results in Figure 5, further details please see the revised paper.)	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] The following table shows the performance of our model on the PASCAL VOC 2012 semantic segmentation task, where DSO-NAS-cls represents the architecture searched on ImageNet with block structure sharing and DSO-NAS-seg represents the architecture searched on PASCAL VOC segmentation task.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have also updated that table with additional results, which show that adding in the tensor component improves upon the strong baseline of the SIF embedding method.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We updated section 2.3 to reflect this more clearly.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] In retrospect, we agree that this was confusing and have removed the [x] notation from the indicator function.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We revised the x-axis from “generations” to parameter “updates” in the latest revision.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] In the latest revision, we also included the curve where the topologies are allowed to be changed, which leads to better performance, but does not necessarily preserve the initial structure.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have updated the draft to include this detail.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] - We added a mathematical justification paragraph in Section 3.3 “An Importance Sampling Perspective”.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] Furthermore, we present an experimental investigation of these different objectives (Sec. 6.4).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We clarified this in Section 2 of the revised draft.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have fixed all the typos as suggested in the revised version.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] In the revision, we have added the following justification on privacy quantification in Section 2, Section 4 and Section 5.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] However, as stated in the last paragraph of Sec. 7.2, we presented in Appendix B.4 a procedure to assist our methods in handling discrete internal RVs.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have edited the corresponding sentence to make it less assertive.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] The Chi is defined in Section 3 as the vector containing all camera poses and point depths.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] Since our method also solves for these unknowns as in classic methods, we did not redefine the Chi.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have revised the abstract to clarify this point.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] 2. Following your suggestions, we add a study on the diversity of agents (presented in Appendix A of the updated paper).	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have added a new section (Sec. 4) to discuss the differences between the objective used for CDN, when performing variational inference for BNNs, and in the variational information bottleneck (VIB) framework.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] Based on your comments, we’ve performed additional ablations to measure the impact of the co-reference mechanisms.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] k-addition definiteness in the spherical setting: we have added the formal condition that the k-addition be well-defined, and a proof that for two points this condition indeed recovers x != y / (k ||y||^2) - see Theorem 1.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have adjusted the word.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We will further clarify this in the paper.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] In page 4 in the revised version (footnote 3), we have clarified this and notified its potential for future work.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We’ve updated the paper to mention this.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] We have further clarified the explanation given in the text and included another ablation experiment  (row 4 of Table 5) to confirm its usefulness.	1.0
"Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] Proofs that the space ""interpolates smoothly with curvature"": we added formal proofs (see theorems 2 and 3) that all the operations are differentiable, i.e. the gradients are equal from both the left and right at 0, w.r.t. curvature, for the chosen models of hyperbolic and spherical geometry."	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_done [SEP] 8. Thank you for pointing out several typos. We have fixed all of them in the revision.	1.0
Too strong assumptions in analysis [SEP] rebuttal_done [SEP] We show that our method outperforms the baselines across multiple environments.	1.0
Too strong assumptions in analysis [SEP] rebuttal_done [SEP] In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.	1.0
Too strong assumptions in analysis [SEP] rebuttal_done [SEP] Theoretically, to address this comment, we added Theorem 1 (See in the Appendix D) that states the dynamics of $D$ with Lipschitz constraint follows Eqn. (10) *around the equilibrium*. Therefore, the stability analysis and our proposed method in Sec. 4 still applies to vanilla WGAN because control theory mainly focuses on the stability *around the equilibrium* [*2].	1.0
Too strong assumptions in analysis [SEP] rebuttal_done [SEP] Specifically, we apply NF-GAN to SN-GAN [*5] and NF-GAN provides a significant improvement on the state-of-the-art inception score on CIFAR-10 (from 8.22 to 8.45).	1.0
Too strong assumptions in analysis [SEP] rebuttal_done [SEP] Finally, we added new results in Table 1 in the revision, which shows that the same technique of negative feedback can further improve the state-of-the-art method of SN-GAN [*5].	1.0
Too strong assumptions in analysis [SEP] rebuttal_done [SEP] -> For more on this we refer to the newly added Section “Scope” in the revision.	1.0
Too strong assumptions in analysis [SEP] rebuttal_done [SEP] See Figures 2,3,5 for more learning curve results and baseline comparisons and Figure 6 for qualitative metric analysis.	1.0
Missing details for reproducibility of result [SEP] rebuttal_structuring [SEP] We have added an explanation of what we mean by “learning dynamics of deep learning” in the last paragraph of the first page.	1.0
Missing details for reproducibility of result [SEP] rebuttal_structuring [SEP] 1. How or why is the benefit.	1.0
Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc) [SEP] rebuttal_concede-criticism [SEP] Thanks for pointing out the issues with our presentations.	1.0
Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc) [SEP] rebuttal_concede-criticism [SEP] We included embedding details in the paper because a reviewer from the venue we previously submitted to requested these details to be in the paper.	1.0
Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc) [SEP] rebuttal_done [SEP] In addition, we added more details about the data as you suggested.	1.0
Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc) [SEP] rebuttal_done [SEP] We revised the notations in the paper to make formulation clearer.	1.0
Lacking details on datasets [SEP] rebuttal_concede-criticism [SEP] Thanks for pointing out the issues with our presentations.	1.0
Lacking details on datasets [SEP] rebuttal_concede-criticism [SEP] While we fully agree that more complex datasets with more complex questions would bring new challenges, these are ones we purposely put aside (such as the general unavailability of ground-truth layouts for vanilla NMN, the need to consider an exponentially large set of possible layouts for Stochastic N2NMN, etc.) We believe that it is highly valuable for the research community to know what happens in the simple ideal case of SQOOP, where we can precisely test our specific generalization criterion.	1.0
Lacking details on datasets [SEP] rebuttal_concede-criticism [SEP] We included embedding details in the paper because a reviewer from the venue we previously submitted to requested these details to be in the paper.	1.0
Lacking details on datasets [SEP] rebuttal_reject-request [SEP] Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.	1.0
Lacking details on datasets [SEP] rebuttal_done [SEP] We also include further details on the construction of training set.	1.0
Lacking details on datasets [SEP] rebuttal_done [SEP] Following your suggestion, we further studied idf scoring.	1.0
Lacking details on datasets [SEP] rebuttal_done [SEP] We have added a pseudo-code description of TTS-GAN training algorithm to the updated submission.	1.0
Lacking details on datasets [SEP] rebuttal_done [SEP] In addition, we added more details about the data as you suggested.	1.0
Lacking details on datasets [SEP] rebuttal_done [SEP] We computed idf scores on the monolingual English corpus released by WMT18 and experimented with BERTScore computed with the Roberta-large model.	1.0
Lacking details on datasets [SEP] rebuttal_done [SEP] We revised the notations in the paper to make formulation clearer.	1.0
Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification. [SEP] rebuttal_done [SEP] Our experimental results and statements presented in the manuscript are fully reproducible and verifiable.	1.0
Missing implementation details of related work used as baselines [SEP] rebuttal_done [SEP] In addition, we have included a brief description of the graph neural network architecture used in Paliwal et al (2019).	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_structuring [SEP] “The hyperparameter selection regime (and the experiments used to find them) is not described”	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_structuring [SEP] Q4: Can the authors show concrete examples on how the attacks are generated? The details are especially unclear on LOAN.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_structuring [SEP] 3) Relation to safe reinforcement learning	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_structuring [SEP] 3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_structuring [SEP] Q1: In appendix A.2 the authors explain how the range of is set for the different experiments. However it's not clear how is this range used in practice ? Do you sample uniformly in this range to train the linear interpolation ?	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_structuring [SEP] Q2. Whole sequence reconstruction results:	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_structuring [SEP] * All claims about running time should be corroborated by controlled experiments:	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_structuring [SEP] - How many images did you have in the experiment?	1.0
Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc) [SEP] rebuttal_done [SEP] We have added a note to that effect to the article.	1.0
Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc) [SEP] rebuttal_done [SEP] We have modified the caption of Fig. 2 to be more specific.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] Re: not clear if the classifier weight difference is well defined	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] We agree much detail on embeddings can be condensed or moved to Appendix.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] We included embedding details in the paper because a reviewer from the venue we previously submitted to requested these details to be in the paper.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] (7) Sorry, for this unfortunate formulation!	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] Thanks for pointing out the issues with our presentations.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] We agree that our main contribution is to cast a graph convolution network as a binary classifier learning to discriminate clean from noisy data and show its excellent results for few-shot learning.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] > You are right in noting that the classifier weights might capture dissimilar yet useful features for two similar tasks, and hence the classifier weight difference might under-predict the transfer potential.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] Sorry for the possible confusion.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] Although most papers in this field only report results on the trimmed video datasets, we do agree that more complicate cases should be tested.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] > Sorry about the lack of clarity!	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] Thanks for the suggestion.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] That said, we were indeed not able to demonstrate end-to-end gains in vision.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] A1: Thanks for pointing this out.	1.0
"Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_concede-criticism [SEP] But you are right, we could easily extend our approach to account for more flexible distributions by using a ""diagonal plus rank-one"" structure diag(a)+uu^T, with vectors a and u, as noted by Louizos and Welling ( 2016) [3] (the increase of parameters is negligible: adding additional vector u)."	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] (c) From the result in (b) we conclude that the particular manner of the pre-training does not matter.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] In this paper, we showed that discriminative regularization could make VAE possible to conduct both class conditional generation and classification with one integrated model.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Basically a baseline is subtracted, and GAE is introduced.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] # For each class name	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] X_L : clean set	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] A^c = build_graph(X_Z^c)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] #Take the clean examples belonging to this class	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] X_Z : noisy set	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] #Clean examples always get weight 1	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] #Noisy examples get the learned weight	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] #Only consider noisy examples with the class name in the text	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Use the relevance weights for noisy examples when learning the classifier	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Then, when new task 2 is coming, DiVA generates images and its labels of task 1 and learns both task 2 and the generated task 1 simultaneously.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Thus, we do not need to train an additional classifier, e.g., deep CNN, which is necessary for other works, including Narayanaswamy et al. There is also classifier integrated VAE such as [6].	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] prediction = argmax(scores)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Generally, CL systems assume that each task comes sequentially, and an agent can not directly access previous experience [2].	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Firstly, we train DiVA with task 1 that consists of real images and labels.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Therefore also the choice of first training set (98% coverage or full coverage) does not influence adversarial resistance.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Also, we train DiVA sequentially for each task with one same model.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] X_L^c = concatenate(X_L^c, X_z^c)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] # Build the graph for this class, and learn the GCN for cleaning	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Given test image Q	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] C_L : class set	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] (Use of labels and novelty) In GR-based approaches, the quality of generated samples is crucial to keep the performance of previous tasks.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] r_i = 1.0	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Ignoring the stopwords can help us get rid of the disturbance of unnecessary high-frequency words (such as function words) being the topic, as the standard practice for TF-IDF topic extraction.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] v = extract_feature(Q)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Generally, conditioning on a generative model yields higher quality samples than unconditional one and makes it possible to generate class-balanced samples [4]; the importance of conditional generation is also described in section 6.1 in our paper.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Because a deep neural network is vulnerable to even single-pixel perturbation [5], the difference can seriously affect the classification performance of GR-based algorithms.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Given a partial tree, there can be more than one way to complete the layout.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Given a 10%, 50% and 80% DFS partial layout, the mean number of completions is 3.63, 1.24, and 1.17 respectively.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] We suspect that it is this unusual aspect of the CNE formulation that makes it original with respect to the state-of-the-art.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] For c in C_L:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] scores = W^T v	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] X_L^c : subset of X_L with label c	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] By narrowing distribution discrepancy between real and generated images using the domain translation technique, we were able to alleviate the catastrophic forgetting problem successfully (Table 2).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] This training gives similar results to the training in stages.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] The posterior is a distribution for the network, such that finding the best embedding is indeed a maximum likelihood problem (not a maximum a posteriori problem), even though the likelihood function is computed as a posterior given a prior for the network and a conditional for the embedding given the network.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] The difference with [6] is the use of class-conditional priors; more details are explained at the response (Difference with CDVAE) for reviewer 3 and section 4.1 in our paper.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] W = train_classifier(X_L^c, r)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] #Learn a classifier jointly for all classes.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] for i in X_L^c:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] for i in X_Z^c:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] To clarify our training process, we provide a brief summarization.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] If we use labels, we can construct a conditional generative model.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] #Add the noisy examples to the list of training images for this class	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] According to the explanation above, we think the spatial relations or grammatical nuances would not be so important in this task if we take the images as topic guidance.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] M^c = GCN_model(X_L^c, X_Z^c, A^c)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] r_i = assign_relevance(M^c(X_Z^c(i))	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] X_Z^c = filter_by_text(X_Z)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] Given a 10%, 50% and 80% BFS partial layout, the mean number of completions of the layout is 2.97, 1.23 and 1.17 respectively.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] (Domain translation) Even though the conditional generation improves the quality of the generated samples, there is still a big difference between real and generated images.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_summary [SEP] We exactly follow the assumption.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Therefore a simple and effective heuristic for choosing \lambda is to look at the validation set of MNIST and choose the highest \lambda that still results in high accuracy (e.g. >. 0.97).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (a) We have evaluated the adversarial resistance when training a Boltzmann machine with 256 fully connected latent variables directly on the 8x8 patches.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In fact, we believe that trying to construct noise-free deep models with a specific mutual information of data and parameters for the purpose of generalization would be an interesting research direction.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The hyper-parameters related to DS-softmax (such as lambda) are tuned according to the performance on a validation dataset.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Instead, we propose to predict many meaningful intermediate values that can simply be summed to obtain a set utility.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (1) We want to evaluate if the generated graphs are scale-free graphs in the direct evaluation for dataset scale-free graphs.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] But the changes produced in the system may also depend on the characteristics of the dataset and the learning algorithms used.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We started with the reference topology like: ResNet18, LeNet, etc. then we reduce the number of kernels and layers keeping similar accuracy.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We observe that at the beginning of the super net training, the operators are not sufficiently trained, and their contributions to the overall accuracy are not clear, but their cost differences are always significant.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Please also see our answer to the next question for more details.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] For training, we have between 20k and 40k samples for all models, and beyond these numbers we don’t see much improvement.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] | 0.2     | 98.02 | 598	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We use warmup training since in our ImageNet experiments.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (4) We used a matrix-variate normal (MVN) to reduce the parameters of the model.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] By this definition of states, an efficiency search can still be achieved.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We actually tested ReLu with good results, but we chose tanh because it generates weights in the range of (-1,1) avoiding large values given by ReLu.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] pGAN produces poisoning attack points that are close to the decision boundary, “pushing the decision boundary away” from the source class (i.e. the same class as the labels of the poisoning points) towards the samples of the target class.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The parameter alpha controls the importance/priority of each of the objectives.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The main premise behind guiding our siamese networks is to find very simple, yet effective ways to capture some of the variation in the data, through weak supervision.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We believe that this is due to the conceptual advance made in CNE.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] 2. Compared with other gradient estimation techniques such as Reinforce, Gumbel Softmax balances the variance/bias of the gradient estimation with respects to weights.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] A: Generally, Eq.10 is an idea of behavior cloning algorithm.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] - The classification loss has implicit dependency with input conditions by minimizing the KL divergence in Equation 2.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The heuristic we use to tune group lasso lambda is to increase lambda, starting from a small value, until it hurts the performance.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] A7 L is the cardinal of the union of classes with labeled examples in at least one domain.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] For a small machine (16 units) of full hidden connectivity we can observe the noise rejection behaviour, as shown in appendix D.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] A1: We pick the ranges using two criteria: qualitatively acceptable and quantitatively under a fixed threshold for FID score.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] .	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] It removed an entire set of m x V parameters and got us better results in all our experiments so we decided to use it.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (2) There are many classical evaluation metrics focusing on measuring the similarities or distance of two distributions.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (1) The equations are indeed very related.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Yet in order to generate the audio signal, we simply generate the real and imaginary parts of the STFT coefficients such that we can convert them to waveform using inverse STFT.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In each episode (training batch), our algorithm solves a small classification problem which contains N classes each having K support and Q query examples (e.g., N=5, K=1, Q=15, totally 80 examples).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Furthermore, in a Bayesian setting p(theta|...) would play the role of a approximate posterior, which would require variational inference (VI), and thus a different objective,  to estimate it.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] However, please, note that the discriminator’s loss is decoupled from the classifier’s loss.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Thus, we adopt the same hyperparameters and architectures as the prior work, and as a result our work is fairly easy to reproduce.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] - Use different values of T for different tasks (see “discussion” in section 4)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We do not claim that we obtain any theoretical guarantees or properties of the Choquet integral.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In other words, the discriminator does not filter out the points that are used to train the classifier during the training of pGAN.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We describe in Section 3.3 that we use mean squared error (MSE) and mean absolute error (MAE) in our experiments.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] - Reward clipping (see section 3, under Eq.2)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] And for any $k$ is greater than $M c_\eta$, we have the uniform bound (w.r.t. $k$) as stated in the theorem 4.3.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] | 0.75   | 97.36 | 577	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Yes. We agree with the comments that the advantages of the Gumbel Softmax technique are two-fold:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The four metrics in this paper are among the most authoritative and commonly used ones in existing works, e.g., [2][3][4][5].	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We therefore have made many improvements and adaptations by either referring to existing literature, or depending on the specific tasks.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] y = alpha_1 * y_1 + alpha_2 * y_2	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] 4.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We sample architectures every a few epochs, mainly because in our experiments, we want to analyze the behavior of the architecture distribution at different super net training epochs.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The worst case happens when the graph is a complete graph.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Furthermore, in the ELBO we have a fixed value of \lambda = 1.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In fact $\omega$ does not need to predict p, but it gives extra supervision signal and therefore regularizes the prediction.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Furthermore, we have put more details about model architecture as in Appendix A Figure 4 and Figure 6.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In the original version of the paper, all experiments are conducted on trimmed video classification datasets.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The objective we introduce for CDNs differs from the ELBO-based objective in VI in the way the logarithm is placed in the first term of the objective: in the ELBO we have a logarithm inside the expectation, while the logarithm is outside the expectation in the CDN objective (note however, that the sample-based approximations get equivalent if only one sample is used).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Specifically, the proposed GT-GAN that uses L1 norm outperformed all the other comparison methods shown in Table 2,3 and 4.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (2) This paper has given the time complexity in the worst case: O(n^2) as shown in 3.4.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We used this term to distinguish the additional classifier from our integrated encoder that has discriminative power.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] - Use improved training algorithms (e.g. PPO) for more challenging tasks, and slightly adjust reward generation schemes (see “discussion” in section 4, and Appendix A.1).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (3) The experiment demonstrates its effectiveness.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We implemented custom versions of the optimizers we consider (SGD, RMSprop, and K-FAC) that treat the optimization hyperparameters as variables in the computation graph for an optimization step.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Our code is built on top of existing code (Prototypical Networks and Image-to-Image Translation from CycleGAN).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Specifically, the P2F network is trained on the original data, and not on the output frames of the P2P network.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] This analysis is illustrated in figure 3 of our paper.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Regarding your questions: i) demonstrators policies are implemented by search algorithms; ii) the behavior tracker is an LSTM with 128 hidden units; iii) fusion module produces a 32-dim attention vector corresponding to 32 feature maps from the state encoder, and each element of that vector is used to reweight one of the feature map in order to reshape the state feature.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The abstracted game is generated by domain knowledge, such as clustering similar hand strength cards into the  same buckets.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Thus we decided to feature the per-iteration comparison in the main paper, as it is the cleanest one.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We tune lambda by performing a grid search over the range {1e-1, 1e-2, 1e-3, 1e-4, 1e-5}. Because each lambda value gives rise to a learning rate schedule, tuning lambda yields significantly more value than tuning a fixed learning rate.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] A1: There is only one adversarial party in centralized attack.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] To prevent this, we use warmup training to ensure all the candidate operators are sufficiently trained before we optimize architecture parameters.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Also threshold and balancing lambda variables are kept fixed as (0.01 and 10).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The domain of graph deep generative learning methods typically do not require to distinguish or preprocess specific topological types of graphs before applying it, no matter it is strongly- or weakly- connected graph, complete graph, planar graph, scale-free graph, or graphs that have other specific patterns.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (2) L1 norm is commonly used in GAN in relevant domains, e.g., in image-translation domain, for example, reference [1] (with 600+ citations) and reference [6] (with 1300+ citations).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The time complexity of a strongly-connected graph will not be worse than that.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] - For episodes of different length, we can pad or truncate the trajectories into same lengths and apply V-GMM.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] | λ_RU  | Acc.5 | Size |	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Also, as we mentioned in the paper, only one hyper-parameter (group lasso lambda) needs to be tuned.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We were also interested in the mean absolute error because minimizing this loss might be more appropriate in a task such as automatic summarization, in which we don't want to punish a model strong if it makes a few severe mistakes compared to making many small mistakes.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The weight matrix is constructed on the support and query examples in each episode rather than the whole dataset.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In comparison, at epoch-79 and epoch-89, architectures have higher compression rates and accuracy.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In other words, to evade detection or removal of points by algorithms that defend against poisoning attacks, such as the defences we used in our experiment, we want our attack points to be close to the distribution of the genuine data.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] For the problem of mixed precision quantization, this can be problematic.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] >> Comment #13	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] However, it is not really necessary to use Langevin dynamics to get $M$ initial samples, as we can simply using some other initialization distribution and get the $M$ initial samples from that distribution (and by this setting, our dynamics is simply the second phases in Eq (3)).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] They explore the link of limiting mutual information and generalization error mostly in theory (and in particular for adaptive analysis).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Using a diagonal MVN for X \in R^{p x q} one needs pq+p+q parameters.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The difference between epoch-79 vs. epoch-89 is small since the distribution has converged.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We also describe in Section 3.3. that we use Adam as optimizer.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] +---------+---------+-------+	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] At some point, when the fraction of poisoning points increases significantly the decision boundary starts to change in a different (and possibly more abrupt way), so that the false negatives also start to increase.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Approximations like NCE (in conjunction with random projections) would allow us to remove the restriction in the output layer.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] R) We can use any Activation function.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Our approach has 2 important hyperparameters: scaling parameter s used for calculating binary mask from the embedding matrix as well as  λ_RU, that controls the size accuracy trade-off (see Sec. 4.1 “joint training”).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In contrast, we deploy this principle in a practical model structure that is easily applicable to many existing deep and variational learning approaches and provide empirical evidence of the validity of our framework.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Another method is to use PCA or auto-encoder to reduce the dimension into a fixed size and then apply CDP.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The parameter alpha also allows to control the detectability constraints for the attack, which allows us to test the robustness of learning algorithms and defences in different settings, considering more or less aggressive adversaries.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In Figure 6 (right) this happens when the fraction of poisoning points is larger than 25%.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] +---------+--------+--------+	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We pick alpha steps uniformly within the ranges (shifts and rotations are integer steps).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The result of this theorem holds uniformly for any $k$ (not a fixed $k$).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Works on image-translation have proposed and utilized L1 loss and adversarial loss jointly in GAN, for example, reference [1] (with 600+ citations) and reference [2] (with 1300+ citations).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Furthermore, the main idea of this work is to not learn a representation.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We apply the data augmentation both at training and test time.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] All the camera pose including the first camera are initialized with identity rotation and zero translation, which are aligned with the coordinate system of the first camera.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We can see that at epoch-0, where the architecture distribution is trained for only one epoch (close to random sampling), the sampled architectures have much lower compression rate.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] - We have included model and training hyperparameter details in Section 5.1 and Appendix B.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Owing to similar concerns, we recommend using CFS information transfer metric over classifier weight difference (which is also supported by results in Table 2 and Figure 3).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In our ImageNet experiments, we found that ten warmup epochs are good enough.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Let's assume both y_1 and y_2 are in binary and are in {0, 1}. Assuming alpha_1=0.5 and alpha_2=0.25, then the possible values of y are {0, 0.25, 0.5, 0.75}, which essentially extend the effective bit-width to 2 bit.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] This is the same method used in the works we compare to ([1], [5], [6]).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We follow the widely-used episodic paradigm proposed by Matching Networks [1].	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We intentionally use the spectrogram notation as we do not use complex-valued kernels with complex-valued convolution.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The $f^*$ and $g^*$ means the dependent variable of duality gap.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We acknowledge the difficulties of training controllers using vanilla REINFORCE.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We find that the model without stacking is not able to increase the adversarial resistance.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Another limitation of AutoLoss is the necessity of designing the feature vector X, which might require some prior knowledge on the task of interest, such as being aware of a rough range of the possible values of validation metrics, etc.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] This is very fast and efficient.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Therefore, a lot of techniques in addition to vanilla REINFORCE are required to stabilize the training.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Then, we can expect an increase of the false positive rate, which is shown in Figure 6 (centre).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] | 0.002 | 98.22 | 638	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Perhaps the reviewer is incredulous regarding this large increase in performance a method as 'simple' as CNE achieves w.r.t. the state-of-the-art.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] During our development of the training algorithm (See Eq.2, the “discussion” section in Sec.4, and Appendix A.1), we found the vanilla form of REINFORCE algorithm leads to unstable training.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The attack points are therefore not necessarily close to the decision boundary, and thus, the changes produced in the algorithm are more unpredictable and affect the errors for the two classes.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] This is good for the super net's accuracy, but the performance of the super net cannot transfer to the searched architectures in which we have to pick only one operator per layer.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] To avoid exponential enumeration of the predicate orderings sophisticated transformation of the rules has been applied in the Neural LP framework (see [Yang et al. 2017]).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In forward computation pass, the index position of the max (or top-k) values are stored.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] That is, the ratio of the global trigger of DBA pixels to the centralized is 0.992 for LOAN, 0.964 for MNIST, 0.990 for CIFAR and 0.991 for Tiny-imagenet.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In the large extensive game, the initial strategy is obtained from an abstracted game which has a manageable number of information sets.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Similarly, for epoch-9, architectures also have relatively low compression rate.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (3)-(4) To some extent pGAN can control the specific errors produced in the system, as shown both in Figures 5 and 6.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] This formulation is fundamental for the next step of the work in which we are removing the restrictions from the output layer and learning word probability distributions without prior knowledge of the vocabulary size.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Relatedly, one would have to design priors and architecture to achieve a specific mutual information.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (2) Combining L1 loss and adversarial loss is well-recognized and validated.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] It is possible that we are unable to complete the training due to the approximations involved.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Instead of trying to come up with a custom learning rate schedule, which would require deciding how frequently to decay the learning rate, and by what factor it should be decayed, all one needs to do is perform a grid search over a fixed set of lambdas to find an automated schedule that is competitive with hand-designed schedules (which are the result of years of accumulated experience in the field).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In CIFAR-10 experiments, warmup training is not needed.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We want to imply that our proposal is not incompatible with NCE, but we did not yet explore it so, to make the paper more self-contained it is probably best to leave this out.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] - Substitute from the reward a baseline term, which is a moving average (see section 3, Eq.2)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In contrast, the label flipping attack is less subtle as it does not consider detectability constraints.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] - We added a motivation for mixing two different terms in the objective function.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (d) There are a total of 28800 parameters in the Boltzmann machine.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The main challenge here would certainly be to come up with an effective estimator.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We also point out that, as our system is complicated, in taking the limit of $M\to\infty$, we need to ensure that the number of iteration we run is larger than $Mc_\eta$. To be specific, the asymptotic convergence would be	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Specifically, L1 makes T(Gx) share the same rough outline of sparsity pattern like Gy, while under this outline, adversarial loss allows the T(Gx) to vary to some degree.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Outputs of candidate operators are multiplied with some coefficients and summed together.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Before training, all unique entity labels (e.g. B-PER, I-PER, ... etc.) are embedded by assigning them to randomly initialized, continuous vectors of 128 dimensions (this hyperparam is mentioned in Table A.2 of the appendix).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] But we make sure that the total injected triggers (e.g., modified pixels) of DBA attackers is close to and even less than that of the centralized attacker.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] As described above, the proposed approaches are inspired by the way Choquet integrals handle non-additive utility aggregations.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] | 2E-06 | 98.16 | 660	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We will of course release the code.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Specifically, L1 makes generated graphs share the same rough outline of sparsity pattern like generated graphs, while under this outline, adversarial loss allows them to vary to some degree.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] This is common in most security settings to test system’s robustness and resilience in different attack scenarios.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] That said, the formulation is just the re-use of the embedding layer transposed.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The L1 loss is calculated between the predicted ratio mask and the ground truth ratio mask.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Clone a good initialization, and then continuously update the two neural networks using our method.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In fact, We initially experimented with directly feeding blackbox features (e.g. raw vectors of parameters, gradients, momentum, etc.) into controller, but found they empirically contributed little to the prediction, and sometimes hindered transferability (as different models have their parameter or gradient values at different scales).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] With the setup used in section 3, there is no good notion of validation: our model is expected to predict “0” on held-out data.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Also, we would like to remark that this seemingly strange things is in fact the ‘artifact’ caused by the using of Langevin dynamics at beginning to obtain the $M$ initial samples when we designed the practical implementation of the proposed methods.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Note that “time” here refers to number of iterations, not epochs.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We add a table analyzing the sensitivity of the parameter λ_RU observing the expected behavior: higher values of λ_RU lead to a smaller model size, however, reduced G size is positively correlated with the final classification performance of D (smaller G -> lower accuracy of D).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Due to nonlinearities in typical deep models, it is at least not obvious how to calculate the mutual information between data and parameters.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Q4: Yes \theta and \phi are jointly and simultaneously optimized at Eq. 12, though the gradients w.r.t. \phi from the KL divergence is stopped.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We indeed implemented search algorithm with simple heuristics for acceleration for all grid-world tasks.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] to the architecture parameter \theta	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] While in the back propagation pass, the gradient is computed only with respect to these saved positions.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] They have done extensive experiments to show the advantage of such a strategy.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] $$\lim_{k,M \to\infty, \eta \to 0^+} \mathbb{D}_{\text{BL}} (\rho_k, \rho^*)=0$$	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] To clarify, we used the elbow method (used to find an appropriate number of clusters for clustering) and observed that the ‘elbow’ in the accuracy vs dimensions plot was around the 80% accuracy mark for most tasks, and hence, we used 80% as the retention ratio.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We stressed this setup in Section 3.2.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Our methodology is achieved by a trade-off between L1 loss and adversarial loss (GAN-D).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] where the joint limit of k and M requires that $k\eta\to\infty$; $\exp(C\alpha^{2}k\eta)\eta^{2}=o(1)$; $(k\eta)/(Mc)=q(1+o(1))$ with $q>1$. Here if $q \leq 1$, we degenerate to Langevin. But when $q>1$ (intuitively that means, when $M$ is large, the number of iterations we run is larger), our dynamics is different from Langevin, which is what we do in the practice.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Note however, that In a standard Bayesian neural network (BNN), one would assume that \theta is a global random variable (i.e. does not depend on input x), whereas in the CDN, we assume that \theta depends on x and is thus a local random variable.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We are not aware of results establishing SGD is faster in this measure.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (2) In Equation (4) we followed with  p(D | \psi) a standard notation for \sum_n p(y_n | x_n; \psi) (i.e. summation of Equation (3) wrt all data in D) which also can be found e.g. in the work of Graves (2011) [4] and Blundell et al. (2015) [5].	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Let's consider a simplified scenario	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Using our method, however, the sampling ensures that the super net only picks one operator at a time and the behavior can transfer to the searched architectures.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] - Another note: to perform a full wall-clock comparison with algorithms that have different per-iteration costs, one must disentangle and retune various hyperparameter choices, most notably the learning rate schedule.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The version with only 128 hidden units was not able to reduce the relative entropy to the values of the larger, stacked machine.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In contrast, the generator is the element that competes with both the discriminator and the classifier.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The embeddings are then updated along with the rest of the models' parameters during training.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Section 4.2 shows how to use the K-S test to detect leakage, but the same test could tell if the m-set comes from neither the train nor the validation sets.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In our opinion a conceptual advance that achieves a strong boost in accuracy without increasing complexity, is at least as valuable as a method that achieves the same boost in accuracy while also increasing complexity.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] A: (1) Similar to all the existing graph deep generative learning methods for generic graphs, we do not have additional assumptions on the graphs.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We observe AutoLoss has bounded transferability -- while we successfully transfer a controller across different CNNs, we can hardly transfer a controller trained for CNNs to RNNs.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In fact, unlike previous video compositional methods, even when local events are not well aligned or misclassified, long-term modelling with 4D convolution and video-level aggregation with global average pooling are very likely to correct the partial error.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] 2. The Pose2Pose and Pose2Frame networks are trained separately.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] If the degree distribution of generated graphs is the same to the degree distribution of real target graphs, the generated graphs are good.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We observed that generally as \lambda increases, the uncertainty is increasing, while the accuracy is decreasing.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] DARTS [1] does not really sample candidate operators during the forward pass.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] For more complex semantics, we discuss the possibility of using a pre-trained network as guidance.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] {k,l} locate the convolving window inside of the input image	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Our methodology is still general enough which is achieved by a trade-off between L1 loss and adversarial loss (GAN-D), which jointly enforces Gy and T(Gx) to follow a similar topological pattern but may not necessarily the same.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We have in the meantime been able to include struc2vec in the evaluation, again showing superiority of CNE by a wide margin -- showing that it is maybe more complex but certainly not 'stronger' as in 'more accurate'.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] As a result, the search always picks low-cost operators.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] | 2        | 86.82 | 522	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We hypothesize that the optimization behaviors or trajectories of CNNs and RNNs are very different, hence the function mappings from status features to actions are different.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (1) L1 norm is applied to the weight adjacent matrix of the graph.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We then use automatic differentiation to compute the gradient of the meta-objective with respect to the hyperparameters (e.g., the learning rate).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] They include:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Minor concern #1: Value of the Gumbel Softmax function	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] |	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] A: (1) L1 norm is applied to the weight matrix.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (1) In equation (3) we are using scalarization, a well-known technique to solve multi-objective optimization problems (see for example Boyd’s book “Convex optimization” Ch. 4).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The two tower design was necessary since the decision whether theorem T can be rewritten using parameters P requires both pieces of information, so we need to feed them to the network.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We use MSE because it is usually used in regression problems.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Practically speaking, this is handled for us via the embedding layer in PyTorch [4].	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] The sparsity in page 3 means the percentage of pruned words.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In this case, the maximization problem is a multi-objective optimization problem including both the parameters of the discriminator and of the classifier.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] This trick is implemented in modern deep learning frameworks such as tensorflow and pytorch.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Furthermore, in our experiments, we found the performance when using L1 loss and adversarial loss jointly is better than using either of them.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] We want to clarify the few-shot setting.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In deep neural networks, there is a common trick in computing the gradient of operations non-differentiable at some points, but differentiable elsewhere, such as Max-Pooling (top-1) and top-k.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] This is an excellent question.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] This is slightly different from some related AutoML works, such as in [1], where auto-learned neural optimizers are able to produce decent results on even different families of neural networks.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] This is actually one of the core advantages of deep learning based models where the graph patterns are not extracted or pre-identified manually by the human but automatically discovered by the end-to-end deep models.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] For experiments in Sec 4.4, the output of the network is the ratio mask, and the separated audio is generated by an Inverse STFT operated on the input STFT coefficients multiplied by the predicted ratio mask.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Meta-learning discrete schedules involves non-differentiable optimization, which is by nature difficult.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (heavy classifier) A classifier such as resnet.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] 1. It makes the loss function differentiable with respect	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In our paper, we use the tensorflow function tf.nn.top_k() to compute k-nearest neighbor operation.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] On the other side, the discriminator does not exclude poisoning data or select any data point but helps to guide the generator to craft poisoning points that are difficult to detect.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] For experiments in Sec. 4.2 and 4.3, the network’s output is the complex STFT coefficient, the raw waveform is then recovered by inverse STFT using the overlap-and-add method.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] (2) In pGAN the discriminator allows to model detectability constraints for the poisoning points.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] Besides, we do not require $k$ bigger than $M c_\eta$ in the definition of $\tilde{\rho}_k^M$. When $k$ is no more than $M c_\eta$, $\tilde{\rho}_k^M$ and $\rho_k^M$ are stochastic processes with same distribution and thus the Wasserstein distance between them is 0.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In Maze Navigation, the state space is extended to the combination of map status and the agent's inventory.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_answer [SEP] In contrast a fully-factorized diagonal Gaussian needs pq+pq.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_social [SEP] These are indeed good questions.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_social [SEP] - We thank the reviewer for suggesting additional comparisons with specific more complex models, although we feel that calling these methods 'stronger' requires some clarification or support.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_social [SEP] Also note that all code is provided, and we invite the reviewer to replicate our experiments.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_social [SEP] >>> Thanks for pointing out the details.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] ** Addressing comments on the write-up:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Concern 1: Reproducibility	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Minor concern #3: Warmup training	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] (i) Speed of SGD vs GD:	1.0
"Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] ""How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral?"""	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] In addition, the paper would also need to show that such a model does not generalize to a validation set of images. [...]”	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] You may check our updated paper with clarification and new experimental results.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] A: Answer about Evaluation metrics:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] *The reasons for the use of the energy-based formulation are not clear to me.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q4: It is not clear how the initialisation (10) is implemented.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] (4) Baselines:	1.0
"Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] ""What is your loss or your algorithm?"""	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 1) Elaborated derivation of Eq. 10	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] > In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Testing	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 1.  “In particular, Section 4 is a series of empirical analyses, based on one dataset pair….However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.”	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q1: Is there only 1 adversarial party?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] - It wasn't clear how the sparsity percentage on page 3 was defined?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] - How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 1. I do not get the point of bringing up NCE...	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] We list several limitations we discovered during the development of AutoLoss:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q4: Can the authors show concrete examples on how the attacks are generated? The details are especially unclear on LOAN.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] (Experimental settings)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q2. Implementation Details:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] How do we take a limit of $M \to\infty$ ? Does k also go $\infty$?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q: It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Training:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] If so, what was the methodology.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] - Bounded transferability	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] ** Clarity **	1.0
"Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Minor comment 2) - ""How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i? In the extreme case it would be all the permutations."""	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 2.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Towards this, how does the computational complexity scale wrt to the connectedness?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q1. Evaluation Time:	1.0
"Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Re: CFS metric depends on a hyperparameter (the ""retention ratio"")"	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q3. Figure 1 is too abstract:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 2. Why stop words are ignored.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Minor concern #2: Comparison with non-stochastic method such as DARTS	1.0
"Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] - ""In Equation (6), the posterior distribution should be P(X|G) [...]?"""	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Answer about L1 norm:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 2. More implementation details	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] - Non-differentiable optimization	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Specifically, how can mutual information in this context be formally linked to generalization/overfitting?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 4. Details in the experiments to clear up the settings.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q：A lot of clarity is required on the choice of evaluation metric; for example, choice of distance measure?	1.0
"Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?"""	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 3. Query for the relationship module	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] >> As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.	1.0
"Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] ""Some technical details are missing."	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 2. Dilated convolution in paper’s notation.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Do we know how does the connectedness of the input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q5. How the first camera pose is initialized?:	1.0
"Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] ""Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?)."""	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Major concern: Trained sampling vs random sampling	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] More discussions on these questions can be very helpful to further understand the proposed method.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 7) Was crossvalidation used to select the topology?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 1. The authors should provide more details on how the hand-crafted demonstrator agents were made.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] What is the L1 norm applied on?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Please kindly refer to first paragraph in Section 3.3.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] “(*) 3. Scenario discussed in Sec. 4 seems somewhat impractical. [...] one might also need to figure out if it is neither train nor val”	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] “It is unclear whether the data augmentation techniques is applied only at training time or also at test time. In other words: at test time, do you present the original images only or transformed images too?”	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q: how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q: Please explain more how gradients w.r.t hyper-parameters are computed.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q: How to tune lambda?	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Q1: The learning procedure is confusing. It is highly recommended to provide the pseudocode of the proposed method.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] Regarding predicted entity label embeddings,	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] 3. Complex Coefficient vs Spectrograms.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] See general responses #1 and #3.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] - Design white-box features to capture optimization status	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_structuring [SEP] * Missing implementation steps and optimization details:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] We will share all the source code to make sure it is reproducible.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] We will discuss this process and test with different retention ratios in the final version.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] The only new part is the automatic rate tuning  behavior shown for most parameters when BN is used.)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] R1: We will provide the pseudocode in the future versions of the paper:	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] (As noted on p2,  we are working within the standard paradigm of convergence rates in optimization.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] A4. Thanks for your suggestions, we will release code upon the acceptance.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] Moreover, we will ultimately release codes on Github.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] The paper will be updated very soon to include these results.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] We are sorry for not stating this clearly in the theorem and we have revisited the present of the theorem. We will fix this issue in the next revision.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_by-cr [SEP] All code and model weights used in this paper will be made available.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_future [SEP] We will investigate the benefits of more flexible mixing distributions in future work.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_future [SEP] As a potential future work, we will seek for continuous representations of the update schedules and end-to-end training methodologies, as arisen in recent works [2].	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] Meanwhile, we have included more details as suggested in Appendix A, including a visualization of all layers of the different parts of the network. If 1-2 extra pages are allowed, we can include those details to the paper.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] (Added to the paper)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have simplified the architecture described in the paper by combining the networks $\sigma$ and $\omega$, and included the results from this architecture as well, producing a more robust architecture that performs better for multiple rewrite steps (while keeping the original, more complicated solution as one of the baselines).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have added a section in the appendix to include dilated convolution in the paper's formulation.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We updated section 2.2 to relate to the references you mentioned.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] Details on architecture: Agreed, and thanks. We have added some details on the specific network architectures to Appendix C (for ski rental) and Appendix D (for AdWords).	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] (b) We have trained a machine with the same connectivity as the stacked machine directly on the 8x8 patches.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have added the detailed running time for each component in Table 3 in Appendix A of the revised version.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] (refer to section 3.3 in the revised paper.)	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] In addition to implementation details, the appendix has a rather detailed table of the architecture parameters.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] Please refer to our “Discussion” section for more details.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We added an additional figure in Figure 6 in Appendix E, for helping conceptual understanding.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We haved add the above discussion to the latest version as Appendix A.9.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have also provided more details about LOAN dataset and how we attack in Appendix A.1.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have updated the figure to make it more intuitive and contains more details.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] In addition, we added more details about the data as you suggested.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] Additionally, we evaluated our V4D for untrimmed video classification on ActivityNet v1.3, which contains videos of 5 to 10 minutes and typically large time lapses of the videos are not related with any activity of interest.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] Moreover, we investigated the impact of the different objectives empirically and found that the CDN-based objective led to significantly better results, as shown in the newly added Section 6.4 in the revised manuscript.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have made this procedure clear in the revised manuscript.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We revised the notations in the paper to make formulation clearer.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] R) Now we have a pytorch version of the code at https://github.com/adapconv/adaptive-cnn	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have updated the text in the manuscript (under section 2) to make this more clear.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] With MNIST and CIFAR.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] The very competitive result is reported in the appendix of the second version of paper, which demonstrated the generalization and robustness of our V4D.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] Q1: We have added one more line to explain the derivation.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have updated the baseline details in the Appendix B.3.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We clarified this at the end of Section 4.3 in the revised version.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] Q5: In the updated version, we have explicitly pointed out that the gradients w.r.t. \phi from KL divergence is stopped. Thanks for this suggestion.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have modified the text in the implementation details in Sec. 3 and the setup paragraphs in Sec. 4.2, 4.3, and 4.4 to make this point.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have added more details, and plan to release the code.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We added a new Section 4 in the revised version of the paper discussing these differences.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have added more clarifications in the revised version.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have provided more details in the revision and plan to release our code.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have modified the text in Sec. 4.2, 4.3, and 4.4 to make the details more clear.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_done [SEP] We have also revised the submission to disclose more details on how we make these improvements.	1.0
Reasons for rejection (not fit for conference, contains several weaknesses, etc.) [SEP] rebuttal_structuring [SEP] Regarding your comments on a System Demonstration submission to ACL,	1.0
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] We agree that plausibility is indeed important, and that's what our human subject studies try to capture.	1.0
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] Since we provide predictions of the whole sequence to the human evaluator, we are not only evaluating for image realism but also for plausibility of the dynamics.	1.0
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] In early experiments, we trained our pure VAE model on the stochastic shape movement dataset from Babaeizadeh et al. (2018), and our pure VAE was able to model the dataset without any blur and with perfect separation of the possible futures.	1.0
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] The authors of MoCoGAN also mentioned in personal correspondence that the conditional version (i.e. video prediction) was significantly harder to train.	1.0
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] However, that is typically not the case of synthetic datasets.	1.0
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] Furthermore, we are currently running experiments for various weightings of the KL loss and the adversarial loss, and we plan to include additional results that illustrate the trade-offs based on these hyperparameters.	1.0
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] Although MoCoGAN performs well for videos with a single frame-centered actor, it struggles with multiple simultaneously moving entities.	1.0
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] In our case, we found that the model would degenerate to static videos or videos with a cyclic flickering artifact, which are issues that aren't a problem in conditional image generation.	1.0
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] Unlike the VAE models that implausibly erase the small objects that are being pushed in the BAIR dataset, our SAVP model moves those objects in a more plausible way.	1.0
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] Furthermore, while the individual components have indeed been known for video prediction, their combination is novel and not present in prior work, and we demonstrate that this produces state-of-the-art results in terms of diversity and realism.	1.0
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] We noticed the same in earlier iterations of our model.	1.0
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018). [SEP] rebuttal_answer [SEP] Although the combination of VAEs and GANs have been explored recently for conditional image generation (Zhang et al. 2018), the video prediction task is substantially different, with unique challenges, due to spatiotemporal relationships and inherent compounding uncertainty of the future.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_concede-criticism [SEP] 5. We were not aware of this at the time of submission.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_summary [SEP] Indeed, MnasNet achieves similar results with us with less FLOPs.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_summary [SEP] The focus of our work is to propose a new tree prediction problem (layout completion) and introduce Transformer-based approaches for addressing the problem.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_summary [SEP] Instead, our V4D processes videos of RGB frames so that our input is 3D data.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_summary [SEP] This paper utilizes 4D CNN to process videos of point cloud so that their input is 4D data.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_summary [SEP] Moreover, the design space of MnasNet is significant different from other existing NAS methods including ours.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_summary [SEP] This basically makes the methods and tasks quite different.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_summary [SEP] The main critical difference between our work and other HRL works is that we build an abstract MDP, which enables us to plan for targeted exploration; other works also learn skills and operate in latent abstract state spaces, but not necessarily in a way that satisfies the property of an MDP, which can make effectively using the learned skills difficult.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_summary [SEP] However, it is also need to note that MnasNet evaluates more than 8K models, which introduces much higher search cost than our method.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_summary [SEP] Jenatton et al. proposed an approach to predict tree structures by using Bayesian optimization to combine independent Gaussian Processes with a linear model that encodes a tree-based structure.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] We would like to thank reviewer 3 for suggesting the related works that we have missed. These were incorporated in the revision.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] We have cited and discussed the work in the revision.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] We have also incorporated the recent works [1,2] to appear soon at Neurips’19 (their text became available very recently and after the ICLR submission deadline).	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] Thank you for providing this related work and we now cite this paper in the second version.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] A5: We have added the result of MnasNet [2] in Table 2.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] A more careful review of the literature has been incorporated into the introduction section.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] We have changed this to PoweredSGD. If you have any alternative suggestions, please let us know.	1.0
Missing literature review (some literature not included, misses baseline citations, etc) [SEP] rebuttal_done [SEP] We’ve updated the related works section in our recently posted draft to more carefully compare  Please see Sections 1 and 7 for updated related work.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We used a simple 1D CNN where convolution happens over the time dimension, not the spatial dimensions.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] LPIPS linearly calibrates AlexNet feature space to better match human perceptual similarity judgements.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] In early experiments, we trained our pure VAE model on the stochastic shape movement dataset from Babaeizadeh et al. (2018), and our pure VAE was able to model the dataset without any blur and with perfect separation of the possible futures.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] A first impression is that our technique is more general and will probably give looser bounds, but may be applicable to a wider range of problems not only ones that have specifically that type of covariance, just like DEMINE vs Pearson's correlation.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] However, that is typically not the case of synthetic datasets.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] DEMINE-sig maximizes mean([m1,m2,m3])-v.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Aside for the first two predicted frames, our VAE ablation and the SVG model both achieve similar SSIM and LPIPS.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Regarding fMRI experiments, our focus is on demonstrating neural MI estimation and dependency test on fMRI data.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We compare with pearson's correlation because it's another widely used technique that can perform both correlation analysis as well as significance test.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Higher-order and nonlinear covariance tests may make very appealing comparisons and we are looking into it.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] The experiments from our original submission cropped the videos into a square before resizing, and thus discarded information from the sides of the video.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] But at the same time we also have questions on how much additional insight it brings, as it's not an apples to apples comparison, so neither tight or loose estimations diminish the value of both types of approaches.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Better architectures, e.g. transformers over time + graph networks over space could improve performance, but not our focus and we leave that to future work.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] After examining the KTH results further, we realized that our results are likely weaker than they should have been, because we did not use the same preprocessing as prior work.	1.0
Incorrect baselines used [SEP] rebuttal_by-cr [SEP] Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.	1.0
Incorrect baselines used [SEP] rebuttal_by-cr [SEP] Nonetheless, we will expand the related work section to more explicitly compare to data parallelism and non-pipelined approaches to model parallelism (i.e., expand on the first paragraph of related work).	1.0
Incorrect baselines used [SEP] rebuttal_by-cr [SEP] We can include the literature review for training on imbalanced data sets as well as comparison of other methods with negative pre-training in terms of memory use and training complexity in the final version.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] In the MNIST experiments we already included Gumbel top-k sampling, but we will also add this for the other experiments in the revised manuscript.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Thus the difference of 0.01 CR is a direct comparison to that work.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Note that the use of the Gumbel top-k method for compressive sampling is also new, and in fact constitutes a specific case (constrained version with shared weights across distributions) of the proposed deep probabilistic subsampling (DPS) framework.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Regarding the theoretical basis used for the design of the task network; we took a theoretically principled approach by exploiting a model-driven network architecture for the CIFAR10 reconstruction problem.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] However, asynchronous update (e.g., asycn-SGD in Dean et al. [1]) usually utilizes a parameter server to keep track of model parameters (weights) while our pipelined method does not use any parameter server.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] The simple sum baselines and deepsets perform better in this experiments.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] In the meanwhile, we got the interesting idea of negative pre-training on training with imbalanced training data and tested our hypothesis and included in the paper.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Instead, we train the other G_theta(u) to match Q(X), which is more similar to AAE-like works (Engel et al., 2017; Kim et al., 2017; Achlioptas et al. 2017).	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] On the other hand, we do not enforce Q(X) to follow from Gaussian.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] We respectfully disagree with the referee’s conclusions, and will elaborate on the above statements in the following.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] In particular, we were not able to find any official public implementation of the pseudo-count methods.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Regarding the theoretical correctness of deep probabilistic subsampling, in section 3.2 we explain how we incorporate a well-known reparametrization trick, termed the Gumbel-max trick (Gumbel,1954), to sample from a categorical probability distribution.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Gumbel (1954) showed that this reparametrization allows sampling from the original categorical distribution.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Recent state-of-the art work on a relaxation of this trick, termed Gumbel-softmax sampling (Jang et al., 2017) or the concrete distribution (Maddison et al., 2016), allows us to apply this relaxed reparametrization inside a neural network as it enables gradient calculation, which is needed for error backpropagation in the training procedure of the network.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Furthermore, we demonstrate in the extrapolation experiments that standard RNNs tend to overfit.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] We experimented with a third party implementation trying to see if it could play Breakout without extrinsic rewards, but did not achieve sufficient success and found it to be too slow for scaling it up to a large-scale study.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Hence, we think that the comparison is fair and meaningful.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Furthermore, each accelerator obtains a replica of a full model in asycn-SGD training while each accelerator contains only a part of the model in our pipelined method, on the assumption that the full model does not fit into the memory of a single accelerator.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Regarding the referee’s conclusion that the manuscript lacks comparison to the approaches of (Xie & Ermon (2019); Kool et al. (2019); Plötz & Roth (2018): We would like to point out that these three references all together put forward the Gumbel top-k method.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] In addition, we added a thorough comparison of the DPS to LOUPE (Bahadir et al, 2019), a recently proposed data-driven method for subsampling.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] The Gumbel-max reparametrization perturbs the logits of the categorical distribution with Gumbel noise after which, by means of the argmax, the highest value is selected.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] To that end, we unfold the iterations of a proximal gradient scheme (Mardani et al., NeurIPS, 2018), allowing for explicit embedding of the acquisition model (and therewith the learned sampling) in the reconstruction network.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] This allows us to be able to run more and larger experiments on many environments.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] The focus of this work is on problem embedding and its application in a recommendation system that uses problem embedding to project students’ performance for the problems they solved onto the problems that they have not solved yet.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] We would like to ask the reviewer what is believed to be missing from this explanation on the subsampling part of our proposed method.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] => We chose to focus on dynamics-based approaches in this paper because we found them more straightforward to efficiently parallelize than the published pseudo-count methods.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] 1- The idea of using concepts to represent a problem is simple, but using it along with neural network based embedding gives us the opportunity to gain concept continuity as discussed on the last paragraph on page 7 and table 2, which is an active field of research in education.	1.0
"Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Hence, a ""better"" complexity turns out to be prone to overfitting, which shows that larger models are not necessarily better."	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] The difference of the interpretation between PC-GAN and  those AAE-like work is also explained in the second paragraph of Sec 4.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] We are currently running a broader experiment for negative pre-training on other data sets to gain more insight on it, but for the purpose of the task proposed in this work, it outperforms one-shot learning, which cannot be said that is the state-of-the art, but is a common practice. There is no notion of state-of-the-art in training on imbalanced data sets since due to our best knowledge, there is no method that outperforms all the other ones, and the performance of different methods depends more on the nature of the data set.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Note that this shares similarities with the reparameterization trick used for sampling from trained gaussian distributions in a vanilla variational autoencoder.	1.0
"Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] We also compare against an RNN-based approach (abbreviated with ""RNN"" in the paper)."	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] The RCN approach is the smallest modification one can make to implement our idea into a standard RNN.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] Using the evaluation on unseen problems, a problem is recommended that is within the capacity of students close to their boundary to help them learn, and at the same time we cover all the concepts necessary for them to learn.	1.0
Incorrect baselines used [SEP] rebuttal_reject-criticism [SEP] The async-SGD in Dean et al. [1] still falls into data parallelism because each accelerator has a replica of the full model.	1.0
Incorrect baselines used [SEP] rebuttal_done [SEP] This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers.	1.0
Incorrect baselines used [SEP] rebuttal_done [SEP] We redo the visualization in Figure 6 to make the gains provided by SN clearer.	1.0
Incorrect baselines used [SEP] rebuttal_done [SEP] However, due to the reviewers’ concern in the updated draft we compare spectrally-normalized networks to networks with the same architecture except with weight decay, dropout, or batch norm in Appendix A.2.	1.0
Incorrect baselines used [SEP] rebuttal_done [SEP] As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results.	1.0
Incorrect baselines used [SEP] rebuttal_done [SEP] We have added more to the revised version.	1.0
Incorrect baselines used [SEP] rebuttal_done [SEP] We see that using SN can improve the test performance by over 12% for some FGM, PGM, and WRM cases.	1.0
Incorrect baselines used [SEP] rebuttal_done [SEP] We have moved this latter comparison to the main text at the suggestion of Reviewer 1.	1.0
Incorrect baselines used [SEP] rebuttal_done [SEP] To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294).	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Q1: The idea is very related to Yeh et al.’s work which is not mentioned at all.	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] A:	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Q3: Comparison to more baseline, for example models with no message passing.	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] 1. We are not training Restricted Boltzmann Machines (RBMs), but Boltzmann machines where the hidden units can be fully connected.	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Q2: About “comparison with the baselines”:	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] --------------------------------------------------------------------------------------------------------------------------------	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Q1. Comparison with [1, 2, 3, 4].	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] “However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite: Vít Škvára et al. Are generative deep models for novelty detection truly better? at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried.”	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] With augmentation:	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Concern 2: Experiments	1.0
"Missing baselines [SEP] rebuttal_structuring [SEP] Q5: ""The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL."""	1.0
"Missing baselines [SEP] rebuttal_structuring [SEP] 7. ""In related work, no reference to previous work on ""statistical"" approaches to NN"	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Comment 4:	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Q: “Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing”.	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Improved GAN + SNTG [2] (14.93) vs ours (14.34 +/- 0.17)	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] 2. Baseline missing: Random actions from expert	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Adversarial Dropout [1] (11.32) vs ours (11.79 +/- 0.25)	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Q5. Evaluation on adversarial attacks.	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Q1: About the main concern on “novelty, improvement relative to the current state of the art implementations,  and non-linearity of G and D”:	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] 3. Baseline missing: Simple RNN policies that communicate hidden states	1.0
"Missing baselines [SEP] rebuttal_structuring [SEP] framing the synthetic experiments as, ""here are some simple functions for which we would need the additional parameters that we define"" makes sense; but arguing that Hartford et al. ""fail approximating rather simple functions"" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail”"	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Specifically for ESS-BodyShare baseline:	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Domain Adaptation Baselines + Other datasets	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] “Weak baseline”	1.0
"Missing baselines [SEP] rebuttal_structuring [SEP] verification. Is it actually the case that this angle has never been explored so far?"""	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Question 1:	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] We refer the reviewer to the general response for further information.	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] Without augmentation:	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] 3. On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it’s being reported as 6.24.	1.0
Missing baselines [SEP] rebuttal_structuring [SEP] R: - Related work	1.0
Missing baselines [SEP] rebuttal_by-cr [SEP] In any case we will make the code available as soon as possible.	1.0
Missing baselines [SEP] rebuttal_by-cr [SEP] The updated paper will discuss related work in more depth, including the suggested [A] and [B].	1.0
Missing baselines [SEP] rebuttal_by-cr [SEP] To also include previously-published baselines, we are currently running experiments with the recently proposed LOUPE method by Bahadir et al. (2019).	1.0
Missing baselines [SEP] rebuttal_by-cr [SEP] Defining the best combination of techniques to achieve the highest performance is an interesting direction of future work; our preliminary experiments combining Mean Teacher with manifold regularization have shown some improvements and we will include the results in the final version of the paper.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] 2. The 0-GP is not the only contribution of our paper.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] However, Goodfellow et al. (2014) already prove that the discriminator is unable to identify how realistic an input is after several steps of training, if the GAN has enough capacity.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] In addition, we note that the highest performance in many of the mentioned baselines (and with VAT) are obtained with a combination of multiple approaches.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] Next, [3,4] also assume clean training labels, and aim for detecting abnormal test samples after ’clean’ training.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] That is why we use the generator of a trained GAN as an implicit probability density model in our method.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] The main difference between our method and [1, 2] is that we do not directly train the Gaussian mixture model, i.e., generative classifier but we post-process it on hidden feature spaces of pre-trained deep models.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] Yeh et al. (2017) use the discriminator loss of a trained GAN as an indicator of how realistic their restoration is.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] In addition, we study a robust inference method to handle noisy labels in training samples, while they did not.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] As we explained in the paragraph, there is a major theoretical flaw in their method.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] After updating these baselines, we note that our method still achieves state-of-the-art performance in the regime where 1000 and 2000 labels are used for training on CIFAR-10, with and without data augmentation.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] For the state-of-the-art results, we indeed provided a significant improvement on the inception score of CIFAR-10 (from 8.22 to 8.45) using SN-GAN’s architecture as suggested.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] A1: The entire first paragraph of our related work section is focused on Yeh et al.’s work.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] Then, as some useful examples, in this paper, we particularly showed that the technique of negative feedback can be leveraged to stabilize GANs and developed NF-GAN, which was proven to be effective in our experiments.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] Another difference between their work and ours is that they only focus on image inpainting problem, while our method applies to various image restoration problems.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] When our method is compared head-to-head against the proposed method in the mentioned papers, it is competitive and sometimes outperforms them, for instance, in experiments on CIFAR-10 with 4000 labels	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] Note that proposing a generator architecture is not the goal of this paper.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] Ideally the generator will have all the information of the data distribution while the discriminator will have none.	1.0
Missing baselines [SEP] rebuttal_reject-criticism [SEP] Indeed, as agreed by R#2, this is a major novel contribution that provides a unified and promising framework to model the stability of GANs, which includes some recent developments (e.g., Negative Momentum and Reg-GAN, See Sec. 4.1 and Appendix A&C) and also provides us a possibility to explore advanced tools in control theory (e.g., nonlinear control and modern control theory [*3]) to improve both the stability and convergence speed of GANs.	1.0
Missing baselines [SEP] rebuttal_done [SEP] It indeed performs much worse than our full model.	1.0
"Missing baselines [SEP] rebuttal_done [SEP] [Experiments Section] We have significantly updated qualitative and quantitative results in our ""Experiments"" section and now compare our methods against Beta-VAE, DIP-VAE, and InfoGAN."	1.0
Missing baselines [SEP] rebuttal_done [SEP] We thank the referee for bringing the article [V] to our attention and we now have acknowledged the prior work properly in our introduction.	1.0
Missing baselines [SEP] rebuttal_done [SEP] As reported in the updated Table 1, this policy is comparable to the one learned from optimal demonstrations, and it still outperforms baselines which are all trained from optimal demonstrations.	1.0
Missing baselines [SEP] rebuttal_done [SEP] Evidence and comparison to other methods on disentanglement is provided in  Table 9 in Appendix G, where we visualize the correlations between our embedding dimensions.	1.0
Missing baselines [SEP] rebuttal_done [SEP] We have added the suggested baselines in the revision.	1.0
Missing baselines [SEP] rebuttal_done [SEP] We have included a revised plot in Figure 14 at the end of the Appendix (note that this temporary plot will be incorporated to Figure 6), where we use the official implementation of SSIM and replace the VGG metric with the Learned Perceptual Image Patch Similarity (LPIPS) metric (Zhang et al., 2018).	1.0
Missing baselines [SEP] rebuttal_done [SEP] In our experimental section, we added requested references to methods performing better on bAbI, and point out that our goal is not to beat SOTA on bAbI, but to exhibit and overcome drawbacks of DNC.	1.0
Missing baselines [SEP] rebuttal_done [SEP] In the updated draft, we clarify in Section 3.4 that the warping component assumes that videos can be described as transformation of pixels, but that any generator (including the one from Denton & Fergus (2018)) could be used with our losses.	1.0
Missing baselines [SEP] rebuttal_done [SEP] In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.	1.0
Missing baselines [SEP] rebuttal_done [SEP] We showed that GAN-0-GP-sample suffers from the problem.	1.0
Missing baselines [SEP] rebuttal_done [SEP] We added Section 3.5 to point out the differences between the VAE component of our model and the SV2P and SVG models from prior work.	1.0
Missing baselines [SEP] rebuttal_done [SEP] 6. We added the analysis for the 'mode jumping' problem to Section 6.2.	1.0
Missing baselines [SEP] rebuttal_done [SEP] We have also evaluated the success rate when we use the policy learned from the suboptimal demonstration (10% random actions).	1.0
Missing baselines [SEP] rebuttal_done [SEP] We have also updated the text to tone down the claims.	1.0
Missing baselines [SEP] rebuttal_done [SEP] Accordingly, based on your suggestions, and suggestions from other reviewers, we have tried to expand the baselines substantially (specifically, we include three state of the art Domain Adaptation methods as baselines – RevGrad [1], ADDA [2] and CyCADA [3]), and our proposed methods outperform them.	1.0
Missing baselines [SEP] rebuttal_done [SEP] Figure 5 shows the results where 10% actions from the demonstrator are purely random.	1.0
Missing baselines [SEP] rebuttal_done [SEP] We clarified this in Section 2.1 of the revised draft.	1.0
Missing baselines [SEP] rebuttal_done [SEP] Designing more efficient streaming algorithms with machine learning techniques is a relatively new research topic and we have included more related work in our updated version of the manuscript (highlighted in the blue color).	1.0
Missing baselines [SEP] rebuttal_done [SEP] Since we did not sufficiently emphasize that leveraging Gumbel top-k sampling for learning signal subsampling matrices is part of the novelty of the present work, we clarified this in the revised manuscript.	1.0
Missing baselines [SEP] rebuttal_done [SEP] We cite it in the revised draft.	1.0
Missing baselines [SEP] rebuttal_done [SEP] We have evaluated this baseline in the revision (i.e., the “2-LSTM” baseline).	1.0
Missing baselines [SEP] rebuttal_done [SEP] In the response to reviewer 1, we propose a more sophisticated way to find a path from a fake to a real datapoint.	1.0
Missing baselines [SEP] rebuttal_done [SEP] We have added more to the revised version.	1.0
Missing baselines [SEP] rebuttal_done [SEP] The new method highlights the difference between our method and 1-GP.	1.0
Missing baselines [SEP] rebuttal_done [SEP] We also updated our “Probabilistic Interpretation” section with analysis on how the contrastive loss helps us to learn a disentangled representation.	1.0
Missing baselines [SEP] rebuttal_done [SEP] First, with respect to baselines, we have updated the results tables to include the additional baselines mentioned, as well as runs for VAT(+EntMin) with lower numbers of labels on CIFAR-10.	1.0
Missing baselines [SEP] rebuttal_done [SEP] Specifically, we apply NF-GAN to SN-GAN [*5] and NF-GAN provides a significant improvement on the state-of-the-art inception score on CIFAR-10 (from 8.22 to 8.45).	1.0
Missing baselines [SEP] rebuttal_done [SEP] Finally, we added new results in Table 1 in the revision, which shows that the same technique of negative feedback can further improve the state-of-the-art method of SN-GAN [*5].	1.0
Missing baselines [SEP] rebuttal_done [SEP] In the revised draft, we also consider optimization-based adaptive attacks against our method under the black-box setup (see Table 5) and the white-box setup (see Table 10).	1.0
Missing baselines [SEP] rebuttal_done [SEP] With regards to your comment on attacking the current state of the art method for smoothed classifiers, we have added new results to the resubmission (Appendix B), in which we attack the adversarially trained smooth classifier [1].	1.0
Missing baselines [SEP] rebuttal_done [SEP] With the randomness, our approach is still be able to find meaningful probing policy.	1.0
Missing baselines [SEP] rebuttal_done [SEP] We agree with the referee and will therefore include a visualization of the trained distributions using Gumbel top-k sampling and a realization of the sampling pattern.	1.0
Missing baselines [SEP] rebuttal_done [SEP] We updated Section 4.4 to indicate that it is to be expected that although our SAVP model improves on diversity and realism, it also performs worse in accuracy compared to pure VAE models (both our own ablation and SVG from Denton & Fergus (2018)).	1.0
Missing baselines [SEP] rebuttal_done [SEP] The network architecture is illustrated in Figure 16.	1.0
More experiments needed with related work [SEP] rebuttal_by-cr [SEP] We can include the literature review for training on imbalanced data sets as well as comparison of other methods with negative pre-training in terms of memory use and training complexity in the final version.	1.0
More experiments needed with related work [SEP] rebuttal_by-cr [SEP] Discussion about VEEGAN and Lucas et al. will be added to our next revision.	1.0
More experiments needed with related work [SEP] rebuttal_by-cr [SEP] Defining the best combination of techniques to achieve the highest performance is an interesting direction of future work; our preliminary experiments combining Mean Teacher with manifold regularization have shown some improvements and we will include the results in the final version of the paper.	1.0
More experiments needed with related work [SEP] rebuttal_by-cr [SEP] We will clarify this in the final version.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] In addition, we note that the highest performance in many of the mentioned baselines (and with VAT) are obtained with a combination of multiple approaches.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] Next, [3,4] also assume clean training labels, and aim for detecting abnormal test samples after ’clean’ training.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] 3) Providing fair comparisons across a range of very different methods is not easy when other methods aim to solve a different problem.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] The main difference between our method and [1, 2] is that we do not directly train the Gaussian mixture model, i.e., generative classifier but we post-process it on hidden feature spaces of pre-trained deep models.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] The focus of this work is on problem embedding and its application in a recommendation system that uses problem embedding to project students’ performance for the problems they solved onto the problems that they have not solved yet.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] In addition, we study a robust inference method to handle noisy labels in training samples, while they did not.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] In the meanwhile, we got the interesting idea of negative pre-training on training with imbalanced training data and tested our hypothesis and included in the paper.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] 1- The idea of using concepts to represent a problem is simple, but using it along with neural network based embedding gives us the opportunity to gain concept continuity as discussed on the last paragraph on page 7 and table 2, which is an active field of research in education.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] After updating these baselines, we note that our method still achieves state-of-the-art performance in the regime where 1000 and 2000 labels are used for training on CIFAR-10, with and without data augmentation.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] In that regard, the calculation of point-wise variance (our metric of uncertainty) is possible only using our approach, and therefore a direct comparison is not possible, since other supervised methods (explained below) cannot work in this setting where only set of desired images are available. ** We will clarify this unique aspect in the revised version of the manuscript. **	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] We did not consider comparisons with conventional triplet approaches: the message of our paper was not to demonstrate the utility of ordinal embedding approaches over conventional (representation-based) triplet approaches.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] We are currently running a broader experiment for negative pre-training on other data sets to gain more insight on it, but for the purpose of the task proposed in this work, it outperforms one-shot learning, which cannot be said that is the state-of-the art, but is a common practice. There is no notion of state-of-the-art in training on imbalanced data sets since due to our best knowledge, there is no method that outperforms all the other ones, and the performance of different methods depends more on the nature of the data set.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] When our method is compared head-to-head against the proposed method in the mentioned papers, it is competitive and sometimes outperforms them, for instance, in experiments on CIFAR-10 with 4000 labels	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] Using the evaluation on unseen problems, a problem is recommended that is within the capacity of students close to their boundary to help them learn, and at the same time we cover all the concepts necessary for them to learn.	1.0
More experiments needed with related work [SEP] rebuttal_reject-criticism [SEP] It was rather to show that when input representations are NOT available, we provide a scalable approach to solve the ordinal embedding problem.	1.0
More experiments needed with related work [SEP] rebuttal_done [SEP] Detailed results will be included in our revision.	1.0
More experiments needed with related work [SEP] rebuttal_done [SEP] We have implemented and added a comparison to the Gumbel-Sinkhorn method (Mena et al., 2018), which is a customized representation for permutations with these properties, and requires similar techniques (unsupervised objective, permutation sampling, etc.) in order to learn the latent structure.	1.0
More experiments needed with related work [SEP] rebuttal_done [SEP] Regarding the permutation learning experiment, in response to the feedback, we have revised the main text to clarify the setup.	1.0
More experiments needed with related work [SEP] rebuttal_done [SEP] We have also updated the text to tone down the claims.	1.0
More experiments needed with related work [SEP] rebuttal_done [SEP] First, with respect to baselines, we have updated the results tables to include the additional baselines mentioned, as well as runs for VAT(+EntMin) with lower numbers of labels on CIFAR-10.	1.0
Missing theoretical comparisons [SEP] rebuttal_concede-criticism [SEP] Response #3: We agree that the description of three baselines should be more precise, especially the DNN and DNN(resized) baseline.	1.0
Missing theoretical comparisons [SEP] rebuttal_concede-criticism [SEP] The reviewer is right.	1.0
Missing theoretical comparisons [SEP] rebuttal_concede-criticism [SEP] Thank you for these suggestions.	1.0
Missing theoretical comparisons [SEP] rebuttal_concede-criticism [SEP] We recognize that the focus on parameter reduction was perhaps counter productive to making the goal or this work clear.	1.0
Missing theoretical comparisons [SEP] rebuttal_concede-criticism [SEP] Indeed, the proposed regularizer can be interpreted as certain constraints on the Jacobian at the equilibrium point.	1.0
Missing theoretical comparisons [SEP] rebuttal_concede-criticism [SEP] > We agree that LARS/LASSO could act as potential ways to attain reduced dimensions.	1.0
Missing theoretical comparisons [SEP] rebuttal_concede-criticism [SEP] For our use case, we found the greedy forward selection computationally fast enough (of the order of a few minutes), and we observed a significant portion of accuracy captured in a very few dimensions.	1.0
Missing theoretical comparisons [SEP] rebuttal_concede-criticism [SEP] We will make the definitions clearer.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] To assess transfer learning potential reliably, we require both the X and y for the target task (i.e supervision).	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Many multi-agent problems have been studies using simple robot models (point-mass, etc), where more complex and realistic models have used the problem because significantly more challenging.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] There is a success-rate-efficiency tradeoff in CW, as a smaller binary search step number leads to a lower success rate.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Achieving such feature invariance for high-dimensional feature mapping can be a very weak constraint [1], causing limitation in performance.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] The paper may not make this point obvious enough probably because the curves are too thick to reveal the difference.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] -> We stated that defining a trade-off between objectives is not necessary (in case you are referring to this statement), which would, e.g., be necessary when one would scalarize objectives by using a weighted sum.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We see this methodology, as illustrated by our experiments for one model, as the central part of our contribution.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Perhaps the reviewer is incredulous regarding this large increase in performance a method as 'simple' as CNE achieves w.r.t. the state-of-the-art.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We had experiments with categorical variables, however, we faced training stability issues with them.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We propose a method that uses a form of goal-conditioned RL to learn task agnostic low-level policies that can simplify the share control structure across robots.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Based on the official implementation, after every 10 iterations, if the attack is not successful, we increase the radius of the wasserstein ball in which the noise is projected back onto.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] - The combination of our method and DIAYN enables DIAYN to learn manipulation skills efficiently, while DIAYN alone did not learn.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Note that the reason that the examples are more perceptible than those from [1] is that they are made to produce large certified radii and not only cause misclassification (i.e., the entire Gaussian augmented batch needs to get misclassified.) A comparison of the resulting images and average certified radii of those images can be found in the following anonymized link:	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We believe that this is due to the conceptual advance made in CNE.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Indeed, the Wasserstein attack and the other previously mentioned non-$\ell_p$ bounded attacks are alternatives for producing quasi-imperceptible non-$\ell_p$ bounded adversarial examples.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Recently, generative approaches, following image-to-image translation have been shown achieve better domain adaptation, as this constrains the feature embedding to generate the data in a new domain.	1.0
"Missing theoretical comparisons [SEP] rebuttal_answer [SEP] When talking in more general terms about ""deep learning models"", we refer to the proposed methodology for ""investigating deep learning models"", and don't want to claim that we actually evaluate a representative number of models."	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Therefore, it is most natural to also put the number of parameters on a log scale for LEMONADE.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] For the number of parameters, the log scale is natural to cover a large range of sizes: think of a plot of size vs. performance; in order to see anything for small sizes one would typically put the size on a log scale (and we indeed did, see, e.g., Figures 3 and 4).	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Regarding your first concern on the comparison with CW: In short, MarginAttack is able to achieve a higher attack success rate than CW AND a shorter running time.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Furthermore, compared to MISC, the combined method enjoys the benefits brought by DIAYN, such as learning combinable motion primitive with skill-conditioned policy for hierarchical reinforcement learning [1].	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] To show this point clearly, we would like to refer you to the results in our response to reviewer 3, where we scanned through the number of binary search steps and measure the success rate and running time.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] However, often, an assumption can be made that the robots in the environment share similar morphology.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] However, one major advantage of our attack method over the Wasserstein attack may be its simplicity and scalability.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Instead, we feel the more appropriate baseline is performing tasks from a natural language description, as many prior zero-shot works have, and so we have moved this result to the main text, as noted above.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] In most HRL methods, the lower level can be viewed as part of the environment, yet this restructuring of the environment enables faster and more capable learning.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] This is a fundamental problem of measures that look directly at the input data X without considering the nature of the labels y.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] This property allows us to explore various surrogates, where one such construction allows us to interpret CE as a policy gradient method, and another makes the basis for Cakewalk.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] To the best of our knowledge, our paper is the first to use machine learning to design better sketches for any streaming problem.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] In such a case, unsupervised metrics like clustering, BoW etc. would indicate maximum transfer potential, whereas the actual transfer potential would be close to zero (assuming the lengths of reviews aren’t correlated with the sentiment).	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] One might take inspiration from our framework try to use MAML for zero-shot task performance by transforming task representations would require adopting our meta-mapping framework, as well as a number of ideas of our architecture (where do the task representations come from, and how are they used?), and so its not clear to us that this is an appropriate baseline, rather than simply another implementation of our technique.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] I think the author should have some discussions about these related methods.]	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] As can be seen, MarginAttack has a higher success rate than all the versions of CW.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Rescaling an objective, however, is different as it is independent from other objectives: it only depends on that specific objective and which scale is important to the user and the application.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Thus while our algorithm relies on unsupervised learning, the other algorithms fall under the category of supervised learning.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Being a non-GAN based approach, concepts such idt (encouraging the styling network to behave as an identity when given a target domain instance as input) and revMap (constructing source instance back from generated target instance) are not applicable in this scenario, as no instances or images are being generated from a feature embedding.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] These results show a moderate sensitivity of MuLANN, MADA and DANN wrt hyper-parameters and confirm that MuLANN outperforms both MADA and DANN (detailed results available here https://drive.google.com/file/d/1NjtMKF53qmnx4_Jyvh-ofxb0WjzcDvow/view?usp=sharing).	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] The authors of [1] suggest that the Wasserstein PGD attack works best when the attacker takes PGD steps in $ell_p$-norm directions and then project the noise back onto the Wasserstein ball.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Another big difference between the methods mentioned above and our approach is that while they require image pairs (true and corrupted images) for training, our approach only requires uncorrupted images.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Otherwise, $\phi$ is a stationary point instead of the global optimal point of the regularization term.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] The guidance allows more control over the latent space, even in lack of data.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We investigated the impact of varying the learning rate, the weight lambda on the discriminator loss, the weight dzeta of the known-unknown discrimination loss, the learning rate schedule, lambda schedule as well as using different learning rates for pre-trained layers versus from scratch layers (see Table 5 for more detailed information).	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] In our opinion a conceptual advance that achieves a strong boost in accuracy without increasing complexity, is at least as valuable as a method that achieves the same boost in accuracy while also increasing complexity.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Therefore, introducing the $L$ is equivalent to adding a negative-semidefinite matrix to the jacobian matrix of the original dynamics, which do help to stabilize the dynamics.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] However, even with 10 binary search steps, CW is still unable to outperform MarginAttack in terms of success rate.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] For the sake of the argument, let us assume that the X for both sentiment analysis and sentence length is exactly the same set of movie reviews.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] While we were writing the paper we in fact considered presenting Cakewalk as the reviewer suggests.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] The first paragraph of section 3.2 describes this FiLM model and, given the focus on methodology, we considered the description (plus reference to the paper) sufficient here.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] To the best of our knowledge, deterministic neural networks have not been well studied for the integer factorization problem.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We have in the meantime been able to include struc2vec in the evaluation, again showing superiority of CNE by a wide margin -- showing that it is maybe more complex but certainly not 'stronger' as in 'more accurate'.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Since at the equilibrium, $D(x)=0$ for all x, indicating that the equilibrium is a global optimal point of the negative feedback regularization $L = \lambda \int D^2(x)dx$. Therefore, the Hessian matrix $J = \frac{\partial^2 L}{\partial \phi^2}$ is positive-semidefinite.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] [InfoGAN] Compared to InfoGAN, our method is novel in two ways: First, we use separate networks to obtain the image embeddings, which enables us to guide some of these networks with simple functions.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] In these articles the authors have used methods like Bayesian dropout and variational autoencoder to compute uncertainty in the inferred images.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Consequently, the attack is always able to reach a comparable, but slightly weaker, spoofed certified radii (~ 67% that of the shadow attack) at the cost of slightly more perceptible adversarial noise in difficult cases.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] In [4] deterministic neural networks are used, but are able to factor smaller integers, on a more restricted problem, and are fully trained on the subset of all integers.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Any of these methods can alternatively be used for generating non $\ell_p$ bounded attacks.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] On the other hand, with very small numbers of binary search steps, CW still runs slower than MarginAttack.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Re: (W7) Alternatives to CFS / Computational concerns	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] They can generate adversarial examples whose \ell_p norm is large.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We used their official implementation and adapted it to attack the Randomized Smoothed classifier.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] However, attack in Wasserstein distance and some other methods can also do so.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] •There has been some work on computing the uncertainty in an inferred image within a supervised learning framework where pairs of measured and desired images are used for training the network [1, 2].	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Consider the case where the target task is sentiment analysis, and one of the candidate tasks is finding sentence length (SentLen).	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] Meta-RevGrad tries to achieve feature invariance at the embedding level.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] We eventually decided against this approach as CE is a method for adapting an importance sampler, and its convergence guarantees only apply when it is treated as such.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] The convergence guarantees of REINFORCE on the other hand still apply under our surrogate objective framework.	1.0
Missing theoretical comparisons [SEP] rebuttal_answer [SEP] - The accuracy of the proposed simple model exceeds the accuracy of far more complex models by a wide margin, and this consistently over a range of networks (all commonly used networks in this literature), against a range of baselines (all either commonly used baselines, or methods known as achieving state-of-the-art accuracies), and on two important tasks (link prediction and multi-label classification).	1.0
Missing theoretical comparisons [SEP] rebuttal_social [SEP] A1: Thanks for the interesting suggestion.	1.0
Missing theoretical comparisons [SEP] rebuttal_social [SEP] - We thank the reviewer for suggesting additional comparisons with specific more complex models, although we feel that calling these methods 'stronger' requires some clarification or support.	1.0
Missing theoretical comparisons [SEP] rebuttal_social [SEP] Also note that all code is provided, and we invite the reviewer to replicate our experiments.	1.0
Missing theoretical comparisons [SEP] rebuttal_social [SEP] Hope these results will clarify your major concern.	1.0
Missing theoretical comparisons [SEP] rebuttal_by-cr [SEP] Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.	1.0
Missing theoretical comparisons [SEP] rebuttal_by-cr [SEP] * We will refer to these works in the revised version to better orient reader.	1.0
Missing theoretical comparisons [SEP] rebuttal_by-cr [SEP] The paper will be updated very soon to include these results.	1.0
Missing theoretical comparisons [SEP] rebuttal_by-cr [SEP] However, if our paper is accepted, and you feel that the comparison to MAML for basic meta-learning is useful, we will run MAML on our tasks before the camera-ready submission.	1.0
"Missing theoretical comparisons [SEP] rebuttal_done [SEP] [Experiments Section] We have significantly updated qualitative and quantitative results in our ""Experiments"" section and now compare our methods against Beta-VAE, DIP-VAE, and InfoGAN."	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] We tested PicoSAT, MiniSAT, Dimetheus and CaDiCaL and reported the	1.0
"Missing theoretical comparisons [SEP] rebuttal_done [SEP] We now point this out in our ""Discussion"" section."	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] We tried to cover related work thoroughly in section 2.	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] - We added a few sentences to the end of section 2.4 on our speculative intuition regarding what strategy a model may prefer, and we do indeed think that a pairing-based strategy is plausible for convolution-based networks.	1.0
"Missing theoretical comparisons [SEP] rebuttal_done [SEP] *We have also clarified this within the ""Our Contributions"" Section**"	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] Per your suggestion, we ran experiments using the Wasserstein attack.	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] Thanks for pointing out! This is the same algorithm described above in Mehta et al., but we realize that must have been confusing. Fixed.	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] **We have added this comment in Section 3.1**	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] Taking your comments and comments from other reviewers into account, we have made improvements to the experimental section.	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] These include updating the draft to include (i) a detailed analysis of edits performed on SNLI, (ii) results on various datasets using an ELMo based classifier; (iii) concerning your question about larger Bi-LSTMs, we had tried a large Bi-LSTM but it overfit badly.	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] [Gaussian Prior on Latents] In our new experiments, we used uniform distributions to model the generative factors.	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] results in the updated paper.	1.0
"Missing theoretical comparisons [SEP] rebuttal_done [SEP] This point is now addressed in our ""Related Work"" section."	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] We added the related discussion in Appendix E in the revision.	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] We have updated the draft to include this detail.	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] We’ve updated the paper to mention this.	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] In the revision, we have added explanations on the difference/similarity between DNN (resized) and DNN baselines. And explain why we include them as baselines to compare RAN against in Section 3.1.	1.0
Missing theoretical comparisons [SEP] rebuttal_done [SEP] We’ve included these in an expanded discussion of related work with discussion on how they relate to the current dataset.	1.0
Limited generalizability of claims on other datasets [SEP] rebuttal_answer [SEP] (Q2) Equivariance property of the Alt-az convolution	1.0
Limited generalizability of claims on other datasets [SEP] rebuttal_answer [SEP] Q3: alt-az convolution is not well defined on the south pole	1.0
Limited generalizability of claims on other datasets [SEP] rebuttal_answer [SEP] A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.	1.0
Limited generalizability of claims on other datasets [SEP] rebuttal_answer [SEP] (3) We observed that as \lambda increases, in the validation set, the uncertainty is increasing, while the accuracy is decreasing.	1.0
Limited generalizability of claims on other datasets [SEP] rebuttal_answer [SEP] The results show that the CDN objective produces superior results compared to VI and VIB.	1.0
Limited generalizability of claims on other datasets [SEP] rebuttal_answer [SEP] , a simple heuristic that we use is to choose the highest \lambda that allow high validation accuracy (e.g. > 0.97 on MNIST).	1.0
Limited generalizability of claims on other datasets [SEP] rebuttal_answer [SEP] So	1.0
Limited generalizability of claims on other datasets [SEP] rebuttal_answer [SEP] The experiment validates that DSGAN still works well to create complement data for complicate images.	1.0
I would have liked to see results for both trivial baselines like random ranking as well as more informed baselines where we can estimate transfer potential using say k representation techniques, and then use that to help us understand how well it would do on the other representations. [SEP] rebuttal_answer [SEP] > The random baseline (i.e a random ordering of candidate task) is compared in figure 3 (and all the plots in the appendix), where we plot the accuracy boost using the best task till now in the produced recommendation of candidate tasks using different methods.	1.0
I would have liked to see results for both trivial baselines like random ranking as well as more informed baselines where we can estimate transfer potential using say k representation techniques, and then use that to help us understand how well it would do on the other representations. [SEP] rebuttal_answer [SEP] We can clearly see that the random ordering is much worse compared to informed metrics that use representations.	1.0
I would have liked to see results for both trivial baselines like random ranking as well as more informed baselines where we can estimate transfer potential using say k representation techniques, and then use that to help us understand how well it would do on the other representations. [SEP] rebuttal_answer [SEP] Re: (W3) Baselines for transfer learning:	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] We conduct experiments with decoder p(x|z, m) conditioned on the original mask in training set, and observe comparable performance and convergence time.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Due to the combinatorial nature of the solution space, the examples that have been sampled thus far create a distorted representation of the solution space.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] The result can be highly efficient, meaning the agent needs less samples than sampling from the uniform distribution p(τ).	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Reducing the required sample complexity can result in a better empirical risk minimizer.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] It is the trend of the decline in inference accuracy with pipelining is what we study and this trend exists with both our hyperparameters and those at https://github.com/akamaster/pytorch_resnet_cifar10.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] The CDP framework finds the samples that have large errors based on the ‘surprise’ of the trajectory.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] In this case we could get that some x_i=j will occur few times, while some other x_k will not receive the value j at all.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] This way similar classes (grouped under a superclass) together contribute to learning a general prior representing the superclasses and each superclass also provides “guidance” to better train with the few samples assigned to that superclass.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] (Schaul et al., 2015b).	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] R1A2: What makes few-shot learning particularly difficult compared to common machine learning settings is the dearth of training examples, which results in a bad empirical risk approximation for the expected risk and therefore gives rise to an empirical risk minimizer that is sub-optimal.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] In general, we believe multi-modal data is more general than conventional image-text or video-text pairs.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] We trained the network for only 164 epochs with a batch size of 100, which is probably the reason that its inference accuracy is lower than expected.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] By unifying tabular data also as multi-modal (with each attribute as one modality), we show that VSAE provides us a principled way for imputation, capable of generalizing to more data families.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Specifically, we conducted experiments on two types of data:	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Furthermore, its heavy-tail property also guarantees that samples will be diverse	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] is that they protect against sampling biases.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Therefore, given a very large space of hypotheses H, our goal is to further restrict and constrain H using some prior knowledge because a reduced H has reduced sample complexity and thus requires fewer training samples to be trained.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] We argue that to estimate the integral of the loss function L(τ) of the RL agent efficiently, we need to draw samples τ from the buffer in regions which have a high probability, p(τ), but also where L|(τ)| is large.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] .	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Indeed, this phenomenon is studied in detail in the AdaGrad paper (though without assuming a data distribution), and sparse data like ours (one can say our data points are N indicator vectors of length M) is the first motivating example in their paper.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Any density estimation method that can approximate the trajectory density can provide a more efficient proposal distribution q(τ) than the uniform distribution p(τ).	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Gradient updates such as those of AdaGrad and Adam on the other hand will lessen the impact of such deviations as the importance of each case is inversely proportional to the number of previous observations.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Now if we apply vanilla gradient updates this can skew the sampling distribution in random directions.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] The introduction of this prior knowledge in the form of a supergraph in $C^{GAT}$ during training also helps generalize better to the novel samples that are presented to our model in the fine-tuning stage.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] The reason is that the rank-based variant is more robust because it is not affected by outliers nor by density magnitudes.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] The speedup obtained is 1.73X, slightly higher than the 1.71X obtained in our paper, which could be caused by the batch size increase that makes the GPU process more efficient.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Such experimental results support our claim that the proposed generative classifier can improve the robustness against adversarial attacks as it utilizes multiple hidden features (i.e., harder to attack all of them).	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] - To mitigate the influence of very unusual stochastic transitions, we use the ranking instead of the density directly.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Furthermore, this reasoning explains why AdaGrad is superior to Adam: AdaGrad corrects against sampling biases that entail all the examples that have been encountered, while Adam does this only within some exponentially moving time window.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] - Yes, the experiments are mostly in deterministic domains.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] The exact inference accuracy of the model is somewhat orthogonal to our study.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] We feed our model an equal number of positives and negatives (chosen randomly) at each epoch.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] We provide this “prior knowledge” in the form of a “graph of graphs”, namely our super-graph $g^{sup}$, which captures both the latent inter-class and intra-class relationships between classes.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Luckily, modeled as latent random variables, an information term could be added to the objective as in [1, 2].	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] The sampling mechanism should have a property of oversampling trajectories with larger errors/‘surprise’.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Should we adopt the hyperparameters (a batch size of 128) and more training epochs (200 epochs) as shown at https://github.com/akamaster/pytorch_resnet_cifar10 , our ResNet-110 baseline reached 93.59% in inference accuracy, and the pipelined ResNet-110 reached 92.88% in inference accuracy.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] One of the limitation we see from NADPEx is that dropout policies are not directly interpretable from their network structures, while interpretability and composibility are prerequisites for reusing them in more complicated tasks.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Observe that in $g^{sup}$, we build a k-NN graph PER super-class, restricting any flow of information between super-classes, thus further restricting H. We force our model to jointly learn both the superclass and graph class labels.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] In both setups, our inference method is shown to be more robust compared to the softmax inference.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Since, p(τ) is a uniform distribution, i.e., the agent replays trajectories at random, we only need to draw samples which has large errors L|(τ)|.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] One intuitive explanation for why an algorithm that maintains a ‘memory’ of previous gradient updates like AdaGrad or Adam is required	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] For n < 10K, after 300 epochs the model has seen at most 3M negatives out of 15M, and yet still generalizes to the unseen negatives.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] Consider for example the case when the execution is at the start, and the sampling distribution still has maximum entropy.	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] The mask distribution might be easier to learn as compared to data distribution (since the mask is fully-observed)	1.0
design choices are questionable [SEP] rebuttal_answer [SEP] As such deviations will inevitably occur whenever we rely on polynomially sized samples to represent a combinatorial solution space, without such corrections a gradient based adaptive sampling algorithm will almost surely fail.	1.0
Too strong assumptions in analysis [SEP] rebuttal_answer [SEP] Specifically, the noise will add a term to the coefficient update in Lemma 2, and will effect the threshold, tau.	1.0
Too strong assumptions in analysis [SEP] rebuttal_answer [SEP] In the main results, we focus on rule-based agents because it is computationally demanding to train a large population of RL agents, and our focus was not about the worker policies but rather how the manager assesses the workers’ mental states and encourages an optimal collaboration accordingly.	1.0
Too strong assumptions in analysis [SEP] rebuttal_answer [SEP] On the other hand it is a first step towards a multilayer analysis and allows a localized layer-by-layer analysis for the first time.	1.0
Too strong assumptions in analysis [SEP] rebuttal_answer [SEP] Under the noisy case, the recovered dictionary and coefficients will converge to a neighborhood of the true factors, where the neighborhood is defined by the properties of the additive noise.	1.0
Too strong assumptions in analysis [SEP] rebuttal_answer [SEP] Nevertheless, the proposed algorithm can tolerate i.i.d. sub-Gaussian noise, including Gaussian noise and bounded noise, as long as the ``noise’’ is dominated by the ``signal’’.	1.0
Too strong assumptions in analysis [SEP] rebuttal_answer [SEP] In other words, the noise terms will lead to additional terms which will need to be controlled for the convergence analysis.	1.0
Too strong assumptions in analysis [SEP] rebuttal_answer [SEP] In this paper, using a cheap rule-based implementation with randomness has demonstrated the effect of different components of our approach.	1.0
Too strong assumptions in analysis [SEP] rebuttal_answer [SEP] 1. Noise Tolerance — NOODL also has similar tolerance to noise as Arora et. al. 2015 and can be used in noisy settings as well.	1.0
Too strong assumptions in analysis [SEP] rebuttal_answer [SEP] For the dictionary, the noise will result in additional terms in Lemma 9 (which ensures that the updated dictionary maintains the closeness property).	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_by-cr [SEP] We will work on the writing in the future version of this work.	1.0
1. The presentation is somewhat convoluted. [SEP] rebuttal_by-cr [SEP] We will also clarify that the little phrase “After initial experimentation, we opted for the simple proxy…” implies quite extensive experimentation with other plausible proxies that looked promising in individual environments but were not consistently effective across the suite of Atari games.	1.0
1. The presentation is somewhat convoluted. [SEP] rebuttal_by-cr [SEP] Comment 2:	1.0
incorrect claims for related work [SEP] rebuttal_concede-criticism [SEP] Again, we apologize for the confusing use of “ES” abbreviation.	1.0
incorrect claims for related work [SEP] rebuttal_concede-criticism [SEP] After second thoughts, this is absolutely right.	1.0
incorrect claims for related work [SEP] rebuttal_concede-criticism [SEP] As the reviewer says, in both this work and Khadka & Tumer, the RL updates lead to policies that may differ a lot from the search distribution and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_concede-criticism [SEP] We agree that it is essential to justify how the reconstruction error works as a measure of privacy in this paper.	1.0
Incorrect claims in introduction (confusing related work presented in intro, wrong claims) [SEP] rebuttal_done [SEP] We understand your concern and have made the title more specific.	1.0
Incorrect claims in introduction (confusing related work presented in intro, wrong claims) [SEP] rebuttal_done [SEP] We tested PicoSAT, MiniSAT, Dimetheus and CaDiCaL and reported the	1.0
Incorrect claims in introduction (confusing related work presented in intro, wrong claims) [SEP] rebuttal_done [SEP] Tentatively, we chose: “Convergence Properties of Deep Neural Networks on Separable Data”.	1.0
incorrect claims for related work [SEP] rebuttal_done [SEP] We corrected the paper according to this new insight.	1.0
incorrect claims for related work [SEP] rebuttal_done [SEP] We have also updated the text to tone down the claims.	1.0
incorrect claims for related work [SEP] rebuttal_done [SEP] In the abstract of the revised draft, we report our improvement over Co-teaching [5] which is the most recent and state-of-the-art training method.	1.0
incorrect claims for related work [SEP] rebuttal_done [SEP] First, with respect to baselines, we have updated the results tables to include the additional baselines mentioned, as well as runs for VAT(+EntMin) with lower numbers of labels on CIFAR-10.	1.0
incorrect claims for related work [SEP] rebuttal_done [SEP] -Statement about “general class of trees” replaced by “all weighted or unweighted trees”.	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_done [SEP] In the revision, we have added the following justification on privacy quantification in Section 2, Section 4 and Section 5.	1.0
Incorrect assumptions as compared to related work [SEP] rebuttal_done [SEP] k-addition definiteness in the spherical setting: we have added the formal condition that the k-addition be well-defined, and a proof that for two points this condition indeed recovers x != y / (k ||y||^2) - see Theorem 1.	1.0
claims on the datasets is questionable [SEP] rebuttal_done [SEP] Upon request, we have included more extensive experiments following [1] on MNIST/FashionMNIST, and [2] on CMU-MOSI/ICT-MMMO.	1.0
claims on the datasets is questionable [SEP] rebuttal_done [SEP] As suggested by multiple reviewers, we have conducted further experiments on the Gigaword corpus to test PDR on larger corpora.	1.0
claims on the datasets is questionable [SEP] rebuttal_done [SEP] We have now evaluated the adversarial resistance throughout the article for 1000 images randomly selected from the 10000 MNIST test images.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_followup [SEP] We would be appreciated if the reviewer could provide us with the references that showing using only sigmoid could control such a challenging problem of adversaries.	1.0
Correctness of algorithm proposed is questionable [SEP] rebuttal_followup [SEP] We currently thinking about an experiment to better illustrate the intuition of our theory and would appreciate any suggestions.	1.0
incorrect claims for related work [SEP] rebuttal_future [SEP] Finally, as for the further evaluation (such as multiple seed runs and 2nd-momentum estimates), we agree it is interesting, but it is very demanding in computational resources, and we leave it for a systematic future investigation.	1.0
incorrect claims for related work [SEP] rebuttal_future [SEP] We believe it will be a valuable future research direction.	1.0
incorrect claims for related work [SEP] rebuttal_future [SEP] A result of these second thoughts is that one could definitely build an ERL algorithm where the evolutionary part is replaced by CEM.	1.0
claims on the datasets is questionable [SEP] rebuttal_future [SEP] Here we are mostly focusing on relatively clean data (WSJ, TIMIT, Librispeech) following the original wav2vec paper but we would be interested in exploring robustness in the future.	1.0
claims on the datasets is questionable [SEP] rebuttal_future [SEP] At some point we may be interested in actually investigating the same for real-world data, but we think it's unclear right now what exactly such evaluation data should ideally look like, what problems are most interesting, what details to pay attention to. Artificial data allows us to investigate these questions while avoiding the elaborate and expensive process of obtaining real-world data.	1.0
design choices are questionable [SEP] rebuttal_refute-question [SEP] The intuition for NADPEx is given in Section 3.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_social [SEP] It is relatively easy to update our results in the paper with new hyperparameters.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_social [SEP] Please kindly let us know a pointer.	1.0
incorrect claims for related work [SEP] rebuttal_structuring [SEP] - Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?	1.0
incorrect claims for related work [SEP] rebuttal_structuring [SEP] Q3: You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.	1.0
incorrect claims for related work [SEP] rebuttal_structuring [SEP] With augmentation:	1.0
incorrect claims for related work [SEP] rebuttal_structuring [SEP] The reviewer also raises doubts about the fact that the method of Khadka & Tumer (2018) cannot be extended to use CEM.	1.0
incorrect claims for related work [SEP] rebuttal_structuring [SEP] Adversarial Dropout [1] (11.32) vs ours (11.79 +/- 0.25)	1.0
incorrect claims for related work [SEP] rebuttal_structuring [SEP] Without augmentation:	1.0
incorrect claims for related work [SEP] rebuttal_structuring [SEP] Improved GAN + SNTG [2] (14.93) vs ours (14.34 +/- 0.17)	1.0
incorrect claims for related work [SEP] rebuttal_structuring [SEP] Q3: How does negative feedback (NF) influence the training of stable dynamics and further evaluation:	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] Q5: Clarifying values of $k$	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] In regards to why we compare the errors on natural images and those of our adversarial images:	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] 5.  “Figure 6(a) [Figure 5(a) in revised draft] clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.”	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] 4. Missing BLEU scores & the number of parameters:	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] Q6.Typos:	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] I think this point should be discussed in the paper.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] > However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] That is, the issue still exists, and Dreamer is less effective with very long horizon.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] This horizon length is still short compared to the entire horizon length of many MDPs (e.g., 1000).	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] 1. Clarification on Fig. 4.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] (2) Multimodal Experiments:	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] Vacuous bounds in the regime \beta >1.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] Results are reported in Table 10 and Table 11 (Appendix C.5).	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] * Why does the MMD version constitute an improvement? Or is it simply more stable to train?	1.0
"Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] Also, Figure 6 is referenced in the text in the context of binary multiplication (""[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6""), but presents results for addition and factorization only."	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] [R1: In Table 2, it is not clear to me what is the point for comparing errors of the natural images (which measures the misclassification rate of a natural image) and that of the adversarial images (which measures successful attacks rate), and why this comparison helps to support the claim that the attack results in a stronger certificates.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] >In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says “losses”.	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] [R1: In Table 1, for ImageNet, Shadow Attack does not always generate adversarial examples that have on average larger certified radii than the natural parallel, at least for sigma=0.5 and 1.0. Could the authors explain the reason?]	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?	1.0
Incomplete ablation in terms of tables and figures [SEP] rebuttal_structuring [SEP] * Incomplete definition of the metrics:	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_structuring [SEP] R: - FSM vs. Classification performance	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_structuring [SEP] Interpretability of the generative scores:	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_structuring [SEP] See e.g. Irie et al. Interspeech 2019 for better result.	1.0
"correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_structuring [SEP] ""solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization """	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_structuring [SEP] Next, we’ll try to provide some intuition as to why a sampling distribution that assumes independence between the different dimensions can be useful in some cases.	1.0
correctness of results presented is questionable(eg., metrics, complexity, etc) [SEP] rebuttal_structuring [SEP] >> The state of the art on LibriSpeech is not Mohamed at al. 2019.	1.0
Too strong assumptions in analysis [SEP] rebuttal_structuring [SEP] - Q: Algorithm applied layer-by-layer:	1.0
Too strong assumptions in analysis [SEP] rebuttal_structuring [SEP] Q3: The Lip constraints on the discriminator:	1.0
Too strong assumptions in analysis [SEP] rebuttal_structuring [SEP] The convergence analysis is on Z, not on parameters x and hyper-parameters theta.	1.0
Too strong assumptions in analysis [SEP] rebuttal_structuring [SEP] 2.  “It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images. At least this limitation should be pointed out in the paper…	1.0
Too strong assumptions in analysis [SEP] rebuttal_structuring [SEP] Q1: About the main concern on “novelty, improvement relative to the current state of the art implementations,  and non-linearity of G and D”:	1.0
Too strong assumptions in analysis [SEP] rebuttal_structuring [SEP] 2. What is the reason for using rule-based agents in all the experiments?	1.0
generalizability of method on datasets created from different distributions is questionable [SEP] rebuttal_summary [SEP] One of the main advantages of pGAN is the possibility of generating poisoning attacks at scale with detectability constraints capable of targeting large deep networks, where strategies relying on bilevel optimization have a limited applicability.	1.0
generalizability of method on datasets created from different distributions is questionable [SEP] rebuttal_summary [SEP] In our case, for the experiment with MNIST in Figure 2, we used a deep neural network with more than 40,000,000 parameters, 1,000 training points, injecting up to 400 poisoning points.	1.0
generalizability of method on datasets created from different distributions is questionable [SEP] rebuttal_summary [SEP] (Q4) Munoz-Gonzalez et al. (2017) showed an experiment using a Convolutional neural network with 450,000 parameters, trained with 1,000 training points and injecting 10 poisoning points.	1.0
generalizability of method on datasets created from different distributions is questionable [SEP] rebuttal_summary [SEP] This is not the case for pGAN, which is capable of bypassing different defences, including the outlier detection scheme proposed by Paudice et al. (2018a).	1.0
generalizability of method on datasets created from different distributions is questionable [SEP] rebuttal_summary [SEP] Although defences based on outlier detection can be bypassed, as shown by Koh et al. (2017) (Stronger poisoning attacks break data sanitization defences), the complexity of the bilevel problem significantly increases compared to Munoz-Gonzalez et al. (2017).	1.0
generalizability of method on datasets created from different distributions is questionable [SEP] rebuttal_summary [SEP] On the other side, Paudice et al. (2018a) showed that, in many cases, if we don’t consider appropriate detectability constraints, the attack points generated by optimal attack strategies formulated as bilevel optimization problems can be effectively filtered out with appropriate outlier detection, resulting in blunt attacks.	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] However, the results show that our proposed method does not make the network be overfitted to test data as the Reviewer doubted because the difference between the accuracy for validation set and test set are consistent with the values from the previous works.	1.0
"claims on the datasets is questionable [SEP] rebuttal_summary [SEP] (1) low-dimensional tabular data, and (2) high-dimensional data (pixel or text) as ""multimodal"" to better define the overall task of learning from partially-observed data."	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] To avoid using the test set in the retraining process as the Reviewer pointed out, we randomly selected 5K validation images among the original 50K training images in CIFAR-10 dataset, and applied our scheme.	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] Pruning [1]                         11.4                         12.2	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] Then, we observed the training and validation accuracy at each training epoch, and measured the test accuracy once after training.	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] From the Table 2, we can see that our proposed scheme maintains the accuracy of the uncompressed baseline network.	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] On the other hand, the CIFAR-10 dataset does not include a separate validation set, so we had to use the test set in the retraining process.	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] Therefore, we believe that our proposed compression method does not suffer from the concerned overfitting problem regardless of the types of neural networks or dataset.	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] VWM (Ours)	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] 11.4                         12.4	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] Baseline	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] 11.5                         12.2	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] The test accuracy in the above table is about 1 % less than the accuracy which we reported in the originally submitted manuscript because the number of training data was decreased as part of the data set is used as a validation set.	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] As shown, VSAE consistently outperforms baseline models across the added experiments as well.	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] Compression scheme   Validation Error (%)    Test Error (%)	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] Note that even the uncompressed baseline network exhibits similar accuracy difference between the validation error and the test error compared with the compressed networks.	1.0
claims on the datasets is questionable [SEP] rebuttal_summary [SEP] In fact, in case of PTB and Wikitext-2 corpus, we already used the provided validation set and measured the test PPW only once after training (Table 2) in the original manuscript.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] To some extent, they play the role of the former: they inform researcher intuition and lay a solid foundation for scientific dialogue.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] We apply plus-one smoothing to handle such words.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] Currently, there are 5 datasets in our experiments.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] The lookup table is formed as the mapping of token (only topic words) index to image id.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] Then, the retrieval method is applied as the tensor indexing from the sentence token (only topic words) index to image ids, which is the same as the procedure of word embedding.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] We believe that the total of our results makes a complete conference paper.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] We have found that this leads to worse performance, likely because of the domain shift.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] Learning image representations takes only about 2 minutes for all the 29,000 images in Multi30K using 6G GPU memory for feature extraction and 8 threads of CPU for transforming images.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] The extracted features are formed as the “image embedding layer” with the size of (29000, 2400) for quick accessing in neural network.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] The extra computation is negligible.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] This knowledge (e.g. the superiority of trees to chains, the sensitivity of layout induction to initialization, the emergence of spurious parameterization in end-to-end learning), will guide researchers in choosing, designing and troubleshooting their models, as they now know what to expect modulo the optimization challenges that they may face.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] The retrieved image ids are then sorted by frequency.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] The time of obtaining image data for MT sentences for EN-RO dataset, for example, is approximately less than 1 minute by tensor operation in GPU.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] The field of language understanding with deep learning is not easily amenable to mathematical theoretical investigations and, with that in mind, rigorous minimalistic studies like ours are arguably very important.	1.0
Lacking details on datasets [SEP] rebuttal_answer [SEP] The candidate sentences generated by MT systems may contain words that never appear in the test set.	1.0
Missing implementation details of related work used as baselines [SEP] rebuttal_answer [SEP] Importantly, our purpose in this task is to show that, **all other things being equal**, a neuromodulated plastic LSTM can outperform a standard LSTM in realistic settings.	1.0
Missing implementation details of related work used as baselines [SEP] rebuttal_answer [SEP] We believe that outperforming standard LSTMs (again, all else being equal) on their “workhorse” task domain (language processing) is worthy of notice, especially given the ease of implementation of our method which requires only adding a few lines of codes (<10) to a standard LSTM implementation and can then be used as a drop-in replacement to standard LSTM.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] In those cases there is generally an upper bound on a penalty function that should never be exceeded, including during training itself.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] While our approach can similarly be applied to upper bounds on penalties, there’s unfortunately no guarantee that the constraints will be satisfied at every moment during training, but only at convergence.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] Our current implementation only allows up to 5 images in a single 2015 TITANX GPU with 12GB memories.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] yes but not necessarily. Alpha can be used to control the expected proportion of non-zero entries, but as long as the probability of a sparse configuration is random uniform, our mechanism guarantees that any sampled index is almost orthogonal to any other sampled index, so it's easier achieve the same while guaranteeing the sparsity in the inputs.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] As such it is not clear how these methods would apply to our specific experimental setups.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] For testing, there might be overlapping during sampling due to the limit length of video.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] For training, we have between 20k and 40k samples for all models, and beyond these numbers we don’t see much improvement.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] However, our V4D inference algorithm in section 3.4 guarantees that only the non-overlapping action units will interact with each other during testing.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] Each image takes about 2.3GB memory on average, and most of the memory is consumed by the CNN features and matrix operation.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] These algorithms generally restrict policy updates to remain within the constraint-satisfying regime.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] We pick alpha steps uniformly within the ranges (shifts and rotations are integer steps).	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] We had 7500 images in total.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] It is indeed the case that constrained MDPs are often considered in safe RL.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] But it is straightforward to concatenate multiple 5-frame segments to reconstruct a complete sequence, which is demonstrated in the comparison with CodeSLAM in Figure 7 of the revised version.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] A1: We pick the ranges using two criteria: qualitatively acceptable and quantitatively under a fixed threshold for FID score.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] We had 3 concept classes, and 2500 images for each concept.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] This is because we implemented the whole pipeline using tensorflow in python, which is memory inefficient, especially during training.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] 2.During training, we uniformly divide the whole video into U sections and randomly select one action unit from each section.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] It is also straightforward to implement our BA-Layer in CUDA directly to reduce the memory consumption of matrix operation and push the number of frames.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_answer [SEP] So there are no overlaps for training.	1.0
Missing details for reproducibility of result [SEP] rebuttal_answer [SEP] We used PyTorch as the framework for implementing all the network modules.	1.0
Missing details for reproducibility of result [SEP] rebuttal_answer [SEP] Typically it took < 10 hours to get a converged result by our approach on a single Nvidia Tesla V100 GPU.	1.0
Missing details for reproducibility of result [SEP] rebuttal_answer [SEP] For instance, the work by Saxe et al in 2013 is entitled “Exact solutions to the nonlinear dynamics of learning in deep linear neural networks”.	1.0
Missing details for reproducibility of result [SEP] rebuttal_answer [SEP] Specifically, the game environment and the worker agents were implemented in Python and it runs at a speed of more than 300 steps per second.	1.0
Missing details for reproducibility of result [SEP] rebuttal_answer [SEP] It usually refers to the evolution of weights and outputs of neural networks throughout training.	1.0
Missing details for reproducibility of result [SEP] rebuttal_answer [SEP] Yes, we do plan to open source our implementation.	1.0
Missing implementation details of related work used as baselines [SEP] rebuttal_by-cr [SEP] Moreover, we will ultimately release codes on Github.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_by-cr [SEP] We will mention the total number in the main text.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_done [SEP] With MNIST and CIFAR.	1.0
Lack of experimental details (no of images, sampling criteria, etc) [SEP] rebuttal_done [SEP] R) Now we have a pytorch version of the code at https://github.com/adapconv/adaptive-cnn	1.0
Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc) [SEP] rebuttal_future [SEP] As a potential future work, we will seek for continuous representations of the update schedules and end-to-end training methodologies, as arisen in recent works [2].	1.0
Lacking details on datasets [SEP] rebuttal_reject-criticism [SEP] 1. We aimed to provide a broad variety of example applications (playing tennis, walking, fencing, dancing), while mainly focusing on the most complicated (tennis) application, for a thorough analysis of our method.	1.0
Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc) [SEP] rebuttal_summary [SEP] In the end, we found the heat-map more informative.	1.0
Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc) [SEP] rebuttal_summary [SEP] (3) We have shown the analysis of the translation quality against noise in Figures 4 and 5.	1.0
Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc) [SEP] rebuttal_summary [SEP] We also generated line plots (multiple curves in one plot) and 3D mesh plots to show the dependency.	1.0
Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc) [SEP] rebuttal_summary [SEP] It can be seen that the generated graphs show the similar randomness pattern as the real graphs.	1.0
Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc) [SEP] rebuttal_summary [SEP] In Figure 5 (see in the supplementary material), each logarithm plot in each column show the power-law trend of each randomly generated graph, which will look linear in such a logarithm plot.	1.0
Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc) [SEP] rebuttal_summary [SEP] Moreover, the larger the graph is (see the graph size of 150), the smaller the randomness is, and the clearer the power-law trend is, which verifies that the translation quality of our method.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_reject-request [SEP] No, the equation is correct as stated.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_reject-request [SEP] Although GAN-based models show promising imputation results, they usually fail to model data distribution properly.	1.0
Missing details on methodology (eg., use of notation, use on tasks, etc) [SEP] rebuttal_reject-request [SEP] Therefore, we do not consider them as our baseline models.	1.0
