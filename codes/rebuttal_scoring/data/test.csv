descs	scores
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_concede-criticism [SEP] In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.	0.3508773619358619
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_concede-criticism [SEP] 1) For the presentation, we apologize for our typos and unclear statement in the paper. And your advice is so helpful. We will modify it.	0.649122638064138
"3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] R: ""The paper can benefit from a proofreading."""	0.1334172456785002
"3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] Q3. Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)"	0.1712191370351586
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] > I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me.	0.2439874587384383
"3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way."""	0.4513761585479028
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_reject-criticism [SEP] We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.	0.1975795937322861
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_reject-criticism [SEP] We did our best to write the paper such that it includes all details needed to fully understand the proposed method and its theoretical background.	0.2815511087403978
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_reject-criticism [SEP] The referee indicates (in sharp contrast to referee 3) that the paper is not nicely written, nor easy to follow.	0.5208692975273159
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.	0.0166666666666666
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.	0.026618503397122
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] A: Thanks, we indeed corrected serveral typos like this in the new version of the paper.	0.044354014210802
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] - We have carefully proofread the manuscript and fixed the typos in the revised version. Could reviewer be more specific about the odd formulations, so that we can improve them?	0.0481233166545627
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We fully agree with this criticism. Following your suggestion (and also that of reviewer # 2) we have moved some material from the appendix to the main text.	0.0600752592671472
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We have modified our expression, typos and grammar errors.	0.1302674973265725
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] - To improve the clarity of the paper, we move the exact set-up into the earlier section, Section 2.1 “Environments”.	0.18515567822868
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We updated the paper to make this clear in Section 3, under Importance Weighting.	0.2344794189742683
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] >> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.	0.2542596452741784
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.	0.3508773619358619
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] Thus we decided to feature the per-iteration comparison in the main paper, as it is the cleanest one.	0.649122638064138
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_done [SEP] >> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.	0.3508773619358619
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_done [SEP] New suggested structure and related suggestions: These are nice suggestions and explain why the structure was confusing. We’ve worked on these to come close to the suggested structure.	0.649122638064138
Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc) [SEP] rebuttal_done [SEP] We renamed all the models based on the original papers and their properties.	0.3508773619358619
Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc) [SEP] rebuttal_done [SEP] The results are presented in Appendix A.1.	0.649122638064138
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_concede-criticism [SEP] In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.	0.1334172456785002
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_concede-criticism [SEP] We agree much detail on embeddings can be condensed or moved to Appendix.	0.1712191370351586
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_concede-criticism [SEP] We included embedding details in the paper because a reviewer from the venue we previously submitted to requested these details to be in the paper.	0.2439874587384383
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_concede-criticism [SEP] Adding a table that summarizes referred gradient penalties is a good idea.	0.4513761585479028
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] 1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.	0.0640340158384878
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] Based on the Reviewer’s comments, we added more description about the schemes we adopted from [1] and [2] in Appendix A.1 and A.2 of the revised manuscript.	0.0731054190644934
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] We revised the notations in the paper to make formulation clearer.	0.0855332424172482
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] 2. We extended the Related works section to include papers which address the mode collapse problem.	0.1037089558670764
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] A table summarizing the referred gradient penalties is also added.	0.1330930952284554
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] 1. A background section is added with basic information about GANs and a definition of generalization.	0.1896577481857959
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] 3. Another MNIST experiment is added to Section 6.1 to further demonstrate the effectiveness of our method in preventing overfitting.	0.3508675233984424
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] We added new experiments showing the agent can learn to manipulate two balls.	0.163813582448939
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] The simulation will be released with the work for others to use and build on multi-agent learning methods.	0.2334347317478641
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] >> Comment #8, #9	0.2972101275128807
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] Thus, our sentence the reviewer refers to.	0.3055415582903162
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_structuring [SEP] Remark 1. Expression and detail	0.1975795937322861
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_structuring [SEP] We would like to clarify one of the central points of this paper, as the cons presented are built upon a misunderstanding of this point.	0.2815511087403978
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_structuring [SEP] For the astrophysics setting, we provide more information in the appendix, Sec. 5, and for the medical application we refer to [1] for full details.	0.5208692975273159
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_by-cr [SEP] 5. We will add more details about the experiments to the appendix.	0.1975795937322861
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_by-cr [SEP] Thank you for that suggestion: we will update the organization of the paper to make the main body more self-contained.	0.2815511087403978
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_by-cr [SEP] We will publish the code to compute conductance after the blind-review phase.	0.5208692975273159
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] (6) We updated the appendix to address this. See “Training convergence” in Appendix D.2	0.0465545656078372
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] 5. Implementation details are added to the appendix.	0.0515009778334933
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.	0.057754681517639
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] We have appropriately revised the two claims and cited them in the latest version.	0.0659366434715179
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] In the new revision, we have added a discussion section to make a case for this.	0.0771459723731007
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] Fig. 3 and other evaluations have been updated for the new test set.	0.0935396615821408
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] In the paper, we include many details on the environment rewards and design as we consider these simulation tasks part of the contribution of the work.	0.1200428239093447
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] 6. We added the analysis for the 'mode jumping' problem to Section 6.2.	0.1710613384561072
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] 5. We have added detailed hyperparameter settings for CW and EAD in the revision in the supplemental materials.	0.3164633352488186
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] [A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.	0.1334172456785001
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] The updated paper will split Figure 5 into two to increase clarity.	0.1712191370351586
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.	0.2439874587384383
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Also, we have added the sentence ``Models are fitted independently for the noisy image, the noiseless image, and the noise.'', and rewrote the paragraph	0.4513761585479028
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] > Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.	0.0540159036531358
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] [Q] Clarification and exposition of plots.	0.0605749135945383
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] [R1: The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table.]	0.0691563081565013
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] I hope to see some discussions about this. Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.]	0.0809128579180634
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] Q4: Clarification of Figure-4 (Section-4.2)	0.0981068757205273
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] 1. Better explanatory texts for natural statistics comparison.	0.1259039706128623
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] > I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.	0.1794135120688543
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] Q3. Figure 1 is too abstract:	0.3319156582755169
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_by-cr [SEP] We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.	0.0375
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_by-cr [SEP] - Table 3 is indeed confusing, this is a good point. We will correct it.	0.3208333333333333
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_by-cr [SEP] We will certainly consider renaming the approach and fixing this in Table 2.	0.3208333333333333
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_by-cr [SEP] we were trying to cram different experiments (with different regularization) in the same figure which is understandably confusing and needs to be corrected.	0.3208333333333333
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] Thanks for this; we have updated the draft to make the presentation clearer.	0.0979214001226644
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] Thanks also for catching several typographic errors. We have addressed them in the new draft.	0.0979214001226644
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] We have added the missing labels in the latest version of the paper. Thank you for pointing it out.	0.0979214001226644
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] For the Figure 4, we added the description of the underlined numbers.	0.1395377459145343
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] We have modified the caption for Fig. 2 and text in Sec 2.4 to be more clear about the natural statistics analysis.	0.1988412131123173
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] We also added additional explanation for 'D' of Figure 2 in its caption.	0.3678568406051547
Unclear intro (eg. contributions) [SEP] rebuttal_done [SEP] We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.	0.3508773619358619
Unclear intro (eg. contributions) [SEP] rebuttal_done [SEP] We have added a reference to what we mean by the classical approach in the related works section.	0.649122638064138
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).	0.0781068773141067
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] 1) For the presentation, we apologize for our typos and unclear statement in the paper. And your advice is so helpful. We will modify it.	0.0913848418340474
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] (Notations) Thank you for commenting this.	0.1108039684446293
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] We apologize for the confusions in Section 3.1.	0.1421984327951226
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] That said, we agree it is worth investigating the performance of LSTM on this problem further.	0.2026331680715508
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] We agree with your point regarding the wealth of graph neural network studies.	0.3748727115405429
Unclear description of method [SEP] rebuttal_answer [SEP] The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.	0.0125
Unclear description of method [SEP] rebuttal_answer [SEP] Regarding the first point, your intuition is exactly correct and a slightly simpler discussion of this phenomenon can be found in [1].	0.013465909090909
Unclear description of method [SEP] rebuttal_answer [SEP] Your interpretation of section 3 is exactly right.	0.0146105113636363
Unclear description of method [SEP] rebuttal_answer [SEP] It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.	0.0199518426944514
Unclear description of method [SEP] rebuttal_answer [SEP] >> Comments #1, #11	0.0326243548626046
Unclear description of method [SEP] rebuttal_answer [SEP] - In order to make it easier for readers to understand the differences between different models and how they are related to InfoNCE, we have added a summary in Table 1.	0.0563276246510066
Unclear description of method [SEP] rebuttal_answer [SEP] Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)	0.081503167024879
Unclear description of method [SEP] rebuttal_answer [SEP] As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.	0.1107918890191514
Unclear description of method [SEP] rebuttal_answer [SEP] We set parameters as follows.	0.1174174757408574
Unclear description of method [SEP] rebuttal_answer [SEP] As to the online setting, thanks for pointing us to the “short-horizon bias” paper.	0.1499825266873704
Unclear description of method [SEP] rebuttal_answer [SEP] For more detail on PER, please refer to the original PER paper [Schaul et al 2016].	0.1578781556310944
"Unclear description of method [SEP] rebuttal_answer [SEP] 5. ""Additionally, in section 6.4, the results in Figure 2 also does not look very"	0.232946543234039
Unclear description of method [SEP] rebuttal_social [SEP] We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.	0.3508773619358619
Unclear description of method [SEP] rebuttal_social [SEP] >>> Thanks for pointing out the details.	0.649122638064138
"Unclear description of method [SEP] rebuttal_structuring [SEP] ""Some technical details are missing."	0.0361205226470929
Unclear description of method [SEP] rebuttal_structuring [SEP] Q8. Terminology consistency through the paper:	0.0439971324540729
Unclear description of method [SEP] rebuttal_structuring [SEP] ** Clarification on Mathematics in Section 3.1 **	0.0452351291483727
Unclear description of method [SEP] rebuttal_structuring [SEP] Q2: Regarding other minor comments	0.0486719111217291
Unclear description of method [SEP] rebuttal_structuring [SEP] Q7. Attention should be given to the notation in formulas (3) and (4):	0.0497758621656242
"Unclear description of method [SEP] rebuttal_structuring [SEP] It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way."""	0.0606264773903007
Unclear description of method [SEP] rebuttal_structuring [SEP] p2-3, Section 3.1 - I found the equations impossible to read. What	0.0709331087404761
Unclear description of method [SEP] rebuttal_structuring [SEP] Abstract node notations? It is not clear in the model section.	0.0860065096042962
Unclear description of method [SEP] rebuttal_structuring [SEP] Not sure why Eqns. 2 and 9 need any parentheses	0.1103750498642193
Unclear description of method [SEP] rebuttal_structuring [SEP] - the sentence under eq. (2)	0.1572841641560967
Unclear description of method [SEP] rebuttal_structuring [SEP] 2. Each component described in Figure 1 is not explained enough.	0.2909741327077189
Unclear description of method [SEP] rebuttal_by-cr [SEP] In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.	0.1334172456785002
Unclear description of method [SEP] rebuttal_by-cr [SEP] A2: Thanks for your notification! We will polish our paper and rewrite the corresponding part in the next revision.	0.1712191370351586
Unclear description of method [SEP] rebuttal_by-cr [SEP] We will edit Appendix F to include more detailed information and cover this important point.	0.2439874587384383
Unclear description of method [SEP] rebuttal_by-cr [SEP] 2) Thanks for pointing this out, we’ll correct it in the next revision.	0.4513761585479028
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).	0.0991274512750463
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] We agree that setup we selected is destined to fail, but it was done on purpose to illustrate the presence of spurious solutions.	0.1201921274650668
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] => We believe that both are valuable insofar as generating discussion within the community and leading to follow-up experimentation.	0.1542466413058904
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] In addition to that, we have shown that curiosity could be a very strong baseline to compare against in future papers.	0.219801450657675
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] Note that proposing a generator architecture is not the goal of this paper.	0.4066323292963215
Unclear description of method [SEP] rebuttal_future [SEP] While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.	0.1975795937322861
Unclear description of method [SEP] rebuttal_future [SEP] Since this is the first paper on this topic, we chose to focus on introducing the problem and providing Transformer-based approaches as a baseline for future work.	0.2815511087403978
Unclear description of method [SEP] rebuttal_future [SEP] This is indeed what our results show in Figure 5 in the appendices, illustrating our metric for individual samples.	0.5208692975273159
Unclear description of method [SEP] rebuttal_done [SEP] 5. We have added further empirical evidence to show that in the revision.	0.0092301840745655
Unclear description of method [SEP] rebuttal_done [SEP] 1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.	0.0098380788826152
Unclear description of method [SEP] rebuttal_done [SEP] This should provide a more intuitive introduction to our work, whilst maintaining cornerstone concepts.	0.0126102918291057
Unclear description of method [SEP] rebuttal_done [SEP] - We move the exact setup (Section “2.1 Environments” in the new version) in early sections to improve the clarity of the paper.	0.0128387086497842
Unclear description of method [SEP] rebuttal_done [SEP] Nevertheless your troubles compelled us to restate the meaning of this notation at the time of its first usage.	0.0133147183385375
Unclear description of method [SEP] rebuttal_done [SEP] We have added clarifying comments at the beginning of section 4.	0.0140399822227822
Unclear description of method [SEP] rebuttal_done [SEP] We corrected the notations of section 3 to match with later sections.	0.0141509534716246
Unclear description of method [SEP] rebuttal_done [SEP] We fully agree with this criticism. Following your suggestion (and also that of reviewer # 2) we have moved some material from the appendix to the main text.	0.0143842327868277
Unclear description of method [SEP] rebuttal_done [SEP] -> Added a comment in the newly written “Scope” section in the revision	0.0146842289061167
Unclear description of method [SEP] rebuttal_done [SEP] We have reorganized this section, as shown in our updated paper.	0.0149987964850367
Unclear description of method [SEP] rebuttal_done [SEP] We have updated the introduction to rephrase and clarify the lower bound claims.	0.015119186213374
Unclear description of method [SEP] rebuttal_done [SEP] In our revision we have made an effort to outline this in greater detail.	0.0153487895666089
Unclear description of method [SEP] rebuttal_done [SEP] We have updated the paper with these recommendations in Section 7.	0.0160389417470078
Unclear description of method [SEP] rebuttal_done [SEP] 9. We incorporated the changes as you suggested.	0.0178055972863306
Unclear description of method [SEP] rebuttal_done [SEP] We edited the text to address variables more gently and to explain the arrow sign.	0.0181656415006547
Unclear description of method [SEP] rebuttal_done [SEP] We updated the paper to make this clear in Section 3, under Importance Weighting.	0.0184303916704701
Unclear description of method [SEP] rebuttal_done [SEP] We have added a detailed description on the data setting to Section 6.3 of the supplementary material.	0.0185848956464771
Unclear description of method [SEP] rebuttal_done [SEP] Indeed, thank you. We have updated the text to place more emphasis on this contribution.	0.0187907276152829
Unclear description of method [SEP] rebuttal_done [SEP] + We corrected as many typos as we could find, we would be very thankful for pointing out any further typos!	0.0195462244543918
Unclear description of method [SEP] rebuttal_done [SEP] The added/modified text are highlighted in the blue color.	0.0201916970101509
Unclear description of method [SEP] rebuttal_done [SEP] -> Thanks, we agree; we re-organized our paper accordingly.	0.0222050650205771
Unclear description of method [SEP] rebuttal_done [SEP] A2: Thank you for your careful review in catching these mistakes. We have updated the typo in the revision.	0.0233455023956278
Unclear description of method [SEP] rebuttal_done [SEP] This miscommunication also encouraged us to change our signs to the curly version as suggested by you.	0.0258289206816832
Unclear description of method [SEP] rebuttal_done [SEP] eqn (8), (12): thanks for pointing these out, we will fix this in the paper.	0.0291891144549257
Unclear description of method [SEP] rebuttal_done [SEP] Thanks for pointing out -- we apologize for misusing “exploding or vanishing gradients” and have revised the paper to be accurate.	0.0365323052197634
Unclear description of method [SEP] rebuttal_done [SEP] In the discussion in section 5, we also analyze how the error may affect the tasks downstream.	0.0366200780323099
Unclear description of method [SEP] rebuttal_done [SEP] We will produce a reworked introduction where graphs play a larger role.	0.0371760698396647
Unclear description of method [SEP] rebuttal_done [SEP] We will update the labels in the ablation table to make this more clear.	0.0436251564841517
Unclear description of method [SEP] rebuttal_done [SEP] We have updated notations in Equations 1 and 2.	0.046878921055783
Unclear description of method [SEP] rebuttal_done [SEP] We have added the detailed PPO-based training algorithm in Appendix A.1.	0.0568407413219494
Unclear description of method [SEP] rebuttal_done [SEP] 3. You are right, it is a quite weak attack and we have removed it from the table (just mention it in the text).	0.0695745206232902
Unclear description of method [SEP] rebuttal_done [SEP] A: For space reasons we provided only a short description of CTAugment, and how it differs from AutoAugment. We will include a longer treatment in the appendix.	0.0926577305102254
Unclear description of method [SEP] rebuttal_done [SEP] Also, for an easy understanding of the whole CL process with DiVA, we added another figure in Appendix E.	0.1714136060023021
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_concede-criticism [SEP] [A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.	0.3508773619358619
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_concede-criticism [SEP] We agree that some of our observations under constant epoch budgets in sections 2 and 3 have been made in previous work.	0.649122638064138
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_answer [SEP] We provide clarification for the two main questions of the Reviewer below.	0.3508773619358619
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_answer [SEP] We thank the reviewer for pointing us to the work of Huand et al. Indeed this is an interesting method that would allow us to most likely achieve similar or better results with less computational overhead.	0.649122638064138
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.	0.1334172456785002
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] See ensuing discussion in p.18 following equation 36.	0.1712191370351586
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] After taking a close look, we make the following observations:	0.2439874587384383
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] Finally, another contribution is that we propose a discretized algorithm.	0.4513761585479028
Not enough originality in results (not surprising) [SEP] rebuttal_structuring [SEP] > “The results are not strong. And, unfortunately, the model contribution currently is too modest.”	0.1975795937322861
Not enough originality in results (not surprising) [SEP] rebuttal_structuring [SEP] [Q] Limited amount of new insight.	0.2815511087403978
Not enough originality in results (not surprising) [SEP] rebuttal_structuring [SEP] More detailed comments are addressed below.	0.5208692975273159
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.	0.0758318876649472
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] All of these insights are supported by a fair and unbiased rigorous experimental process.	0.1238000069772663
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] Although our primary contributions are empirical, we also provided a detailed theoretical discussion in section 2, where we give a clear and simple account of why the two regimes arise.	0.1238000069772663
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models.	0.1238000069772663
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] We would also like to emphasize that we make a significant contribution to the debate regarding SGD and generalization.	0.1939535207626336
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] Additional experiments in the appendix G go further and study how the optimal learning rate schedule changes as we increase the epoch budget.	0.35881457064062
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_structuring [SEP] > I am reluctant to give a higher score due to its incremental contribution.	0.1975795937322861
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_structuring [SEP] More specifically, we would like to draw the reviewer's attention to the following two papers:	0.2815511087403978
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_structuring [SEP] We found it important to revisit this topic in the light of recent substantial improvements to dynamics models (see below).	0.5208692975273159
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.	0.3508773619358619
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] Besides the important technical difference described above, we highlight the empirical performance of Dreamer.	0.649122638064138
In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion. [SEP] rebuttal_reject-criticism [SEP] On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models.	0.3508773619358619
In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion. [SEP] rebuttal_reject-criticism [SEP] All of these insights are supported by a fair and unbiased rigorous experimental process.	0.649122638064138
Incremental novelty of method as compared to related work [SEP] rebuttal_concede-criticism [SEP] We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:	0.3508773619358619
Incremental novelty of method as compared to related work [SEP] rebuttal_concede-criticism [SEP] Two reviewers complained that it was difficult to tell from the text which contributions are novel and which also appear in previous works.	0.649122638064138
Incremental novelty of method as compared to related work [SEP] rebuttal_mitigate-criticism [SEP] Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).	0.3508773619358619
Incremental novelty of method as compared to related work [SEP] rebuttal_mitigate-criticism [SEP] 3. We clarify the differences to some other recent papers in our reply to reviewer 1.	0.649122638064138
Incremental novelty of method as compared to related work [SEP] rebuttal_done [SEP] It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.	0.1334172456785002
Incremental novelty of method as compared to related work [SEP] rebuttal_done [SEP] Therefore, we updated Figure 7 in original manuscript to Figure 6c in the updated manuscript according to the new data.	0.1712191370351586
Incremental novelty of method as compared to related work [SEP] rebuttal_done [SEP] The assumptions used for the simulation and analysis data have been updated in Figure 6c of the revised manuscript.	0.2439874587384383
Incremental novelty of method as compared to related work [SEP] rebuttal_done [SEP] Turning to our generalization experiments in sections 4 and 5.	0.4513761585479028
"Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] 3. ""Given the existing body of literature, I found the technical novelty of this paper rather weak"""	0.1975795937322861
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] Please refer to Section 3.1 and Section 3.4 for more details.	0.2815511087403978
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] Turning to our generalization experiments in sections 4 and 5.	0.5208692975273159
Suggest missing related work [SEP] rebuttal_answer [SEP] See ensuing discussion in p.18 following equation 36.	0.3508773619358619
Suggest missing related work [SEP] rebuttal_answer [SEP] After taking a close look, we make the following observations:	0.649122638064138
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_concede-criticism [SEP] We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:	0.3508773619358619
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_concede-criticism [SEP] Two reviewers complained that it was difficult to tell from the text which contributions are novel and which also appear in previous works.	0.649122638064138
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_done [SEP] It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.	0.3508773619358619
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_done [SEP] Turning to our generalization experiments in sections 4 and 5.	0.649122638064138
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.	0.3508773619358619
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] We consider our contributions in Secs. 4 and 5 as one step toward that final goal.	0.649122638064138
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] We provide clarification for the two main questions of the Reviewer below.	0.1975795937322861
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] But this description mischaracterizes the fundamental problem that we have identified and proposed a solution for.	0.2815511087403978
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] 6. While Theorem 4.7 is new and may be of independent interest in the optimization community,  it is not the main contribution in this paper.	0.5208692975273159
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] 1. Comments about the contributions and novelty	0.1975795937322861
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] More detailed comments are addressed below.	0.2815511087403978
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] Q1: Robot design were explored in (Sims, 1994) etc. The novelty of the paper is fairly incremental.	0.5208692975273159
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.	0.0465545656078372
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] As a combination of problem setting and proposed solution, we do believe we have addressed an important problem, and made a novel contribution.	0.0515009778334933
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.	0.057754681517639
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Please note that what’s done in Secs. 4 and 5 is not straight-forward and has not been reported before.	0.0659366434715179
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] If the reviewer thinks our method is an incremental contribution or similar to previous algorithms, please list the specific references.	0.0771459723731007
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] A2: Please refer to Q1.	0.0935396615821408
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] To further showcase our work with respect to prior art, we added (Sims, 1994) as an additional baseline in the latest revision.	0.1200428239093447
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Please check the degraded images in Table 3.	0.1710613384561072
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] We believe that judging the novelty of a NAS paper solely by its architecture space is unfair.	0.3164633352488186
Limited novelty in theoretical contribution [SEP] rebuttal_mitigate-criticism [SEP] We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).	0.3508773619358619
Limited novelty in theoretical contribution [SEP] rebuttal_mitigate-criticism [SEP] We believe this combination method could be used for other things, and have presented it here as a proof of concept rather than a definitive survey with all possible uses.	0.649122638064138
Limited novelty in theoretical contribution [SEP] rebuttal_done [SEP] We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.	0.1975795937322861
Limited novelty in theoretical contribution [SEP] rebuttal_done [SEP] Nevertheless, we have added a comparison between our method and the exact solution for a small Boltzmann machine of 16 units in the appendix.	0.2815511087403978
Limited novelty in theoretical contribution [SEP] rebuttal_done [SEP] By mentioning this statement, if the reviewer means the missing of some principled approaches like ODIN in our comparisons, we would like to inform the inclusion of ODIN results in the revised version of the paper.	0.5208692975273159
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] After taking a close look, we make the following observations:	0.1975795937322861
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] See ensuing discussion in p.18 following equation 36.	0.2815511087403978
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] We highlight the differences to DGR [3] in the Sec. 2 of our work.	0.5208692975273159
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_structuring [SEP] 1. We first want to point out the main contributions of the paper.	0.3508773619358619
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_structuring [SEP] Q1. Comparison with [1, 2, 3, 4].	0.649122638064138
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_done [SEP] We clarified this in Section 2.1 of the revised draft.	0.1975795937322861
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_done [SEP] * In relation to the connection to IWAE, we have included a detailed discussion in Appendix E.1.	0.2815511087403978
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_done [SEP] We now have added related work about video compositional methods in section 2.3 in the second version of the paper.	0.5208692975273159
While sensible, this seems to me to be too minor a contribution to stand alone as a paper. [SEP] rebuttal_done [SEP] We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.	0.3508773619358619
While sensible, this seems to me to be too minor a contribution to stand alone as a paper. [SEP] rebuttal_done [SEP] We also added a second artificial dataset to further analyze the behavior of the approaches.	0.649122638064138
Missing explanation of utility of proposed method [SEP] rebuttal_done [SEP] We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.	0.3508773619358619
Missing explanation of utility of proposed method [SEP] rebuttal_done [SEP] Moreover, we have detailed some limitations of Euclidean spaces in Appendix B.	0.649122638064138
Limited insights based on design choices [SEP] rebuttal_answer [SEP] We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.	0.3508773619358619
Limited insights based on design choices [SEP] rebuttal_answer [SEP] After examining the KTH results further, we realized that our results are likely weaker than they should have been, because we did not use the same preprocessing as prior work.	0.649122638064138
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] [Q] Limited amount of new insight.	0.1975795937322861
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.	0.2815511087403978
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] a. Limited contribution to the qualitative understanding of the optimizers	0.5208692975273159
Limited insights based on design choices [SEP] rebuttal_by-cr [SEP] We will publish the code to compute conductance after the blind-review phase.	0.3508773619358619
Limited insights based on design choices [SEP] rebuttal_by-cr [SEP] We are currently rerunning the KTH experiments and we plan to update the results in the paper.	0.649122638064138
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] All of these insights are supported by a fair and unbiased rigorous experimental process.	0.0991274512750463
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] We did our best to write the paper such that it includes all details needed to fully understand the proposed method and its theoretical background.	0.1201921274650668
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.	0.1542466413058904
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models.	0.2198014506576749
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] The referee indicates (in sharp contrast to referee 3) that the paper is not nicely written, nor easy to follow.	0.4066323292963214
Limited insights based on design choices [SEP] rebuttal_done [SEP] In the new revision, we have added a discussion section to make a case for this.	0.1975795937322861
Limited insights based on design choices [SEP] rebuttal_done [SEP] Moreover, we have detailed some limitations of Euclidean spaces in Appendix B.	0.2815511087403978
Limited insights based on design choices [SEP] rebuttal_done [SEP] We added Section 3.5 to point out the differences between the VAE component of our model and prior work.	0.5208692975273159
Limited contribution in problem definition [SEP] rebuttal_reject-criticism [SEP] We are of course willing to further specify any details that the referee misses in the current paper. We would therefore like to kindly invite the referee to be specific about the details that he/she would like to be added to the manuscript.	0.3508773619358619
Limited contribution in problem definition [SEP] rebuttal_reject-criticism [SEP] We would first like to refer the referee to third paragraph of the introduction, where we explicitly formulate the main shortcoming of compressed sensing:	0.649122638064138
Not enough originality in results (not surprising) [SEP] rebuttal_accept-praise [SEP] A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.	0.0
Incremental novelty of method as compared to related work [SEP] rebuttal_answer [SEP] We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.	0.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_concede-criticism [SEP] [A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.	0.0
Limited novelty in theoretical contribution [SEP] rebuttal_concede-criticism [SEP] It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.	0.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_done [SEP] We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).	0.0
Suggest missing related work [SEP] rebuttal_done [SEP] * In relation to the connection to IWAE, we have included a detailed discussion in Appendix E.1.	0.0
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_mitigate-criticism [SEP] 3. We clarify the differences to some other recent papers in our reply to reviewer 1.	0.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] As we explained at the common response, we started our research from clear open questions.	0.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.	0.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] However, our work differs in two major ways:	0.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] We have added a sentence in the introduction emphasizing this crucial point.	0.0
"Incremental novelty of method as compared to related work [SEP] rebuttal_structuring [SEP] In response to your comment that ""similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts"" we would like to take this opportunity to clarify the novelty of our approach."	0.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_structuring [SEP] [Q] The only issue with this paper is its degree of novelty, which is narrow.	0.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.	0.0
"While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited. [SEP] rebuttal_answer [SEP] Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."	0.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] We think generating such justifications is a great next step and hope that our work will foster such interesting future research.	0.0
"Missing explanation of utility of proposed method [SEP] rebuttal_answer [SEP] Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."	0.0
Limited impact of results [SEP] rebuttal_done [SEP] We will update the labels in the ablation table to make this more clear.	0.0
In terms of method, the guidance for learning Siamese networks are designed heuristically (e.g. edges, colors, etc.) which limits its applicability over various datasets; I think that designing more principled approach to build such guidances from data should be one of the key contributions of the paper. [SEP] rebuttal_mitigate-criticism [SEP] Further, we added our unsupervised analyses to show that the method works even without explicit guidance on all tested datasets.	0.0
Limited insights based on design choices [SEP] rebuttal_refute-question [SEP] *Please also see reply to reviewer #2 on a similar question of evaluating against other methods*	0.0
Limited impact of results [SEP] rebuttal_reject-criticism [SEP] [A]  We respectfully disagree.	0.0
Limited contribution in problem definition [SEP] rebuttal_structuring [SEP] Re. somewhat limited contribution:	0.0
"Missing explanation of utility of proposed method [SEP] rebuttal_structuring [SEP] Concerning the point "" It is not even clear that the final compression of the baselines would not be better."	0.0
Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training. [SEP] rebuttal_structuring [SEP] Following the reviewer’s suggestion, we have added a figure that shows the dynamics of neuromodulation in the cue-response task (Figure 3, in the Appendix).	0.0
Limited contribution in problem definition [SEP] rebuttal_summary [SEP] In terms of GR, we are trying to address the two open questions mentioned above.	0.0
"Improper presentation of results in tables and figures [SEP] rebuttal_answer [SEP] 5. ""Additionally, in section 6.4, the results in Figure 2 also does not look very"	0.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] Q2: The main contribution is listed as follows:	0.0
Lack of discussion on datasets (size, motivation for use, etc) [SEP] rebuttal_answer [SEP] Please see the last paragraph in page 5.	0.0
-The experimental section do not clarify the benefits of the proposed approach. [SEP] rebuttal_answer [SEP] Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)	0.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_concede-criticism [SEP] Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).	0.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_concede-criticism [SEP] A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.	0.0
Incorrect citation styles [SEP] rebuttal_done [SEP] We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.	0.0
Lack of discussion of analysis [SEP] rebuttal_done [SEP] In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.	0.0
Lack of discussion on datasets (size, motivation for use, etc) [SEP] rebuttal_mitigate-criticism [SEP] So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.	0.0
Unclear description of method [SEP] rebuttal_refute-question [SEP] >>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.	0.0
Unclear problem definition [SEP] rebuttal_structuring [SEP] Q2. It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.	0.0
Unclear description of method [SEP] rebuttal_summary [SEP] In terms of GR, we are trying to address the two open questions mentioned above.	0.0
Unclear description of method [SEP] rebuttal_reject-request [SEP] Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.	0.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_concede-criticism [SEP] Thanks very much for the suggestions.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_concede-criticism [SEP] 1) For the presentation, we apologize for our typos and unclear statement in the paper. And your advice is so helpful. We will modify it.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_concede-criticism [SEP] A : We apologize to the reviewer for the lack of clarity in the manuscript.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_concede-criticism [SEP] 2) For the presentation: we will try to modify it.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] - Q: How do Section 2 & 3 fit together?	1.0
"3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] Q3. Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)"	1.0
"3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way."""	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] Now,	1.0
"3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] 1. ""The clarity is below average. In Section 2 the main method is introduced. However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately."	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] “This paper is a slightly difficult read [...] because there is not one main coherent argument or goal for the paper.[...]. Yes the different sections are related but it is does not feel like they build upon each other to help form a clearer picture of memorization within neural networks.”	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] Question 3:	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] “I think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area... For instance, the omission of a results discussion section or a conclusion are clearly not reader friendly”	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] 3.  “Some parts of the paper feel long-winded and aimless….In general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] > I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] 5. Grammar mistakes and typos.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] WRITING:	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] Remark 2. Expression, and details (ex. number of iterations, stopping criteria, typos and grammar errors)	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_structuring [SEP] For example	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_reject-criticism [SEP] The referee indicates (in sharp contrast to referee 3) that the paper is not nicely written, nor easy to follow.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_reject-criticism [SEP] We therefore think it is natural to study both of them.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_reject-criticism [SEP] Although it is true that our paper can roughly be divided into two section, we want to stress that these sections are inextricably linked due to the nature of their topics, since we see invariance as a limit case of inverse stability.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_reject-criticism [SEP] We did our best to write the paper such that it includes all details needed to fully understand the proposed method and its theoretical background.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] “What would have been nicer would be to perform some sort of ablation study, where the authors studied how the representational capacity of the network changed as a result of them introducing the universal linear transmission layer.”	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] A: Thanks, we indeed corrected serveral typos like this in the new version of the paper.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We discuss this in more detail below, and hope this should clarify any misunderstandings.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We updated the section to address all of your feedback.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] Hereby, \tilde\mu is a noise-corrupted version of the new parameters \mu, and we obtain a limit on the mutual information between \mu and D. We simplified Figure 2 and 8 to make this more clear.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We realized that the naming was very confusing and consequently, we renamed \tilde\theta to \tilde\mu in the noise-injected model.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We updated the paper to make this clear in Section 3, under Importance Weighting.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We have modified our expression, typos and grammar errors.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] In this section, we also redefine the “state” based on your suggestions.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] >> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] - To improve the clarity of the paper, we move the exact set-up into the earlier section, Section 2.1 “Environments”.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We fully agree with this criticism. Following your suggestion (and also that of reviewer # 2) we have moved some material from the appendix to the main text.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] - We have carefully proofread the manuscript and fixed the typos in the revised version. Could reviewer be more specific about the odd formulations, so that we can improve them?	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_done [SEP] We tried to fix grammatical mistakes as much as possible in the revision.	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] Re: (W8) The proposed  CLF weight difference method has some concerning aspects as well. For example say we had two task with exact opposite labels.	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] Empirically, with random parameter initialization most experiments manage to converge and give fairly good controllers.	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] However, as we have elaborated in the text below Eq.2, to reduce the variance and stabilize the training, we have made the following adaptations referring to previous works [1,2]:	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] - Substitute a moving average B (defined in text) from the reward	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] Moreover, AutoLoss is not restricted to REINFORCE, but open to any off-the-shelf policy optimization method, e.g. for large-scale tasks such as NMT, we introduce PPO to replace REINFORCE, and adjust the reward generation scheme accordingly (see the paragraph “Discussion”).	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] - Another note: to perform a full wall-clock comparison with algorithms that have different per-iteration costs, one must disentangle and retune various hyperparameter choices, most notably the learning rate schedule.	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] >> Comment #1	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] We have also updated Table.1 to show the variance.	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] Thus we decided to feature the per-iteration comparison in the main paper, as it is the cleanest one.	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] - Clip the final reward to a given range	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] Almost all main results are averaged over multiple runs as explicitly indicated in the main text and the table or figure captions (e.g. see captions of Table.1 and Fig.2).	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] See Fig.2 and Fig.3(R) where vertical bars indicate variances.	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_answer [SEP] We empirically found the two techniques significantly stabilize the controller training.	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_done [SEP] A3: We have changed the word to “impressive” in the revision. However, DSO-NAS indeed outperforms DARTS on ImageNet dataset as illustrated in Table2.	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_done [SEP] “What would have been nicer would be to perform some sort of ablation study, where the authors studied how the representational capacity of the network changed as a result of them introducing the universal linear transmission layer.”	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_done [SEP] New suggested structure and related suggestions: These are nice suggestions and explain why the structure was confusing. We’ve worked on these to come close to the suggested structure.	1.0
Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc) [SEP] rebuttal_done [SEP] We also improved clarity in the revised version.	1.0
Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc) [SEP] rebuttal_done [SEP] The results are presented in Appendix A.1.	1.0
Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc) [SEP] rebuttal_done [SEP] In addition, we have also included experiments where we condition on only 2 frames instead of 10 frames, in order to test on a setting with more stochasticity.	1.0
Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc) [SEP] rebuttal_done [SEP] We fixed those issues to match the preprocessing used by Denton & Fergus (2018).	1.0
Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc) [SEP] rebuttal_done [SEP] We’ve updated the related works section in our recently posted draft to more carefully compare  Please see Sections 1 and 7 for updated related work.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_concede-criticism [SEP] We included embedding details in the paper because a reviewer from the venue we previously submitted to requested these details to be in the paper.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_concede-criticism [SEP] Thanks for pointing out the issues with our presentations.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_concede-criticism [SEP] Adding a table that summarizes referred gradient penalties is a good idea.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_concede-criticism [SEP] We agree much detail on embeddings can be condensed or moved to Appendix.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] 1. A background section is added with basic information about GANs and a definition of generalization.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] We have revised our paper to address your concerns as follows:	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] The writing of this part and the whole paper was revised.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] In addition, we added more details about the data as you suggested.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] 2. We extended the Related works section to include papers which address the mode collapse problem.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] Based on the Reviewer’s comments, we added more description about the schemes we adopted from [1] and [2] in Appendix A.1 and A.2 of the revised manuscript.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] We revised the notations in the paper to make formulation clearer.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] 3. Another MNIST experiment is added to Section 6.1 to further demonstrate the effectiveness of our method in preventing overfitting.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] A table summarizing the referred gradient penalties is also added.	1.0
Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc) [SEP] rebuttal_done [SEP] We’ve updated the related works section in our recently posted draft to more carefully compare  Please see Sections 1 and 7 for updated related work.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] To better characterize Gaussian mean field inference on the original model, we aim to find an inference procedure on p’ so that both algorithms result in exactly the same outcome, e. g. the same calculations are executed when running the corresponding program.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] Regardless of the parallelization techniques, the maximum speedup of a 2-GPU system is 2X compared to a 1-GPU system.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] >> Comment #5	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] In [1], the authors investigate several features and develop a controller that can adaptively adjust the learning rate of the ML problem at hand, similarly in a data-driven way.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] We note that [4] is a state-of-the-art method which is able to help GAN to scale to massive datasets and it is used in BigGAN paper.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] - We first scale the intrinsic and the extrinsic reward between 0 and 1 and then use equal weights for these two rewards.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] They are the same -- there is a typo leading to confusion in the sentence “...in Figure 1 where we set different \lambda in l_2 = \lambda |\Theta|_2...”; which should be “...in Figure 1 where we set different \lambda in l_2 = \lambda |\Theta|_1...”.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] Hence, AutoLoss fits into more problems (as we’ve shown in the paper).	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] This should avoid placing too much emphasis on the cleaner images in the beginning of the MNIST test set.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] Communication between these two partitions is necessary to enable activation and gradient transfers.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] - Yes, MISC can deal with the case, when there are multiple objects of interest.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] Our particular configuration allows our method to be decentralized, making the individual network for each agent more straightforward.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] For data parallelism, a model is duplicated and placed onto 2 GPUs, each GPU containing a full copy of the model.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] For the imagenet experiment, we used the code from [4] which is available on github.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] By contrast, AutoLoss offers a more generic way to parametrize and learn the update schedule.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] We are also interested in generalization to different numbers of agents after training, which is also problematic for centralized methods.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] Thanks for pointing us to these two works.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] >> Comment #8, #9	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] In short, decentralized learning will allow for more general methods, and HRL enables the learning of sophisticated controllers.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] To obtain a close to perfect speedup of 2X, the communication overhead must be almost non-existent and the workload needs to be perfectly balanced between the 2 GPUs.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] On the other hand, for pipelined parallelism, a model is divided into two partitions (on the assumption that it cannot fit in a single device): one is mapped onto GPU 0 while the other is mapped onto GPU 1, each GPU obtaining only a part of the model.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] In our implementation, we obtained a speedup of 1.81X for ResNet-362, which is equivalent to 90% utilization of each GPU.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] The code for all experiments will be released after the review process.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] - the adapted, noise-injected model p’ has the structure \mu -> \tilde\mu -> D (containing a bottleneck).	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] - the original, noise-free model p has the structure \theta -> D (no bottleneck) while	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] Note that only if generative and inference model are adapted simultaneously we end up with equivalence.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] We show that there is such an inference procedure on the noisy model, and it has the character of MAP.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] The simulation will be released with the work for others to use and build on multi-agent learning methods.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] In [2], the authors propose to manually balance the training of G and D by monitoring how good G and D are, assessed by three quantities and realized by simple thresholding.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] Hereby, \mu (the mean of the Gaussian q) and \theta (the original parameter in p) correspond to \mu (the MAP point-mass of q’) and \tilde\mu (the noise-injected version of \mu in p’).	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] Thus, our sentence the reviewer refers to.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] On the other hand, our GAN-0-GP is robust to the problem and is able to produce better interpolation between modes.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_answer [SEP] - For the opposite situation, we can use negative mutual information rewards to encourage the agent to learn to “avoid” some objects.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_structuring [SEP] For the astrophysics setting, we provide more information in the appendix, Sec. 5, and for the medical application we refer to [1] for full details.	1.0
"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_structuring [SEP] > ""However, the real-world experiments are not necessarily the easiest to read."""	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_structuring [SEP] Now,	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_structuring [SEP] We would like to clarify one of the central points of this paper, as the cons presented are built upon a misunderstanding of this point.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_structuring [SEP] 1. the difficult to train the network	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_structuring [SEP] Comment 3:	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_structuring [SEP] The reviewer notes the need to emphasize how and why to use this approach.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_by-cr [SEP] Thank you for that suggestion: we will update the organization of the paper to make the main body more self-contained.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_by-cr [SEP] We will publish the code to compute conductance after the blind-review phase.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] 5. We have added detailed hyperparameter settings for CW and EAD in the revision in the supplemental materials.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] In addition, we have simplified the architecture described in the paper by combining the networks $\sigma$ and $\omega$, and included the results from this architecture.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] In the paper, we include many details on the environment rewards and design as we consider these simulation tasks part of the contribution of the work.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] Hereby, \tilde\mu is a noise-corrupted version of the new parameters \mu, and we obtain a limit on the mutual information between \mu and D. We simplified Figure 2 and 8 to make this more clear.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] We realized that the naming was very confusing and consequently, we renamed \tilde\theta to \tilde\mu in the noise-injected model.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] See Figures 2,3,5 for more learning curve results and baseline comparisons and Figure 6 for qualitative metric analysis.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] We have appropriately revised the two claims and cited them in the latest version.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] 4. WGAN-GP is included to our ImageNet experiment.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] We have fixed it in the latest version.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] We showed that GAN-0-GP-sample suffers from the problem.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] 6. We added the analysis for the 'mode jumping' problem to Section 6.2.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] Fig. 3 and other evaluations have been updated for the new test set.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] We have now evaluated the adversarial resistance throughout the article for 1000 images randomly selected from the 10000 MNIST test images.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] In the new revision, we have added a discussion section to make a case for this.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] 5. Implementation details are added to the appendix.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_done [SEP] We show that our method outperforms the baselines across multiple environments.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] The x-axis was scaled according to the number of updates.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] The experiment you suggest (picking the best hyper-parameter after the first X episodes) is exactly what we investigated in Figure 5 (left subplot).	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] A normalized rank of 1 corresponds to all the N outcomes (seeds) of a variant being ranked at the top N positions in the joint ranking.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] In other words, given a computing budget, which model should you pick?	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] The “fixed reference” is described in Appendix C, and corresponds to the most commonly used settings in the literature.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] How many hyperparameter settings would you need to consider to achieve a certain quality?	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] REINFORCE however is sensitive to the objective values, and it appears that Adam somewhat mitigates this problem.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] -This means that of all the models we trained for the study presented in Table 1 which did not use Orthogonal Regularization, only 16% were amenable to truncation.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Not surprisingly, both Cakewalk and OCE work best with it.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Also, we have added the sentence ``Models are fitted independently for the noisy image, the noiseless image, and the noise.'', and rewrote the paragraph	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Of all the models which we trained for the study presented in Table 2 which did use Orthogonal Regularization, 60% were amenable to truncation.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] but it is not far off, and it handily outperforms untuned arms, allowing us to remove some of the hyper-parameter tuning burden.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Figure 4 then further aggregates these normalized ranks across 15 Atari games.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] The FID from the plot is the estimate of the min FID computed by bootstrap estimation and the line-plots show this relationship.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] All outcomes are then jointly ranked, and the corresponding ranks are averaged across seeds.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] The empirical result is that it works well for some games but not others, and better for some modulation classes than others, but overall it’s not reliable.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] In this case, our data is a classical use which is explored in the AdaGrad paper uses as a motivating example.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Furthermore, in Figure 1, for the FID distribution plots, we can group the methods visually (according to the loss function) by drawing a slightly shaded rectangle around results with the same loss (e.g. https://goo.gl/6YeUL1).	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] - The 3d plot conceptually represents class-specific one mode Gaussians.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] The bandit is not guaranteed to reproduce the performance of the best arm for a couple of reasons: (a) the signal f(z) it obtains is noisy, (b) if is myopic in that it reflects only current performance not future learning, and (c) the dynamics are non-stationary, so the best arm changes over time.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Models that capture image priors well might not transfer to spectrograms or raw waveforms.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] All the outcomes are then jointly ranked, and the ranks are averaged across seeds.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Unlabelled data refers to only the domain of the meta-test data, but the meta-test data is never used in meta-training.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Finally, these averaged ranks are normalized to fall between 0 and 1.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] 1/ Figure 4: We have added labels and the sentence ``Early stopping can mildly enhance the performance of DD; to see this note that in panel (a), the minimum is obtained at around 5000 iterations and not at 50,000.'' in the caption to clarify.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] L_da is essentially the sum of L_gan and L_cycle.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] - Methods that apply a surrogate objective work best with AdaGrad.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Note that these joining rankings are done separately per subplot (ie modulation class).	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] [A] Say that you had access to a GPU and had to train a model (loss+penalty+architecture).	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Our aim was to show that in the case where the human-engineered topology needs to be preserved, it is better to co-evolve the attributes and controllers with NGE rather than only training the controllers (controllers are trained from scratch for both NGE and baselines).	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] The bandit does not generally do better than the best fixed arm in hindsight -- in general, this would still need to be identified --	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Being able to tell from the classifier output (using e.g. the confidence) if a set of images comes from the training or the validation set is a good indicator of how much the network has memorized these images.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] However, this is not as effective as applying a surrogate objective, and REINF_Z with Adam is outperformed by OCE_0.1 with AdaGrad in all measures.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Thus the reason that no fixed arm is always good does not depend on the inter-seed variability as much as on the fact that the best arm differs in different games.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] The updated paper will split Figure 5 into two to increase clarity.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] This analysis is intended to contrast the natural statistical differences among the representations, to indicate that different modeling approaches are needed for each of them.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] For all these reasons, the bandit we use is a conservative one that tends to spread the probability mass among decent-looking arms, while suppressing obviously sub-optimal arms.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] This is not reflected in Table 1, which is merely a presentation of how the introduced modifications impact performance.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_answer [SEP] Similar to the non-adversarially trained smooth classifier included in the original submission, we can produce adversarial examples for the SmoothAdv classifier which on average produce larger certified radii than their natural example counterpart.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] 2. table 2: Dynamic -> Adaptive?	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] “2. Figure 3: [the term CDF] is confusing”	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] Q4: Clarification of Figure-4 (Section-4.2)	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] Response to other minor points:	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] Q2. The input and output types of each block in Figure 1. should be clearly stated, and the figures are almost useless because the captions contain very little information.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] “4. Fig. 3 (right): It is not clear why the fact that the classifier is able to predict which dataset the image ‘m’ corresponds to is useful or practical, as this seems to be a property of the set ‘m’ rather than the property of the trained classification model (f_\theta). Please clarify. [...]”	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] Is this something the reader should understand from Table 1?	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] > I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.	1.0
"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] Minor comment 3) - ""I would suggest a different name other than Neural-LP-N..."""	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] Is it a baseline with the best hyperprameters in hindsight?	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] [R1: The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table.]	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] [Q] Clarification and exposition of plots.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] Thanks for pointing this out!	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] Q: Typo:. The “Inf” in Tabel 1	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] 1. Better explanatory texts for natural statistics comparison.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] [Q] The graphs were difficult to parse.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] 2. In Figure 3, the baseline got different perplexity between 3(a) and 3(b).	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] Q3. Figure 1 is too abstract:	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] > In Section 3.1 : “Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.” For me, this is not particularly clear.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] Main comment 2:	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_structuring [SEP] I hope to see some discussions about this. Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.]	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_by-cr [SEP] We will update the caption to make it less ambiguous.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_by-cr [SEP] - Table 3 is indeed confusing, this is a good point. We will correct it.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_by-cr [SEP] We will clarify this in the caption too.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_by-cr [SEP] we were trying to cram different experiments (with different regularization) in the same figure which is understandably confusing and needs to be corrected.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_by-cr [SEP] We will certainly consider renaming the approach and fixing this in Table 2.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_by-cr [SEP] As for the figures, we are aware of this and will try to make them more clear in a revised version.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_by-cr [SEP] We will provide additional details in the caption of the plot.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] >	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] Thanks also for catching several typographic errors. We have addressed them in the new draft.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] Per your request, we have attacked the work of [2] and reported results of attacking the pre-trained SmoothAdv classifiers (available in [3]) in Appendix B.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] In the revision, we have described what the numbers are representing in more detail.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] We also added additional explanation for 'D' of Figure 2 in its caption.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] We tried to add more information to the figures in the revision.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] We have modified the caption for Fig. 2 and text in Sec 2.4 to be more clear about the natural statistics analysis.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] *	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] We made this clear in the main body of the text.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] We have updated the figure to make it more intuitive and contains more details.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] We revised the x-axis from “generations” to parameter “updates” in the latest revision.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] For the Figure 4, we added the description of the underlined numbers.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] In the latest revision, we also included the curve where the topologies are allowed to be changed, which leads to better performance, but does not necessarily preserve the initial structure.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] * We included a table showing accuracy numbers for different values of beta and M (see p. 6, Table 1) for the latent bottleneck sizes K=256 (Figure 2) and K=2 (Figure 3).	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] Response #8: We have re-plotted Figure 3 and Figure 4 to improve the readability.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] In relation to the figures, we have improved these in the revision.	1.0
Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot) [SEP] rebuttal_done [SEP] We have added the missing labels in the latest version of the paper. Thank you for pointing it out.	1.0
Unclear intro (eg. contributions) [SEP] rebuttal_done [SEP] We updated the changes in the latest version accordingly.	1.0
Unclear intro (eg. contributions) [SEP] rebuttal_done [SEP] We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings.	1.0
Unclear intro (eg. contributions) [SEP] rebuttal_done [SEP] We have added a reference to what we mean by the classical approach in the related works section.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] Re: not clear if the classifier weight difference is well defined	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] 1) For the presentation, we apologize for our typos and unclear statement in the paper. And your advice is so helpful. We will modify it.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] That said, we agree it is worth investigating the performance of LSTM on this problem further.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] (Notations) Thank you for commenting this.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] We feel that the field of geometric deep learning, from which part of the direction of this work originated, is important to keep as part of the development of graph-based machine learning models.	1.0
"Unclear description of method [SEP] rebuttal_concede-criticism [SEP] 1. About equation 8, indeed there is a typo and should be a ""partial"" sign in front of the ""\delta"" function in the numerator."	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] - Thanks for the feedback.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] 3/ We agree and have changed `batch normalization' to `channel normalization' throughout.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] >> We will highlight the connection to the case of single input channel and DeepSets permutation invariance universality.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] You are correct that without any robustness training it is possible to find adversarial examples with distortion less than 0.1 for some inputs.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] We agree with your point regarding the wealth of graph neural network studies.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] Thanks for pointing this out.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] We have missed out on a detailed description of how to set up some hyper-parameters.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] Thank you for pointing this sentence out!	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] > You are right in noting that the classifier weights might capture dissimilar yet useful features for two similar tasks, and hence the classifier weight difference might under-predict the transfer potential.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] It is indeed unclear.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] We see how the approach taken did not provide an intuition about the problem as well as it could have.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] Very good Catch	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] All the same, we agree that the original Figure 3 was confusing in this respect, and have rerun this experiment with a lower minimum threshold for log(I) to make the point clearer in the graph.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] We apologize for the confusions in Section 3.1.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] Thanks for the suggestion.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] All we were trying to say is that each baseline’s training duration was chosen independently to prevent overfitting.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] [A] We agree and will provide more details.	1.0
Unclear description of method [SEP] rebuttal_concede-criticism [SEP] We acknowledge that the time complexity of our pre-training methods was not well explained in Appendix F. In Figure 2 (a) we show that we only sample one node per graph.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The original reason we used the previous definition was that we thought it would show more clearly what the core property is, namely that omnidirectionality can be used to nail down one precise solution. But we are now convinced that the best introductory formulation is the geometric one, as it offers an intuition of omnidirectionality.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Your formulation would therefore require every entry of Ax to be negative, while for omnidirectionality one entry would be sufficient as long as the others are non-positive.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We had experiments with categorical variables, however, we faced training stability issues with them.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We consider self-modulation as an architectural change in the line of changes such as residual connections or gating: simple, yet widely applicable and robust.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The main premise behind guiding our siamese networks is to find very simple, yet effective ways to capture some of the variation in the data, through weak supervision.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Regarding our use of ‘axioms’: We follow the economics literature in using axioms as normative concepts, i.e., to denote desirable properties that a neuron importance methods.	1.0
"Unclear description of method [SEP] rebuttal_answer [SEP] 5. ""Additionally, in section 6.4, the results in Figure 2 also does not look very"	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Note Eq. 2 calculates the posterior, i.e., p(w|D) and according to the Bayes theorem, it is p(w|D) \propto p(D|w)p(w)	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] However, beta does affect the quality of the generative samples (blurriness).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Yes, we use parameter sharing in the one cluster of feature groups.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] To allow the model to access ancestry nodes during decoding, one way is to concatenate the parent node latent representation with the input of each step for decoding children, and then feed the concatenated vector to LSTM (e.g., Dong & Lapata ACL 2016).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] They make the training of the controller much easier compared to other RL tasks with higher dimensional features or larger output space.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] A2: The experiments on SHREC17 does show all three spherical methods slightly under-perform some other state-of-the art approaches, We believe this is mainly due to the information losses introduced in the spherical projection process which retains only the convex portion geometric information.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Additive composition vs. tensor: as discussed in our introduction (and illustrated by the qualitative results in Tables 1 and 2), we believe that linear addition of two word embeddings may be an insufficient representation of the phrase when the combined meaning of the words differs from the individual meanings.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] 9. In white-box setting, we perform grid search / binary search for parameter epsilon (or c for CW) for all algorithms.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] robust to perturbation of size epsilon = 0.1. Without any adversarial training,	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We hope that that this paper could ignite a discussion around what transformation can be created to impose prior knowledge into the model.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] And not the use in the mathematical literature, which is to denote statements that are self-evidently true.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Regarding equation I_{DIM}, it is supposed to contain two g_{\omega} and no g_{\psi} as we use one network for encoding both the sentence and n-grams.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The 3D models presented on ModelNet and Shrec’17 are of arbitrary genus which prevents us from using spherical parameterization method.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] (When SST uses Gaussian noise, ours are also degraded.)	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Formally, it should be F = {f_1, …, f_|U|} and ~F = {~f_1, …, ~f_|U|}	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] On the other hand, we didn’t observe it harms on NMT task noticeably.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Hence, the averaging happens implicitly during the search.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] For (1), as we specify, F-BERT is a reliable metric for MT.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] About replacing the ReLU non-linearity in DDPG and TD3 prior work with tanh, we spotted that we could get much better results on several environments with the latter.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Another disadvantage is the use of lat-lon grid which introduces unevenness of the perception field (due to the high resolution near the poles, and low resolution at the equator).	1.0
"Unclear description of method [SEP] rebuttal_answer [SEP] Thus, we say there are ""explicit feature combinations"" in TabNN."	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The prior work by Hsu et al. showed that the oracle trained by deep learning has high accuracy (see Section 5.3 in their paper): for Internet traffic data, the AUC score is 0.9, and for search query data, the AUC score is 0.8.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The smallest singular values are directly linked to inverse stability for points from the same input polytope (where the linearization is exact).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The number of training iteration and thresholding epsilon are very important parameters in our algorithm and have a considerable correlation with each other.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Your interpretation of section 3 is exactly right.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Their unlabeled data came from only in 4 classes, however, we selected unlabeled data in all classes.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We used Figure 1 and the combination matrices to show what exactly is happening when we combine the models, and how the models mathematically combine.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] But indeed, in order to preserve the details and increase the receptive field, simply enlarge the 3D kernels to cover the holistic video will bring enormous computation cost.	1.0
"Unclear description of method [SEP] rebuttal_answer [SEP] ""Implicit feature combinations"" is not efficient as it introduces much more trainable parameters, and has a risk of over-fitting."	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] These prior transformation could be something that we observed in biology, for example, we observe global-local information disentanglement in our perception.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Cohen et al 2018 and ours obtain similar performances because both of the methods used anisotropic filters, the former achieves rotational invariant using SO(3) rotations for filters, while ours achieves rotational invariant using alt-az rotation of filter and SO(2) rotation augmentation of input shapes.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] So there is no surprise that our algorithm’s distortion is not the best.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We agree that the current state of the research should be stated as using approximate spectrogram inversion.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The main difference lies in whether the feature combination information is explicitly introduced into model structure or not.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We are using word pieces in all experiments, and we compute IDF using word pieces.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Other details are the same as those of the first experiment.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] adversarial examples (or counter-examples for property verification) with L_inf	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] => Animal vs non-animal	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Linear classifiers with IP addresses represented as individual bits are unlikely to work well because their expressive power is limited.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We have however summarized the relevant aspects: the fact that all probabilities P_ij, although there are n^2 of them, can be represented using much fewer parameters, and the fact that they can be fitted highly efficiently (in our experiments, even on the largest networks, this always took only a tiny proportion of the total computation time).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Notably, however, log(I) is incredibly small before any of this training for eps=0.1, demonstrating how it is important to not only think in terms of whether any violations are present, but also how many: here less the proportion of violating samples is less than 10^-100 at eps=0.1 for most of the datapoints.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] This is very fast and efficient.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] On the other hand, as mentioned by the other reviewer, distortion is usually not that essential in adversarial attacks as long as it is maintained in a reasonable range.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In other words, they are implied and can be computed automatically and highly efficiently based on prior work, after one has decided on which prior knowledge to use.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] How is precision and NDCG calculated	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] As a result, pre-training via context prediction has linear time complexity.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] N is the sum of all frequencies; i.e., N = \sum_{ i \in U }	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] BERTScore computed with Multilingual-BERT is better than most existing metrics except on few low-resource languages.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] A :	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The stopping criterion is that the accuracy of the current iteration reaches the average accuracy of the previous 20 steps.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We only take one of the terms from the full objective function and mix it with MLM.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] However, since the ancestry has a variable-number of nodes (as decoding proceeds)	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We combine it with MLM which is designed for learning (contextual) word representations, since our overall goal is to create better representations for both the sentence and each word in the sentence.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Thus, 2 sentence input tasks and 1 sentence input tasks cannot be compared using the metric.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] A recent result [1] proves that there is a fundamental tradeoff between accuracy and realism, for all problems with inherent ambiguity.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Applying a classical transfer learning algorithm might improve performance even further, as then we could fine-tune results on C_p.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In this example, \theta and \tilde\theta never appear in the same model (they are part of p and p’, respectively).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Thus, instead of comparing transfer learning methods, we evaluate the transferrability of both our own structural features as well as those of competitors.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] If the stopping iteration is much less than 100 times, the epsilon growth rate should be reduced so that the data is added more slowly.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] As mentioned in the paper (4. Experiments), our supervised learning performs slightly better than conventional SSL algorithms because of different settings such as learning rate and Gaussian noise on the input layer.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Is there other hard-coded disentanglement in biology?	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Regarding missing values:	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] With the setup used in section 3, there is no good notion of validation: our model is expected to predict “0” on held-out data.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] 2. Compared to GMM,  V-GMM has a natural tendency to set some mixing coefficients close to zero and generalizes better.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Other non-spherical method (such as volumetric or multi-view based methods)  can only be generalized into unknown orientations using SO(3) rotation augmentations, their representation of 3D shapes are either too sparse (voxel model) or too redundant (multi-view projections).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Indeed, GPipe [2] incurs less memory footprint than our pipelining scheme and PipeDream [1] because it only saves the activations at the boundary of each model partition and re-computes the activations of the model during the backward pass.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We mainly account the success of this simple training strategy to the simplicity of the model, the relatively low dimensionality of our input features, and the simplified action space (though all  three suffice to obtain a good controller in the current settings).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] For instance, at the very least, we would like our classifier to express a DNF hypothesis of the form:	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Re: (W4) Metrics for ranking of transfer don't make sense (and some are missing).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] While in the back propagation pass, the gradient is computed only with respect to these saved positions.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We hypothesize the tradeoff is insignificant on NMT, as in our multi-task setting, slightly over-optimizing one task objective usually does not have irreversible negative impact on the MT model (as long as the other objectives are optimized appropriately later on).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] 2) It seems that the role of depth in performance is more subtle than standard intuition would dictate.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] -Truncation introduces a train-test disparity in G’s inputs--at sampling time, G is given a distribution it has effectively never seen in training.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] To reconcile this with the commonsense intuition that “deeper is better”, our answer is twofold.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] 1) As in [1] and [2] it is often possible to find configurations or architectural modifications where the covariance matrix doesn’t approach its fixed point over depths often considered in machine learning.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Future improvements can be added by using:  (1) less lossy input in spherical projection methods (e.g. on top of SEF, and EDF, we can also add other statistic information such as the minimum distance of intersection, mean distance of intersections or standard deviation of the intersection and so on to reduce the information losses); (2) extend the spherical parameterization method on to the general 3D shapes.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] If the correlation of some rows is arbitrarily small (but non-zero) between remaining and removed rows, the upper bounds can be arbitrarily small.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We agree that there are other approaches that may offer different model quality vs. inference speed tradeoffs; we simply highlight that K-matrices are one promising method, especially given their important theoretical properties.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] positive - it unlikely to be true that an undefended network is predominantly	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We simply intended to clip the reward to reduce variances, and fount it effectively improved training.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Although the feature combination information are not explicitly provided, one neuron in FCNN can learn a linear combination of its input features.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] When the sequential Bayesian optimization chooses the next set of hyperparameter combinations to test we run the model once (per hyperparameter combination) and report the scores to the optimizer.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] If the second case holds, one would need to consider different linearizations of the network and thus extend the analysis to movements between the polytopes.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] When this is the case one can safely increase the depth without sacrificing accuracy.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Yet with our 4D kernels, a simple k x k x k x k will cover the interaction from the 1st frame to the (kT+1)th frame, because 4D convolution can go beyond space and time, making the long range interaction possible.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In black-box setting, we care more about query complexity and thus did not perform the grid search/binary search steps to avoid extra queries in finding the best epsilon/lambda.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] For more complex semantics, we discuss the possibility of using a pre-trained network as guidance.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In experiment 1.2 we use beta=1.0 which is the same as using the original VAE objective.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] 1. GMM can be trained reasonably fast for RL agents.	1.0
"Unclear description of method [SEP] rebuttal_answer [SEP] The successful CNN model also uses ""explicit feature combinations"", as it only combines the local pixels."	1.0
"Unclear description of method [SEP] rebuttal_answer [SEP] - You're right, this sentence was a bit vague, we rephrased it to: ""these models can indeed learn and utilize more abstract concepts (approximate numbers) than mere superficial pattern matching (""red square"" etc)""."	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] 2. We would like to argue that constrained optimization based formulation itself is not designed to achieve better distortion compared with regularized optimization based formulation.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] where the number of features between the tasks are of the same size.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] This will lead to better/ closer distortions for all methods.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We set parameters as follows.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] If the stopping iteration significantly exceeds 100 iterations, the epsilon growth rate should be increased so that the data is added more easily.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] ==> Data setting	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] This trick is implemented in modern deep learning frameworks such as tensorflow and pytorch.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In TabNN, we leverage GBDT to find feature combinations and then construct model structure according to them.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In deep neural networks, there is a common trick in computing the gradient of operations non-differentiable at some points, but differentiable elsewhere, such as Max-Pooling (top-1) and top-k.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We allow 5 iterations as a deviation from 100 iterations and the growth rate of epsilon is left unchanged in this interval.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Experimentally, we compared the three spherical convolution methods in Table 3 using Shrec’17 perturbed shape retrieval experiment.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] At each iteration k-1, the network predicts the outputs, i.e., O1, O2 and \alpha.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] For example, in FCNN, as all features are connected to the neurons in the next layer, there are no feature combination information in the model structure.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] A : SST has a network structure similar to other papers.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] - MISC-p works similarly to PER.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] At this point the network becomes untrainable.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In our paper, we use the tensorflow function tf.nn.top_k() to compute k-nearest neighbor operation.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] , to directly access these nodes during decoding, attentional mechanisms would be an efficient way, which is one of our motivations to use Transformer models that are attention-based.	1.0
"Unclear description of method [SEP] rebuttal_answer [SEP] The algorithm itself trades-off exploration and exploitation and it can explore hyperparameters ""close"" to the existing ones if they seem promising."	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The difference of structure was that the selection network is added and Gaussian noise and the mean only batch norm are not used.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We hope that after the earlier clarifications, the equation C[b] = sum_{j:h(j)=b} f_j  is more clear now.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] As to the online setting, thanks for pointing us to the “short-horizon bias” paper.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] To explain Eq.5 and Eq. 6, the training works as follows:	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] 2. The details of derivative estimation can be found in Section 3.1 (especially equation 9 and 10 in our updated version.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] With this lower value of log(P_min), we see the 75 percentile of log(I) over the samples quickly decrease as robustness training proceeds for eps=0.2.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We experimented similar to the [1] and they categorized according to the animal.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In order to model the interaction of 1st frame and the (kT+1)th frame, a 3D kernel of at least (kT+1) x k x k has to be applied, which brings linear increasing of computation cost.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We use a warping-based generator, from prior work (Ebert et al. 2017), and include a comparison to SVG for completeness.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] And the $\Leftrightarrow$ definition of $\mathcal{F}$ means equivalence.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] As a first step, we provide a careful empirical evaluation of its benefits.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The main goal of our submission is to experimentally show that our pipelined training, using stale weights without weight stashing [1] or micro-batching [2], is simpler and does converge.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Aside for the first two predicted frames, our VAE ablation and the SVG model both achieve similar SSIM and LPIPS performance.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The purpose of experiments is to show that the SST algorithm is comparable to the conventional SSL algorithms.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] LSTMs are indeed a strong model for tree prediction on previous tasks.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The ranked list (in the decreasing utility of transfer learning gain) is then considered the ‘gold’ set.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In each episode (training batch), our algorithm solves a small classification problem which contains N classes each having K support and Q query examples (e.g., N=5, K=1, Q=15, totally 80 examples).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] While AutoLoss is amenable to different policy optimization algorithms, we empirically find PPO performs better on NMT, but REINFORCE performs better on GANs.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] For parameters, 4D kernels are k times larger than 3D kernels.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Instead, we provide a systematic analysis of the effect of the loss function on this task (which could be applied to any generator).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] After evolving for k steps, the Generative Model is used to generate data which is subsequently compared with the observations at time t+k.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] (In previous versions, the training iterations of fixed mode had been fixed.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] - In order to make it easier for readers to understand the differences between different models and how they are related to InfoNCE, we have added a summary in Table 1.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] (In previous versions, the growth ratio of epsilon for CIFAR-10 was applied to SVHN and CIFAR-100.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The performance of a simple online algorithm would likely depend on the type of classifier used and input feature representation.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] (dt in Algorithm 1) dt means the domain translation explained at section 5.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Our suspicion is that if G is not encouraged to be “smooth” in some sense, then it is likely that G will only properly generate images given points from the untruncated distribution.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Regarding the first point, your intuition is exactly correct and a slightly simpler discussion of this phenomenon can be found in [1].	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Other conceivable transformations are (i) removal of colour information by converting image to grey scale, (ii) removal of orientation information with random rotation and positional shift, (iii) removal of temporal correlation using shuffling in a time-series data and etc.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The observation that imposing orthogonality constraints improves amenability to truncation is empirical.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In International Conference on Learning Representations, 2016.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] For experiment 1.1, different beta produce similar disentanglement results, we use beta=20 to produce the figures as it created nicest looking samples.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Precision@K, Reciprocal Rank and NDCG are among the popular information retrieval metrics to compare a recommended list against a gold ranked list.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Of course, LSTM equipped with Attention would achieve the same benefit.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In the updated manuscript we state both these facts explicitly and state much earlier that ‘i’ is the index for entities.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] However, since the epsilon growth rate is different for each dataset, as the reviewer mentioned, we have performed the cross-validation for SVHN and CIFAR-100 and modified our results.) As a result, the epsilon is gradually increased in log-scale by 10 times every 33 iterations in CIFAR-10 and SVHN.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] f and g are learned in an end-to-end way in our approach.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In fact, a recent challenge held at ECCV 2018 in such a problem [2] evaluates all algorithms on both of these axes, as neither adequately captures performance.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We could actually remove the distortion column, instead, we chose to include it just to show that we did not trade a lot of distortions (to make problem much easier) and thus gains speedup.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] They are not parameters: they are numbers between 0 and 1 representing the prior probability of a link between nodes i and j (i.e. prior to seeing the embedding).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] While the validation accuracy is evaluated using the cross-validation, we set the number of training iteration to be 100 so that the model is trained enough until it saturates.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Thus, this Lemma provides an intuition how hard it is to globally control inverse stability with a vanilla architecture (linear mapping followed by ReLU).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The main difference is that MISC-p uses the estimated mutual information quantity as a priority, while PER uses the TD-error as a priority for replay.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The weight matrix is constructed on the support and query examples in each episode rather than the whole dataset.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We speculate that encouraging G’s filters to have minimum pairwise cosine similarity means that, when exposed to distribution shift, the network’s features are less correlated and less likely to align and amplify an activation path it would otherwise have learned to scale properly.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Our approach is similar but not identical.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Figure 3 presents the accuracy boost using the best task till now in the produced recommendations for the candidate tasks using different methods.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] As we explain in the paper, classifier weight difference metric is only applicable in cases	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] However, the re-computation still incurs pipeline bubbles during training.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We train our structural features on C_r and show that with no re-training they can achieve state-of-the-art results of C_p.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We then use breadth-first search to extract a K-hop neighborhood of the node, which takes at most linear time with respect to the number of edges in the graph.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In the first experiment, the iteration number remains fixed and the growth rate of epsilon is adjusted so that the validation accuracy saturates near the settled iteration number.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Then, the optimization algorithm takes these scores into account when selecting the next set of hyperparameters.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We also propose Residual 4D Blocks to ease the optimization and preserve short-term details.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] After we get the per-example feature representation f_{\varphi}(x_i) for x_i, we feed it into the graph construction module g_{\phi}. The output of this module is a one-dimensional scalar.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] As expected, anisotropic filter perform  better than the isotropic filter proposed in Esteves et al 2018 which limits the model capacity.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The problem with your formulation (from the point of view of our notation) lies in the usage of the inequality sign since we defined it in the notation section to be element-wise.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] >> Comments #1, #11	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] -- Lemma 1: Yes, your assumption is correct in general for variational posterior.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] 3. We only use a basic density estimation method, such as V-GMM, in our framework as a proof of concept for the idea “Curiosity-Driven Experience Prioritization via Density Estimation”.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] From an intuitive perspective, using lambda>1 is essentially a “relax and tighten” step by first relax the constraint to make the problem easier, and then tighten it back to the real constraint.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Our early experiments with LSTM did not yield good results on this spatial layout problem.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We have indicated in the revision the existence of this bias -- this bias was observed on the GAN task -- overtraining G can increase IS in a short term, but may lead to divergence in a long term as G becomes too strong.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] - To improve the clarity, we clarify why we chose to use V-GMM, among the three basic density estimation methods, including KDE, GMM, and V-GMM.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We hypothesize that models which are not amenable end up learning mappings which, when given truncated noise, either attenuate or amplify certain activation pathways, leading to extreme output values (hence the observed saturation artifacts).	1.0
"Unclear description of method [SEP] rebuttal_answer [SEP] The statement that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction"" is a criticism of per-pixel losses, and not of VAEs in general. We clarified in the introduction that VAEs can indeed model joint distributions of pixels."	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Owing to similar concerns, we recommend using CFS information transfer metric over classifier weight difference (which is also supported by results in Table 2 and Figure 3).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Experiments Detail ( data setting, threshold, number of iterations, animal vs nonanimal)	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We uses beta=40 for all clustering experiments which had been searched from beta=\{1, 10, 20, 30, 40, 50, 60\} for the best digit identity clustering results.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We first solve a discrete optimization to find the permutation (matching) between the predictions at k-1 and the ground truth (GT).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We follow the widely-used episodic paradigm proposed by Matching Networks [1].	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The paper does achieve this goal, on a number of networks.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Currently, the spherical parameterization method only works for genus-0 closed object.	1.0
"Unclear description of method [SEP] rebuttal_answer [SEP] distortion less than 0.1 (at least on some images) should be able to find."""	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] which is simply Eq. 1.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Thanks to reviewer3, we incorporated this information into the revision.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] (IP address = a1) or (IP address  = a2) or ...	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Our scheme saves all activations instead of re-computing them to eliminate pipeline bubble, thus achieving better utilization for the accelerators (GPUs).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Then, we use this permutation to order GT and back-propagate the losses to update w at iteration k. Please note that cardinality loss does not depend on this permutation variable.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The word “sketch” in Algorithm 1 refers to the storage used by SketchAlg.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] f_i.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We further compare our recommendations of candidate tasks generated using CFS and classifier weight difference methods against the gold ranked list.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The upper bounds (Lemma 9) and the correlation effect are interesting, as they show how a well-conditioned matrix (subset of rows almost orthogonal) may become instable due to the removal of rows via ReLU.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In forward computation pass, the index position of the max (or top-k) values are stored.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] For example, in [3] note that although the authors were able to train a 10k hidden layer network, they did not observe any improvement in accuracy.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Therefore, we experimented with the popular setting.	1.0
"Unclear description of method [SEP] rebuttal_answer [SEP] In contrast, ""explicit feature combinations"" let model focus on the more important feature combinations and is more efficient."	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The series are indeed finite, we skipped the last index for simplicity.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] 2) Do points from other input polytopes map to the epsilon ball?	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] As explained in the description, items not stored in unique buckets “are fed to the remaining B − Br buckets using a conventional frequency estimation algorithm SketchAlg”.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] > To compute the gold set, we first train a neural network for each of the candidate tasks and then use the pre-trained sentence encoder (part of the network) from the candidate task to fine-tune on the target task.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] You can see for several samples that were not initially robustness to eps=0.1 perturbations (log(I) > log(P_min)), the value of log(I) decreases steadily as the robust training procedure is applied.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] When the network is deep enough that the covariance matrix has reached its fixed point, the distribution of the outputs of the network will be independent of the inputs.	1.0
"Unclear description of method [SEP] rebuttal_answer [SEP] The differences we want to point out is that, on the one hand, (approximate) numbers are a more abstract concept than, for instance, object types like ""cat"", ""chair"", etc as they can be combined with any object type. On the other hand, being able to utilize such representations to answer practical questions like whether ""most"" applies is more interesting than just being able to classify which representation applies."	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In the second experiment, we leave the epsilon fixed and simply train the model until the stopping criterion is satisfied.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] So in order to reduce the parameters, we apply k x k x 1 x 1 kernels in most experiments, as mentioned in section 3.2 and section 4.2.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] However, when considering an epsilon ball around activations, two main questions arise: 1) Are all points in the ball reachable from the considered input polytope?	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Other destiny estimation methods can also be applied in this framework.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] =	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We also point out that our DynamicConv model with K-matrices in the decoder attains a comparable BLEU score with the state-of-the-art from two years ago – the Transformer model, which continues to enjoy widespread use today – while having over 60% higher sentence throughput and 30% fewer parameters than this model.	1.0
"Unclear description of method [SEP] rebuttal_answer [SEP] Thus, we say there are ""implicit feature combinations"" in FCNN."	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Since evaluating generator architectures is not the emphasis of this paper, we did not test the importance of the warping component nor test on videos where this hypothesis is less suitable.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] These metrics are meaningful in our case, for instance, Reciprocal Rank tells us how many tasks we need to consider as per our recommendation before we hit the highest performing candidate task.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Another advantage is that our network allows local filters and local-to-global multi-level spherical features extraction.	1.0
"Unclear description of method [SEP] rebuttal_answer [SEP] An example for an implausible method would be to rely on color/shape cues to solve instances involving ""most"", which doesn't make sense for the abstract meaning of ""most""."	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Compared to non-spherical method, spherical image is one of the most compact representation for 3D shape analysis, the spherical convolution methods rely on no data augmentation (for Type I and Type II) or reduced data augmentation (for Type III, only SO(2) rotation augmentation is required).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The “relax and tighten” idea has been widely used in constrained optimization, and we adapted this idea to Frank-Wolfe algorithm to make it even faster.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In the case of CIFAR-100, the epsilon is increased by 10 times in log-scale every 27 iterations.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The entity and location embeddings  e_{i,t} and lambda_{i,t} are d-dimensional vectors, although we also overload the symbols to refer to abstract nodes in the model’s knowledge graphs.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] For (2), Roberta-Large performs consistently well for to-English language pairs.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We also note that Deep InfoMax for learning image representations mixes multiple terms in their objective function.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The results are less conclusive for from-English language pairs.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In addition, positional encoding in Transformer also allows us to easily model spatial locations of UI elements.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Here we clarify some of the proposed advantages of the method.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Q4: The paragraph motivating the alt-az convolution on page 4 is not very clear.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Compared to SO(3) spherical convolution method (Cohen et al. 2018), our network is computationally efficient (in terms of network model), but we have to admit that this is at the price of required data augmentation	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] These numbers are such that the prior knowledge of the types described are satisfied in expectation.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Considering a video of size UxTxHxW, where U is number of action units, and T,H,W means temporal length, height and width of each action unit.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Generalization of spherical parameterization methods to objects with arbitrary topology will be one of the future work.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The other two spherical convolution methods (Cohen et al 2018 and Esteves et al 2018) use a lat-lon grid and conduct convolution in the Fourier space, which has a common disadvantage: the Fourier transform does not support local spherical filters.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] This ensures that the IDF is the same for all MT systems that are tested.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Our scheme has less memory footprint than PipeDream because it does not stash weights.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] >> Comment #7	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] *	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] GMM is also much faster in inference compared to Kernel Density Estimate (KDE) (Rosenblatt, 1956).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We also found in our ablation study that using only strong augmentation (i.e., replacing weak augmentations with strong augmentations) resulted in very poor performance for ReMixMatch, suggesting that anchoring towards a weaker augmentation is important.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We do not refer to this procedure as “prediction” since the initial state z_t for the forward interpolation was obtained by making use of the full data.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Regarding unknown words handling, we computed the IDF on the reference sentences in the test set.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] For more detail on PER, please refer to the original PER paper [Schaul et al 2016].	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] -> The latter: we compared with the models with the closest match in # of parameters.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In this work we refer by filtering to the process of inferring the optimal latent state z_t  at time t, using observations x_{1:t} from the trial up to time t, not including observations to the future of t. By forward interpolation we refer to the process of smoothing, (inferring optimal z_t from observations of the complete trial x_{1:T}, including points to the future of t), and then evolving the inferred z_t with the learned VIND dynamics.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Epsilon is increased in log-scale and begins at a very small value (10^(−5)) where no data is added.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] The growth rate of epsilon is determined according to when the validation accuracy saturates.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] We want to clarify the few-shot setting.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] For details about how P_ij are fitted given such prior knowledge constraints, and how they can be represented efficiently, we have to refer to Adriaens et al. (2017) and van Leeuwen et al. (2016).	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] Our DIM is primarily designed to improve sentence and span representations.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] There are largely two sets of options, (1) Among P, R, F; and  (2) What model to use.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] In our experiments, we found that the disentanglement of global and local information is very robust to different values of beta.	1.0
Unclear description of method [SEP] rebuttal_answer [SEP] LPIPS linearly calibrates  AlexNet feature space to better match human perceptual similarity judgements.	1.0
Unclear description of method [SEP] rebuttal_social [SEP] Thanks for pointing this out.	1.0
Unclear description of method [SEP] rebuttal_social [SEP] Response:  Thank you.	1.0
Unclear description of method [SEP] rebuttal_social [SEP] 2) We thank the reviewer for this remark.	1.0
Unclear description of method [SEP] rebuttal_social [SEP] >>> Thanks for pointing out the details.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] 1. Are e_{i,t} and lambda_{i,t} vectors?	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] ** Addressing comments on the write-up:	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Comment: The proposed method seems to be specifically designed for the	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] - ""In Equation (2), How is P_ij defined exactly [...]?"""	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Additional question 3:	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] In addition, the paper would also need to show that such a model does not generalize to a validation set of images. [...]”	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Q：I did not completely follow the arguments towards directed graph deconvolution operators.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Q1: About the main concern on “novelty, improvement relative to the current state of the art implementations,  and non-linearity of G and D”:	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] - There are a few points here:	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] - the sentence under eq. (2)	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] Remark 3. ""As the base classifier is different for various baselines, it is hard to compare the methods."""	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] “Section 5 is somewhat less clear than the previous sections. The authors should more clearly define what the private, public and evaluation sets are, right from the beginning. The purpose of the public set is explained only in section 5.2.”	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] 2. Difference between ""implicit feature combinations"" and ""explicit feature combinations"""	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Q2: There is a typo in equation 6	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] predict class A. Is this indeed the case?	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Q: “The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing”	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Q7. Attention should be given to the notation in formulas (3) and (4):	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] (Importance and motivation)	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way."""	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] A and not class B. While the generation of this type of explanations	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] - The network architecture :	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] ** Clarification on Mathematics in Section 3.1 **	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] For your questions:	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] 3. About ""encourage parameter sharing""."	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] U^m in Eq 1:	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] - Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum."""	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] 2.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] -Q: Upper bounds and inverse stability:	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] R2: ""However, it isn't entirely clear if the primary contribution is showing that 'curiosity reward' is a potentially promising approach or if game environments aren't particularly good testbeds for practical RL algorithms"""	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] [1] Odena, Augustus, et al. ""Realistic Evaluation of Semi-Supervised Learning Algorithms."" (2018)"	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] 2. Each component described in Figure 1 is not explained enough.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] “I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?”	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Comment: typographical errors...	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] >For example, it is unclear to me why some larger models are not amenable to truncation.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] [Q] Bayesian optimization and variance.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] “1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] - The term p(w) in Eq 2:	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] - The problem setting description is neither formal nor intuitive which made it very hard for me to understand exactly the problem you are trying to solve.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Q2: Regarding other minor comments	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Q8. Terminology consistency through the paper:	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] somewhat novel, from the text it seems that the proposed method may not	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] p2-3, Section 3.1 - I found the equations impossible to read. What	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] Q: I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3."	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Reference:	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] 2. For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] It is still not clear why self-modulation stabilizes the generator towards small conditioning values.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] [Response for 2]	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] generation of contrastive explanations, i.e.  why the model predicted class	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] - The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!"	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] 4.4, law of total variation -- define """	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] *	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] R1: ""What are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper)."""	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] is	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?"""	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] ==> Iterations & Threshold	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] be able to indicate what part of the image content drove the model to	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] ""It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)"""	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] - In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e."	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] - Eq 5 confusion :	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] * Invertible ? Decodable ? Approximate inversion ?	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] “It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix. “	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Generalizing this to the multi-channel input as the next step could make the proof more accessible ”	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Also, it took me a long time to figure out that ‘i’ is used to index each entity (it is mentioned later).	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] [R1: Lambda sim and lambda s are used interchangeably. Please make it consistent. ]	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] What is the relationship between \theta and \tilde\theta exactly?	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] - the sentence ""Because the identity of the datapoint can never be learned by ..."" What is the identity of a data point?"	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Thank you for this stylistic critique.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Scalars?	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] The reasons are the following:	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Abstract node notations? It is not clear in the model section.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] > Many of the parameters here are also unclear and not properly defined/introduced.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] 2. Description of variables	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] 2. ""I also struggled a little to understand what is the difference between forward interpolate and filtering"""	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] 1. ""The clarity is below average. In Section 2 the main method is introduced. However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately."	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Not sure why Eqns. 2 and 9 need any parentheses	1.0
"Unclear description of method [SEP] rebuttal_structuring [SEP] ""I am not convinced that the measure theoretic perspective is always necessary to convey the insights, although I appreciate the desire for technical correctness."" / ""Generally speaking it seems like a lot of technicalities for a relatively simple result: marginalizing a distribution onto a lower-dimensional surface."""	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] Here are our responses to your concerns in “Cons” and “Minor comments”.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] - Your F and \tilde{f} are introduced as infinite series.	1.0
Unclear description of method [SEP] rebuttal_structuring [SEP] It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] => Thank you. We will add more details on the architectures to the appendix.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] A2: Thanks for your notification! We will polish our paper and rewrite the corresponding part in the next revision.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] To avoid confusion, we will shorten line 10 to “feed i to SketchAlg”.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] 2) Thanks for pointing this out, we’ll correct it in the next revision.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] It can also be written as $f\in \mathcal{F}\rightarrow 1-f\in mathcal{F}$. And we will modify other improper presentation.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] A4: Thanks for the comments, as you suggested, we will rewrite this paragraph in the new version, and acknowledge the importance and effectiveness of the recent work on the group equivariance and rotation invariant networks.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] We plan on replacing the iterative slow spectrogram inversion with Griffin-Lim by faster decoding with Multi-head Convolutional Neural Networks, arXiv:1808.06719, Sercan O. Arik et al.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] We wanted $L_H$ to be defined for our theorem statements, but we can see how it is confusing as is. We will make it clear that $L_H$ is the smoothness parameter of $H$.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] We will clarify this in the paper.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] We will update this section to make it clearer.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] We agree with your suggestions and we will revise our paper accordingly.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] We will edit Appendix F to include more detailed information and cover this important point.	1.0
Unclear description of method [SEP] rebuttal_by-cr [SEP] In the new version to be uploaded soon, this will be further clarified.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] We found empirically that GAN with simple DeepSet-like discriminator most of the times fails to learn to generate point clouds even after converging, however, it does sometimes results in reasonable generations (although worse than proposed PC-GAN).	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] We observe that the global structure contains more information than just digit identity.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] For each output we have a loss defined for each experiment in the text.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] It also contains information such as whether or not there are distracting digits in the image.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] U^m is simply U powered by the cardinality variable.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] For the permutation (O2), we need 4!=24 outputs.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] In particular, we hope our paper stimulates both, an interest in trying out more realistic/stochastic environments, *and* further research on curiosity as a potential useful reward.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] In section 3.3, in the paragraph after Eq.7 line 2, we explain the mechanism to obtain U. For each experiment, we also report the tuned value for U.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] It is defined in our section on notation.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] A: We actually found that using stronger augmentations in MixMatch resulted in divergence.	1.0
"Unclear description of method [SEP] rebuttal_reject-criticism [SEP] + In fact we did introduce our subscript notation of the kind ""b |_{y>0}""."	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] Instead, we argued that 3D kernels usually are not large enough to cover the holistic video so that Max Pooling operations are applied in most 3D CNNs to enlarge the receptive field.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] We did not argue the computation cost of 3D kernels in section 3.2.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] So the message here is that we need additional constraints for GANs with simple DeepSet-like discriminator to exclude such bad solutions and lead to a more stable training.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] The use of 30 clusters helps us identify the grouping of other types of global information in addition to the digit identity.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] All these, we argue, are valuable to the progress and health of the field.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] For example, for the set size with maximum cardinality 4, we need 5 outputs ( \alpha) for cardinality m = {0, 1,...4}. If the state of each set is 5 for the detection experiment, we need 4*5=20 outputs for the state  loss (O1).	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] Then, as some useful examples, in this paper, we particularly showed that the technique of negative feedback can be leveraged to stabilize GANs and developed NF-GAN, which was proven to be effective in our experiments.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] Regarding our proofs, they are all self-contained.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] Note that proposing a generator architecture is not the goal of this paper.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] Other architectures like RNN might work, but they are not permutation invariant, which is a desirable property for set data like point clouds.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] In ALL our experiments, we use Res-net 101 (mentioned several times on page 6 and 8).	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] A good generator and discriminator would definitely be a solution as well.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] More comparisons between using RNN and DeepSets for other tasks on set data can refer to Zaheer et al., (2017).	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] The purpose of the counterexample is only to show that there exists some spurious solutions to GANs with general DeepSets-style discriminator for point clouds.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] This term is clearly defined at the beginning of section 3, first paragraph, line 6 “U is the unit of hyper-volume in ...” .	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] So, we do not consider the argument to be unrealistic as we often observe the degeneracy.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] Therefore, the identity clustering performance does not directly translate into the ability to disentangle local and global variables.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] We agree that setup we selected is destined to fail, but it was done on purpose to illustrate the presence of spurious solutions.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] We are not concern with improving upon baseline but rather to confirm that our method can disentangle global-local information and the further analysis have shown that the grouping corresponds to more than just the digit identity.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] In addition to that, we have shown that curiosity could be a very strong baseline to compare against in future papers.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] Yet this causes the loss of detailed information.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] It’s important to note, though, that BERTScore is an improvement over the commonly used Bleu across the board.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] Our recommendation to use F1, while potentially not optimal in specific cases, generally performs very well and much better than Bleu.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] However, solutions during optimization might not always correspond to such good solutions and can also correspond to the demonstrated spurious solutions.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] We only need to define the number of outputs and use the set loss defined in Eq. 5 and 6.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] Note that our described methodology can be applied to any network architecture.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] => We believe that both are valuable insofar as generating discussion within the community and leading to follow-up experimentation.	1.0
Unclear description of method [SEP] rebuttal_reject-criticism [SEP] Indeed, as agreed by R#2, this is a major novel contribution that provides a unified and promising framework to model the stability of GANs, which includes some recent developments (e.g., Negative Momentum and Reg-GAN, See Sec. 4.1 and Appendix A&C) and also provides us a possibility to explore advanced tools in control theory (e.g., nonlinear control and modern control theory [*3]) to improve both the stability and convergence speed of GANs.	1.0
Unclear description of method [SEP] rebuttal_future [SEP] Exploring how to continue to improve these structured compression approaches, while retaining the efficiency and theoretical benefits of K-matrices, is an exciting question for future investigation.	1.0
Unclear description of method [SEP] rebuttal_future [SEP] - Eval metrics	1.0
Unclear description of method [SEP] rebuttal_future [SEP] Similar to residual connections, gating, dropout, and many other recent advances, more fundamental understanding will happen asynchronously and should not gate its adoption and usefulness for the community.	1.0
Unclear description of method [SEP] rebuttal_future [SEP] This is indeed what our results show in Figure 5 in the appendices, illustrating our metric for individual samples.	1.0
Unclear description of method [SEP] rebuttal_future [SEP] Since this is the first paper on this topic, we chose to focus on introducing the problem and providing Transformer-based approaches as a baseline for future work.	1.0
"Unclear description of method [SEP] rebuttal_done [SEP] We mention this in the paper (""Since MixMatch uses a simple flip-and-crop augmentation strategy, we were interested to see if replacing the weak augmentation in MixMatch with AutoAugment would improve performance but found that training would not converge."") but will emphasize this more in the next draft."	1.0
Unclear description of method [SEP] rebuttal_done [SEP] (3) To make our description clearer, we have updated our paper in Section 3.2.2, e.g, by adding “To get the nth hop information Aij, row filter decodes all the (n+1)-th hop information of outgoing edges of Vi and column filter decodes all the (n+1)-th hop information of incoming edges of Vj.”	1.0
"Unclear description of method [SEP] rebuttal_done [SEP] [Experiments Section] We have significantly updated qualitative and quantitative results in our ""Experiments"" section and now compare our methods against Beta-VAE, DIP-VAE, and InfoGAN."	1.0
Unclear description of method [SEP] rebuttal_done [SEP] A: For space reasons we provided only a short description of CTAugment, and how it differs from AutoAugment. We will include a longer treatment in the appendix.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We will produce a reworked introduction where graphs play a larger role.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have updated notations in Equations 1 and 2.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] 2. Equation 1 typo fixed.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] (1)This is typo. It is Q.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have included a revised plot in Figure 14 at the end of the Appendix (note that this temporary plot will be incorporated to Figure 6), where we use the official implementation of SSIM and replace the VGG metric with the Learned Perceptual Image Patch Similarity (LPIPS) metric (Zhang et al., 2018).	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have fixed the typos.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] In the updated draft, we clarify in Section 3.4 that the warping component assumes that videos can be described as transformation of pixels, but that any generator (including the one from Denton & Fergus (2018)) could be used with our losses.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We updated the paper to make this clear in Section 3, under Importance Weighting.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] Please refer to our “Discussion” section for more details.	1.0
"Unclear description of method [SEP] rebuttal_done [SEP] 3. Definition of Hard Thresholding (HT) — As per the recommendation of the reviewer, we have repeated the definition of hard-thresholding (HT) initially presented in the ""Notation"" sub-section, in Section 2 for clarity."	1.0
Unclear description of method [SEP] rebuttal_done [SEP] Indeed, thank you. We have updated the text to place more emphasis on this contribution.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] Fixed. Thank you.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have added a detailed description on the data setting to Section 6.3 of the supplementary material.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have added clarifying comments at the beginning of section 4.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] For the third and fourth comment, thanks for pointing out and we have added the constraint-based methods in the related works section, and stressed that we are dealing with “causality in mean” in section 3.1.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] In the discussion in section 5, we also analyze how the error may affect the tasks downstream.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] Nevertheless your troubles compelled us to restate the meaning of this notation at the time of its first usage.	1.0
"Unclear description of method [SEP] rebuttal_done [SEP] We now point this out in our ""Discussion"" section."	1.0
Unclear description of method [SEP] rebuttal_done [SEP] The expectations are now taken over random variables (A and B) and the function takes particular values (a and b) of these random variables.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have updated the draft to be more clear.	1.0
"Unclear description of method [SEP] rebuttal_done [SEP] We now write ""law of total variance"" instead of ""law of total variation"" to avoid any ambiguity."	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have updated the paper with these recommendations in Section 7.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] -> Thanks, we agree; we re-organized our paper accordingly.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] - We have improved notations by adding explicit definitions before they are used in Section 2 and Section 4, and added a short description of Deep InfoMax in Section 4.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have reorganized this section, as shown in our updated paper.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] (Fixed on the paper)	1.0
Unclear description of method [SEP] rebuttal_done [SEP] A2: Thank you for your careful review in catching these mistakes. We have updated the typo in the revision.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We consistently use the term “feature-metric BA” and “basis depth maps” through the paper now.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] The citation of that part is obscure and has been modified.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] [Gaussian Prior on Latents] In our new experiments, we used uniform distributions to model the generative factors.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] it should be (N)x(N) instead of (N+1)x(N+1). (Fixed on the paper)	1.0
Unclear description of method [SEP] rebuttal_done [SEP] + As you suggested we changed our inequality notation to curly brackets to make it visually clearer that we are dealing with vectors.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] + We tried to improve the readability by increasing structure of longer segments of text e.g. by introducing informal titles.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] + We corrected as many typos as we could find, we would be very thankful for pointing out any further typos!	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We corrected the notations of section 3 to match with later sections.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We will update the labels in the ablation table to make this more clear.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] 3. Indeed D^c_k and D^u_k should have been clearly defined there; we clarified this notation in the updated version of the submission.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We fully agree with this criticism. Following your suggestion (and also that of reviewer # 2) we have moved some material from the appendix to the main text.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have fixed the misspelling in “differentiable”.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] -> Added a comment in the newly written “Scope” section in the revision	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have updated Equation 1 and the paragraph above so that I(...) is consistently a function of two variables.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] 3. You are right, it is a quite weak attack and we have removed it from the table (just mention it in the text).	1.0
Unclear description of method [SEP] rebuttal_done [SEP] 9. We incorporated the changes as you suggested.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] In the revision, we have added analysis in section 4.2 and section 5 on how the learned causal matrix can be used downstream, for example in RL/IL and interpretability of neural nets.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We also removed the redundant subindex ‘1’, because all points ‘q’ are on the first frame.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We edited the text to address variables more gently and to explain the arrow sign.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have addressed these and uploaded a new draft to reflect the changes.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We added Section 3.5 to point out the differences between the VAE component of our model and the SV2P and SVG models from prior work.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] (in the revised version of the paper Section “2.3 Density Estimation Methods”)	1.0
Unclear description of method [SEP] rebuttal_done [SEP] In Section 3.4, we clarified what frames the discriminator takes, and in Section 4.3 we added a description of the deterministic version of our model.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We changed the parameters from ‘d’ to ‘d \cdot p’ which is a 3D point.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have updated the introduction to rephrase and clarify the lower bound claims.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] Also, for an easy understanding of the whole CL process with DiVA, we added another figure in Appendix E.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] In addition, we have simplified the architecture described in the paper by combining the networks $\sigma$ and $\omega$, and included the results from this architecture.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] This should provide a more intuitive introduction to our work, whilst maintaining cornerstone concepts.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We updated the paper and hope that the discussion is clearer now.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We updated the section to address all of your feedback.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have added a performance comparison of K-matrices with other structured replacements such as circulant, Fastfood, ACDC, and Toeplitz-like in Appendix B.4.3, showing that K-matrices yield faster inference with similar BLEU score.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] 1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We realized that this is confusing and have therefore renamed \tilde\theta to \tilde\mu.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] Thanks for pointing out -- we apologize for misusing “exploding or vanishing gradients” and have revised the paper to be accurate.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] The added/modified text are highlighted in the blue color.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have added the detailed PPO-based training algorithm in Appendix A.1.	1.0
"Unclear description of method [SEP] rebuttal_done [SEP] This explanation is now clearly mentioned in the paper, and motivates a future work direction which consists in using ""neural architecture search"" for RL problems, the performance of algorithms being a lot dependent on such architecture details."	1.0
Unclear description of method [SEP] rebuttal_done [SEP] Specifically, we apply NF-GAN to SN-GAN [*5] and NF-GAN provides a significant improvement on the state-of-the-art inception score on CIFAR-10 (from 8.22 to 8.45).	1.0
Unclear description of method [SEP] rebuttal_done [SEP] In our revision we have made an effort to outline this in greater detail.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] - We move the exact setup (Section “2.1 Environments” in the new version) in early sections to improve the clarity of the paper.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We have clarified this in the submission.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] eqn (8), (12): thanks for pointing these out, we will fix this in the paper.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] Finally, we added new results in Table 1 in the revision, which shows that the same technique of negative feedback can further improve the state-of-the-art method of SN-GAN [*5].	1.0
Unclear description of method [SEP] rebuttal_done [SEP] This miscommunication also encouraged us to change our signs to the curly version as suggested by you.	1.0
Unclear description of method [SEP] rebuttal_done [SEP] We updated Section 4.4 to indicate that it is to be expected that although our SAVP model improves on diversity and realism, it also performs worse in accuracy compared to pure VAE models (both our own ablation and SVG from Denton & Fergus (2018)).	1.0
Unclear description of method [SEP] rebuttal_done [SEP] In Section 3.5 and A.1.2, we clarified that the latent variables are sampled at every time step.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_concede-criticism [SEP] The work of Lakshminarayanan et al. indeed showed that deterministic ensembles can improve on the performance of MC-dropout techniques and provides a foundation for ours.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_concede-criticism [SEP] We agree that some of our observations under constant epoch budgets in sections 2 and 3 have been made in previous work.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_answer [SEP] Novelty of the paper	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_answer [SEP] Taking into account more advanced ensemble methods is definitely of interest.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_answer [SEP] We thank the reviewer for pointing us to the work of Huand et al. Indeed this is an interesting method that would allow us to most likely achieve similar or better results with less computational overhead.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_answer [SEP] And as Beluch et al. (2018) showed, this can be valuable in an active learning setting.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] The method is different from ours, except in the limiting case where M = 1 and beta =1, in which case it coincides with the beta-VAE and also with our method.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] Finally, another contribution is that we propose a discretized algorithm.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] After taking a close look, we make the following observations:	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] In practice, we can only use the discretized one instead of the ideal continuous process to solve problems.	1.0
"Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] A similar argument goes in the direction of an ""Importance weighted Variational Information Bottleneck""."	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] We have not explored if and how using more expressive posteriors such as the qIW (p. 17, equation 35) can help the supervised bottleneck formulations in VDB or VIB.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] We address the comments about novelty in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ), for instance concerning the relationship to previous work, and the regularization penalty ||f||_M we propose.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] Although Dupuis& et.al's work establishes beautiful and complicated mathematical theory for replica exchange Langevin diffusions, they does not consider the discretization at all.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] It is not entirely clear however, why we would want to do so when modulating beta already suffices to tune the VAE towards autoencoding (low beta) or autodecoding behavior (high beta) depending on the requirement at hand.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] For beta values other than 1, a naive trick would be to plant qIW in liue of qphi in equation 37 (p. 18) to get a beta-IWAE of sorts.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] Our theory quantifies the discretization error and convergence rate and hence, ensures the validity to use the discretized algorithm.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] In particular, this implies we cannot trade-off reconstruction fidelity for learning more meaningful representations by incorporating bottleneck constraints.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] See ensuing discussion in p.18 following equation 36.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] One of our contribution is applying standard mathematical tools to a specific machine learning problem.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] For M > 1, the IWAE bound does not admit a decomposition like the standard ELBO (see equation 29 and 36) into a reconstruction loss term and a regularization term.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] The IWAE bound is known to be equivalent to the ELBO in expectation with a more complex approximate posterior qIW (see p.17, equation 34 and 35 and references therein in Appendix E.1).	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_answer [SEP] Indeed, the model is a minor contribution and, especially in light of a more thorough evaluation of RGCN, the sum attention RGAT results do not improve on those in Schlichtkrull et al. (2017).	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_structuring [SEP] [Response for 1]	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_structuring [SEP] More detailed comments are addressed below.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_structuring [SEP] Q1: About the main concern on “novelty, improvement relative to the current state of the art implementations,  and non-linearity of G and D”:	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_structuring [SEP] [Q] Limited amount of new insight.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] Besides, the large deviation rate function in our paper is different with that of Dupuis's since we use an alternative approach.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] While many papers have proposed that small batches may generalize better than large minibatches, it was recently pointed out by Shallue et al. that none of these experiments provide convincing evidence for this claim, because no experiment to date has compared small and large batch training under a constant step budget with a realistic learning rate decay schedule while independently tuning the learning rate at each batch size.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] Additional experiments in the appendix G go further and study how the optimal learning rate schedule changes as we increase the epoch budget.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] We also provide intriguing results as we vary the epoch budget, which demonstrate that the optimal learning rate which maximizes the test accuracy does not decrease as the epoch budget rises.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] Although our primary contributions are empirical, we also provided a detailed theoretical discussion in section 2, where we give a clear and simple account of why the two regimes arise.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] We quantify the acceleration effect by both large deviation and chi^2 divergence.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] This supports the notion that SGD has an optimal “temperature” which biases it towards solutions that generalize well.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] [A] Our paper presents many useful insights, namely: NS-GAN performs well, spectral norm is a good default normalization technique, gradient penalty should also be considered, even in combination with spectral norm but will cost substantially more in terms of computational resources, popular metrics such as KID and FID result in the same relative ordering of the models so there is no point in computing both, most Resnet tricks do not matter, etc.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] We are the first to run this experiment and conclusively establish that SGD noise does enhance generalization in popular models/datasets.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] We choose such a form of rate function because it is connected to the Dirichlet form, and hence, the convergence of chi^2 divergence.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] We would also like to emphasize that we make a significant contribution to the debate regarding SGD and generalization.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] All of these insights are supported by a fair and unbiased rigorous experimental process.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] Then, as some useful examples, in this paper, we particularly showed that the technique of negative feedback can be leveraged to stabilize GANs and developed NF-GAN, which was proven to be effective in our experiments.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] We believe this is an important contribution.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] Although this work is inspired from Dupuis's work, their setting is MCMC and they only investigate by large deviation.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_reject-criticism [SEP] Indeed, as agreed by R#2, this is a major novel contribution that provides a unified and promising framework to model the stability of GANs, which includes some recent developments (e.g., Negative Momentum and Reg-GAN, See Sec. 4.1 and Appendix A&C) and also provides us a possibility to explore advanced tools in control theory (e.g., nonlinear control and modern control theory [*3]) to improve both the stability and convergence speed of GANs.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_structuring [SEP] *	1.0
"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_structuring [SEP] The proposed method adapt a previously presented hierarchical clustering method in the ""standard space"" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model."	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_structuring [SEP] More specifically, we would like to draw the reviewer's attention to the following two papers:	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_structuring [SEP] The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_structuring [SEP] We found it important to revisit this topic in the light of recent substantial improvements to dynamics models (see below).	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_structuring [SEP] > Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_structuring [SEP] Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] Therefore, we did not include a comparison with Sparsely-Gated MoE in our article and only compare with full softmax.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] - VAE-nCRP [4]: VAE+(nCRP+GMM)	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] In addition, Dreamer propagates gradients through transitions in a learned features, making it effective for high-dimensional control tasks.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] Besides the important technical difference described above, we highlight the empirical performance of Dreamer.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] Also, if we take a naive pipelined approach of iterative training between the hierarchical Gaussian mixture model and representation learning, then this work would be an obviously incremental work.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] 1) Novelty of the convergence analysis: The paper by Yuan et al. did not present proof of convergence in the discrete-time setting.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] - Variational Deep Embedding (VaDE) [3]: VAE+GMM	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] It cannot achieve speedup because each expert still contains full softmax space as we mentioned in the background section (page 2 line 21st) and method section (page 2 last 4th line).	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] On the other hand, convergence analysis of momentum methods in non-convex setting is an important but under-explored area  (Yan et al., 2018).	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] It was designed to increase the model expressiveness.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] SVG(0) does not use a dynamics model.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] DS-8       | 0.257 | 0.448 | 0.530 | 2.84x |	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] Another work of the latter approach is:	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] Please take a look at Theorems 1 and 2 in [Yan18] and Theorem 3 in [Bernstein18].	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] [A2] VAE imposes a single Gaussian prior on embeddings, which leads to 1) the over-regularization, and 2) poor representations [1,2,5].	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] And since it is slower than the standard softmax by definition, we chose not to compare with it in the paper.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] while Sparse-Gated MoE (MoE) was not designed to do so.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] This is fundamentally different from Sparse-gated MoE. We divide the output space into multiple overlapped subsets.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] SVG clearly differs from Dreamer in that it only considers 1-step model predictions in SVG(1) or multi-step predictions without value function in SVG(∞).	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] While in full softmax or MoE, the complexity is linear with output dimension.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] To find top-k predictions, we only search a few subsets.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] A conclusion of the SVG paper was that the model did not yield substantial practical benefits beyond 1-step predictions.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] Our work attempts to the latter approach, which proposes a new prior called a hierarchical-versioned Gaussian mixture distribution prior to the first trial of hierarchical density estimation in the embedding space.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] The authors only provided convergence of the ODE models.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] - VAE with a VampPrior [5]: VAE+ a variational mixture of posteriors prior.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] Therefore, the recently published researches can be divided into two branches: 1) designing of an objective function by introducing the additional regularized terms, or 2) constructing of a more flexible prior.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] The contribution of these studies lies on 1) the formalization as a unified model based on the newly proposed prior, though not the original technique proposed by the authors, and 2) demonstrating the superiority of the prior.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] As expected, the Sparsely-Gated MoE does not achieve speedup in terms of softmax inference.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] Our algorithm addresses speed up in softmax inference.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_reject-criticism [SEP] Our work is for softmax inference speedup	1.0
In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion. [SEP] rebuttal_reject-criticism [SEP] All of these insights are supported by a fair and unbiased rigorous experimental process.	1.0
In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion. [SEP] rebuttal_reject-criticism [SEP] [A] We respectfully disagree: we believe that for GAN practitioners our paper presents many useful insights, namely: NS-GAN performs well, spectral norm is a good default normalization technique, gradient penalty should also be considered, even in combination with spectral norm but will cost substantially more in terms of computational resources, popular metrics such as KID and FID result in the same relative ordering of the models so there is no point in computing both, most resnet tricks do not matter, etc.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_concede-criticism [SEP] We admit that the computation process of F-pooling and the ECCV method is the same.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_concede-criticism [SEP] We apologise for this.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_concede-criticism [SEP] Two reviewers complained that it was difficult to tell from the text which contributions are novel and which also appear in previous works.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_concede-criticism [SEP] We are aware of the related work you mention.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_mitigate-criticism [SEP] A common criticism of this analogy is that SGD noise is not Gaussian when the batch size is small.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_mitigate-criticism [SEP] 2.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_mitigate-criticism [SEP] 3. We clarify the differences to some other recent papers in our reply to reviewer 1.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_mitigate-criticism [SEP] Our paper is the first to relate the two regimes of SGD to the popular analogy between SGD and stochastic differential equations (SDEs).	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_mitigate-criticism [SEP] -- It is worth noting that VCL has achieved very good results on most of these benchmarks, so it is very hard to outperform it with a large margin.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_mitigate-criticism [SEP] Our discussion in section 2 clarifies why the two regimes arise in practical deep learning models for which these conditions may not hold.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_mitigate-criticism [SEP] We will discuss their contribution explicitly in the updated text.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_mitigate-criticism [SEP] To our knowledge, we are the first to show that the analogy between SGD and SDEs holds for non-Gaussian short-tailed noise (appendix B).	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_mitigate-criticism [SEP] 1. Ma, Bassily and Belkin also introduced the notion of two regimes, however their theory holds for convex losses in the interpolating regime.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_mitigate-criticism [SEP] As we show in later sections, this perspective is crucial to understanding the influence of batch size and learning rate on test accuracy.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_done [SEP] Therefore, we updated Figure 7 in original manuscript to Figure 6c in the updated manuscript according to the new data.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_done [SEP] We conducted additional simulations to evaluate the runtime benefit of the proposed method compared to that of the method in [1].	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_done [SEP] The assumptions used for the simulation and analysis data have been updated in Figure 6c of the revised manuscript.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_done [SEP] Turning to our generalization experiments in sections 4 and 5.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] However, there are some important, in our opinion, differences between those two models, which also result in an improved training speed and stability of CWAE compared to WAE-MMD (see refined experiments in section 5, as well as figures in the appendix showing comparisons between proposed CWAE and WAE and SWAE models in the Appendix).	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] b) All algorithms should optimize both G and theta for a fair comparison.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] Q2: a) Optimizing both the controller and the hardware has been previously studied in the literature.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] The reviewer has noticed that the cw-distance resembles that of a U-statistic MMD estimate, and thus the proposed model very much resembles MMD itself.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] Is it worth using a neural graph?	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] The differences are:	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] POINTS 1 AND 2 OF THE REVIEW	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] Please refer to Section 3.1 and Section 3.4 for more details.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] The appropriate clarifications are given in the appendix B.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_structuring [SEP] Turning to our generalization experiments in sections 4 and 5.	1.0
Suggest missing related work [SEP] rebuttal_answer [SEP] The method is different from ours, except in the limiting case where M = 1 and beta =1, in which case it coincides with the beta-VAE and also with our method.	1.0
Suggest missing related work [SEP] rebuttal_answer [SEP] For beta values other than 1, a naive trick would be to plant qIW in liue of qphi in equation 37 (p. 18) to get a beta-IWAE of sorts.	1.0
Suggest missing related work [SEP] rebuttal_answer [SEP] In particular, this implies we cannot trade-off reconstruction fidelity for learning more meaningful representations by incorporating bottleneck constraints.	1.0
Suggest missing related work [SEP] rebuttal_answer [SEP] After taking a close look, we make the following observations:	1.0
"Suggest missing related work [SEP] rebuttal_answer [SEP] A similar argument goes in the direction of an ""Importance weighted Variational Information Bottleneck""."	1.0
Suggest missing related work [SEP] rebuttal_answer [SEP] We have not explored if and how using more expressive posteriors such as the qIW (p. 17, equation 35) can help the supervised bottleneck formulations in VDB or VIB.	1.0
Suggest missing related work [SEP] rebuttal_answer [SEP] For M > 1, the IWAE bound does not admit a decomposition like the standard ELBO (see equation 29 and 36) into a reconstruction loss term and a regularization term.	1.0
Suggest missing related work [SEP] rebuttal_answer [SEP] The IWAE bound is known to be equivalent to the ELBO in expectation with a more complex approximate posterior qIW (see p.17, equation 34 and 35 and references therein in Appendix E.1).	1.0
Suggest missing related work [SEP] rebuttal_answer [SEP] It is not entirely clear however, why we would want to do so when modulating beta already suffices to tune the VAE towards autoencoding (low beta) or autodecoding behavior (high beta) depending on the requirement at hand.	1.0
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_concede-criticism [SEP] We apologise for this.	1.0
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_concede-criticism [SEP] Two reviewers complained that it was difficult to tell from the text which contributions are novel and which also appear in previous works.	1.0
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_done [SEP] Turning to our generalization experiments in sections 4 and 5.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] ,  CWAE in generally learns faster than WAE-MMD, and has smaller dispersion of the cost-function during the learning process (see Figures 7 and 8, Appendix F).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] We agree that a more thorough study of the training method would be desirable but in this article we are concentrating on reporting the results on increased adversarial resistance.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] Interestingly, RtF [1] also does not consider the class-conditional priors even though they consider a classifier integrated VAE similar to CDVAE.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] We believe that these results should be interesting to the community even if some doubts about the training procedure remain.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] One caveat is that for general target function, the output needs to be properly scaled since our current analysis in Section 5 relies on linearizing the network.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] Note that in our bias-variance decomposition, only the bias term depends on the target function.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] As stated in our paper, many works tried to solve the problem of stochastic/statistical back-propagation.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] For instance, the bias under vanishing initialization is the same as that of least squares regression on the input, which can be solved under isotropic prior on $\beta$ via decomposing the activation function similar to Appendix C.5.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] While we don't claim that this explains the performance of the method under all circumstances, it gives an indication of how well it works.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] DiVA: $\mathbb{E}_{q_{\theta}\mathrm{(z|x)}}[\mathrm{log}\ p_{\theta '}(\mathrm{x|z)}] - D_{KL}[q_{\theta}\mathrm{(z|x)} || \hat{q}_{\phi}\mathrm{(z|c)]} + \lambda \mathbb{E}_{q_{\theta}\mathrm{(z|x)}}[\mathrm{log}\ \hat{p}_{\phi '}\mathrm{(c|z)}]$	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] CDVAE: $\mathbb{E}_{q_{\theta}\mathrm{(z|x)}}[\mathrm{log}\ p_{\theta '}(\mathrm{x|z)}] - D_{KL}[q_{\theta}\mathrm{(z|x)} || p\mathrm{(z)]} + \lambda \mathbb{E}_{q_{\theta}\mathrm{(z|x)}}[\mathrm{log}\hat{p}_{\phi '}\mathrm{(c|z)}]$	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] As a result, we can stably generate more realistic samples than CDVAE.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] In other words, our result on the variance (including Theorem 4) would still be valid for other targets, such as two-layer neural network.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] 2. When the target function is a multiple-neuron neural network, deriving the bias term can be challenging.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] (3) Please refer to our main contributions (ii)-(iii).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] As we discussed in section 4.1, below the table for Algorithm 1, the key difference is that we consider class-conditional Gaussian distributions as priors for variational posteriors.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] NGE has about 2x performance of (Sims, 1994) in both fish and walker environments.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] Lastly, we want to obtain a differentiable building block that can be used in standard neural nets.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] We consider our contributions in Secs. 4 and 5 as one step toward that final goal.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] We have shown in our experiments (see Fig. 3) that using a single RBM as a pre-processor will not result in increased adversarial resistance.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] As a result, CDVAE sometimes generates ambiguous samples (Figure 2 (c)).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] Since CDVAE assumes the prior as unit Gaussian for all classes and optimizes classification loss simultaneously with the KL divergence, the latent space does not follow the prior exactly.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] Propagating a gradient through a sampling based building block, while possible, would be considerably harder.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] A comparison of mean field training vs. constrastive divergence for RBMs has been made by [III,IV].	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] Additionally, since we do not have the noise in the learning process given by the random choice of the sample from normal density	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] However, we note that under the same setup, the bias may be obtained when the teacher is a slightly more general single-index model, i.e. $y=\psi(\beta^\top x)$ with Lipschitz link function $\psi$, equivalent to a single-neuron network.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] In contrast, we assume class-wise specific Gaussian for each class.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] The unrolling of the mean field iterations (see Fig. 1) provides a straightforward to achieve this.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_summary [SEP] Summarizing, in the proposed CWAE model, contrary to WAE-MMD, we do not have to choose parameters.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] But this description mischaracterizes the fundamental problem that we have identified and proposed a solution for.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] b) Instead of recombining the reward and cost directly on the environment side and learning a single value estimate, we train a critic network to output both return and penalty value estimates as well as the Lagrangian multipliers themselves, effectively providing more structure to the critic.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] Further, we experimented with the same threshold in table 4 of the new version and the results have shown that out of class unlabeled data are added even with an extremely small threshold such as 0.99999 (epsilon = 10^-5).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] Our method targets a better in-domain reconstruction, as depicted by Figure 1”.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] a) We introduce pointwise, per-state constraints to learn more consistent behavior compared a single global constraint, and regress the resulting state-dependent Lagrangian multipliers using a neural network to exploit generalization across similar states.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] We address the comments about novelty in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ), for instance concerning the relationship to previous work, and the regularization penalty ||f||_M we propose.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] A : It is a module that estimates the confidence of the softmax output according to the inputs of the classification network.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] Note that we already cite two of the suggested references by Reviewer 4, namely “Towards the limit of network quantization” and “ThiNet: A filter level pruning method for deep neural network compression” in our work.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] This contrasts our method, which effectively retains the knowledge in the generator using HAT like neuron masking and only loses information through “natural” forgetting.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] Novelty: Our method aims to solve the fundamental issue of d-separation in disentangled representation learning.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] Compression ratio	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] The method is not a heuristic; it is theoretically motivated by use of the double gradient, and inspired by the success of this in meta learning (e.g. MAML [1]).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] 6. While Theorem 4.7 is new and may be of independent interest in the optimization community,  it is not the main contribution in this paper.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] As pointed out in the Sec. 2 of our work, Deep Generative Replay (DGR) tries to prevent forgetting in the generator by retraining it from scratch every time a new data chunk becomes available.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] As we state in our introduction, using codebooks to compress networks is not new, as well as using a weighted k-means technique.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] The selection network is trained with sigmoid and binary cross-entropy in a supervised manner.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] 2. We further we would like to clarify a possible confusion of the proposed method to be a combination of Deep Generative Replay (DGR)[6] and HAT[2].	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] In our original paper (in table 10), there already exist results of softmax output for in or out of class unlabeled data with 0.9999 thresholds.	1.0
"Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] Thus, in DGR the generator would lose information at each replay step since the quality of generated samples highly depends on the quality of samples generated by the prior generator causing ""semantic drift""."	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] c) We show that we can train a single, bound-conditional policy that can optimize penalty across a range of bounds and can be used to dynamically trade off reward and penalty.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] Since softmax output is a relative value, the softmax output can be high for some out of class unlabeled data.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] This allows us to use “complete” learned representation during learning and inference of the subsequent tasks as well as speed up the training (no replay of G is involved).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] The selection network has advantages in out of class unlabeled data.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] It allows for a theoretically consistent way of obtaining factorisation in the posterior without any information-theoretic penalties.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] And the threshold is not 0.5 but high because selection network is learned with many ’1’ labels with close to 100 % training accuracy.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] However, as we state in the paper: “The closest work we are aware of is the one by Choi et al. (2016), but the authors use a different objective (their weighted term is derived from second-order information) along with a different quantization technique (scalar quantization).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_answer [SEP] We only combine the different terms appropriately for the actor update.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] However, there are some important, in our opinion, differences between those two models, which also result in an improved training speed and stability of CWAE compared to WAE-MMD (see refined experiments in section 5, as well as figures in the appendix showing comparisons between proposed CWAE and WAE and SWAE models in the Appendix).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] BERT-based models in the low-resource case is not very surprising	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] R: - Theoretical contribution	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] The differences are:	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] Extending result to other target functions:	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] POINTS 1 AND 2 OF THE REVIEW	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] In contrast to PlaNet, the difference is that this work learns an actor-critic model in place of online planning with the cross entropy method.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] **ANSWER**	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] (Difference with CDVAE) To clarify the difference our DiVA with CDVAE, we write derivations for both models here.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] 1.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] The reviewer has noticed that the cw-distance resembles that of a U-statistic MMD estimate, and thus the proposed model very much resembles MMD itself.	1.0
"Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] Hence, the effectiveness and advantage of the proposed methods are not clear."""	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] For completeness we will list these below:	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] 1. “The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily”	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] We refer the reviewer to the general response for details.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] Concern 1: Concerns with title “Meta Domain Adaptation”	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] that  “in the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed”.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] Q1: Robot design were explored in (Sims, 1994) etc. The novelty of the paper is fairly incremental.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] 1. The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] > 1.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] A:	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] “…unlike as advertised, the paper does not address	1.0
"Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] ""Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed."	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] Q2: “The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.”	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] … “	1.0
"Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] What is ""Selection Network""?"	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] R3: That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] We have addressed this by responding to the specific questions below.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] More detailed comments are addressed below.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] The appropriate clarifications are given in the appendix B.	1.0
"Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] The reviewer stated ""There exists more principled approaches for *selecting* out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors"":"	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_structuring [SEP] We agree that the problem might be significantly more difficult for different target functions, and would like to make the following remarks:	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] We note that only NGE among all the baselines has the ability to optimize both the graph G and the controller parameters.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Existing solutions are not effective in this setting, restricting their use in the real world.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] First, they learn a Q function rather than just a V function.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] With suitable settings, the shift consistency of F-pooling is much better.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Thus, GBDT can learn the stable and robust feature combinations.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Overall, we think that we have made an important contribution to Meta-Learning literature, by identifying its limitation for few-shot learning under domain shift, and proposed a solution to tackle this problem.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] In short, unfortunately, we have failed to understand how you could connect our technique to proximal functions and proximal gradient descent.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] 1) Proximal gradient descent is meant to solve a convex optimization problem while our aim is to solve a non-convex problem in which each local minimum exhibits vastly different test accuracy after compression.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] At a high level, previous approaches can be grouped into those using Reinforce gradients with V baselines (A3C, PPO, ACER) and those using deterministic or reparameterization gradients of learned Q functions (DDPG, SAC, MVE, STEVE).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] We believe that judging the novelty of a NAS paper solely by its architecture space is unfair.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Moreover, since our kernel is naturally introduced with the sliced approach and kernel smoothing, the choice of regularization parameter is given by the Silverman's rule of thumb, and depends on the sample size	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Arora et al. (2017) provide the standard definition of generalization error for GANs which is very different from the standard generalization error considered in supervised learning.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] However, we defend the novelty of our work.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] 2) Finding a particular flat minimum is the key to obtaining good model compression (and good generalization as well).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Such as how to deal with the imaginary part and the zero-padding of convolutions.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] We respectfully disagree and believe our contributions are significant.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Please check the degraded images in Table 3.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Consequently, the CWAE has, while being trained, potentially less stochastic perturbation then WAE-MMD.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] .	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Investigating many different local minima would be only available with large learning rate (as we have chosen for our experiments) and/or large amount of weight distortion.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] The values of our work are not how the output of F-pooling is computed.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Jumping to another local minimum from a certain minimum would not be easily achieved by convex optimization methods.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Furthermore, problem embedding is used by the recommender system to project the performance of students on the problems they solved onto other problems that they have not solved.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] While formulating a proximal function for model compression might be an interesting idea (if search space is highly limited) as the reviewer suggested, we believe that our proposed method is fundamentally different from proximal gradient descent approaches due to the following reasons:	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] 2) the learning of GBDT is just based on statistical information over full dataset.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] We also do think that the problem setting we have proposed is an important problem that deserves attention, and has not been studied in the meta-learning paradigm.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] In previous works, they even don’t give an operable definition shift-equivalence when down sampling involved.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] 1. We are not training Restricted Boltzmann Machines (RBMs), but Boltzmann machines where the hidden units can be fully connected.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Even though proximal gradient descent selects step size only considering convergence, Figure 1 can lead to the results such as Figure 2(b) which cannot be obtained if only local exploitation is employed.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] By contrast, we target towards more general situations in Secs. 4 and 5 where deterministic chain rule might not be applicable, such as for non-parameterizable (continuous) RVs.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Furthermore, no work in the literature theoretically guarantees that spectral normalization closes the generalization gap for either adversarial supervised learning or GAN unsupervised learning.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] The idea indeed was inspired by other works in auxiliary learning, but only to the extent that we also use auxiliary tasks to improve performance of a principal task.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Moreover, we discuss some practical problems of F-pooling.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] By just having the most sophisticated concept extractor, the concept continuity cannot be retrieved.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Dreamer is a novel algorithm that belongs to the family of actor critic methods.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] If the reviewer thinks our method is an incremental contribution or similar to previous algorithms, please list the specific references.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Addressing this setting in our framework gives us a direction to improve the practical utility of meta-learning solutions for few-shot learning.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Instead, the values are the strict definition of shift-equivalence and the theoretical properties of F-pooling.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] We observed that in many cases (like log-likelihood), the logarithm of the probability function works better, since it increases the role of examples with low-probability.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] We are glad that you also agree that setting makes sense (“... the combination … is fair”).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] As for the novelty of this method, although it seems more like an engineering solution, we believe that it makes a significant contribution in the field of deep active learning.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Specifically, while Reinforce estimators typically learn V functions, these are only used to reduce the variance of the gradient estimate rather than directly maximizing them with respect to the actor.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Due to the properties of the constructed Cramer-Wold kernel, we are able to substitute in the distance the sample estimation d(X,Y) of d(X,N(0,I)) given by its exact formula.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Since stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015) focuses mainly on reparameterizable RVs, deterministic chain rule as mentioned in main contribution (ii) can be readily applied.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] 4) Our effort to introduce optimal distortion step size and learning rate for a given compression problems is connected to exploration, not exploitation (which potentially supported by proximal functions where convergence matters).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] To the best of our knowledge, the traditional methods (such as (Sims, 1994)) require re-optimizing parameters of the controllers from scratch for each different topologies, which is computationally demanding and breaks the joint-optimization.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Finding a flat minimum has been known to be a difficult work as shown in the paper “On large-batch training for deep learning: generalization gap and sharp minima”, ICLR 2016.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Due to the inherent difference between supervised and unsupervised learning problems, the notion of generalization is defined differently between them.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Having concept embedding, concept continuity can be reached as is discussed in the last paragraph on page 7 and some other examples are given in table 2.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] In most experiments, the total time cost of GBDT part in TabNN is about several minutes, while the NN part often needs several hours for training.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] These images are damaged so badly that TV cannot recover any meaningful thing.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Please note that what’s done in Secs. 4 and 5 is not straight-forward and has not been reported before.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] As a combination of problem setting and proposed solution, we do believe we have addressed an important problem, and made a novel contribution.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Moreover, we argue the videos of (Sims, 1994) might be confusing as it mixes the results of policy evolution from human-designed robots and structure evolution.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] 1- There are two reasons that concept and problem embedding are performed in this work.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Note that by just having the concepts of problems that are not in numerical form, performance projection may not be feasible and there is a need for using other methods like embedding.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Second, the actor only maximizes the Q value predicted for the current time step rather than maximizing multi-step value estimates.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] While MVE and STEVE learn dynamics models (from proprioceptive inputs), the dynamics are not directly used to update the policy.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] 3) While proximal gradient descent can be useful to find a certain local minimum close to the starting point given a convex constraint, wide exploration (associated with possibly transient accuracy loss in the initial training as shown in Figure 2.(b)) is necessary to escape from a point with sharp loss surface.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] While there are the approaches that aim to detect out-distribution sets, they have not been designed for the selection purposes as we do.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] To further showcase our work with respect to prior art, we added (Sims, 1994) as an additional baseline in the latest revision.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] According to the best knowledge of the authors, the Cramer-Wold kernel (which defines the Cramer-Wold metric), except for the classical RBF kernel, is the only known characteristic kernel which has closed form for radial gaussians, and we believe the respective computations in other cases (like the inverse quadratic kernel used in WAE-MMD), would be highly nontrivial.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Such an exploration, however, cannot be obtained by a proximal function since we need to investigate lots of different local minima with different amount of flatness in loss surface.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] (contrary to WAE-MMD, where the parameters are chosen by hand, and in general do not depend on the sample size)	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] A2: Please refer to Q1.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Please refer to our general response for more of F-pooling’s values.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] A: While ReMixMatch comprises many components (some of which are new), we believe our ablation study justifies the reason why each component exists. If there are additional ablation experiments that you think would be helpful for us to run, please let us know.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] As a handcrafted prior, TV performs much worse than our data-driven baseline method in these tasks.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Considering concept continuity is an important matter in education.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] As we emphasized many times in our paper, the success of DNN in domains such as image, speech and text, is built on the comprehensive exploration of the locality-based patterns, which motivates us to first find such patterns of features in tabular data automatically and then build up NN architecture based on these discovered patterns.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Graph neural network formulation is KEY here, enabling it to perform this efficient policy transfer.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] We have observed interesting patterns, e.g. similar problems are more likely to be solved correctly at the same time or wrong at the same time.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Specifically, we identify that the principle of image-to-image translation is very suitable for this setting, and apply those concepts to boost the performance of few-shot learning under domain shift.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] In comparison, Dreamer uses reparameterization gradients of V functions by backpropagating the value estimates through the latent dynamics.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] This way, we have an idea of what problems should be recommended to them and which problems should not by having an evaluation of their ability to solve unseen problems and recommend problems in the boundary of their capacity, not way beyond, and to recommend problems in a way that covers all concepts necessary for students to learn.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Thus, instead of using an additional weighting parameter lambda (as in WAE-MMD) whose aim is to balance the MSE and divergence terms, we decided to automatically (independently of dimension) balance the two terms of the loss function, by taking the logarithm of the divergence.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Thus, GBDT is just a tool we adopt to mine the patterns and do feature grouping since GBDT is an efficient and convenient method for these pre-processing tasks: 1) GBDT is very fast.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] This is the core idea of this paper.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] A2: We think you underestimate the difficulty of those restoration problems.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Instead, they only serve for computing multi-step Q targets for learning the Q critic.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] 1. Novelty: To the best of our knowledge, this is the first paper presenting a simple solution to generating useful auxiliary tasks in a self-supervised manner.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] And in fact, most existing NAS papers share the same architecture search space, the main differences between them is the search strategy.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Moreover, we never claim the main contribution of our work lies in augmenting the search space.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] We firmly believe that our search space exploration method based on optimal distortion step size and amount of weight distortion enable us to produce better local minima well-suited to various model compression techniques.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] We prove that one can utilize our GO to sequentially back-propagate gradient though non-parameterizable continuous RVs, namely the statistical chain rule mentioned in main contribution (ii).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Thus, no gradients are backpropagated through the dynamics model for learning the actor or critic.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] Actor-critic algorithms that use analytic gradients of Q critics differ from Dreamer in two ways.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] We have identified a novel problem setting, which is closer to the real world setting, than what has been studied so far under the meta-learning paradigm.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_reject-criticism [SEP] CWAE, as compared to WAE-MMD, has no parameters (while WAE-MMD has two).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_mitigate-criticism [SEP] While this result may not look surprising, to the best of our knowledge it was not addressed before for the specific case of BERT.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_mitigate-criticism [SEP] In [1], the authors claim that a large pre-trained model can be very helpful in transfer learning scenarios, and they also suggest how to best fine-tune BERT.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_mitigate-criticism [SEP] GAN inference and adversarial training seek different goals.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_mitigate-criticism [SEP] Adversarial training addresses a supervised learning task while GAN inference focuses on an unsupervised learning problem.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_mitigate-criticism [SEP] We believe this combination method could be used for other things, and have presented it here as a proof of concept rather than a definitive survey with all possible uses.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_mitigate-criticism [SEP] However, the focus of the paper is to contribute a new prediction problem and adapts and applies the Transformer model for this problem to establish a benchmark for future exploration, which we believe has values.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_done [SEP] Nevertheless, we have added a comparison between our method and the exact solution for a small Boltzmann machine of 16 units in the appendix.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_done [SEP] By mentioning this statement, if the reviewer means the missing of some principled approaches like ODIN in our comparisons, we would like to inform the inclusion of ODIN results in the revised version of the paper.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_done [SEP] We fully agree with the reviewer, that CWAE is a model based on the kernel as the divergence measure for distributions, and consequently can be seen as a modified variant of WAE-MMD (we have added the respective comments in the paper, see the extended introduction, and added a section B in the appendix, which discusses the comparison in more details).	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_done [SEP] In fact, we have mostly changed the name from “Meta Domain Adaptation” to “Meta Learning with Domain Adaptation”, and the rest of the paper is almost identical, which we believe addresses the concerns of false advertising.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_done [SEP] We updated Section 4.4 to indicate that it is to be expected that although our SAVP model improves on diversity and realism, it also performs worse in accuracy compared to pure VAE models (both our own ablation and SVG from Denton & Fergus (2018)).	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] The method is different from ours, except in the limiting case where M = 1 and beta =1, in which case it coincides with the beta-VAE and also with our method.	1.0
"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] A similar argument goes in the direction of an ""Importance weighted Variational Information Bottleneck""."	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] We have not explored if and how using more expressive posteriors such as the qIW (p. 17, equation 35) can help the supervised bottleneck formulations in VDB or VIB.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] However, even if f_j(z) is not much high, the softmax output can be close to 1 with extremely smaller values for other f(z)s	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] = 1 / ( 1 + exp(-f_j(z)) × (exp(f_1(z)) + ... + exp(f_j-1(z)) + exp(f_j+1(z)) + ... exp(f_n(z)))	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] where z, f(z), and g(z) represent the final layer of the backbone network, classification network, and selection network respectively.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] Thereby we introduce Dynamic Generative Memory (DGM) - an adversarially trainable generative network endowed with neuronal plasticity through efficient learning of sparse attention mask for layer activations.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] The sigmoid function : 1 / (1 + exp(-g(z)))	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] It is not entirely clear however, why we would want to do so when modulating beta already suffices to tune the VAE towards autoencoding (low beta) or autodecoding behavior (high beta) depending on the requirement at hand.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] The softmax function : exp(f_j(z)) / sigma(exp(f_k(z))	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] For beta values other than 1, a naive trick would be to plant qIW in liue of qphi in equation 37 (p. 18) to get a beta-IWAE of sorts.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] In particular, this implies we cannot trade-off reconstruction fidelity for learning more meaningful representations by incorporating bottleneck constraints.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] Even at 0% of the non-animal data, performance is lower than the fixed mode of the sigmoid.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] See ensuing discussion in p.18 following equation 36.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] We highlight the differences to DGR [3] in the Sec. 2 of our work.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] because:	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] The formulae of the softmax and sigmoid are as follows.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] The softmax output : 1 / ( 1 + exp(-f_j(z)) × 0) = 1	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] Hereby we extend the idea of [2] to generative networks.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] For M > 1, the IWAE bound does not admit a decomposition like the standard ELBO (see equation 29 and 36) into a reconstruction loss term and a regularization term.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] The IWAE bound is known to be equivalent to the ELBO in expectation with a more complex approximate posterior qIW (see p.17, equation 34 and 35 and references therein in Appendix E.1).	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] As you said, if f_j(z) is very high and the other f(z)s are moderate, it can work like sigmoid.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] Although epsilon was 10^(−4) (threshold = 0.9999), an average of about 800 unlabeled data was added for the case of 100% of the non-animal data.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] We experimented with a high softmax output threshold (epsilon = 10^(−4)).	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_answer [SEP] First, we address the catastrophic forgetting problem in continual learning.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_structuring [SEP] Comment: Although the idea of generating contrastive explanations is	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_structuring [SEP] Q1. Comparison with [1, 2, 3, 4].	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_structuring [SEP] Dhurandhar et al., arXiv:1802.07623.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_structuring [SEP] Regarding your comments about the triviality of our paper,	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_structuring [SEP] quite interesting, it is not that novel. See Kim et al., NIPS’16,	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_structuring [SEP] In particular:	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_done [SEP] We now have added related work about video compositional methods in section 2.3 in the second version of the paper.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_done [SEP] We are now also citing the paper Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_done [SEP] * In relation to the connection to IWAE, we have included a detailed discussion in Appendix E.1.	1.0
While sensible, this seems to me to be too minor a contribution to stand alone as a paper. [SEP] rebuttal_done [SEP] A: To give more clues about the good behavior of the algorithm, we added results about the accuracy of the sampled trajectories on the artificial datasets (for which we have the ground truth on who infected whom).	1.0
While sensible, this seems to me to be too minor a contribution to stand alone as a paper. [SEP] rebuttal_done [SEP] Results show that our approach actually performs better infector choices than CTIC which does not consider the history of the diffusion in its infection probabilities.	1.0
While sensible, this seems to me to be too minor a contribution to stand alone as a paper. [SEP] rebuttal_done [SEP] We report the rate of good infector choices (i.e., the rate of I_i that equal the ground truth) for our approach and the others.	1.0
While sensible, this seems to me to be too minor a contribution to stand alone as a paper. [SEP] rebuttal_done [SEP] We also added a second artificial dataset to further analyze the behavior of the approaches.	1.0
Missing explanation of utility of proposed method [SEP] rebuttal_done [SEP] Moreover, we have detailed some limitations of Euclidean spaces in Appendix B.	1.0
Missing explanation of utility of proposed method [SEP] rebuttal_done [SEP] We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data.	1.0
Missing explanation of utility of proposed method [SEP] rebuttal_done [SEP] This is in accordance with the previous related work on non-Euclidean embeddings.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] We can justify PDR theoretically as an inductive bias on the language model.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] LPIPS linearly calibrates AlexNet feature space to better match human perceptual similarity judgements.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] A1: Many state-of-the-art approach, including SRCNN and SRGAN, has their own implicitly defined degradation function.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] Such experimental results support our claim that the proposed generative classifier can improve the robustness against adversarial attacks as it utilizes multiple hidden features (i.e., harder to attack all of them).	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] A RNN based language model models the first dependence (and more long range ones) and our proposed PDR tries to model the second one.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] Although we proposed Past Decode Regularization (PDR) with language modeling in mind to exploit the symmetry between the input and output vocabulary (and the corresponding word embedding and softmax layer), any model/task that has this symmetry can potentially use a PDR term.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] Aside for the first two predicted frames, our VAE ablation and the SVG model both achieve similar SSIM and LPIPS.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] Re: (W6) Motivation for CFS	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] In a similar spirit, we wanted to quantitatively study this surprising phenomenon, and we were curious about how densely is information encoded in representations.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] Currently, the proposed method cannot be directly applied to multi-layer sigmoid belief networks (without the procedure in Appendix B.4).	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] The experiments from our original submission cropped the videos into a square before resizing, and thus discarded information from the sides of the video.	1.0
"Limited insights based on design choices [SEP] rebuttal_answer [SEP] In a unidirectional language model, we cannot look into the future tokens and hence we use the output distribution as a proxy for the ""true second word"" and decode the distribution of the first word."	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] Thus the PDR term can be thought of as biasing the language model to retain more information about the distribution of the first word given the second word in a bigram.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] The observed bigrams in a language are not random and the distribution of the second word given the first word in a bigram is not uniform.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] They use their function F to generate training samples during their training process, while we use our explicitly defined function F during the inference process.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] We believe that procedure could be useful for the inference of models like the multi-layer sigmoid belief networks.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] In early experiments, we trained our pure VAE model on the stochastic shape movement dataset from Babaeizadeh et al. (2018), and our pure VAE was able to model the dataset without any blur and with perfect separation of the possible futures.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] However, that is typically not the case of synthetic datasets.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] As for the conditional independency, it is actually removed after marginalizing out additional continuous RVs (which could be non-reparameterizable RVs like Gamma).	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] In both setups, our inference method is shown to be more robust compared to the softmax inference.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] Further, there are a few initial works that show that certain characteristics like length [1][2], sentiment [3], presence and absence of tokens like brackets [4] are densely captured in a single dimension of the representation space.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] Also note that one can strengthen the aforementioned procedure by inserting more additional continuous internal RVs into the inference model to enlarge its (marginal) description power.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] Similarly, the distribution of the first word given the second word will be far from uniform.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] > There is a rich literature concerning what information is captured in the representations.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] If the assumed degradation function F is not exactly the function in real scenarios, both these state-of-the-art approach and our method will suffer.	1.0
Limited insights based on design choices [SEP] rebuttal_answer [SEP] After examining the KTH results further, we realized that our results are likely weaker than they should have been, because we did not use the same preprocessing as prior work.	1.0
"Limited insights based on design choices [SEP] rebuttal_structuring [SEP] 2a) - ""The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications."""	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] a. Limited contribution to the qualitative understanding of the optimizers	1.0
"Limited insights based on design choices [SEP] rebuttal_structuring [SEP] ""Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed."	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] Question 3:	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] Perhaps other referee will have a clearer opinion.	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] The reviewer notes the need to emphasize how and why to use this approach.	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] While the paper is a pleasant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation.	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] Q5. Evaluation on adversarial attacks.	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] - There are a few points here:	1.0
"Limited insights based on design choices [SEP] rebuttal_structuring [SEP] Hence, the effectiveness and advantage of the proposed methods are not clear."""	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] Q1: The degradation function F is challenging to obtain in real world scenarios.	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] a. Limited contribution to the qualitative understanding of the optimizers:	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] *	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] We have addressed this by responding to the specific questions below.	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] -- Comment on scale / speed for large instances of combinatorial optimization:	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] MOTIVATION:	1.0
Limited insights based on design choices [SEP] rebuttal_structuring [SEP] Re: Utility of the methods is a bit unclear	1.0
Limited insights based on design choices [SEP] rebuttal_by-cr [SEP] We are currently rerunning the KTH experiments and we plan to update the results in the paper.	1.0
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] The referee indicates (in sharp contrast to referee 3) that the paper is not nicely written, nor easy to follow.	1.0
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.	1.0
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] Furthermore, we believe (although acknowledge that this is subjective) that curiosity-driven questions about how the information is encoded are interesting: while they might not be useful in a way that is easily measurable by quantifiable metrics, they provide insights that can help guide future work.	1.0
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models.	1.0
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] A: While ReMixMatch comprises many components (some of which are new), we believe our ablation study justifies the reason why each component exists. If there are additional ablation experiments that you think would be helpful for us to run, please let us know.	1.0
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] We did our best to write the paper such that it includes all details needed to fully understand the proposed method and its theoretical background.	1.0
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] So it is unfair to criticize our motivation just because we explicitly write out the degradation function F.	1.0
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] With the rapid development of industrial and scientific knowledge graphs, we believe (and agree with the Reviewer #2) that learning rules that involve multiple modalities is an important and relevant problem.	1.0
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] Indeed, such rules can not only be used for data cleaning and completion, but they are also themselves extremely valuable assets carrying human-understandable structures that support both symbolic and subsymbolic representations and inference.	1.0
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] The importance of a proper hyperparameter search protocol is emphasized by Choi et al., 2019 (published after our submission and under review at ICLR).	1.0
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] [A] Our paper presents many useful insights, namely: NS-GAN performs well, spectral norm is a good default normalization technique, gradient penalty should also be considered, even in combination with spectral norm but will cost substantially more in terms of computational resources, popular metrics such as KID and FID result in the same relative ordering of the models so there is no point in computing both, most Resnet tricks do not matter, etc.	1.0
Limited insights based on design choices [SEP] rebuttal_reject-criticism [SEP] This is missing in existing papers, which consider best attained performance alone.	1.0
Limited insights based on design choices [SEP] rebuttal_done [SEP] Moreover, we have detailed some limitations of Euclidean spaces in Appendix B.	1.0
Limited insights based on design choices [SEP] rebuttal_done [SEP] However, as stated in the last paragraph of Sec. 7.2, we presented in Appendix B.4 a procedure to assist our methods in handling discrete internal RVs.	1.0
Limited insights based on design choices [SEP] rebuttal_done [SEP] In the revised draft, we also consider optimization-based adaptive attacks against our method under the black-box setup (see Table 5) and the white-box setup (see Table 10).	1.0
Limited insights based on design choices [SEP] rebuttal_done [SEP] We added Section 3.5 to point out the differences between the VAE component of our model and prior work.	1.0
Limited insights based on design choices [SEP] rebuttal_done [SEP] We have made an explicit statement of this in the revised manuscript.	1.0
Limited insights based on design choices [SEP] rebuttal_done [SEP] This is in accordance with the previous related work on non-Euclidean embeddings.	1.0
Limited insights based on design choices [SEP] rebuttal_done [SEP] The purpose of adding adversarial losses to a pure VAE is to improve on blurry predictions where the latent variables alone cannot capture the uncertainty of the data.	1.0
Limited insights based on design choices [SEP] rebuttal_done [SEP] We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data.	1.0
Limited insights based on design choices [SEP] rebuttal_done [SEP] We will incorporate this discussion in the updated version of the paper.	1.0
Limited insights based on design choices [SEP] rebuttal_done [SEP] We have included a revised plot in Figure 14 (note that this temporary plot will be incorporated into Figure 6), where we use the official implementation of SSIM and replace the VGG metric with the LPIPS metric (Zhang et al., 2018).	1.0
Limited contribution in problem definition [SEP] rebuttal_reject-criticism [SEP] We would first like to refer the referee to third paragraph of the introduction, where we explicitly formulate the main shortcoming of compressed sensing:	1.0
Limited contribution in problem definition [SEP] rebuttal_reject-criticism [SEP] We respectfully disagree with the referee’s conclusion that the method does not support a significant contribution.	1.0
Limited contribution in problem definition [SEP] rebuttal_reject-criticism [SEP] Then, in the list of main contributions, we write:	1.0
Limited contribution in problem definition [SEP] rebuttal_reject-criticism [SEP] We propose a fully-probabilistic generative model for trainable sampling, that exploits both the underlying data distribution and information to solve the downstream task of interest.	1.0
Limited contribution in problem definition [SEP] rebuttal_reject-criticism [SEP] “DPS: A new regime for task-adaptive subsampling using a novel probabilistic deep learning framework for jointly learning a sub-Nyquist sampling scheme with a predictive model for downstream tasks”	1.0
Limited contribution in problem definition [SEP] rebuttal_reject-criticism [SEP] Subquestion 2:	1.0
Limited contribution in problem definition [SEP] rebuttal_reject-criticism [SEP] This opens up a vast array of new opportunities in compressed sensing.	1.0
Limited contribution in problem definition [SEP] rebuttal_reject-criticism [SEP] “These [compressed sensing] methods, however, are lacking in the sense that they do not fully exploit both the underlying data distribution and information to solve the downstream task of interest.”	1.0
Limited contribution in problem definition [SEP] rebuttal_reject-criticism [SEP] Our generative model builds upon recent advances on Gumbel max and top-k reparameterizations and their relaxations, showing for the first time how discrete sample selection can be done in a data-driven and task-adaptive fashion.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_answer [SEP] However, instead of trying to directly solve the main problem (estimate the frequency of each element), our oracle is a subroutine that tries to predict the best resource allocation --i.e., it tries to answer the question of which elements should be given their own buckets and which should share with others.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_answer [SEP] Vassilvitskii ’18, Purohit et al, NIPS’18, belong to a growing class of studies that use a machine learning oracle to improve the performance of algorithms.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_answer [SEP] In Kraska’18 and Mitzenmacher’18, the oracle tries to directly solve the main problem, which is: “is the element in the set?” An analogous approach in our case would be to train an oracle that directly outputs the frequency of each element.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_answer [SEP] This motivates our design to split heavy and light items using the learned model, and apply separate algorithms for each type.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_answer [SEP] There are other differences.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_answer [SEP] Our paper, as well as the works of Kraska et al ’18, Mitzenmacher ’18,  Lykouris &	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_answer [SEP] In contrast, in existence indices, all collisions count equally.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_answer [SEP] Finally, our theoretical analysis is different from M'18 due to the intrinsic differences between the two problems, as outlined in the previous paragraph.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_answer [SEP] All such papers use a learned oracle of some form.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_answer [SEP] The key differences are in what the oracle does, how it is used, and what can be proved about it.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_answer [SEP] For example, the main goal of our algorithm is to reduce collisions between heavy items, as such collisions greatly increase errors.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_concede-criticism [SEP] We agree that this method of composing simple functions to compute more complex ones is intuitive, and may not be very surprising, but we think that this helps data and model efficiency in a different manner than presented in previous papers.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_concede-criticism [SEP] We agree with the reviewer that the model novelty is relatively incremental.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_concede-criticism [SEP] It is true, that one can describe the method as a (non-trivial) combination of beta-vae + GAN.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_concede-criticism [SEP] We are glad that the reviewer agrees that we are tackling a long standing and important problem and acknowledge the fact that neither the definition of constrained MDPs nor the application of Lagrangian relaxation to solve these problems is novel by itself.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_concede-criticism [SEP] We should have stated our exact technical contributions more clearly and have adapted the paper to do so.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_concede-criticism [SEP] We acknowledge this problem and agree with you about a possible misinterpretation.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_concede-criticism [SEP] We developed our approach based on Transformer models.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_concede-criticism [SEP] We admit that the computation process of F-pooling and the ECCV method is the same.	1.0
Limited novelty in theoretical contribution [SEP] rebuttal_concede-criticism [SEP] As far as scaling up the tasks and problem sizes, we are showing a method of combination here, and are scaling up the problem sizes continuously.	1.0
Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology) [SEP] rebuttal_done [SEP] Just for additional reference, we tested Sparsely-Gated MoE with different experts in PTB dataset; we compared the results to DS-Softmax.	1.0
Suggest missing related work [SEP] rebuttal_done [SEP] We are now also citing the paper Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016.	1.0
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_mitigate-criticism [SEP] A common criticism of this analogy is that SGD noise is not Gaussian when the batch size is small.	1.0
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_mitigate-criticism [SEP] 2.	1.0
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_mitigate-criticism [SEP] Our paper is the first to relate the two regimes of SGD to the popular analogy between SGD and stochastic differential equations (SDEs).	1.0
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_mitigate-criticism [SEP] Our discussion in section 2 clarifies why the two regimes arise in practical deep learning models for which these conditions may not hold.	1.0
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_mitigate-criticism [SEP] We will discuss their contribution explicitly in the updated text.	1.0
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_mitigate-criticism [SEP] To our knowledge, we are the first to show that the analogy between SGD and SDEs holds for non-Gaussian short-tailed noise (appendix B).	1.0
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_mitigate-criticism [SEP] 1. Ma, Bassily and Belkin also introduced the notion of two regimes, however their theory holds for convex losses in the interpolating regime.	1.0
When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches). [SEP] rebuttal_mitigate-criticism [SEP] As we show in later sections, this perspective is crucial to understanding the influence of batch size and learning rate on test accuracy.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] Fortunately, we were able to find an existing solution that satisfies the requirements: CycleGAN. Any other domain translators that satisfy the conditions can be used or newly studied.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] 1. We should translate only the style (a global pattern of a specific domain) as keeping outline patterns of given images.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] 2. We should consider an unpaired domain translation between real and generated images because the generated images are sampled randomly.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] By introducing class conditional priors induced by the mutual information maximization, DiVA yields class-wise discriminative one mode Gaussians for latent variable z.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] Naturally, DiVA can conduct both class prediction and class conditional sample generation with one integrated model.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] We also consider the poor results on MUTAG to be significant and informative for the rest of the community in developing more powerful models that can be applied to relational graphs.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] We assumed that this is due to the vulnerability of neural networks [3] triggered by different distributions of pixel values between real and generated images.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] However, we would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] To narrowing the distribution gap, we needed a solution that satisfies two conditions (also described in section 5):	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance model contributions of the paper.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] This leads us to a more theoretical formulation for classification-regularized VAE.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] (Zhang et al. 2018)	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] However, this is not a mathematics conference after all.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] Thus, we defined the two domains: real domain and sample domain.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] With the solution, we could make a breakthrough for GR-based methods.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] .	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] To the best of our knowledge, this is the first successful approach for a GR-based algorithm to start to resist the catastrophic forgetting problem with a natural image dataset.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] Since they do not consider the conflict between the unit Gaussian prior and discriminative loss for the latent variable z, their models generate ambiguous samples that negatively affect the performance of incremental learning, which is discussed in section 4.1 in our paper.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] Our first open question was that why other GR-based algorithms [1, 2] assume unit Gaussian priors even though they integrate classification loss into their VAE formulation.	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs	1.0
Not enough originality in results (not surprising) [SEP] rebuttal_mitigate-criticism [SEP] The second open question was that why GR-based algorithms suffer from serious catastrophic forgetting in natural image datasets, even though generated samples are not completely noisy.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] ————————————————————————————	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] For example, we have already begun porting this scheme to a neural cross-lingual summarization project of ours.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] We have found that occasional distortions (not compressing weights for every mini-batch like previous techniques), relatively large learning rate, and training batches in full-precision (unlike previous ones which store compressed weights during entire training) would be the key to recovering or even increasing the accuracy.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] - Our pruning method is fundamentally different from the previous ones because we do not incorporate a masking layer.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] - Low-rank approximation results on PTB (Figure 2) shows even higher compression rate compared with weight pruning (Table 3), which is surprising to us because pruning has been known to show much higher compression ratio compared with SVD (fine-grain vs. coarse-grain or structured).	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] The main difference between our method and [1, 2] is that we do not directly train the Gaussian mixture model, i.e., generative classifier but we post-process it on hidden feature spaces of pre-trained deep models.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Response: Dhurandhar et al. does use the term constrastive explanation.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] We think this is also an interesting form of explanation, but a different	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] While the weight formats after model compression follow well known ones, our model compression method is significantly different from the existing ones.	1.0
"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] classification of ""A.""  This is a different constrast than ours"	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] /	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] ( 22.27 / 0 )	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] While previous pruning ideas keep zero weights during training, we do not have any zero weights at any moment except at the weight distortion step.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] ( 20.24 / 1.2 )	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Entity pre-training accounted for a 0.63% increase in our ablation experiment.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] We believe that our paper suggests a wide view on how model compression should be performed.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] As we discussed in the paper, investigating various local minima is crucial for good model compression.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Furthermore, problem embedding is used by the recommender system to project the performance of students on the problems they solved onto other problems that they have not solved.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] The large range in our estimate is because [4] trained for a wide range (60-200) of epochs.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] supervised	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] (17.84/2,338.8 )	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] activation function / softmax (error/added data) / sigmoid (error/added data)	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] (20.71 / 864.0 )	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] By just having the most sophisticated concept extractor, the concept continuity cannot be retrieved.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] If our technique is a simple extension from the previous ones, we could not obtain such impressive results with high compression rate and improved accuracy.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] ( 18.72 / 2580.0 )	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] dataset as a whole for examples that help explain.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] 100%	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] This shows the limitation of thresholding with softmax.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] (18.35 / 3,350.4 )	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] training. “Do not perform quantization at every batch, but instead recover accuracy through full-precision training, high learning rate, and occasional quantization” is the key message.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] - To the best of our knowledge, our specific implementation of entity pre-training is novel.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] A: We do not agree with your opinion.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] The result of new SSL problems on the CIFAR-10 dataset with 5 runs are as follows:	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] We undertook an extensive initial phase of experiments where we discovered non-trivial contributions for achieving SOTA performance and fast convergence.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] - As opposed to previous models (e.g. [2], [3]) we were able to drop all recurrent architectures, which reduced training times substantially.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Having concept embedding, concept continuity can be reached as is discussed in the last paragraph on page 7 and some other examples are given in table 2.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] 50%	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] - Overall, our occasional compression is a significant one since we can greatly reduce amount of computation overhead from compression.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] why is it not class B?	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Our technique can be ported to other neural, multi-task setups with a strong dependence between tasks (e.g. where a model must learn to perform some task before attempting to learn one or more dependent tasks jointly).	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] In addition, we study a robust inference method to handle noisy labels in training samples, while they did not.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] (18.27 / 4,306.8)	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] 1- There are two reasons that concept and problem embedding are performed in this work.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] 0%	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Note that by just having the concepts of problems that are not in numerical form, performance projection may not be feasible and there is a need for using other methods like embedding.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] - Training models after compression in order to recover accuracy is as important (if not more) as compressing weights.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] On the CoNLL04 test set, it accounts for a 1.26% boost in performance, which is large relative to historic improvement on this corpus [1].	1.0
"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] contrasts ""A"" to ""B"" (rather than ""present for A"" to ""absent for A"")."	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] - Quantization is performed also in a very different way.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] 25%	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] problem: for a given example (perhaps not even from the training set),	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] For the ADE corpus in particular, we advance SOTA RE performance by >10%, which is substantially larger than improvements have been historically [5].	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Our work is one of the first architectures for joint NER and RE to do this.	1.0
"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] However, they look at the question of ""why A?""  Contrastive in their case"	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Considering concept continuity is an important matter in education.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] (18.38 / 1470.0 )	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Unlike previous ones, we do not consider quatization during	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Kim et al. also has a different form of model criticism; they look at the	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] (19.04 / 811.2 )	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] We look at a different	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] We have observed interesting patterns, e.g. similar problems are more likely to be solved correctly at the same time or wrong at the same time.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] This way, we have an idea of what problems should be recommended to them and which problems should not by having an evaluation of their ability to solve unseen problems and recommend problems in the boundary of their capacity, not way beyond, and to recommend problems in a way that covers all concepts necessary for students to learn.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Next, [3,4] also assume clean training labels, and aim for detecting abnormal test samples after ’clean’ training.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] that	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Because previous papers do not report their training times, we contacted the authors of a comparable method [4] for their training times and found that our method converged between 3-35X times faster for the ACE04, ADE, and CoNLL04 corpora (keeping in mind that we did not train on the same hardware).	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] It is also complementary to [6] (published in ACL this year), by demonstrating similar performance without the need for templated queries, which, as pointed out in our introduction, may become a limiting factor where domain expertise is required to craft such questions (e.g., for biomedical or clinical corpora).	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] - Exploring large search space in much wider area is suggested in this paper through large distortion step and large learning rate (note that many compression-aware techniques perform compression at every batch has distortion step of “1” while much smaller learning rate for retraining that normal training is chosen).	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] refers to whether something is or is not present that drives the	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] - Our low-rank approximation is also unique one since 1) we do not alter the structure for training even after performing SVD, 2) very high learning rate associated with transient accuracy loss is allowed for DeepTwist, and 3) we change SV spectrum continuously while the previous ones perform SVD only once (in practice, retraining low-rank approximated model has been considered to be very difficult, if not impossible).	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] - Even though our pruning method is even simpler compared to the previous ones, compression rate is significantly better or very close to the one based on sophisticated Bayesian inference model.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] Our model serves as a strong baseline for future studies on joint NER and RE architectures and provides guidance on how to best integrate a pre-trained language model into such an architecture.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] 75%	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] one.	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] (20.33 / 1,711.2 )	1.0
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. [SEP] rebuttal_reject-criticism [SEP] ( 20.07 / 315.6 )	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] - VAE-nCRP [4]: VAE+(nCRP+GMM)	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] As for the novelty of this method, although it seems more like an engineering solution, we believe that it makes a significant contribution in the field of deep active learning.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] Also, if we take a naive pipelined approach of iterative training between the hierarchical Gaussian mixture model and representation learning, then this work would be an obviously incremental work.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] - Variational Deep Embedding (VaDE) [3]: VAE+GMM	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] We don’t claim to be the first to generate questions from logical form, but the experiments within show that our approach is superior to standard approaches in the literature.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] - VAE with a VampPrior [5]: VAE+ a variational mixture of posteriors prior.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] Our work attempts to the latter approach, which proposes a new prior called a hierarchical-versioned Gaussian mixture distribution prior to the first trial of hierarchical density estimation in the embedding space.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] i) We focus on showing the uncertainty representation in these methods suffer from overconfident predictions and that combining the two methods into a stochastic ensemble can be of great benefit and improve on the quality of the uncertainty.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] Therefore, the recently published researches can be divided into two branches: 1) designing of an objective function by introducing the additional regularized terms, or 2) constructing of a more flexible prior.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] The contribution of these studies lies on 1) the formalization as a unified model based on the newly proposed prior, though not the original technique proposed by the authors, and 2) demonstrating the superiority of the prior.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] As you mentioned, data is notoriously scarce and deep learning methods rarely work on small dataset problems.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] Another work of the latter approach is:	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] ii) We believe the true novelty to be in applying them in an active learning setting, and in particular on a small dataset problem (i.e. the size of the final dataset acquired during AL is only a small fraction of the entire available unlabelled dataset).	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_reject-criticism [SEP] [A2] VAE imposes a single Gaussian prior on embeddings, which leads to 1) the over-regularization, and 2) poor representations [1,2,5].	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] A common criticism of this analogy is that SGD noise is not Gaussian when the batch size is small.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] We showed that the resulting algorithm, which intercalates a gradient step and a FPI step yields very good results in well-known, difficult tasks such as dimensionality expansion in the single cell data or the WFOM task.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] Our paper is the first to relate the two regimes of SGD to the popular analogy between SGD and stochastic differential equations (SDEs).	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] Crucially, to establish that SGD noise enhances generalization, one must show that small batch sizes generalize better than large batch sizes under constant step budgets, with realistic learning rate decay schedules, and one must independently tune the learning rate at each batch size.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] To the best of our knowledge, the traditional methods require re-optimizing theta from scratch for each different topology, which is computationally demanding and breaks the joint-optimization.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] Moreover, since our kernel is naturally introduced with the sliced approach and kernel smoothing, the choice of regularization parameter is given by the Silverman's rule of thumb, and depends on the sample size	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] However Shallue et al. recently argued no previous work had provided convincing empirical evidence for this claim.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] This is the reason why such approximate posteriors have not been proposed before.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] Indeed in their abstract, they state ‘We find no evidence that larger batch sizes degrade out-of-sample performance’.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] Consequently, the CWAE has, while being trained, potentially less stochastic perturbation then WAE-MMD.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] Naively, the feature also seems to be a curse because the variational approximation is rendered intractable for the case of nonlinear dynamics.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] .	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] (contrary to WAE-MMD, where the parameters are chosen by hand, and in general do not depend on the sample size)	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] We believe this is an important contribution.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] Graph neural network formulation is KEY here, enabling it to perform this efficient policy transfer.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] We would like to reiterate that the novelty of the paper is <i>twofold.</i>	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] Thus, instead of using an additional weighting parameter lambda (as in WAE-MMD) whose aim is to balance the MSE and divergence terms, we decided to automatically (independently of dimension) balance the two terms of the loss function, by taking the logarithm of the divergence.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] These results provide the first convincing empirical evidence that SGD noise does enhance generalization in well-tuned networks with learning rate decay schedules.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] By “optimizing both G and theta”, we meant to indicate that the learned controllers can be transferred to the next generation even if the topologies are changed (instead of throwing away old controllers).	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] To our knowledge, we are the first to show that the analogy between SGD and SDEs holds for non-Gaussian short-tailed noise (appendix B).	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] In another recent paper, Zhang et al. argued that optimization in deep learning is well described by a noisy quadratic model which predicts that increasing the batch size should always enhance performance under constant step budgets.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] This feature is powerful because it uses known information about the true posterior in the design of the approximate one.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] We note that only NGE among all the baselines has the ability to do that.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] We observed that in many cases (like log-likelihood), the logarithm of the probability function works better, since it increases the role of examples with low-probability.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] NGE approximately doubles the performance of previous approach (Sims, 1994) as shown in Q1.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] As we show in sections 4 and 5, this perspective is crucial to understanding the influence of batch size and learning rate on test accuracy.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] In section 4, we are the first authors to perform this experiment and confirm that the final test accuracy of SGD does degrade for very large batch sizes under both constant epoch and constant step budgets, contradicting the claims of both Shallue et al and Zhang et al.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] The second novelty is a method to deal with this intractability, via the Laplace approximation and the fixed-point iteration method.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] First and foremost, we propose the use of a novel variational approximate posterior that shares the nonlinear dynamics with the generative model.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] CWAE, as compared to WAE-MMD, has no parameters (while WAE-MMD has two).	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] Due to the properties of the constructed Cramer-Wold kernel, we are able to substitute in the distance the sample estimation d(X,Y) of d(X,N(0,I)) given by its exact formula.	1.0
Not enough novelty in experiments (seems similar to previous work) [SEP] rebuttal_reject-criticism [SEP] Furthermore, we show in section 5 that the optimal SGD temperature which maximizes the test accuracy is almost independent of the epoch budget.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_structuring [SEP] Technical contribution: marginal.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_structuring [SEP] [Novelty compared to Mitzenmacher’ 18]	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_structuring [SEP] It is thus very hard to know if this new approach brings any improvement to previous work.”	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_structuring [SEP] The section concludes that if the second dataset has small variances, it will get higher likelihood. But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).”	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_structuring [SEP] We only know that the proposed approach finds near-optimal solutions with a difference of 0.01 competitive ratio.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_structuring [SEP] Compressability is evaluated, but that was already present in the previous work.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_structuring [SEP] Second, with respect to novelty, we would like to re-iterate our contributions since they may not have been clear.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_structuring [SEP] 3. Response:	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_structuring [SEP] You mention that our work is incremental to the work on benchmarking of optimizers.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_structuring [SEP] -- “No comparison has been made between their approach and other previous approaches.	1.0
Overall not original enough (primary claim, limited evaluation, limited novelty) [SEP] rebuttal_structuring [SEP] However there are also several important differences:	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] (Use of labels and novelty) In GR-based approaches, the quality of generated samples is crucial to keep the performance of previous tasks.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] After further optimization, we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] In this paper, we showed that discriminative regularization could make VAE possible to conduct both class conditional generation and classification with one integrated model.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] Generally, conditioning on a generative model yields higher quality samples than unconditional one and makes it possible to generate class-balanced samples [4]; the importance of conditional generation is also described in section 6.1 in our paper.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] While preparing for the rebuttal, we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] E.g., [Mehta et al. 2007] proposes the algorithm to solve Adwords, and proves that it achieves the optimal CR of 1-1/e ~ 0.63 (i.e., no matter what the online input sequence is, you get >= 1-1/e of the optimal solution in hindsight if you knew the instance offline).	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] We generated random 512-by-512 matrices with pruning rate ranging from 70 % to 95 % and simulated the number of parameters fed to PEs in 10000 cycles.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] The proposed method outperformed both baseline method and [1] in all simulation results.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] We provide new understandings of RNNs by connecting their generalization properties to their empirical success.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] The difference with [6] is the use of class-conditional priors; more details are explained at the response (Difference with CDVAE) for reviewer 3 and section 4.1 in our paper.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5).	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] We could observe that proposed parallel weight decoding based on the second Viterbi decompressor allowed 10 % to 40 % more parameters to be fed to PEs than the previous design [1].	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] These give the analytical benchmarks.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2).	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] Thus, we do not need to train an additional classifier, e.g., deep CNN, which is necessary for other works, including Narayanaswamy et al. There is also classifier integrated VAE such as [6].	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] If we use labels, we can construct a conditional generative model.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3):	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds.	1.0
Incremental novelty of method as compared to related work [SEP] rebuttal_summary [SEP] (1) The original algorithms papers which found optimal worst case algorithms [Karlin et al. 1986, Mehta et al. 2007].	1.0
While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited. [SEP] rebuttal_answer [SEP] This shows a clear distinction between different methods and an important result: when $\ell_2$ normalizing atoms, dictionary learning may converge to a result for which the ratio of activity between the most activated and the least activated is of the order 2.	1.0
While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited. [SEP] rebuttal_answer [SEP] However, SPAMS is a great inspiration for our framework.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] Indeed, analogous to e-SNLI for SNLI, Abductive-NLI can be extended to “e-Abductive-NLI” by providing explanations that justify the selected hypothesis.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] Abductive Commonsense Reasoning, a critical capability in human reasoning, is relatively less studied in NLP research.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] O2: Chad ensured that he took a picture to remember the event.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] To support this line of research, our work introduces a dataset that focuses explicitly on this important reasoning capability.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] H2: Chad waited after a game and met Barry.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] Furthermore, several recent works [1,2,3,4] have shown the presence of annotation artifacts in crowdsourced datasets -- which poses a significant challenge for dataset curation.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] A key distinction between e-SNLI and Abductive-NLI is that the explanations in e-SNLI serve the purpose of justifying model decisions.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] For the above example, a possible explanation for selecting H2 could be: “People need to be physically co-located to take a picture with someone. Meeting online does not mean two people are physically co-located”.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] The e-Abductive-NLI task would require models to generate an explanation for selecting H2.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] Consider the following example that BERT fails to predict correctly:	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] H1: Chad got to meet Barry Bonds online, chatting.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] O1: Chad loves Barry Bonds.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] In contrast, the goal of Abductive-NLI and Abductive-NLG is to select or generate explanatory hypotheses for given observations.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] We appreciate the opportunity to briefly restate our contributions and to discuss its significance.	1.0
Limited contribution in problem definition [SEP] rebuttal_answer [SEP] Our work makes the following contributions:	1.0
Missing explanation of utility of proposed method [SEP] rebuttal_answer [SEP] This shows a clear distinction between different methods and an important result: when $\ell_2$ normalizing atoms, dictionary learning may converge to a result for which the ratio of activity between the most activated and the least activated is of the order 2.	1.0
Missing explanation of utility of proposed method [SEP] rebuttal_answer [SEP] However, SPAMS is a great inspiration for our framework.	1.0
"Limited impact of results [SEP] rebuttal_done [SEP] We mention this in the paper (""Since MixMatch uses a simple flip-and-crop augmentation strategy, we were interested to see if replacing the weak augmentation in MixMatch with AutoAugment would improve performance but found that training would not converge."") but will emphasize this more in the next draft."	1.0
Limited impact of results [SEP] rebuttal_done [SEP] However, as stated in the last paragraph of Sec. 7.2, we presented in Appendix B.4 a procedure to assist our methods in handling discrete internal RVs.	1.0
In terms of method, the guidance for learning Siamese networks are designed heuristically (e.g. edges, colors, etc.) which limits its applicability over various datasets; I think that designing more principled approach to build such guidances from data should be one of the key contributions of the paper. [SEP] rebuttal_mitigate-criticism [SEP] [Principled Guidance] The design of guidances is heuristic, but as illustrated in Figure 2 and in Table 2, they are easy to design and are effective.	1.0
Limited insights based on design choices [SEP] rebuttal_refute-question [SEP] Again drawing the analogy of playing Go, the objective is mostly on training an agent that can make competitive moves rather than very fast moves, and there is no known “general-purpose” strategy to accomplish this.	1.0
Limited insights based on design choices [SEP] rebuttal_refute-question [SEP] Thus we don't compare to the running time of offline solvers, but to the worst-case competitive ratio of the optimal online algorithms.	1.0
Limited insights based on design choices [SEP] rebuttal_refute-question [SEP] Note that this is not similar to the case of solving an offline combinatorial problem via integer programming or other solvers, since our problems are online, i.e., the instance is not known beforehand, so there is no comparison to such “general-purpose” solvers.	1.0
Limited insights based on design choices [SEP] rebuttal_refute-question [SEP] The point of this work is only to see if ML can find optimal algorithms, and not about doing it faster than the known theoretical algorithms.	1.0
Limited impact of results [SEP] rebuttal_reject-criticism [SEP] A: We actually found that using stronger augmentations in MixMatch resulted in divergence.	1.0
Limited impact of results [SEP] rebuttal_reject-criticism [SEP] The main conclusions of our work (about NS-GAN, spectral normalization, and gradient penalty) hold across several datasets and architectures.	1.0
Limited impact of results [SEP] rebuttal_reject-criticism [SEP] To our knowledge, this is only the second work which attempts to fairly and systematically compare GANs in a large-scale setting.	1.0
Limited contribution in problem definition [SEP] rebuttal_structuring [SEP] (Importance and motivation)	1.0
Limited contribution in problem definition [SEP] rebuttal_structuring [SEP] Question 2:	1.0
Limited contribution in problem definition [SEP] rebuttal_structuring [SEP] Discussion about e-SNLI:	1.0
Missing explanation of utility of proposed method [SEP] rebuttal_structuring [SEP] MOTIVATION:	1.0
"Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training. [SEP] rebuttal_structuring [SEP] E.g., when is modulation strong and when is it not used """	1.0
"Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training. [SEP] rebuttal_structuring [SEP] Re: ""- no qualitative analysis on how modulation is actually use by the systems."	1.0
Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training. [SEP] rebuttal_structuring [SEP] This figure shows that while neuromodulation clearly reacts to reward, this reaction is complex and varies both within each episode and between runs.	1.0
Limited contribution in problem definition [SEP] rebuttal_summary [SEP] i) proposes and formalizes two novel tasks of Abductive Inference and Abductive Generation,	1.0
Limited contribution in problem definition [SEP] rebuttal_summary [SEP] ii) presents a new dataset in support of these tasks collected through careful crowdsourcing design and an adversarial filtering algorithm,	1.0
Limited contribution in problem definition [SEP] rebuttal_summary [SEP] iii) establishes strong baselines on the task proving the difficulty of the tasks and	1.0
Limited contribution in problem definition [SEP] rebuttal_summary [SEP] However, this is particularly challenging in real-world settings: the agent may observe different tasks sequentially, and an individual task may not recur for a long time.	1.0
Limited contribution in problem definition [SEP] rebuttal_summary [SEP] In this settings, a learned model might overfit to the most recently seen data, forgetting the rest, a phenomenon referred to as catastrophic forgetting, which is a core issue CL systems aim to address [2].	1.0
Limited contribution in problem definition [SEP] rebuttal_summary [SEP] iv) analyses the types of commonsense reasoning that current state of the art models fall short on.	1.0
Limited contribution in problem definition [SEP] rebuttal_summary [SEP] Recently, GR-based methods, inspired by the generative nature of the hippocampus as a short-term memory system in the primate brain [3], have been widely studied to address the catastrophic forgetting problem.	1.0
Limited contribution in problem definition [SEP] rebuttal_summary [SEP] To step forward to artificial general intelligence, we should further consider making an agent that can learn and remember many tasks incrementally [1].	1.0
"Improper presentation of results in tables and figures [SEP] rebuttal_answer [SEP] distortion less than 0.1 (at least on some images) should be able to find."""	1.0
Improper presentation of results in tables and figures [SEP] rebuttal_answer [SEP] positive - it unlikely to be true that an undefended network is predominantly	1.0
Improper presentation of results in tables and figures [SEP] rebuttal_answer [SEP] With this lower value of log(P_min), we see the 75 percentile of log(I) over the samples quickly decrease as robustness training proceeds for eps=0.2.	1.0
Improper presentation of results in tables and figures [SEP] rebuttal_answer [SEP] adversarial examples (or counter-examples for property verification) with L_inf	1.0
Improper presentation of results in tables and figures [SEP] rebuttal_answer [SEP] robust to perturbation of size epsilon = 0.1. Without any adversarial training,	1.0
Improper presentation of results in tables and figures [SEP] rebuttal_answer [SEP] You can see for several samples that were not initially robustness to eps=0.1 perturbations (log(I) > log(P_min)), the value of log(I) decreases steadily as the robust training procedure is applied.	1.0
Improper presentation of results in tables and figures [SEP] rebuttal_answer [SEP] Notably, however, log(I) is incredibly small before any of this training for eps=0.1, demonstrating how it is important to not only think in terms of whether any violations are present, but also how many: here less the proportion of violating samples is less than 10^-100 at eps=0.1 for most of the datapoints.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] Regarding unknown words handling, we computed the IDF on the reference sentences in the test set.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] To better characterize Gaussian mean field inference on the original model, we aim to find an inference procedure on p’ so that both algorithms result in exactly the same outcome, e. g. the same calculations are executed when running the corresponding program.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] 2. Black-box plug-and-play model collapse calibration.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] This ensures that the IDF is the same for all MT systems that are tested.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] The general goal of the paper is to empirically assess memorization in neural networks, and in particular the important question of implicit memorization, which is important for privacy: does a network trained for classification remember an image, or a set of images ?	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] 1. A pilot study of mode collapse existence in GAN.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] - the adapted, noise-injected model p’ has the structure \mu -> \tilde\mu -> D (containing a bottleneck).	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] We are using word pieces in all experiments, and we compute IDF using word pieces.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] - the original, noise-free model p has the structure \theta -> D (no bottleneck) while	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] >> Comment #15	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] Note that only if generative and inference model are adapted simultaneously we end up with equivalence.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] We show that there is such an inference procedure on the noisy model, and it has the character of MAP.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] 2. Metric to detect mode collapse in GAN models without any labels (ground truth or pseudo-labels).	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] Hereby, \mu (the mean of the Gaussian q) and \theta (the original parameter in p) correspond to \mu (the MAP point-mass of q’) and \tilde\mu (the noise-injected version of \mu in p’).	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] Hence, the invariance is qualitative, whereas for stability we need to quantify singular values.	1.0
3. The paper is not nicely written or rather easy to follow. [SEP] rebuttal_answer [SEP] This aspect is empirically evaluated in sections 4 and 5, and section 3 is a preliminary study of the memorization capabilities for systems explicitly trained to memorize (this serves as a qualitative upper-bound for implicit memorization).	1.0
Lack of discussion on datasets (size, motivation for use, etc) [SEP] rebuttal_answer [SEP] >> Comment #6	1.0
Lack of discussion on datasets (size, motivation for use, etc) [SEP] rebuttal_answer [SEP] [1] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. ICLR 2016.	1.0
Lack of discussion on datasets (size, motivation for use, etc) [SEP] rebuttal_answer [SEP] Once trained, the controller guides the training of a new task model on another two partitions D_{train}^T, D_{val}^T. Trained task models are evaluated on D_{test}. Baseline methods use the union of D_{train}^C, D_{val}^C, D_{train}^T, D_{val}^T for training/validation.	1.0
Lack of discussion on datasets (size, motivation for use, etc) [SEP] rebuttal_answer [SEP] For regression, classification and NMT, we split data into 5 partitions D_{train}^C, D_{val}^C, D_{train}^T, D_{val}^T, D_{test}. AutoLoss uses D_{train}^C and D_{val}^C to train the controller.	1.0
Lack of discussion on datasets (size, motivation for use, etc) [SEP] rebuttal_answer [SEP] For GANs that do not need a validation or test set, we follow the same setting in [1] for all methods.	1.0
-The experimental section do not clarify the benefits of the proposed approach. [SEP] rebuttal_answer [SEP] And this is separately validated across multiple classes of modulations.	1.0
-The experimental section do not clarify the benefits of the proposed approach. [SEP] rebuttal_answer [SEP] Absolutely, this is a difficult issue: there is no perfect middle ground where it is possible to study the contributions in their simplest instantiations while at the same time verifying their practical effectiveness.	1.0
-The experimental section do not clarify the benefits of the proposed approach. [SEP] rebuttal_answer [SEP] Other details are the same as those of the first experiment.	1.0
-The experimental section do not clarify the benefits of the proposed approach. [SEP] rebuttal_answer [SEP] To isolate effects, our experimental section includes many variants and ablations, allowing us to state with confidence that modulating behaviour using the bandit improves performance compared to uniform (no bandit) or untuned (fixed modulation) baselines.	1.0
-The experimental section do not clarify the benefits of the proposed approach. [SEP] rebuttal_answer [SEP] (In previous versions, the training iterations of fixed mode had been fixed.	1.0
-The experimental section do not clarify the benefits of the proposed approach. [SEP] rebuttal_answer [SEP] At the same time, it’s worth recognising that, by design, the method proposed will try to cater to the underlying learning algorithm and would ideally generate samples that would benefit the underlying learning procedure.	1.0
-The experimental section do not clarify the benefits of the proposed approach. [SEP] rebuttal_answer [SEP] We have opted to place the bulk of our emphasis on a realistic scenario (Atari with a Rainbow-like learning agent) that practitioners of Deep RL would find relevant.	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_concede-criticism [SEP] They would have a very low weight difference score though they are ideal representations for each other	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_concede-criticism [SEP] > You are right. For the very same reason, we take the inverse of the difference of normalized absolute classifier weights (Section 4.2).	1.0
Improper explanation of results (as in advantages, why are results better, etc) [SEP] rebuttal_concede-criticism [SEP] We agree vanilla REINFORCE can exhibit high variance.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_concede-criticism [SEP] We understand, although we tried our best to condense the complicated nature of these applications.	1.0
Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc) [SEP] rebuttal_concede-criticism [SEP] Very Good Catch	1.0
Incorrect citation styles [SEP] rebuttal_done [SEP] We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings.	1.0
Lack of discussion of analysis [SEP] rebuttal_done [SEP] We have simplified the architecture described in the paper by combining the networks $\sigma$ and $\omega$, and included the results from this architecture as well, producing a more robust architecture that performs better for multiple rewrite steps (while keeping the original, more complicated solution as one of the baselines).	1.0
Lack of discussion of analysis [SEP] rebuttal_done [SEP] + We added a clear and formal definition of the “binary” diagonal matrices representing the application of ReLU.	1.0
Lack of discussion of analysis [SEP] rebuttal_done [SEP] (Section 3.1)	1.0
Lack of discussion on datasets (size, motivation for use, etc) [SEP] rebuttal_mitigate-criticism [SEP] Ordinary statistical methods (like kernel two-sample tests) do not work well due to the high dimensionality and the complex nature of image data.	1.0
Lack of discussion on datasets (size, motivation for use, etc) [SEP] rebuttal_mitigate-criticism [SEP] To the best of our knowledge, currently, there is no perfect metric to measure the distance between a training set and a test set.	1.0
"Unclear description of method [SEP] rebuttal_refute-question [SEP] Response: The goal of this paper is not to answer ""why A?"" but rather"	1.0
"Unclear description of method [SEP] rebuttal_refute-question [SEP] There are other papers that seek to answer the question of ""why A?"" but"	1.0
"Unclear description of method [SEP] rebuttal_refute-question [SEP] ""why A and not B?""  The visual answer to the two questions may be similar,"	1.0
Unclear description of method [SEP] rebuttal_refute-question [SEP] change to make it a B and not an A, as a way of explaining this contrast.	1.0
Unclear description of method [SEP] rebuttal_refute-question [SEP] that is not our focus.	1.0
Unclear description of method [SEP] rebuttal_refute-question [SEP] We seek to highlight what in the image would need to	1.0
Unclear description of method [SEP] rebuttal_refute-question [SEP] but it may not.	1.0
Unclear problem definition [SEP] rebuttal_structuring [SEP] - The problem setting description is neither formal nor intuitive which made it very hard for me to understand exactly the problem you are trying to solve.	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] A: (1) Our decoder is symmetric to the encoder in their architectures.	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] Response: The aim of section 2.1 is to motivate limiting mutual information for the purpose of generalization.	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] However, this is particularly challenging in real-world settings: the agent may observe different tasks sequentially, and an individual task may not recur for a long time.	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] In this settings, a learned model might overfit to the most recently seen data, forgetting the rest, a phenomenon referred to as catastrophic forgetting, which is a core issue CL systems aim to address [2].	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] This geometric formulation of the definition is not only the origin of the naming, but it is also a mathematically sound formulation similar to the one you suggested “A is full rank and there does not exist any X such that Ax < 0”.	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] We link generalization problems reported in the literature to the introduced information measure.	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] To get the nth hop information of edge <i,j>, row filter decodes all the (n+1)-th hop information of outgoing edges of node i and column filter decodes all the (n+1)-th hop information of incoming edges of node j.	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] The encoder does n-hop edge information aggregation from the input graphs and learns the latent representation of nodes.	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] The information necessary to identify or distinguish between training samples is quantified by the empirical entropy, and we called it the identity of the samples.	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] Recently, GR-based methods, inspired by the generative nature of the hippocampus as a short-term memory system in the primate brain [3], have been widely studied to address the catastrophic forgetting problem.	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] Such results indicate that our technique of NF-GAN can still benefit the state-of-the-art variants of GANs (e.g., SN-GAN).	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] Then, we first decode the node embedding to get the n-hop aggregated information on edges by node-to-edge layer and then we further decode the n-hop aggregated information layer by layer by n-layers back to get the output adjacency matrix.	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] (2) Different from image deconvolution, for each hidden channel, we have two filters vertical to each other, i.e., one is a column vector while the other is a row vector.	1.0
Unclear description of method [SEP] rebuttal_summary [SEP] To step forward to artificial general intelligence, we should further consider making an agent that can learn and remember many tasks incrementally [1].	1.0
Unclear description of method [SEP] rebuttal_reject-request [SEP] Regarding the clustering result, we believe that the resulting accuracy number cannot be used to compare the quality of the clustering methods.	1.0
Unclear description of method [SEP] rebuttal_reject-request [SEP] Nevertheless, we believe this mechanism still has some potential and is currently overlooked by most deep neuroevolution researchers, so we decided to keep the importance mixing study in Appendix B rather than just removing it.	1.0
Unclear description of method [SEP] rebuttal_reject-request [SEP] For the last suggestion, we currently define the law of total variance(variation) in the preliminaries so we did not repeat the definition in Section 4.4.	1.0
