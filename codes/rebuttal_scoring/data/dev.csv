descs	scores
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.	0.3508773619358619
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] In addition, we are also interested in the influence of the number of sentence-image pairs inspired by your suggestion.	0.649122638064138
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] > The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.	0.1975795937322861
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] See general responses #1 and #3.	0.2815511087403978
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] While a feature extractor backbone network may have some influence, we would like to highlight three points.	0.5208692975273159
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_reject-criticism [SEP] We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.	0.3508773619358619
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_reject-criticism [SEP] - The datasets we used are as large as the datasets used in other related work in the area.	0.649122638064138
"Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_done [SEP] You can check the details in the appendix of the paper, the paragraph of ""Applying Our Proposed Metric on FFHQ""."	0.3508773619358619
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_done [SEP] We have revised the toy experiments to include a heteroscedastic regression task (see Section 6.1.).	0.649122638064138
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] Please also refer to the main contribution (ii) of our response to Reviewer 2.	0.3508773619358619
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.	0.649122638064138
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] We will incorporate these results in the experiments section and post the updated manuscript shortly.	0.1975795937322861
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] Are added to section 4 and appendix E	0.2815511087403978
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] Finally, we have conducted further experiments on larger corpora, specifically the Gigaword corpus.	0.5208692975273159
generalizability of results is questionable [SEP] rebuttal_concede-criticism [SEP] We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.	0.3508773619358619
generalizability of results is questionable [SEP] rebuttal_concede-criticism [SEP] Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).	0.649122638064138
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.	0.3508773619358619
generalizability of results is questionable [SEP] rebuttal_answer [SEP] We also strongly agree that testing additional embeddings would be very interesting!	0.649122638064138
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.	0.0781068773141067
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] “The conclusions focus on the importance of section 3 and the results of the experiments performed. Do the conclusions accurately reflect the opinions of the author?”	0.0913848418340474
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] Comment: The reported results are mostly qualitative. I find the set of	0.1108039684446293
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] provided qualitative examples quite reduced.	0.1421984327951226
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] the authors to update the supplementary material in order to show extended	0.2026331680715508
"generalizability of results is questionable [SEP] rebuttal_structuring [SEP] 1. Response to the ""Weaknesses"" part and the comparison with GBDT"	0.3748727115405429
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.	0.0991274512750463
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] The quantitative results in Table 3 further validate our approach.	0.1201921274650668
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] We do not consider the conclusions of the experiments from Section 3  to be more important than those of the other sections, in fact quite the opposite.	0.1542466413058905
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] >>> At first, we want to clarify the few-shot network architecture setting.	0.219801450657675
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] To better relieve the reviewer's concern, we implemented our algorithm with ResNet architecture on miniImagenet dataset and show the results as follow:	0.4066323292963215
generalizability of results is questionable [SEP] rebuttal_done [SEP] We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.	0.02681534878198
generalizability of results is questionable [SEP] rebuttal_done [SEP] We added this clarification to Section 3.4.	0.0285686775190388
generalizability of results is questionable [SEP] rebuttal_done [SEP] New suggested structure and related suggestions: These are nice suggestions and explain why the structure was confusing. We’ve worked on these to come close to the suggested structure.	0.0305923148107708
generalizability of results is questionable [SEP] rebuttal_done [SEP] We decided to move it to an appendix after reading the feedback from the three reviewers.	0.0329562950606381
generalizability of results is questionable [SEP] rebuttal_done [SEP] Response: We have added a supplementary section, adding more qualitative	0.0357576157104752
generalizability of results is questionable [SEP] rebuttal_done [SEP] We will update the labels in the ablation table to make this more clear.	0.0447760150304092
generalizability of results is questionable [SEP] rebuttal_done [SEP] We have added the statistical significance results to the revised version.	0.0464569345832508
generalizability of results is questionable [SEP] rebuttal_done [SEP] Thus our reported metrics are correct and justified for this problem, though we have clarified the exact nature of the replicates in the text to ensure this is not misleading.	0.0502131577460323
generalizability of results is questionable [SEP] rebuttal_done [SEP] Since we were concerned that adding this information to the plots would make them harder to read	0.0516855012950355
generalizability of results is questionable [SEP] rebuttal_done [SEP] R) We added a new experiment for a real life application; testing different topologies.	0.0646489164919878
generalizability of results is questionable [SEP] rebuttal_done [SEP] We redo the visualization in Figure 6 to make the gains provided by SN clearer.	0.0783868680232577
generalizability of results is questionable [SEP] rebuttal_done [SEP] By the way, the ERL paper is now published at NIPS, but it was not the case yet when we submitted ours. We updated the corresponding reference.	0.1005964826642342
generalizability of results is questionable [SEP] rebuttal_done [SEP] We believe that this is a strong baseline, and shows the applicability of APO in practical settings.	0.1433497943396562
generalizability of results is questionable [SEP] rebuttal_done [SEP] The test accuracies using RMSprop and K-FAC with APO are shown in our response to all reviewers at the top.	0.2651960779432328
Lack of experiments [SEP] rebuttal_done [SEP] We added more baselines to further strengthen the significance of our work with respect to the previous approaches.	0.1975795937322861
Lack of experiments [SEP] rebuttal_done [SEP] Taking your comments and comments from other reviewers into account, we have made improvements to the experimental section.	0.2815511087403978
Lack of experiments [SEP] rebuttal_done [SEP] Please refer to the revised version for numerical evaluations in Section 6.	0.5208692975273159
Improper comparison of related work in terms of implementation [SEP] rebuttal_concede-criticism [SEP] We agree much detail on embeddings can be condensed or moved to Appendix.	0.1334172456785002
Improper comparison of related work in terms of implementation [SEP] rebuttal_concede-criticism [SEP] We included embedding details in the paper because a reviewer from the venue we previously submitted to requested these details to be in the paper.	0.1712191370351586
Improper comparison of related work in terms of implementation [SEP] rebuttal_concede-criticism [SEP] However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.	0.2439874587384383
Improper comparison of related work in terms of implementation [SEP] rebuttal_concede-criticism [SEP] That said, we agree it is worth investigating the performance of LSTM on this problem further.	0.4513761585479028
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.	0.3508773619358619
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] This shows that additional modules help.	0.649122638064138
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Upon your suggestion, we would also add this random baseline in table 2 as well.	0.0991274512750463
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] The difference has been discussed in Sec. 1 and Sec. 5	0.1201921274650668
"Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."	0.1542466413058904
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] A4: Thanks for pointing out the related work. In fact, our method is distinct from these methods.	0.2198014506576749
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 2. We further add newly obtained results on the WMT18 challenge.	0.4066323292963214
"Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] Concerning the point "" It is not even clear that the final compression of the baselines would not be better."	0.1975795937322861
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] Please refer to Section 3.1 and Section 3.4 for more details.	0.2815511087403978
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] 1. For the baseline models reported:	0.5208692975273159
Improper comparison of related work in terms of implementation [SEP] rebuttal_by-cr [SEP] We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.	0.3508773619358619
Improper comparison of related work in terms of implementation [SEP] rebuttal_by-cr [SEP] We will edit the related work section to include the above discussion.	0.649122638064138
Improper comparison of related work in terms of implementation [SEP] rebuttal_future [SEP] This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.	0.3508773619358619
Improper comparison of related work in terms of implementation [SEP] rebuttal_future [SEP] Since this is the first paper on this topic, we chose to focus on introducing the problem and providing Transformer-based approaches as a baseline for future work.	0.649122638064138
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] We revised the notations in the paper to make formulation clearer.	0.1334172456785002
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] We have added that to the revised version (see p.6 and Appendix A).	0.1712191370351586
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.	0.2439874587384383
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] - We also followed your suggestion and show the domain weights of MDMN on the Amazon dataset for comparison (see the new Section 5.4).	0.4513761585479028
Less datasets used [SEP] rebuttal_concede-criticism [SEP] We apologize for unclear description of experimental settings.	0.1975795937322861
Less datasets used [SEP] rebuttal_concede-criticism [SEP] We agree that further investigation is needed for mutual information, and we are currently working on it.	0.2815511087403978
Less datasets used [SEP] rebuttal_concede-criticism [SEP] you are right, even if the focus of the paper is not on getting the best possible score on language modelling, different settings would make this point not only more convincing, but clearer.	0.5208692975273159
Less datasets used [SEP] rebuttal_structuring [SEP] 7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.	0.1334172456785002
Less datasets used [SEP] rebuttal_structuring [SEP] As regards the implications of the paper, we would like to address this in the context of the reviewer’s comment that increased performance is not surprising given the additional supervision provided.	0.1712191370351586
Less datasets used [SEP] rebuttal_structuring [SEP] See general responses #1 and #3.	0.2439874587384383
Less datasets used [SEP] rebuttal_structuring [SEP] Thank you for pointing out the other datasets in algebraic word reasoning.	0.4513761585479028
Less datasets used [SEP] rebuttal_done [SEP] First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.	0.0540159036531358
Less datasets used [SEP] rebuttal_done [SEP] We’ve included these in an expanded discussion of related work with discussion on how they relate to the current dataset.	0.0605749135945383
Less datasets used [SEP] rebuttal_done [SEP] We have added a section B.6 in the appendix and figures illustrating these results.	0.0691563081565013
Less datasets used [SEP] rebuttal_done [SEP] We would like to thank reviewer 3 for suggesting the related works that we have missed. These were incorporated in the revision.	0.0809128579180634
Less datasets used [SEP] rebuttal_done [SEP] The results are shown in Figure 4 in the updated version of the paper.	0.0981068757205273
Less datasets used [SEP] rebuttal_done [SEP] As for the synthetic curves experiment, we updated the paper with a justification.	0.1259039706128623
Less datasets used [SEP] rebuttal_done [SEP] Furthermore, we included two additional tasks, attesting to the ability of Transformer to model a wide range of distributions over training curves.	0.1794135120688542
Less datasets used [SEP] rebuttal_done [SEP] We have modified our expression, typos and grammar errors.	0.3319156582755168
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.	0.1334172456785002
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] Providing a controlled experiment is always challenging, though the elements mentioned by the reviewer were specifically selected as to reduce various confounds.	0.1712191370351586
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] Next, we wonder how would the reviewer correct the confounds mentioned with regard to the clique experiment.	0.2439874587384383
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] First, we’d like to emphasize that the clique problem studied in the paper is far from being a toy problem.	0.4513761585479028
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.	0.1334172456785002
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] ; these results are statistically significant.	0.1712191370351586
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] We have revised the toy experiments to include a heteroscedastic regression task (see Section 6.1.).	0.2439874587384383
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] Secondly, we agree with the reviewer that comparing with the Gauss-Newton algorithm will be interesting and have updated such a comparison in Appendix B in the revised version according to the reviewer’s suggestions:	0.4513761585479028
Limited improvement over baselines [SEP] rebuttal_concede-criticism [SEP] Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.	0.3508773619358619
Limited improvement over baselines [SEP] rebuttal_concede-criticism [SEP] 2) For the experiment, we will train our experiments longer and modify our network.	0.649122638064138
Limited improvement over baselines [SEP] rebuttal_summary [SEP] As the reviewer can observe the scale of the experimental evaluation is significantly different.	0.1975795937322861
Limited improvement over baselines [SEP] rebuttal_summary [SEP] Details could be found in Appendix C.	0.2815511087403978
Limited improvement over baselines [SEP] rebuttal_summary [SEP] Furthermore, we also add additional experiments about generating complement data in CelebA, which is a more complex dataset.	0.5208692975273159
Limited improvement over baselines [SEP] rebuttal_answer [SEP] It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.	0.0991274512750463
Limited improvement over baselines [SEP] rebuttal_answer [SEP] As to the online setting, thanks for pointing us to the “short-horizon bias” paper.	0.1201921274650668
Limited improvement over baselines [SEP] rebuttal_answer [SEP] 3. We will include WGAN-GP to the baselines for the sake of completeness.	0.1542466413058904
Limited improvement over baselines [SEP] rebuttal_answer [SEP] Upon your suggestion, we would also add this random baseline in table 2 as well.	0.219801450657675
Limited improvement over baselines [SEP] rebuttal_answer [SEP] >> Comments #1, #11	0.4066323292963215
Limited improvement over baselines [SEP] rebuttal_structuring [SEP] Q4. Updated abstract and performance evaluation.	0.3508773619358619
Limited improvement over baselines [SEP] rebuttal_structuring [SEP] I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.	0.649122638064138
Limited improvement over baselines [SEP] rebuttal_by-cr [SEP] Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper.	0.1975795937322861
Limited improvement over baselines [SEP] rebuttal_by-cr [SEP] If the paper gets accepted, we will expand the experiments with fixed frequencies in the final version.	0.2815511087403978
Limited improvement over baselines [SEP] rebuttal_by-cr [SEP] 3)For the experiment: we will spend some time to train GANs with more iteration and modify it.	0.5208692975273159
"Limited improvement over baselines [SEP] rebuttal_done [SEP] We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to ""The Common concerns about experimental setting and results""."	0.0334280831994757
Limited improvement over baselines [SEP] rebuttal_done [SEP] We have now added additional domain adaptation baselines, designed in the setting suggested by Reviewer 2.	0.0362694474366044
Limited improvement over baselines [SEP] rebuttal_done [SEP] Taking your comments and comments from other reviewers into account, we have made improvements to the experimental section.	0.0396948713701846
Limited improvement over baselines [SEP] rebuttal_done [SEP] We have modified Remark 3.4 (and added Remark 3.5) to make this clear in the updated version.	0.0511601146927878
Limited improvement over baselines [SEP] rebuttal_done [SEP] We have included that in Figure 7 of the revised version (Appendix E).	0.0511601146927878
Limited improvement over baselines [SEP] rebuttal_done [SEP] We updated section 2.2 to relate to the references you mentioned.	0.0511601146927878
Limited improvement over baselines [SEP] rebuttal_done [SEP] R)We added a new experiment where we show how the performance of adaptive kernels improves with the increment of parameters.	0.0656554859787829
Limited improvement over baselines [SEP] rebuttal_done [SEP] We have added the detailed PPO-based training algorithm in Appendix A.1.	0.1384477432182124
Limited improvement over baselines [SEP] rebuttal_done [SEP] Thanks also for catching several typographic errors. We have addressed them in the new draft.	0.1384477432182124
Limited improvement over baselines [SEP] rebuttal_done [SEP] We have added a new section 6.3  to the supplement that includes visualizations of the attention mechanism both over the course of training and within episodes.	0.1384477432182124
Limited improvement over baselines [SEP] rebuttal_done [SEP] We are re-programming DSN and experimental results will be added.	0.2561285382819513
Experimental study not strong enough [SEP] rebuttal_summary [SEP] Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.	0.3508773619358619
Experimental study not strong enough [SEP] rebuttal_summary [SEP] Reviewer 1 further asks for evaluation on more games.	0.649122638064138
Experimental study not strong enough [SEP] rebuttal_answer [SEP] These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.	0.0781068773141067
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)	0.0913848418340474
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Moreover, we investigate the mixing distribution learned in Appendix G.	0.1108039684446293
Experimental study not strong enough [SEP] rebuttal_answer [SEP] - We set parameters as follows.	0.1421984327951226
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Q1:  It would be nice to see a better case made for spherical convolutions within the experimental section.	0.2026331680715508
Experimental study not strong enough [SEP] rebuttal_answer [SEP] perturbation by providing a counter-example (violation).	0.3748727115405429
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.	0.3508773619358619
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] However, we only present the most important results in this paper due to the space limit.	0.649122638064138
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.	0.0526183844839748
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] > Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.	0.0590078799447676
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Q3: The evaluation section lacks experiments that evaluate the computational savings.	0.0673674268464801
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Section 3 too much redundancy	0.1099810366155483
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Missing experiments to validate nature of bounds.	0.1099810366155483
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] The long-term agenda / research program is indeed two-fold:	0.1099810366155483
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Responding to R1's additional feedback:	0.1723032513183127
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Q3: The authors invite five volunteer graduate students to annotate the selected example.	0.3187599475598196
Experimental study not strong enough [SEP] rebuttal_by-cr [SEP] Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.	0.1975795937322861
Experimental study not strong enough [SEP] rebuttal_by-cr [SEP] We plan to add SGD as a reference algorithm (as suggested by another reviewer).	0.2815511087403978
Experimental study not strong enough [SEP] rebuttal_by-cr [SEP] 3)For the experiment: we will spend some time to train GANs with more iteration and modify it.	0.5208692975273159
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.	0.0991274512750463
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Indeed we do not provide a perfect explanation in this submission, however the factors we have considered and the well-designed numerical investigations could be helpful for future studies on this topic.	0.1201921274650668
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] We have also added to our discussion, making clear that our method represents a significant advantage over competing methods when detailed atomic information is available.	0.1542466413058904
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] The results are comparable but the added term in setting II shows benefits on some datasets.	0.2198014506576749
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Based on the above, we cannot concur with the judgement “the experiments in this paper is (are) also not convincing”.	0.4066323292963214
"Experimental study not strong enough [SEP] rebuttal_done [SEP] Please refer to our updated ""Experiments"" section."	0.0115600565067098
Experimental study not strong enough [SEP] rebuttal_done [SEP] We add more analysis with the state of the art in Section 4.2, especially about the case that other methods outperforms our method.	0.0119402935401055
Experimental study not strong enough [SEP] rebuttal_done [SEP] Fig. 3 and other evaluations have been updated for the new test set.	0.0135353104844176
Experimental study not strong enough [SEP] rebuttal_done [SEP] We have updated the baseline details in the Appendix B.3.	0.0136049016684216
Experimental study not strong enough [SEP] rebuttal_done [SEP] - We add the experimental details in the Appendix.	0.0144236214802329
Experimental study not strong enough [SEP] rebuttal_done [SEP] On the other hand, for all the arguments in Section 2.2, we have added the citation to support them.	0.0159118197014129
Experimental study not strong enough [SEP] rebuttal_done [SEP] A quantitative comparison can be found in Table 1, as well as a qualitative comparison in Fig. 14.	0.0163764427188682
Experimental study not strong enough [SEP] rebuttal_done [SEP] Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.	0.0193424475835678
Experimental study not strong enough [SEP] rebuttal_done [SEP] We added an illustrative example in the introduction to give an intuitive understanding of invariance, stability and their relationship.	0.0220826276579065
Experimental study not strong enough [SEP] rebuttal_done [SEP] Please refer to the revised version for numerical evaluations in Section 6.	0.0258366743597507
Experimental study not strong enough [SEP] rebuttal_done [SEP] 3. We have added the values for chosen $\gamma$ in the updated version (see caption of Figure 1).	0.1368490330534339
Experimental study not strong enough [SEP] rebuttal_done [SEP] We have included a new section 6.3 in the appendix of our revised draft that visualizes the behavior of our attention mechanism, as well as how it evolves over the course of training.	0.195008098332589
Experimental study not strong enough [SEP] rebuttal_done [SEP] As suggested, we have added further analysis of failure cases, and describe strategies for negative mining from these examples.	0.2482876442631789
Experimental study not strong enough [SEP] rebuttal_done [SEP] We have revised the toy experiments to include a heteroscedastic regression task (see Section 6.1.).	0.2552410286494042
Need more experimental results [SEP] rebuttal_summary [SEP] Note the compression rates are the same as the data in Table 3 in the original manuscript.	0.3508773619358619
Need more experimental results [SEP] rebuttal_summary [SEP] Reviewer 1 further asks for evaluation on more games.	0.649122638064138
Need more experimental results [SEP] rebuttal_structuring [SEP] We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.	0.1975795937322861
Need more experimental results [SEP] rebuttal_structuring [SEP] Both the results on the development set and on the test set should be reported for the validity of the experiments.	0.2815511087403978
Need more experimental results [SEP] rebuttal_structuring [SEP] Responding to R1's additional feedback:	0.5208692975273159
Need more experimental results [SEP] rebuttal_done [SEP] Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.	0.1975795937322861
Need more experimental results [SEP] rebuttal_done [SEP] (3) Of course! We have moved the test accuracy (which previously was only given in the Appendix and thus hard to find) to the legends of the plots to make it more easily accessible.	0.2815511087403978
Need more experimental results [SEP] rebuttal_done [SEP] Are added to section 4 and appendix E	0.5208692975273159
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] R4: An example is presented in Figure 3 but is not expanded upon in the main text.	0.3508773619358619
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] So, two responses are given below.	0.649122638064138
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_done [SEP] We have modified parts of our paper to reflect these arguments better.	0.1334172456785002
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_done [SEP] Please see the (updated) last paragraph of Section 5, which explains this comparison in detail.	0.1712191370351586
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_done [SEP] We redo the visualization in Figure 6 to make the gains provided by SN clearer.	0.2439874587384383
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_done [SEP] We now present the validation accuracy instead in Appendix F.	0.4513761585479028
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.	0.0362423289050409
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] 2) For the experiment, we will train our experiments longer and modify our network.	0.0393229085776574
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We appreciate your acknowledgment of our contributions and pointing out that the properties may only need to be satisfied at some critical points during training deep neural nets.	0.0430367252494965
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] If the reviewers feel strongly about this, we can move it to appendix, however we feel it helps to provide a complete picture.	0.047609371449088
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] But we agree this was not a good choice, since this simplification appears obvious to the reader.	0.0533905157550535
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We agree that, ideally, a comparison with SOTA architectures would be desirable.	0.0609541973803305
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We agree the IR-based metrics have limitations.	0.0713164650418791
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.	0.0864713115823742
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] That said, we agree it is worth investigating the performance of LSTM on this problem further.	0.1109716774548606
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] you are right, even if the focus of the paper is not on getting the best possible score on language modelling, different settings would make this point not only more convincing, but clearer.	0.1581348739371265
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] Response #6: We agree that it is important to justify how the reconstruction error works as a measure of privacy in this paper.	0.2925496246670925
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.	0.1975795937322861
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] We believe that the adversarial robustness part of our paper has become much stronger than before, thanks to your suggestion.	0.2815511087403978
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] This shows that additional modules help.	0.5208692975273159
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.	0.0132751845861122
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.	0.0133203428484025
"Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."	0.0139100237239957
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.	0.014303185741312
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Describing all aspects of these techniques would require substantially more space and hence we refer to the original work for precise formulation.	0.0148821040974591
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] But this description mischaracterizes the fundamental problem that we have identified and proposed a solution for.	0.0149771579807659
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We believe this is a feature not a bug: as we showed in Sec. 4.1, our algorithm does not need to be more complex.	0.0179275945041234
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We see this methodology, as illustrated by our experiments for one model, as the central part of our contribution.	0.0187208115638091
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Please refer to our overall comments on this question (and also a few more details in reply to Reviewer#1’s similar question).	0.0191591300482945
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We provided pointers to the files in the code anonymously shared on OpenReview.	0.0197153274748465
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] [A] Most of these are described in Section 2 (in particular, discussion on regularization and penalties is in Section 2.2).	0.0235965130044841
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This would allow us to show similar local convergence guarantees to those proven by other works in the area (see Appendix A).	0.0236946762644754
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The simulation will be released with the work for others to use and build on multi-agent learning methods.	0.0250803898028196
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Please refer to our general comment above on why our unguided case performs better now.	0.028084633329259
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.	0.029431614280865
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.	0.0295305686120991
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] yes we have test several layers with adaptive kernels. but we focus on report the results on the first layer to highlight the contribution	0.0298240781098235
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The extendibility of the Neural LP framework is a very important and relevant question, which we also mentioned explicitly as a possible future work direction.	0.0350279914555876
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] perturbation by providing a counter-example (violation).	0.0478969578191201
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] As to the online setting, thanks for pointing us to the “short-horizon bias” paper.	0.0506789052857497
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 4. We updated the paper with a justification of our action discretization scheme.	0.0670428060900636
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Higher-order and nonlinear covariance tests may make very appealing comparisons and we are looking into it.	0.0674370631511184
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We now clarify these reasons in Section “2.3 Density Estimation Methods” of the revised paper.	0.0751471139820304
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] >> Comments #2, #3	0.1078366864216662
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] >> Comments #1, #11	0.1994991398217165
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.	0.1975795937322861
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.	0.2815511087403978
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] We could definitely investigate further why this mild ensembling yields a small performance increase, but we see this as tangential to the overarching points of the paper.	0.5208692975273159
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.	0.1334172456785001
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] 1.Thank you for the insightful suggestion.	0.1712191370351586
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] Thanks for pointing us towards this.	0.2439874587384383
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] Thank you for mentioning the existing learning curve modeling methods.	0.4513761585479028
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 3) - ""The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks."	0.0583485756361285
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Q3: The evaluation section lacks experiments that evaluate the computational savings.	0.0666145117639945
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.	0.0779389239031752
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] “Lemma 3 is too trivial.”	0.1643500088106676
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] We also responded to this suggestion in the public comment. For your convenience, we have copied our response here:	0.1643500088106676
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Concerning the point "" It is not even clear that the final compression of the baselines would not be better."	0.1643500088106676
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Responding to R1's additional feedback:	0.3040479622646984
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.	0.047110945775231
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We will update the manuscript with these additional results and discussion and post it shortly.	0.0517724144707791
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We agree however that a clearer introduction of the terminology would be clearly helpful and we plan to add this to the final submission.	0.0521165044381057
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] According to the Reviewer's comment we will extend Section 5 on experimental results by showing more detailed analysis.	0.054950184278729
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] The updated paper will change the emphasis, and clarify that a closer, more faithful, learning progress proxy remains future work.	0.0580591076561352
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.	0.0729567743952927
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] Note, that regarding the methodology there were some misunderstandings (which we try to avoid for future readers in the revised version).	0.0884602210282278
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We will clarify the sentence in section 2.	0.1135241168714937
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] Thanks for carefully reading our manuscript and noticing this typo which we will correct.	0.1617719951359806
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We are now running additional experiments with a deeper encoder and with more filters.	0.2992777359500251
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.	0.0540159036531358
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] - The datasets we used are as large as the datasets used in other related work in the area.	0.0605749135945383
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] We have also added to our discussion, making clear that our method represents a significant advantage over competing methods when detailed atomic information is available.	0.0691563081565013
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Please check the degraded images in Table 3.	0.0809128579180634
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Providing a controlled experiment is always challenging, though the elements mentioned by the reviewer were specifically selected as to reduce various confounds.	0.0981068757205273
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Next, we wonder how would the reviewer correct the confounds mentioned with regard to the clique experiment.	0.1259039706128623
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] First, we’d like to emphasize that the clique problem studied in the paper is far from being a toy problem.	0.1794135120688542
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Dupuis’s work includes similar results but in a different form.	0.3319156582755168
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.	0.3508773619358619
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] As we explained at the common response, we started our research from clear open questions.	0.649122638064138
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] That said, we agree that using different architectures would strengthen our point and make the paper more convincing.	0.0640340158384878
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] Substantiating this claim in the general case will likely require a large-scale study, which we plan to perform in the future.	0.0731054190644934
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.	0.0855332424172482
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.	0.1037089558670764
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] Yet analysing how well such approach performs in practice is still an open problem, which we leave for future work.	0.1330930952284554
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] We would definitely explore this further, and add a detailed analysis on computational efficiency of different methods to reduce dimensions.	0.1896577481857959
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] Since this is the first paper on this topic, we chose to focus on introducing the problem and providing Transformer-based approaches as a baseline for future work.	0.3508675233984424
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.	0.0072075317756003
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Taking your comments and comments from other reviewers into account, we have made improvements to the experimental section.	0.0072075317756003
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have updated Table 2 and the discussion in section 5 to include these additional results.	0.0072075317756003
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] In the revision, we have also added a more detailed comparison with other methods in sections 4.2 and 4.3, showing the strength of our method.	0.0072082974382084
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have updated the paper with these recommendations in Section 7.	0.0077558797115669
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Furthermore, we present an experimental investigation of these different objectives (Sec. 6.4).	0.0078532333819597
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] See details in Table 1 (bottom) in the revision and our post about common concerns.	0.007996286800736
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have included a more detailed analysis with new visualizations in the updated paper.	0.0085034616355455
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We clarified this in Algorithm 1.	0.0086536631422295
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have extended the contribution part (in the introduction) and added Sections A and B to the Appendix, to make things clearer.	0.008710510785277
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Thank you for this suggestion, we have now clarified this connection in Section 3.	0.0088795146608273
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] The results are in Appendix A.3 of our revised version.	0.0089375575639
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have included  a comparison with method [3] (see general comment to all reviewers).	0.0090014218981433
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Therefore, we updated Figure 7 in original manuscript to Figure 6c in the updated manuscript according to the new data.	0.0094817074174751
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have added the labels in the text and Table 1 to make this more clear.	0.0096261474784885
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] The assumptions used for the simulation and analysis data have been updated in Figure 6c of the revised manuscript.	0.0101829984372824
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] It is true that we did not quite prove that, so we have reworded the text to tone down this claim.	0.0117473799838419
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Nevertheless, we agree that our findings have mostly a theoretical value, so we have adjusted the wording to reflect that.	0.0117923673968327
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] On evaluating general-purpose models only, we may have phrased this badly in the paper, and have updated it.	0.0119540029169972
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We added the more experimental data of runtime analysis to address the Reviewer's main concern.	0.0128180957625599
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have thus modified the text to make it clear that we mean “statistically significant” only.	0.0130571371855493
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have updated the introduction to rephrase and clarify the lower bound claims.	0.0138030433613907
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have included these results in Appendix A.6 of the revised version.	0.0141348971542072
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] In the paper, we include many details on the environment rewards and design as we consider these simulation tasks part of the contribution of the work.	0.0143622961347278
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] The added/modified text are highlighted in the blue color.	0.0149774191743243
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have now added additional domain adaptation baselines, designed in the setting suggested by Reviewer 2.	0.0182850737806274
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] 3. We have added the values for chosen $\gamma$ in the updated version (see caption of Figure 1).	0.0214551943741666
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We now present the validation accuracy instead in Appendix F.	0.0241048883347272
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We redo the visualization in Figure 6 to make the gains provided by SN clearer.	0.0241793168805244
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.	0.0246357193999601
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] To show that we can easily include these features, we have included in our appendix some results including non-structural features.	0.0253192763319483
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] As suggested, we have added further analysis of failure cases, and describe strategies for negative mining from these examples.	0.027755083180572
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] During the rebuttal period, we conducted additional experiments on adversarial robustness as you suggested:	0.0284259299171361
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] However, we also trained our model using verb-object pairs, and we have updated section 5 as well as the appendix to include these additional results.	0.0305398791286434
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We also updated our “Probabilistic Interpretation” section with analysis on how the contrastive loss helps us to learn a disentangled representation.	0.0356639390100316
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] All the above points have now been made much clear in the new version of the paper, in particular we added an appendix dedicated to the swimmer benchmark.	0.0377027195082274
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Following the suggestions, we added additional results for the associative recall task for many network variants.	0.0466496726562433
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] c) We added more detailed descriptions of the adversarial meta-learning baseline and in-depth analysis on the results.	0.052346165399741
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have added the detailed PPO-based training algorithm in Appendix A.1.	0.0528099831833026
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] -> Added a discussion on CNNs in the new “Scope” Section in the revision	0.0583378153414288
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have added a comparison between APO and PBT in appendix Section H, Figure 15.	0.0837650662115631
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We now have added related work about video compositional methods in section 2.3 in the second version of the paper.	0.154964362612283
Need more interesting problem definition [SEP] rebuttal_concede-criticism [SEP] We apologize if we painted an incorrect picture by calling it a “simple example” and a “staple introductory problem”.	0.3508773619358619
Need more interesting problem definition [SEP] rebuttal_concede-criticism [SEP] Reviewer #1 is absolutely right -- we don’t know yet how to scale this to more difficult combinatorial problems.	0.649122638064138
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] We respectfully disagree with the reviewer's main comment that the experiments are not large scale.	0.3508773619358619
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] Our paper simply shows ONE example for this.	0.649122638064138
Lack of analysis [SEP] rebuttal_concede-criticism [SEP] As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.	0.1975795937322861
Lack of analysis [SEP] rebuttal_concede-criticism [SEP] We appreciate your acknowledgment of our contributions and pointing out that the properties may only need to be satisfied at some critical points during training deep neural nets.	0.2815511087403978
Lack of analysis [SEP] rebuttal_concede-criticism [SEP] 1. You are right.	0.5208692975273159
Lack of analysis [SEP] rebuttal_structuring [SEP] [The analysis is relatively straightforward]	0.1334172456785002
Lack of analysis [SEP] rebuttal_structuring [SEP] Please kindly refer to Appendix A for more detailed results.	0.1712191370351586
Lack of analysis [SEP] rebuttal_structuring [SEP] 3. The authors should provide ablation study and analysis of their CTAugment.	0.2439874587384383
Lack of analysis [SEP] rebuttal_structuring [SEP] Following the reviewer’s suggestion, we have added a figure that shows the dynamics of neuromodulation in the cue-response task (Figure 3, in the Appendix).	0.4513761585479028
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.	0.3508773619358619
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] We have provided substantial analysis and visualizations on what AutoLoss has learned in our *initial submission*. Below, we summarize them for your reference:	0.649122638064138
Lack of analysis [SEP] rebuttal_future [SEP] We leave more comprehensive studies on diversity to future work.	0.1975795937322861
Lack of analysis [SEP] rebuttal_future [SEP] We agree that broader analysis beyond global-local disentanglement is desirable and we hope to perform more experiments in a follow up work.	0.2815511087403978
Lack of analysis [SEP] rebuttal_future [SEP] It is definitely a good idea to analyse the correlation between changes in classification accuracy and in FSM values, thank you. We will rigorously investigate this in future work.	0.5208692975273159
Lack of analysis [SEP] rebuttal_done [SEP] In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.	0.0640340158384878
Lack of analysis [SEP] rebuttal_done [SEP] We added a new section (6.2) and figure (Fig. 5) for these experiments.	0.0731054190644934
Lack of analysis [SEP] rebuttal_done [SEP] -> For more on this we refer to the newly added Section “Scope” in the revision.	0.0855332424172482
Lack of analysis [SEP] rebuttal_done [SEP] We updated the discussion in Sec. 3.2 in the revision to make this clearer.	0.1037089558670764
Lack of analysis [SEP] rebuttal_done [SEP] We updated the draft to include a longer treatment in the appendix.	0.1330930952284554
Lack of analysis [SEP] rebuttal_done [SEP] R) New data was added to the paper exercising multiple topologies, in a wider range of applications.	0.1896577481857959
Lack of analysis [SEP] rebuttal_done [SEP] 2. Following your suggestions, we add a study on the diversity of agents (presented in Appendix A of the updated paper).	0.3508675233984424
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_answer [SEP] Moreover, we investigate the mixing distribution learned in Appendix G.	0.0
Less datasets used [SEP] rebuttal_answer [SEP] The reviewer raises an important point about the tested single images.	0.0
Lack of analysis of proposed method [SEP] rebuttal_answer [SEP] Given the limited space provided, it would be difficult to fit a convergence analysis in our paper.	0.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] Please also refer to the main contribution (ii) of our response to Reviewer 2.	0.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_answer [SEP] As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.	0.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Moreover, we investigate the mixing distribution learned in Appendix G.	0.0
Lack of analysis [SEP] rebuttal_answer [SEP] Your interpretation of section 3 is exactly right.	0.0
Less datasets used [SEP] rebuttal_by-cr [SEP] We are training DeepCluster now on a significantly less busy image and will report results in the coming days.	0.0
Need more interesting problem definition [SEP] rebuttal_by-cr [SEP] The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.	0.0
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1). [SEP] rebuttal_by-cr [SEP] Thanks for pointing this out! We will make the presentation of the results consistent by highlighting the respective number.	0.0
generalizability of results is questionable [SEP] rebuttal_by-cr [SEP] 3)For the experiment: we will spend some time to train GANs with more iteration and modify it.	0.0
More variations of experiments needs to be added [SEP] rebuttal_by-cr [SEP] 4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.	0.0
Lack of analysis [SEP] rebuttal_by-cr [SEP] Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.	0.0
Lack of discussion of performance of method on different datasets [SEP] rebuttal_concede-criticism [SEP] As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.	0.0
Need more experimental results [SEP] rebuttal_concede-criticism [SEP] We agree that any improvements compared to RGCN are marginal.	0.0
Experimental study not strong enough [SEP] rebuttal_concede-criticism [SEP] 2) For the experiment, we will train our experiments longer and modify our network.	0.0
More variations of experiments needs to be added [SEP] rebuttal_concede-criticism [SEP] It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.	0.0
generalizability of results is questionable [SEP] rebuttal_contradict-assertion [SEP] Please check the degraded images in Table 3.	0.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_done [SEP] To show that we can easily include these features, we have included in our appendix some results including non-structural features.	0.0
"Experimental study not strong enough [SEP] rebuttal_followup [SEP] - Could you be more specific on what do you expect for ""larger studies"" and ""general study”?  This will be helpful for improving our work."	0.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_followup [SEP] Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?	0.0
Less datasets used [SEP] rebuttal_future [SEP] We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.	0.0
More variations of experiments needs to be added [SEP] rebuttal_future [SEP] However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.	0.0
Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.	0.0
Need more interesting problem definition [SEP] rebuttal_mitigate-criticism [SEP] This is, once again, motivated by making something work with modest amount of computation.	0.0
Incomplete details on perfromance of the method [SEP] rebuttal_other [SEP] This comment was also made in the official blind review #2.	0.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] *Please also see reply to reviewer #2 on a similar question of evaluating against other methods*	0.0
Experimental study not strong enough [SEP] rebuttal_refute-question [SEP] *Please also see reply to reviewer #2 on a similar question of evaluating against other methods*	0.0
Lack of ablation on different datasets/ sizes [SEP] rebuttal_reject-criticism [SEP] Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.	0.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_reject-criticism [SEP] We respectfully disagree with the reviewer’s suggestion that a fundamentally distinct resource warrants only a whitepaper.	0.0
Lack of analysis of proposed method [SEP] rebuttal_social [SEP] We appreciate such detailed and rigorous convergence analysis provided in [1] and [2].	0.0
Limited improvement over baselines [SEP] rebuttal_social [SEP] Thank you for pointing out paper [3].	0.0
generalizability of results is questionable [SEP] rebuttal_social [SEP] A: Thanks for the review comment.	0.0
Less datasets used [SEP] rebuttal_social [SEP] We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.	0.0
Experimental study not strong enough [SEP] rebuttal_social [SEP] We will gladly provide files with the trained weights and also fully trained neural networks on request.	0.0
More variations of experiments needs to be added [SEP] rebuttal_social [SEP] 2. Thank you for your suggestion.	0.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_structuring [SEP] So, two responses are given below.	0.0
Lack of experiments [SEP] rebuttal_structuring [SEP] Missing experiments to validate nature of bounds.	0.0
Incorrect claims about performance in tables and figures [SEP] rebuttal_structuring [SEP] “Judging from Table 1, the proposed method does not seem to provide a large contribution.	0.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_structuring [SEP] We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.	0.0
Less datasets used [SEP] rebuttal_summary [SEP] Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.	0.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.	0.0
Need more experimental results [SEP] rebuttal_reject-request [SEP] We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.	0.0
generalizability of results is questionable [SEP] rebuttal_reject-request [SEP] Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.	0.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] In short, we verified that the results we present are valid over a various number of parameters of the network, like the learning rates (figure 2) but also sparsity and the size of the dictionary (see Response To AnonReviewer3 @ https://openreview.net/forum?id=SyMras0cFQ&noteId=BylQtQPHRX ).	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] As in Sandin, 2017 paper we have shown similar results in OMP.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] These results indicate that a modest number of pairs would be beneficial.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] We choose the sample at percentile $0.01\%, 0.1\%, 1\%, 10\%, 20\%, 30\%, 40\%, 50\%, 60\%, 70\%, 80\%, 90\%$. We conduct the neighboring analysis on these selected samples.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] FFHQ is a public face dataset contains $56,138$ images, without repeating identities.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] The BLEU scores are 33.55 and 33.71 respectively for COCO only and Multi30K+COCO.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] In addition, we are also interested in the influence of the number of sentence-image pairs inspired by your suggestion.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] Furthermore, we would like to clarify that our metric is proposed to measure the collapse of GAN's learned distribution.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] We still observe a gap between $\mathcal{R}_{obs}$ and $\mathcal{R}_{ref}$, which demonstrates that FFHQ dataset has dense mode, even without repeating identities.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] We first randomly pick $1k$ images to form the S set and sort the S set according to the number of neighbors within distance 0.3.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_summary [SEP] We randomly split the pairs of Multi30K into the proportion in [0.1, 0.3, 0.5, 0.7, 0.9], the corresponding BLEU scores are [33.07, 33.44, 34.01, 34.06, 33.80] respectively.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] 2. Applying our metric on the training set of FFHQ	1.0
"Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] R: ""The authors explain how they trained their own model but there is no mention on how they trained benchmark models"""	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] See for example some of the recent domain adaptation papers [1, 2, 3].	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] While a feature extractor backbone network may have some influence, we would like to highlight three points.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] We also show in the paper the application to a one-layer convolution network and our preliminary results show that we can extend this to a hierarchical network.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] 1.  “In particular, Section 4 is a series of empirical analyses, based on one dataset pair….However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.”	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] See general responses #1 and #3.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] 2. Impact of paired sentence-image dataset:	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] … feature extractor backbone”	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_structuring [SEP] As regards the experiments: “fairly small datasets	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_reject-criticism [SEP] Again, CNE outperforms all other methods in accuracy by a wide margin, and is substantially faster as well.	1.0
"Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_reject-criticism [SEP] Second, we had already done the comparison ""against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties"" but we had initially omitted to include this supplementary data (that takes the form of a single jupyter notebook which allows to reproduce all results)."	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_reject-criticism [SEP] We remained with synthetic tasks for two reasons: 1) to illustrate the method in settings we thought would be clearer, 2) because as we highlight in the future directions, there is a lack of meta-learning datasets that contain as structured of relationships between tasks as we consider.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_reject-criticism [SEP] Most domain adaptation experiments use MNIST, USPS, SVHN, which are comparable in size to our Omniglot experiments.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_reject-criticism [SEP] - The datasets we used are as large as the datasets used in other related work in the area.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_reject-criticism [SEP] The Prob2Vec method is performing well on this not relatively big data set, which is our goal, but if we have a bigger data set (as we have right now with more than 2400 problems), Prob2Vec may even have a better performance since with more data we can have a more precise concept and problem embedding.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_reject-criticism [SEP] is the nature of the application since creating mathematical problems is a creative process, so it is hard to have a very big data set.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_reject-criticism [SEP] The other popular benchmark is using the Office-dataset, which also we have used (although a more recent version of a similar dataset, i.e., office-home – more suitable for meta-learning evaluation, as it has larger number of classes).	1.0
"Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_reject-criticism [SEP] (Taskonomy, for example, has at best a notion of ""similarity"" in terms of transfer.) We are working on creating several such datasets, but we think that this paper as it stands is a useful contribution that illustrates the concept and ideas -- the datasets themselves will also require further description, and including them in a paper of this length would likely result in even more material being cut, and so a less clear presentation."	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_reject-criticism [SEP] 2- The data set being small	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_done [SEP] We added this explanation in the new version of the paper.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_done [SEP] To demonstrate CNE's superior scalability, we included another network with around 200.000 nodes and around 1.000.000 edges (http://snap.stanford.edu/data/loc-Gowalla.html), run on a basic single CPU laptop.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_done [SEP] We have revised the toy experiments to include a heteroscedastic regression task (see Section 6.1.).	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_done [SEP] The results are included in the revised manuscript.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_done [SEP] For every model with an embedding space (i.e., all except CTIC), we set its dimension to $d=50$ (larger dimensions induce a more difficult convergence of the learning without significant gain in accuracy).	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_done [SEP] We have empirically shown in the paper that the mode collapse still occurs despite balanced training data.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_done [SEP] Yes. We add the external MS COCO image caption training set and evaluate on the EN-RO task for quick evaluation.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_done [SEP] We have now included it in an anonymized format.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] In our updated revision, we also include the training time comparison for the DynamicConv model in Appendix B.4.2 (in this case, the modified model with K-matrices actually trains slightly faster than the baseline).	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] Additionally, for the speech preprocessing and ShuffleNet experiments, we compare the total wall-clock training time of our K-matrix approach to that of the baseline approach, in both cases finding that the training time required by our approach is at most 20% longer than that of the baseline approach.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] The paper does achieve this goal, on a number of networks.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] However, hybrid training is able to bring the accuracy of most networks studied in our paper up to a comparable level of the non-pipelined baseline as shown in the evaluation section of our paper.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] Our scheme saves all activations instead of re-computing them to eliminate pipeline bubble, thus achieving better utilization for the accelerators (GPUs).	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] (A) In Fig. 1, the difference comes from the definition of node y^(i).	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] Regarding the convergence and speed of training, we would like to stress that all hyperparameters for training were kept the same as those for training the default model architecture, other than those we explicitly mentioned as being tuned (e.g. learning rate for the speech experiment).	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] The main goal of our submission is to experimentally show that our pipelined training, using stale weights without weight stashing [1] or micro-batching [2], is simpler and does converge.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] Our scheme has less memory footprint than PipeDream because it does not stash weights.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] Yes, Lemma 1 shows that our deep GO will reduce to Rep when Rep is applicable.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] Indeed, GPipe [2] incurs less memory footprint than our pipelining scheme and PipeDream [1] because it only saves the activations at the boundary of each model partition and re-computes the activations of the model during the backward pass.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] We agree that better performance could be obtained by running the initial model for more epochs, but our goal is to stay close to the standard training of Imagenet models, i.e. 90 epochs with an initial learning rate of 0.1, divided by 10 every 30 epochs.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] (B) If you were interested in the difference in Fig. 2 (a)(b), the reasons include (1) the standard Rep cannot be applied to Gamma RVs; (2) both GRep and RSVI are designed to approximately reparametrize Gamma RVs; (3) GO generalizes Rep to non-reparameterizable RVs; or in other words, GO is identical to the exact Rep for Gamma RVs.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] The accuracy drops for some models in a pure pipelined training.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] Our pipelined method is different from data parallelism in the following way (for a 2-GPU example).	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] For deterministic deep neural networks, node y^(i) is the activated value after an activation function, where deterministic chain rule can be readily applied; while for deep GO gradient, node y^(i) might be the sample of a non-reparameterizable RV, where deterministic chain rule is not applicable.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_answer [SEP] However, the re-computation still incurs pipeline bubbles during training.	1.0
"In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] [Experiments Section] We have significantly updated qualitative and quantitative results in our ""Experiments"" section and now compare our methods against Beta-VAE, DIP-VAE, and InfoGAN."	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] We compare the performance of the model with and without PDR and using no other regularization.	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] We truncated the vocabulary by keeping approximately 100k words with the highest frequency.	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] To really make this clear, we have updated the paper to demonstrate that even if we do not use C_p^{val} for model selection, and instead select from the same class of models we previously generated by using a randomly selected held-out set C_r^{val}, we still obtain state-of-the-art performance (0.892 [0.885 +/- 0.009]).	1.0
"In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] We now address this point in our ""Discussion"" section."	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] We have added a performance comparison of K-matrices with other structured replacements such as circulant, Fastfood, ACDC, and Toeplitz-like in Appendix B.4.3, showing that K-matrices yield faster inference with similar BLEU score.	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] We obtained a valid/test perplexity of 44.0/42.5 for the model with PDR and 44.3/43.1 for the model without PDR, showing a gain of 0.6 in the test perplexity by using PDR.	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] Finally, we have conducted further experiments on larger corpora, specifically the Gigaword corpus.	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] We used the same validation and test sets as (Yang et al. 2017).	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_done [SEP] Are added to section 4 and appendix E	1.0
generalizability of results is questionable [SEP] rebuttal_concede-criticism [SEP] We are working on Fast Spectrogram Inversion using Multi-head Convolutional Neural Networks, arXiv:1808.06719, Sercan O. Arik et al. to replace Griffin-Lim inversion ; two possible improvements we expect are much faster (towards real-time) sound rendering and better audio quality.	1.0
generalizability of results is questionable [SEP] rebuttal_concede-criticism [SEP] Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] In fact, a recent challenge held at ECCV 2018 in such a problem [2] evaluates all algorithms on both of these axes, as neither adequately captures performance.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] -- Thank you.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] The direct interaction with 3D latent space becomes even more interesting when we pipeline our model with fast-spectrogram inversion.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Our key insight backed up with extensive empirical evidence is that a combination of graph-level and node-level methods (Figure 1) is important because it allows the model to capture both local and global semantics of graphs.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] any video generator (including the one from Denton & Fergus (2018)) could be used with our losses.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Therefore, TabNN is a better general solution for tabular data as it can cover more scenarios.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] We agree that it is important to understand why some pre-training strategies work better over others.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] We can use them independently according to the feature types of data. And they can be used together for the data with mixed feature types.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] (1) The noise is introduced by the dropout function in each convolution layer.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] At first we validated that our models could perform well in term of training/test spectrogram reconstructions with only 3 latent dimensions, some reasons that we found interesting to enforce this are more related to a possible music/creative application of the model: less synthesis/control parameters for the user (and controls which may then be more expressive), direct visualization of the latent space which is turned into a 3D synthesis space from which users may draw and decode sound paths or create other interaction schemes, a denser latent space that may be better suited for random sampling/interpolations.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] 3. Unsupervised results: For the unsupervised setting, in addition to our face dataset and CelebA, we also present the results on the chairs and cars in the Appendix (See Figure 5, Figure 7, Figure 12, Figure 13).	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] It's also worth noting that with a simpler feed-forward posterior and a unit Gaussian prior, our VAE ablation and SVG achieve similar performance on various metrics.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] A recent result [1] proves that there is a fundamental tradeoff between accuracy and realism, for all problems with inherent ambiguity.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Thank you for bringing up this valuable point.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] (2) The way we add noise	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Then, each block of size 9 is indexed by an integer between 0 and 255: such integer can be stored using 8 bits or 1 byte (as 2^8 = 256).	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] The size of the compressed model is the sum of the sizes of the compressed layers.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Finally, we deduce the overall compression ratio which is the size of the compressed model divided by the size of the non-compressed model.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] We also strongly agree that testing additional embeddings would be very interesting!	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] The memory footprint of a compressed layer is split between the indexing cost (one index per block indicating the centroid used to encode the block) and the cost of storing the centroids.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] We also found in our ablation study that using only strong augmentation (i.e., replacing weak augmentations with strong augmentations) resulted in very poor performance for ReMixMatch, suggesting that anchoring towards a weaker augmentation is important.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] is well-recognized and commonly-used in generative deep learning models[1].	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Let us detail it further here.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Although the SVG generator is simpler than ours, ours is just a simple variation from Ebert et al. (2017).	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Thus, as we have 128 x 128 blocks, the indexing cost is 128 x 128 x 1 byte = 16,384 bytes = 16 kB.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] So, if samples are close to each other, the margin is smaller, and vice versa.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Instead, we provide a systematic analysis of the effect of the loss function on this task (which could be applied to any generator).	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Moreover, compared with previous NN based solutions for tabular data, TabNN outperforms them significantly.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] The proposed TabNN can overcome these shortages and achieve comparable accuracy with GBDT.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Phrase similarity results: the tensor component T(v_a,v_b,.) does yield improvement over all other weighted additive methods in 5 out of 6 cases, as shown in Table 3.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Since proposing a strong generator architecture is not the goal of this paper,	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] R) As in traditional CNNs, the increment of the number of kernels in a layer produces some saturation, with a marginal increment of accuracy.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] We provide an example of the computation of compression ratio in Section 4.1, paragraph “Metrics”.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] We agree that better performance could be obtained by running the initial model for more epochs, but our goal is to stay close to the standard training of Imagenet models, i.e. 90 epochs with an initial learning rate of 0.1, divided by 10 every 30 epochs.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] As stated in Section 2, D&W NNs and many related models can work well with high dimensional sparse features, which are usually in the form of one-hot encoding converted from categorical features.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Say we quantize a layer of size 128 × 128 × 3 × 3 with 256 centroids and a block size of 9.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Dropout functions by randomly ignore 50% of neuron’s output of a network in our mode by a uniform distribution.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Finally, we have to store 256 centroids of dimension 9 in fp16, which represents 256 x 9 floats (fp16) = 256 x 9 x 2 = 4,608 bits = 4.5 kB.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] The noises added in GANs aim to enable the diversities in the generated graphs to avoid the problem that GANs tend to favor producing same output rather than spreading it evenly over the domain.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] The reviewer raises an interesting point about total training time, which includes the time to pre-train a GNN and the time to fine-tune it on a downstream task.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] Similar to the non-adversarially trained smooth classifier included in the original submission, we can produce adversarial examples for the SmoothAdv classifier which on average produce larger certified radii than their natural example counterpart.	1.0
generalizability of results is questionable [SEP] rebuttal_answer [SEP] In our experiments it was observed that 5 dynamic kernels generates comparable level of abstraction than 30 traditional convolutional kernels.	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] ** Addressing comments on the write-up:	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work.	1.0
"generalizability of results is questionable [SEP] rebuttal_structuring [SEP] However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal."""	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] * Sound quality is disappointing and with artifacts:	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] More importantly, the results presented are quite meager.	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] --------------------------------------------------------	1.0
"generalizability of results is questionable [SEP] rebuttal_structuring [SEP] 4. ""Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem."	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] 4. How sensitive are the results to the number of adaptive kernels in the layers.	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] However, the results are a bit misleading in their reporting of the std error.	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] Q: It is not clear how the noise is introduced in the graphs. I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] 1)it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.	1.0
"generalizability of results is questionable [SEP] rebuttal_structuring [SEP] This is a major concern."""	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] “The conclusions focus on the importance of section 3 and the results of the experiments performed. Do the conclusions accurately reflect the opinions of the author?”	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] The main point of the reviewer is that the novelty of our approach is limited with respect to the Evolutionary RL (ERL) algorithm, and that improvement is sometimes small.	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] 3. Benchmark Dataset and Compared with Deep and Wide (D&W) NNs	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] R: - Statistical significance	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] In this regard, I encourage	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] 2. As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] RE: Analysis of different pre-training strategies	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] >>> It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.	1.0
"generalizability of results is questionable [SEP] rebuttal_structuring [SEP] """	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] provided qualitative examples quite reduced.	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] the authors to update the supplementary material in order to show extended	1.0
"generalizability of results is questionable [SEP] rebuttal_structuring [SEP] 1. Response to the ""Weaknesses"" part and the comparison with GBDT"	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] [R4: Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] RE: Total running time	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] 2)	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] If this is a method for image recognition,	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] * Insufficient justification of the 3D latent space:	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] Comment: The reported results are mostly qualitative. I find the set of	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] To address this point, below, we give the results of the total training time as well as the amortized total time over different downstream tasks.	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] --------------------------+--------------+---------------+	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] (2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.	1.0
generalizability of results is questionable [SEP] rebuttal_structuring [SEP] qualitative results of the explanations produced by their method.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Our method belongs to the first one, which contains much fewer layers than the ResNet setting.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] In contrast, the proposed TabNN works better on another kinds of tabular data, with numerical features and low-cardinality categorical features.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] However, VAE cannot force the unseen classes with high reconstructed error.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] A: We actually found that using stronger augmentations in MixMatch resulted in divergence.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] In contrast, NN can be learned by mini-batch fashion and therefore can learn from streaming data naturally.	1.0
"generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] >>> We want to clarify that ""Label Propagation"" in Table 1 and Table 2 is a strong baseline."	1.0
"generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Due to the above reason, it is expected that ""our sampled reconstruction results are not good as VAE""."	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] It combines label propagation method [8] with episodic meta-learning.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] The quantitative results in Table 3 further validate our approach.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] In contrast, our rigorous investigation highlights their strong generalization capabilities and relates them to the specific design of NMNs.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] The usage of transductive inference makes this baseline outperform most published state-of-the-art methods.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Since there are many dummy dimensions in one-hot encoding, TabNN is hard to learn the useful features combinations from them.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] >>> At first, we want to clarify the few-shot network architecture setting.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Actually, these NNs perform very well in such datasets, even better than GBDT.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] To better relieve the reviewer's concern, we implemented our algorithm with ResNet architecture on miniImagenet dataset and show the results as follow:	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] As we argued in our conclusion, these findings should be of a highest importance to researchers working on end-to-end NMNs, which is a very popular research direction nowadays.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] It is expected that images from the seen classes should be reconstructed better than those reconstructed from unseen classes.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] 1. We aimed to provide a broad variety of example applications (playing tennis, walking, fencing, dancing), while mainly focusing on the most complicated (tennis) application, for a thorough analysis of our method.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Therefore, TabNN and D&W NNs are orthogonal with each other.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] We believe in few-shot learning, this is a large improvement.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] We do not consider the conclusions of the experiments from Section 3  to be more important than those of the other sections, in fact quite the opposite.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Our method removes this trade-off—-we decouple ‘disentanglement of the latents’ from ‘generation quality’, specifically by having a two-stage training process.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] As stated in the response to review 1, our goal is not inventing a model to beat GBDT but developing a model to cover the scenarios not suitable for GBDT such as some applications need online updating.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Response: We do use different subsets of the train set for different replicates.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] 2) Novelty of experiments: The current paper presents substantially more comprehensive experiments for benchmarking the proposed class of optimizers against other popular optimization methods for deep learning tasks.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] This, in addition to the potential to accelerate initial convergence, makes the proposed PoweredSGD methods useful in many potential applications.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] It can be seen that we beat TADAM for 1-shot setting.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] We would like to emphasize that this is possible only because of the two-stage training process (please see comments to Reviewer 2 regarding d-separation).	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Last but not least, an important part of our investigation (which the review does not discuss) is the systematic generalization analysis of popular end-to-end NMN versions, that shows how making NMNs more end-to-end makes them more susceptible to finding spurious solutions.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] For 5-shot, we outperform all other recent high-performance methods except for TADAM.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Note that the seen class, car, still can be reconstructed well by our method in Fig 8 (at the last row).	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] So, we combine DSGAN with VAE to deal with this issue.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] -- Improving disentangled representation learning over beta-VAE: Beta-VAE obtains disentangled representations by explicitly posing a trade-off between the ‘quality of disentanglement’ (factorisation of the posterior) vs. the image reconstruction quality.	1.0
"generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] For example, in miniImagenet, TPN outperforms label propagation with 1.44% and 1.25% for 1-shot and 5-shot respectively, but this advantage grows to 3.20% and 1.68% with ""Higher Shot"" training."	1.0
"generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] ""The next contender"" model in your comment is the GBDT, which indeed works well for tabular data."	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Currently, there are two common network architectures: 4-layer ConvNets (e.g., [1][2][3]) and 12-layer ResNet (e.g., [4][5][6][7]).	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Notably, chain-structured NMNs were used in the literature prior to this work (e.g. in the model of Jonshon et al multiple filter_...[...] modules are often chained), so the fact that tree-structured NMNs show much stronger generalization was not obvious prior to this investigation and should be of a high interest to the research community.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Thus, it is more reasonable to compare TADAM with ResNet version of our method.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] However, GBDT suffers from two shortages, as stated in Section 2 and the responses to reviewer 3.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] This allows us to potentially have much higher disentanglement, while still maintaining image quality, unlike beta-VAE where the quality of generation would necessarily be compromised.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Reading prior work on visual reasoning may lead a researcher to conclude, roughly speaking, that NMNs are a lost cause, since a variety of generic models perform comparably or better.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] Moreover, the performance of TPN over label propagation is not very small.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] While we agree that the results that we report may not shock the reader (although perhaps hindsight bias plays a role in what people find surprising or not after reading an article) we find them highly interesting and not at all easily predictable.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] The improvements are even larger for tieredImagenet with 4.68% and 2.87%.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-criticism [SEP] In Novelty detection, we use the reconstruction error as a criterion to determine whether an image comes from seen class or unseen class.	1.0
"generalizability of results is questionable [SEP] rebuttal_done [SEP] We mention this in the paper (""Since MixMatch uses a simple flip-and-crop augmentation strategy, we were interested to see if replacing the weak augmentation in MixMatch with AutoAugment would improve performance but found that training would not converge."") but will emphasize this more in the next draft."	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] 92.77            72.53	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] (Added to the paper)	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We did so by replacing Figure 1, which was contrasting CEM-RL to CEM, with a figure directly contrasting CEM-RL to ERL.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We have added the statistical significance results to the revised version.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] SGDm (decayed lr)	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We also compared APO to manual learning rate decay schedules.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We added this clarification to Section 3.4.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] This demonstrates the practical applicability of APO for contemporary networks.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] By the way, the ERL paper is now published at NIPS, but it was not the case yet when we submitted ours. We updated the corresponding reference.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] The final test accuracies of the updated model using SGD/SGDm with and without APO are:	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] The ResNet34 with a custom learning rate decay schedule achieves 93-94% test accuracy on CIFAR-10 and ~74% test accuracy on CIFAR-100.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] ,  statistical significance of the the average accuracy and FSM results obtained after completing the last two tasks from each dataset, i.e. the corresponding values of the last two tasks of all the plots in Figures 1, 2, 3 and 4, are now displayed in the tables in Appendix A.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] The test accuracies using RMSprop and K-FAC with APO are shown in our response to all reviewers at the top.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We have also updated that table with additional results, which show that adding in the tensor component improves upon the strong baseline of the SIF embedding method.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] SGD-APO	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] For CIFAR-10/100, we trained for 200 epochs, decaying the learning rate by a factor of 5 three times during training.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] results. Thank you for your suggestion.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] 93.82            74.65	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] These remarks helped us realize that we had to better highlight the differences between our approach and ERL, both in terms of concepts and performance.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] 93.53            73.80	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We also added Table 4, which repeats the phrase-similarity task for verb-object pairs, and shows that the tensor component leads to improvement in most cases.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] SGD (fixed lr)	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] SGDm (fixed lr)	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We updated Section 4.4 to indicate that it is to be expected that, although our SAVP model improves on diversity and realism, it also performs worse in accuracy compared to pure VAE models (both our own ablation and SVG).	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] Per your request, we have attacked the work of [2] and reported results of attacking the pre-trained SmoothAdv classifiers (available in [3]) in Appendix B.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] New suggested structure and related suggestions: These are nice suggestions and explain why the structure was confusing. We’ve worked on these to come close to the suggested structure.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] [Similar Latent Factors] We now use an adaptive margin that depends on the distance between two latent samples.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] SGDm-APO	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] Thus our reported metrics are correct and justified for this problem, though we have clarified the exact nature of the replicates in the text to ensure this is not misleading.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] Since we were concerned that adding this information to the plots would make them harder to read	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] 93.29            73.45	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We believe that this is a strong baseline, and shows the applicability of APO in practical settings.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] 92.97            72.69	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] The results in these tables show that APO is competitive with manual schedules in terms of test accuracy.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] * Updated the baseline model for CIFAR-10/100 from VGG11 to ResNet34.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] 94.59            73.89	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We redo the visualization in Figure 6 to make the gains provided by SN clearer.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] SGD (decayed lr)	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We’ve expanded the discussion of the results as much as possible.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] The updated figures in our paper show that APO is competitive with manual schedules both in terms of test accuracy and training loss.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] R) We added a new experiment for a real life application; testing different topologies.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] Response: We have added a supplementary section, adding more qualitative	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] | CIFAR-10 | CIFAR-100 |	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We will update the labels in the ablation table to make this more clear.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We have updated the baselines in our paper for CIFAR-10 and CIFAR-100, using a larger, modern network, ResNet34, in place of the VGG11 model used previously.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We decided to move it to an appendix after reading the feedback from the three reviewers.	1.0
generalizability of results is questionable [SEP] rebuttal_done [SEP] We see that using SN can improve the test performance by over 12% for some FGM, PGM, and WRM cases.	1.0
Lack of experiments [SEP] rebuttal_done [SEP] Please refer to the revised version for numerical evaluations in Section 6.	1.0
Lack of experiments [SEP] rebuttal_done [SEP] Taking your comments and comments from other reviewers into account, we have made improvements to the experimental section.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_concede-criticism [SEP] That said, we agree it is worth investigating the performance of LSTM on this problem further.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_concede-criticism [SEP] We included embedding details in the paper because a reviewer from the venue we previously submitted to requested these details to be in the paper.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_concede-criticism [SEP] However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_concede-criticism [SEP] Thanks for pointing out the issues with our presentations.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_concede-criticism [SEP] (6) Thank you for this feedback.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_concede-criticism [SEP] From the reviewer’s comments we noticed that, perhaps, the submitted paper, may not have sufficiently clearly explained that the approach is already targeting defences based on outlier detection and in particular that proposed in Paudice et al. 2018a.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] In our experiments we chose the scheme proposed by Paudice et al. 2018a, as it assumes a stronger model for the defender (as mentioned before), which, in our opinion helps to validate the effectiveness of pGAN to craft successful poisoning attacks even in cases where the defender is in control of a fraction of trusted (clean) data points.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] This shows that additional modules help.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] Since our results are derived for MNIST we can only compare to methods in the literature that are evaluated on MNIST.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] The detectability constraints included in our model prevents this defence to detect the generated poisoning points.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] This instability makes their weights difficult to interpret.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] xylophone:  SDR: 8.08   SIR: 12.33   SAR: 11.72	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] Kim et al. (2018) which is concurrent to our work shows similar performance.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] In Figure 8 (left) we can observe that the defence performs worse than the outlier detector and that, when the algorithm is not under attack, the performance slightly decreases, as the algorithm is removing genuine data points that are significant for the training process.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] We already assume that the defender is in control of a fraction of trusted (clean) data points to train the outlier detector, which is a strong assumption in favour of the defender.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] As pGAN produces poisoning points that are correlated, the KNN-based algorithm proposed to do the relabelling is not capable of detecting the poisoning points.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] We acknowledge that other methods perform very well on more sophisticated tasks and have added a reference.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] In the Figure, we can observe that both for MNIST and FMNIST the outlier detection is capable of detecting many poisoning points and the effect of the attack is reduced compared with the results for alpha = 0.1.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] (For information, the complete simulations for this paper take approximately 12 hours --which are easily distributed on a cluster as we multiplied the number of independent learning runs using different classes of parameters, cross-validations and types of sparse coding algorithms - in total approx 500 experiments.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] xylophone:  SDR: 2.04   SIR: 3.61   SAR: 12.13	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] Moreover, some of the genuine points from the target class are incorrectly relabelled, making the problem even worse.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] We can observe that, in this case, the difference in performance is not significant for the different values explored for this threshold.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] For FMNIST, Sever outperforms the outlier detector when the number of poisoning points is reduced, although the degradation of the algorithm as we increase the fraction of poisoning points is faster compared to the outlier detector and the PCA-based defence.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] Label sanitization (as proposed in Paudice et al. 2018b) completely fails to defend against pGAN attack, as shown in Figure 8 (right).	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] In the supplement we included the sensitivity analysis w.r.t. the parameter that controls the fraction of points to be discarded.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] As you mentioned, our weights are theoretically justified while theirs are only heuristically computed.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] Then, we have provided an empirical evaluation of pGAN against 4 different defence mechanisms both in MNIST and FMNIST, showing how our attack bypasses all of these defences.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] congas:        SDR: -0.20  SIR: 0.23   SAR: 14.76	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] Given a partial tree, there can be more than one way to complete the layout.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] Given a 10%, 50% and 80% DFS partial layout, the mean number of completions is 3.63, 1.24, and 1.17 respectively.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] The PCA-based defence proposed by Rubinstein et al. 2009 (Antidote) is also not capable of mitigating pGAN attack.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] congas:        SDR: 1.77  SIR: 2.76   SAR: 11.97	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] We can observe that the error increases as we increase this threshold.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] First in Figure 2 we show the effect of the attack for different values of alpha tested against the outlier-detection-based defence.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] In summary, the revised paper (see the new version uploaded) now provides a comprehensive comparison of different defence mechanisms and shows the effectiveness of pGAN to bypass all of them.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] guitar:          SDR: 5.97   SIR: 7.56   SAR: 12.81	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] The “Sever” defence (Diakonikolas et al. 2019 ICML) is also not robust against pGAN attack.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] guitar:          SDR: 2.17   SIR: 2.78   SAR: 14.19	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] We see that the weights provided by MDMN are not very stable, changing from one source domain to another drastically during training.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] Different outlier-detection-based defences have already been proposed in the literature, such as Steinhardt et al. 2017 (“Certified defenses for data poisoning attacks”), Koh et al. 2018 (“Stronger data poisoning attacks break data sanitization defenses”) or Paudice et al. 2018a, to cite some.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] Given a 10%, 50% and 80% BFS partial layout, the mean number of completions of the layout is 2.97, 1.23 and 1.17 respectively.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_summary [SEP] Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018).	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] > The random baseline (i.e a random ordering of candidate task) is compared in figure 3 (and all the plots in the appendix), where we plot the accuracy boost using the best task till now in the produced recommendation of candidate tasks using different methods.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 1.3) As can be seen in many well-known and recent NMT works ([4], [5])	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Ours (ensemble)	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Thus, our scheme has the advantage of a smaller memory footprint.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] This is essentially similar to weight stashing used in PipeDream, which we compared to in our paper.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] For the first paper (i.e., Gradient descent GAN optimization is locally stable) analyzed the stability of GANs using the Jacobian matrix and adopted a regularization term to stabilize GANs similarly to [*4].	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] A 10% change in FID is visually noticeable.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Our numbers of the baseline transformer model match the results reported in [1].	1.0
"Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] However, since the ancestry has a variable-number of nodes (as decoding proceeds)	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 33.47       48.89	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] LSTMs are indeed a strong model for tree prediction on previous tasks.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 2018	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] To allow the model to access ancestry nodes during decoding, one way is to concatenate the parent node latent representation with the input of each step for decoding children, and then feed the concatenated vector to LSTM (e.g., Dong & Lapata ACL 2016).	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Further, equation (9) in [1] suggests that while weight updates use delayed gradients, the delayed weights (W^(t-K+k)) are used for the weight gradient calculation.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] This includes, among other things, results for modeling periodic behaviors of signals which is not a goal in positional encoding.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Furthermore, it is also shown from these works that it is hard to improve over the transformer baseline, and 0.5-1 BLEU score improvement is already considered substantial.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Additive composition vs. tensor: as discussed in our introduction (and illustrated by the qualitative results in Tables 1 and 2), we believe that linear addition of two word embeddings may be an insufficient representation of the phrase when the combined meaning of the words differs from the individual meanings.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] For the second paper, the authors used the Lyapunov function, which is different from our framework, to analyze the stability of GANs.	1.0
"Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 1.1) We use the transformer model with ""transformer_big"" setting [1], which is a strong baseline that outperforms almost all previously popular NMT models based on CNN [2] and LSTM [3]."	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Our method achieves the state-of-the-art result on this task.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Our scheme saves the activations instead of re-computing them to eliminate pipeline bubble, thus achieving better utilization of the accelerators (GPUs).	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 2. We further add newly obtained results on the WMT18 challenge.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] We compare our method with both the champion translation system MS-Marian (WMT18 En->De challenge champion).	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] , it is a common practice to use transformer as the robust baseline model.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Note that using time as a scalar feature similar to other features is currently the prevalent choice (see the references in the last paragraph of section 2).	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] - Instead of using time as a scalar feature similar to other features (which as the reviewer also pointed out is a naive way of handling time), we identify the characteristics that differentiate “time” from other features and propose a representation that enables exploiting those characteristics.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] However, as the authors note in the paper cited by the reviewer, the accuracy of a pretext task does not translate to downstream task performances, so even a method that is simple on one image’s patches does not necessarily fail.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] - Providing a comprehensive set of experiments showing the merit of Time2Vec for time-series prediction problems where time is an important feature.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Of course, LSTM equipped with Attention would achieve the same benefit.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] WMT En->De	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] - Obviating the need for hand-crafting functions of time by instead enabling these functions to be learned from data, and backing up the representation theoretically as, according to Fourier sine series, it can approximate any function in a given interval (see the last paragraph of our response to reviewer 5).	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] There might be other types of query distributions, such as the one pointed out by the reviewer.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] However, we note that FID rewards both improvements in sample quality (precision) and mode coverage (recall), as discussed in Sec 5 of [1].	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Ours (single)	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 1.2) In addition to the standard baseline models, we also compare our method against all the relevant algorithms including knowledge distillation (KD) and back translation (BT).	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 40.68	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Note, that the conclusions keep unchanged.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Syntactically related word pairs such as adjective-noun and verb-object pairs can have this property.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] In addition, positional encoding in Transformer also allows us to easily model spatial locations of UI elements.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Methods such as Context [4] and Jigsaw [5] could potentially work less well as they would potentially easily find a way to cheat given the limited amount of original data of one image.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] As the reviewer pointed out, the query distribution we use is a natural choice.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] We believe that this method will work well for pretext tasks that rely on learning via detecting and learning invariances, such as Exemplar [1], Colorization [2], and Noise-as-targets [3].	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] MS-Marian (ensemble)	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] A4: Thanks for pointing out the related work. In fact, our method is distinct from these methods.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] While the final representation of Time2Vec resembles that of positional encoding, the motivation behind Time2Vec is completely different than that of positional encoding.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] The new things offered by Time2Vec compared to positional encoding and other previous work include:	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Instead, we adopted a different method to model the dynamics from control theory.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Therefore, an improvement in FID may not always be easily visible, but may indicate a better generative model of the data.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] ---------------------------------------------------------------------------	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] In essence, our scheme is different than [1] in two key aspects: (1) we pipeline both the forward and backward passes of the backpropagation while [1] pipelines only the backward pass.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 2017	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] The difference has been discussed in Sec. 1 and Sec. 5	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] This shows a clear distinction between different methods and an important result: when $\ell_2$ normalizing atoms, dictionary learning may converge to a result for which the ratio of activity between the most activated and the least activated is of the order 2.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] While we can easily assess the former by visual inspection, the latter is extremely challenging.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 41.23	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] -- Thank you for the suggestion regarding the fixed attention map.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Intuitively, our overall approach that separates heavy hitters from the rest should still be beneficial to such query distribution.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 31.9          48.3	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 39.6	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Besides, their method fails to scale-up to large datasets such as CIFAR-10 because of computational issues.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] The follow up work in [2] attempts to reduce the memory footprint through feature replay (i.e., re-computing activations during backward pass, similar to GPipe).	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Our early experiments with LSTM did not yield good results on this spatial layout problem.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Transformer is the state-of-the-art NMT architecture.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Although our representation resembles positional encoding on the surface, it has not been clear in the time-series community if/how/why positional encoding can be used to replace hand-crafted functions of time, and there has been no empirical evidence to show its merit.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] We can clearly see that the random ordering is much worse compared to informed metrics that use representations.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] However, SPAMS is a great inspiration for our framework.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 34.01       49.61	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] , to directly access these nodes during decoding, attentional mechanisms would be an efficient way, which is one of our motivations to use Transformer models that are attention-based.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] 2016	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_answer [SEP] Re: (W3) Baselines for transfer learning:	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] 5 and 6.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] b) All algorithms should optimize both G and theta for a fair comparison.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] The evaluations are reported as follows:	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] Q2: a) Optimizing both the controller and the hardware has been previously studied in the literature.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] Is it worth using a neural graph?	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] ** Language Translation Baselines **	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] ----unsupervised----	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] Please refer to Section 3.1 and Section 3.4 for more details.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] 1. For the baseline models reported:	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] ----supervised----	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] Comparison with SOTA models for counting and relationship detection	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] >  How general is the proposed approach?	1.0
"Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] Please refer to Section 3.4 ""Study on generality of the algorithm"" for more details and Table 4 for full results in our updated paper."	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] - Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] Q4: Related work:	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] [Assuming query distribution is the same as data distribution]	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_structuring [SEP] “the method they propose offers very little that is new when compared to e.g. Vaswani”	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_by-cr [SEP] We will edit the related work section to include the above discussion.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_future [SEP] - Eval metrics	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_future [SEP] Since this is the first paper on this topic, we chose to focus on introducing the problem and providing Transformer-based approaches as a baseline for future work.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] You are right! We have revised the baseline experiments with Bayesian models so that they either use \lambda = 1 or the settings that the original authors recommended, i.e. we only tune \tau in KFLA and set \tau = 0.01 in noisy-KFAC as these are the settings suggested in their respective publications.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] In addition, we added more details about the data as you suggested.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] 5. We have included a comparison to the, to our knowledge, currently most adversarially resistant model on MNIST.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] We compared our model with unsupervised/supervised NMF for sound source separation, a common unsupervised baseline for this task.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] To make this point clearer, we have also updated Figure 2 in the paper, showing the performance of pGAN for alpha = 0, i.e. when no detectability constraints are considered.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] We tried an experiment using the fixed attention map as a baseline, and as expected it performs significantly worse than ours.	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] - We also followed your suggestion and show the domain weights of MDMN on the Amazon dataset for comparison (see the new Section 5.4).	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] We have added that to the revised version (see p.6 and Appendix A).	1.0
Improper comparison of related work in terms of implementation [SEP] rebuttal_done [SEP] For the third and fourth comment, thanks for pointing out and we have added the constraint-based methods in the related works section, and stressed that we are dealing with “causality in mean” in section 3.1.	1.0
Less datasets used [SEP] rebuttal_concede-criticism [SEP] We agree that further investigation is needed for mutual information, and we are currently working on it.	1.0
Less datasets used [SEP] rebuttal_concede-criticism [SEP] you are right, even if the focus of the paper is not on getting the best possible score on language modelling, different settings would make this point not only more convincing, but clearer.	1.0
Less datasets used [SEP] rebuttal_concede-criticism [SEP] We agree that DC-IGN could potentially perform the same task as our model or more.	1.0
Less datasets used [SEP] rebuttal_concede-criticism [SEP] It is something we should have done on our own.	1.0
Less datasets used [SEP] rebuttal_concede-criticism [SEP] As discussed in the comments above (visible only to authors and area chairs), there is an overhead regarding grouping of data into batches in DC-IGN approach.	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] 3. “How much data is sufficient for a model to learn? What is the minimum/maximum size of the data set?”	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] *All or at least some of these decisions would need to be relaxed to make a convincing paper.	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] 4. Maximum useful data (in the marginal sense $T$ for a limited size model, as above):	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] For more details, please see the comment above entitled: “Relaxing Assumption (H2)”.	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] Similarly to the above:	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] Concern 2: Experiments	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] Q1: Missing assumptions about the black-box calibration approaches	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] See general responses #1 and #3.	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] Thank you for pointing out the other datasets in algebraic word reasoning.	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] Q1: It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] (3) Additional experiments:	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] (2) Multimodal Experiments:	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] We would like to quickly address your question about the experiment here.	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] Results are reported in Table 10 and Table 11 (Appendix C.5).	1.0
"Less datasets used [SEP] rebuttal_structuring [SEP] Remark 1. ""the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets)."""	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] Domain Adaptation Baselines + Other datasets	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] 4.  “The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.”	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] As regards the implications of the paper, we would like to address this in the context of the reviewer’s comment that increased performance is not surprising given the additional supervision provided.	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] 3. Benchmark Dataset and Compared with Deep and Wide (D&W) NNs	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] 1.  “In particular, Section 4 is a series of empirical analyses, based on one dataset pair….However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.”	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] >  How much does the image matter for the single-image data set?	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] 5. “Do we really need a large data set or just a subset that covers the data distribution?”	1.0
Less datasets used [SEP] rebuttal_structuring [SEP] 3.  “It would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.	1.0
Less datasets used [SEP] rebuttal_done [SEP] We took the reviewer’s comment judiciously and have added CIFAR-10 (1k, 2k) experiments in Section 6.5 of the supplementary material and their accuracies are comparable with those of the conventional SSL algorithms. (It took a long time to perform 5 runs of test for all additional experiments.)	1.0
Less datasets used [SEP] rebuttal_done [SEP] As for the synthetic curves experiment, we updated the paper with a justification.	1.0
Less datasets used [SEP] rebuttal_done [SEP] We have added a section B.6 in the appendix and figures illustrating these results.	1.0
Less datasets used [SEP] rebuttal_done [SEP] Furthermore, we included two additional tasks, attesting to the ability of Transformer to model a wide range of distributions over training curves.	1.0
Less datasets used [SEP] rebuttal_done [SEP] We did follow reviewer recommendations and performed experiments with LSTMs and QRNN (slightly faster) along with WikiText (which is larger but not intractable), unfortunately we couldn't accommodate all the analysis and changes in time.	1.0
Less datasets used [SEP] rebuttal_done [SEP] As you suggest, we use MS COCO Image captioning dataset to learn a lookup table and apply it to the EN-RO translation task to do the quick evaluation.	1.0
Less datasets used [SEP] rebuttal_done [SEP] R) Added another experiment, where we use DroNet as base to show the benefit of combine Adaptive layers with ResNet, in this experiment we test different configurations to compress the network up to 32X  (Added to the paper)	1.0
Less datasets used [SEP] rebuttal_done [SEP] We updated additional imputation experiments on multimodal datasets (see in Appendix C.5) : CMU-MOSI/ICT-MMMO (Tsai et al. 2019), FashionMNIST/MNIST (Wu et al. 2018).	1.0
Less datasets used [SEP] rebuttal_done [SEP] These include updating the draft to include (i) a detailed analysis of edits performed on SNLI, (ii) results on various datasets using an ELMo based classifier; (iii) concerning your question about larger Bi-LSTMs, we had tried a large Bi-LSTM but it overfit badly.	1.0
Less datasets used [SEP] rebuttal_done [SEP] A1: We performed some additional experiments using the progressive gan generator [1] on CelebA-HQ dataset.	1.0
Less datasets used [SEP] rebuttal_done [SEP] We have modified our expression, typos and grammar errors.	1.0
Less datasets used [SEP] rebuttal_done [SEP] The results are shown in Figure 4 in the updated version of the paper.	1.0
Less datasets used [SEP] rebuttal_done [SEP] We’ve included these in an expanded discussion of related work with discussion on how they relate to the current dataset.	1.0
Less datasets used [SEP] rebuttal_done [SEP] We would like to thank reviewer 3 for suggesting the related works that we have missed. These were incorporated in the revision.	1.0
Less datasets used [SEP] rebuttal_done [SEP] We now report updated cross-val for all results in section 6 including figures 3,4 and in the newly-added figure 5.	1.0
Less datasets used [SEP] rebuttal_done [SEP] We have updated the draft to include this detail.	1.0
Less datasets used [SEP] rebuttal_done [SEP] Concerns regarding overfitting and uncertainty estimation: Given your suggestion, we performed 10-fold cross validation in all tasks and found high quality results and cross-fold consistency.	1.0
Less datasets used [SEP] rebuttal_done [SEP] Accordingly, based on your suggestions, and suggestions from other reviewers, we have tried to expand the baselines substantially (specifically, we include three state of the art Domain Adaptation methods as baselines – RevGrad [1], ADDA [2] and CyCADA [3]), and our proposed methods outperform them.	1.0
Less datasets used [SEP] rebuttal_done [SEP] Upon request, we have included more extensive experiments following [1] on MNIST/FashionMNIST, and [2] on CMU-MOSI/ICT-MMMO.	1.0
Less datasets used [SEP] rebuttal_done [SEP] We have included experimental results evaluating pGAN on CIFAR-10.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] We also do think that the problem setting we have proposed is an important problem that deserves attention, and has not been studied in the meta-learning paradigm.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] Overall, we think that we have made an important contribution to Meta-Learning literature, by identifying its limitation for few-shot learning under domain shift, and proposed a solution to tackle this problem.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] Instead, the objective is designed in a manner that provides information even for partial solutions, thus allowing the tested algorithms to gradually improve the objective.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] Firstly, we want to clarify that our contribution is beyond improving the Gauss-Newton optimization to Levenberg-Marquardt.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] This contribution is achieved by our differentiable LM optimization that allows end-to-end training.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] Next, we wonder how would the reviewer correct the confounds mentioned with regard to the clique experiment.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] In such a regime, we can attribute any performance gains to the algorithms themselves, and not to any prior knowledge that is reflected by the structure of some complex sampling distribution.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] In this respect, this dataset is an important benchmark for clique algorithms very much like CIFAR10 and CIFAR100 are for image classification methods.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] In fact, we provide a very thorough mix of quantitative and qualitative experiments for both supervised and unsupervised settings.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] Since the objective is the only source of information for such algorithms, an all-or-none kind of objective would not be very useful.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] All the algorithms are evaluated on the DIMACS clique dataset which was published as part of the second DIMACS challenge which specifically focused on combinatorial optimization.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] We would like to point out that there are no accepted measures in the field for the quality of learned disentangled representation (see Locatello et. al. [https://arxiv.org/pdf/1811.12359.pdf](https://arxiv.org/pdf/1811.12359.pdf)) and most previous papers in the field include a similar mix of quantitative and qualitative results in their experiments section.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] 4. Experiments: We would argue that our qualitative plots and quantitative metrics are in line with the evaluation used in current SOTA work.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] Over the years, this dataset has become a standard benchmark for clique finding algorithms, and results on it are regularly published.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] Note that none of the tested methods were given enough samples even to recover the graph itself, as most graphs have more than 100 nodes, and we’ve allowed only for 100 |V| samples in each execution.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] We are glad that you also agree that setting makes sense (“... the combination … is fair”).	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] The main research question we try to address is whether algorithms that only rely on function evaluations can recover locally optimal solutions.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] First, we’d like to emphasize that the clique problem studied in the paper is far from being a toy problem.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] More importantly, our contribution is the combination of conventional multi-view geometry (i.e. joint optimization of depth and camera poses) and end-to-end deep learning (I.e. depth basis generator learning and feature learning).	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] In terms of the sampling distribution, as our focus is on the update step, we decided to use the simplest possible sampling distribution we can think of.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] Providing a controlled experiment is always challenging, though the elements mentioned by the reviewer were specifically selected as to reduce various confounds.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] Notably, Cakewalk approaches the performance of the best clique finding algorithms that directly search a graph, and which are tailored to this specific task.	1.0
More variations of experiments needs to be added [SEP] rebuttal_reject-criticism [SEP] Also, we provide all the code so that it can be verified that the reported results are not cherry-picked.	1.0
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] ; these results are statistically significant.	1.0
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] We have updated the paper and included a new section (4.3) showing how pGAN attacks bypass 4 different defence mechanisms, including outlier detection (as in Paudice et al. 2018a), the PCA-based defence in Rubinstein et al. 2009 (Antidote), Sever (Diakonikolas et al ICML 2019), and label sanitization	1.0
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] Secondly, we agree with the reviewer that comparing with the Gauss-Newton algorithm will be interesting and have updated such a comparison in Appendix B in the revised version according to the reviewer’s suggestions:	1.0
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] (Paudice et al. 2018b)	1.0
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] .	1.0
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] We have revised the toy experiments to include a heteroscedastic regression task (see Section 6.1.).	1.0
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] As we can see in the new Section 5.3, our method achieve state-of-the-art performance and outperform all alternatives	1.0
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] In fact, we have mostly changed the name from “Meta Domain Adaptation” to “Meta Learning with Domain Adaptation”, and the rest of the paper is almost identical, which we believe addresses the concerns of false advertising.	1.0
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] Accordingly, based on your suggestions, and suggestions from other reviewers, we have tried to expand the baselines substantially (specifically, we include three state of the art Domain Adaptation methods as baselines – RevGrad [1], ADDA [2] and CyCADA [3]), and our proposed methods outperform them.	1.0
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] In addition, we have added one more competitive baseline (M3SDA) from the DomainNet paper you mentioned, using their public code with a few necessary adjustments for each dataset (e.g., network architecture, etc).	1.0
More variations of experiments needs to be added [SEP] rebuttal_done [SEP] We have conducted further experiments on the Office-Home dataset,  using the ResNet50 as the backbone architecture and changing the classification head to 65 classes.	1.0
Limited improvement over baselines [SEP] rebuttal_concede-criticism [SEP] 2) For the experiment, we will train our experiments longer and modify our network.	1.0
Limited improvement over baselines [SEP] rebuttal_concede-criticism [SEP] Thank you for these suggestions.	1.0
Limited improvement over baselines [SEP] rebuttal_concede-criticism [SEP] We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] Details could be found in Appendix C.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] Regarding the performance of the standard image captioning system, we train a caption model (Show, Attend, and Tell (Xu et al., 2015b)) with fine-tuned encoder (ResNet101) on the COCO dataset to encode the images.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] We employ mean policy to make it linear. And it could be regarded as an integration on a Gaussian approximation of the Monte Carlo estimate according to [3].	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] The result on EN-RO is 33.58.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] We can see from Fig. 10 (Appendix G) that DSGAN can create complement data for complicate images well.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] One of the main advantages of pGAN is the possibility of generating poisoning attacks at scale with detectability constraints capable of targeting large deep networks, where strategies relying on bilevel optimization have a limited applicability.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] We also feel that some of the results being “significantly worse” is one of the main contributions of our paper.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] Furthermore, we also add additional experiments about generating complement data in CelebA, which is a more complex dataset.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] The main point we would like to make is that the bounds are very concise and exactly reduce to that of gradient descent/stochastic gradient descent in the special cases.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] This is not the case for pGAN, which is capable of bypassing different defences, including the outlier detection scheme proposed by Paudice et al. (2018a).	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] A4: ADDA, an unsupervised DA method, proceeds by training sequentially a classifier on Source, then learning the Target feature space by making it indistinguishable from the Source one.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] Yes. Image captioning dataset is absolutely available for creating the lookup table.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] On the other side, Paudice et al. (2018a) showed that, in many cases, if we don’t consider appropriate detectability constraints, the attack points generated by optimal attack strategies formulated as bilevel optimization problems can be effectively filtered out with appropriate outlier detection, resulting in blunt attacks.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] In semi-supervised learning, we follow our competitors to conduct experiments on MNIST, SVHN and CIFAR10.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] The gradients w.r.t. \phi from the KL divergence is stopped for variance reduction with acceptable bias, which we prove with MuProp [1].	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] As a result, the BLEU score is (33.55), which is comparable to the current lookup table (33.78) based on Multi30K, and outperforms the Trans. (base) (32.66).	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] In our case, for the experiment with MNIST in Figure 2, we used a deep neural network with more than 40,000,000 parameters, 1,000 training points, injecting up to 400 poisoning points.	1.0
"Limited improvement over baselines [SEP] rebuttal_summary [SEP] However, the computation complexity for ""close to each other"" would be O(N^2), with N being the number of dropout policies in this batch."	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] (Q4) Munoz-Gonzalez et al. (2017) showed an experiment using a Convolutional neural network with 450,000 parameters, trained with 1,000 training points and injecting 10 poisoning points.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)].	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] We also want to clarify the datasets used in our experiments.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] As intuitively \phi is controlling the distance between dropout policies, it would further remedy the little bias mentioned above.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] In the stochastic gradient setting, the number of gradient evaluation is indeed $T^2$. This is consistent with the result in Bernstein et al. (2018).	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] Thus, the classifier would actually learn two sub-classifiers: one for each domain, which would turn counter-productive in the second step where this strong distinction between source and target would have to be un-learned.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] However this is not applicable to the semi-supervised setting: either target labels would not be used in the first training step, or they would be used but without any domain loss to account for the fact that two domains are being used at the same time.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] In novelty detection, our method is evaluated on CIFAR10, which is also common in this application.	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] Although defences based on outlier detection can be bypassed, as shown by Koh et al. (2017) (Stronger poisoning attacks break data sanitization defences), the complexity of the bilevel problem significantly increases compared to Munoz-Gonzalez et al. (2017).	1.0
Limited improvement over baselines [SEP] rebuttal_summary [SEP] Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] As to the online setting, thanks for pointing us to the “short-horizon bias” paper.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] > The random baseline (i.e a random ordering of candidate task) is compared in figure 3 (and all the plots in the appendix), where we plot the accuracy boost using the best task till now in the produced recommendation of candidate tasks using different methods.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] A6: For the experiments, we directly used the officially released code of Reg-GAN for fair comparison and it uses the ResNet instead of DCGAN architectures.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] The paper does achieve this goal, on a number of networks.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] In contrast, we deploy this principle in a practical model structure that is easily applicable to many existing deep and variational learning approaches and provide empirical evidence of the validity of our framework.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] We also observe significant improvements in both human evaluations, suggesting that the improvement comes from our method and not from evaluation bias.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] Nevertheless, please note that the human baselines we use for Transformer have been tuned by researchers using auto-tuners among other tools.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] When $p_g$ is the same as $p_r$, the gradient of the optimal discriminator in GAN and the optimal critic in WGAN must be 0.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] Our 0-GP can be applied to WGAN as well.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] >> Comments #1, #11	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] The main goal of our submission is to experimentally show that our pipelined training, using stale weights without weight stashing [1] or micro-batching [2], is simpler and does converge.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] Similar to the original GAN, WGAN and WGAN-GP can overfit to the dataset: the distance output by the critic can be larger than the Wasserstein distance between the two distributions.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] Any non-zero centered GP will not help GANs to converge to the optimal equilibrium.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] We mainly account the success of this simple training strategy to the simplicity of the model, the relatively low dimensionality of our input features, and the simplified action space (though all  three suffice to obtain a good controller in the current settings).	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] As AnonReviewer 3 mentioned, our main contribution is developing a new inference method which can be used under any pre-trained deep model.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] We have indicated in the revision the existence of this bias -- this bias was observed on the GAN task -- overtraining G can increase IS in a short term, but may lead to divergence in a long term as G becomes too strong.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] They explore the link of limiting mutual information and generalization error mostly in theory (and in particular for adaptive analysis).	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] We can clearly see that the random ordering is much worse compared to informed metrics that use representations.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] Upon your suggestion, we would also add this random baseline in table 2 as well.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] However, overfitting in WGAN and WGAN-GP is not as severe as in GAN.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] They make the training of the controller much easier compared to other RL tasks with higher dimensional features or larger output space.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] While AutoLoss is amenable to different policy optimization algorithms, we empirically find PPO performs better on NMT, but REINFORCE performs better on GANs.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] This is partly because the gradient in WGAN and WGAN-GP does not explode so mode collapse is much harder to observe.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] We notice that it is submitted to arXive after the submission deadline of ICLR, thus we were unaware of it at the time of submission.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] 2. Due to time constraints, we have not benchmarked our method against more hyperparameter-tuning baselines yet.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] Our 0-GP helps to improve both generalization and convergence of GANs.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] We hypothesize the tradeoff is insignificant on NMT, as in our multi-task setting, slightly over-optimizing one task objective usually does not have irreversible negative impact on the MT model (as long as the other objectives are optimized appropriately later on).	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] On the other hand, we didn’t observe it harms on NMT task noticeably.	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] Re: (W3) Baselines for transfer learning:	1.0
Limited improvement over baselines [SEP] rebuttal_answer [SEP] 3. We will include WGAN-GP to the baselines for the sake of completeness.	1.0
Limited improvement over baselines [SEP] rebuttal_structuring [SEP] > ii) “In table 2, I don’t really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse.”	1.0
Limited improvement over baselines [SEP] rebuttal_structuring [SEP] 3) Mean policy in the KL divergence	1.0
Limited improvement over baselines [SEP] rebuttal_structuring [SEP] 6. On CIFAR10 the results seem to be worse that other methods.	1.0
Limited improvement over baselines [SEP] rebuttal_structuring [SEP] >>> I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.	1.0
Limited improvement over baselines [SEP] rebuttal_structuring [SEP] Specifically:	1.0
"Limited improvement over baselines [SEP] rebuttal_structuring [SEP] Q4: Comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing."""	1.0
Limited improvement over baselines [SEP] rebuttal_structuring [SEP] “Domain Adaptation Baselines”,	1.0
Limited improvement over baselines [SEP] rebuttal_structuring [SEP] - W.r.t. the naive method of using (fixed) coefficients.	1.0
"Limited improvement over baselines [SEP] rebuttal_structuring [SEP] Q2: “Pioneering work is not necessarily equivalent to ""using all the GPUs""”"	1.0
Limited improvement over baselines [SEP] rebuttal_structuring [SEP] > The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper [...]] and further exploration is desirable.	1.0
Limited improvement over baselines [SEP] rebuttal_structuring [SEP] I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.	1.0
Limited improvement over baselines [SEP] rebuttal_by-cr [SEP] These results will be included in the new manuscript.	1.0
Limited improvement over baselines [SEP] rebuttal_by-cr [SEP] 3)For the experiment: we will spend some time to train GANs with more iteration and modify it.	1.0
Limited improvement over baselines [SEP] rebuttal_by-cr [SEP] Please do note that we ran 2 sets of human evaluations (Adequacy and Fluency), as is standard in Machine translation in order to deal with the evaluation bias problem you describe - we took this into account when conducting experiments and will make it more clear in a revised version.	1.0
Limited improvement over baselines [SEP] rebuttal_by-cr [SEP] If the paper gets accepted, we will expand the experiments with fixed frequencies in the final version.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] We have included that in Figure 7 of the revised version (Appendix E).	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] Thanks also for catching several typographic errors. We have addressed them in the new draft.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] A2: This claim is indeed not accurate we have delete this claim in the revision.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] We updated section 2.2 to relate to the references you mentioned.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] As you suggest, we use MS COCO Image captioning dataset to learn a lookup table and apply it to the EN-RO translation task to do the quick evaluation.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] We have modified Remark 3.4 (and added Remark 3.5) to make this clear in the updated version.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] We compared now against: mobileNet, ShuffleNet, HENet, SqueezeNet, we have less number of parameters or better accuracy or both (Added to the paper)	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] Taking your comments and comments from other reviewers into account, we have made improvements to the experimental section.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] We have modified the Remarks to clarify the statements.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] Thank your for your suggestion.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] Since there is no public code for CodeSLAM, we cite its results directly from the CodeSLAM paper.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] We have added the detailed PPO-based training algorithm in Appendix A.1.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] We are re-programming DSN and experimental results will be added.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] We have now added additional domain adaptation baselines, designed in the setting suggested by Reviewer 2.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] R)We added a new experiment where we show how the performance of adaptive kernels improves with the increment of parameters.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] In order to make a fairer comparison we also added another experiment where we compare the accuracy of ResNet18 with 1 adaptive layer against ResNet18, ResNet50 and ResNet101 and it can be seen that the adaptive one performs even better than ResNet50.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] In the abstract of the revised draft, we report our improvement over Co-teaching [5] which is the most recent and state-of-the-art training method.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] We have added a new section 6.3  to the supplement that includes visualizations of the attention mechanism both over the course of training and within episodes.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT.	1.0
Limited improvement over baselines [SEP] rebuttal_done [SEP] Q3: Rather than [2], we employ MuProp to reduce variance in our development of NADPEx.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] (2) Translation in graphs is a new topic and we have not found many datasets in very large scale, so we do not test on much larger nodes. But the scalability experiments can still show the superiority of our model compared to others.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] (c) From the result in (b) we conclude that the particular manner of the pre-training does not matter.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] (3) We typically test small-size graphs because most of the comparison methods can only handle small-size graphs.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] Furthermore, use of this subset for performance evaluation is justified as as C_p^{test} corresponds to latest released structures in C_p, leading to a more accurate assessment of how such methods would perform on unreleased structures (as they do no sequence identity pruning).	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] The prior state-of-the-art approach we compare against, SmartHash, outperforms these approaches by 1.75x and 4x respectively, at the number of frames they report (200M and 50M respectively).	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] Reviewer 1 further asks for evaluation on more games.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] Indeed, the paper mentioned by the reviewer shows that the performance of various self-supervised methods for ResNets does not degrade with the depth as it does for VGG and AlexNets due to the skip-connections.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] Our convergence analysis is done for non-convex objective functions (similar to that of Yan et al. and Bernstein et al.).	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] pix2pixHD -- the Pose2Frame network can be directly compared with the pix2pixHD network, since they both act as mapping functions between dense-pose representations to realistic images.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] The testing data C_p^{test} is that which has been used in the prior works we compare to (Fout et al. 2017; Sanchez-Garcia et al. 2018).	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] Our approach further outperforms SmartHash by over 2x.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] Once the experiments are complete, we should be able to see how SGDM compares with SGD in the experiments.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] In particular, we follow Aytar et al., 2018, and evaluate on 3 of the hardest exploration games from the Arcade Learning Environment.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] We had experimented with fewer layers.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] Therefore also the choice of first training set (98% coverage or full coverage) does not influence adversarial resistance.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] In the non-convex setting, to the best of our knowledge, there are no theoretical results that show benefits of momentum methods over SGD.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] While it is true that the hyperparameter validation set was initially fixed, the switch to use C_r^{val} as above resolves this.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] However, as ResNets have not been originally used to train the methods analyzed in our paper, we have stayed in the bounds that are required for fair comparisons and only used AlexNet.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] The rich semantics of the face embedding feature can be validated by its strong transferability on other visual tasks, e.g. gender/race classification and age regression.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] Combining the Pose2Pose and pix2pixHD networks, would yield significant artifacts (as seen in Fig. 14), and is not suitable for this kind of application.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] We realized that in this case the width of the network should be increased to compensate for the representation power of the network.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] We use the same set of minimally tuned hyperparameters (tuned only on Montezuma’s Revenge) and obtain new state-of-the-art results by over 2x, suggesting that our approach can generalize to new tasks.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] As we already had an extensive set of experiments, we decided not to report that.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] We claim that the embedding features have rich semantics of all kinds of facial attributes, e.g. age, gender, race and so on.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] This training gives similar results to the training in stages.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] For experiments, we speculate that the reason is that the batch size used is too small for (Powered)SGDM to gain an advantage over (Powered)SGD.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] All experiments show that CWAE learning process is stable and repetitive: the standard deviations, for most of the coefficients computed during training are smaller than those of WAE or SWAE models (in particular CWAE minimizes WAE distance faster then WAE-MMD).	1.0
"Experimental study not strong enough [SEP] rebuttal_summary [SEP] Prior studies [Savchenko, Andrey V, ""Efficient facial representations for age, gender and identity recognition in organizing photo albums using multi-output ConvNet"" (2019)] have shown that transfer learning using neural networks pretrained on face recognition can produce highly effective results for gender recognition and age estimation."	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] As can be seen, the use of our different components described in the P2F ablation study (blending mask and regularization, object channel, two pose inputs, discriminator attention on character, etc.), results in much fewer artifacts, making the Pose2Frame network suitable for this application.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] In particular, FuN and Roderick et al., 2017 both report results on Montezuma’s Revenge.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] We do not evaluate on many of the simpler other games (e.g., Breakout), because they do not require sophisticated exploration and can already be solved with current state-of-the-art methods.	1.0
Experimental study not strong enough [SEP] rebuttal_summary [SEP] In this work, the authors show that alternating between a l1-based sparse approximation step and dictionary update based on block co-ordinate descent converges to a stationary point.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] (a) We have evaluated the adversarial resistance when training a Boltzmann machine with 256 fully connected latent variables directly on the 8x8 patches.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] We often get questions about why our estimators give MI numbers lower than MINE and why are we claiming that our estimator is better.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] We find that the model without stacking is not able to increase the adversarial resistance.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] But the changes produced in the system may also depend on the characteristics of the dataset and the learning algorithms used.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] With suitable settings, the shift consistency of F-pooling is much better.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] This is because the default speech pipeline already uses different learning rates for different portions of the network, so there is no clear choice a priori for the learning rate of the “preprocessing layer” (note that most methods, including K-matrices, do not seem to be overly sensitive to the choice of this learning rate).	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Then, we can expect an increase of the false positive rate, which is shown in Figure 6 (centre).	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] We chose quantization schemes using binary codes in the experimental results because 1) binary codes are being widely studied and 2) we can focus on the practical issues on binary codes.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] The good performance on non-rigid shape is mainly contributed by the use of bijective spherical parameterization method, which obtains the input spherical image without topological information losses.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] The attack points are therefore not necessarily close to the decision boundary, and thus, the changes produced in the algorithm are more unpredictable and affect the errors for the two classes.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] pGAN produces poisoning attack points that are close to the decision boundary, “pushing the decision boundary away” from the source class (i.e. the same class as the labels of the poisoning points) towards the samples of the target class.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Since all of quantization techniques in Table 1 and Table 2 follow the form of binary codes with the same q bits, comparisons have been made under the same computational savings (thus, model accuracy is emphasized).	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] For	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] 1. Training humanoid controllers is of orders of magnitude more difficult than training cheetah (Schulman, 2017).	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] The stopping criterion is that the accuracy of the current iteration reaches the average accuracy of the previous 20 steps.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Regarding ease of training and hyperparameter tuning, we would like to re-emphasize that for all experiments, all hyperparameters for training were kept the same as those for training the default model architecture, other than those we explicitly mentioned as being tuned.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] (3)-(4) To some extent pGAN can control the specific errors produced in the system, as shown both in Figures 5 and 6.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Therefore, “gold standard” method acts as the “best-possible-performer”, and is used as a benchmark to evaluate all the different generative models on how “real” the graphs they can generate: the closer (and better) their performance is to the “gold standard” one, the “more real” their generated graphs are.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] For a small machine (16 units) of full hidden connectivity we can observe the noise rejection behaviour, as shown in appendix D.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] It is possible that we are unable to complete the training due to the approximations involved.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] The other comparable techniques shown in Table 1, are not ``online’’ and/or require stringent initializations, in terms of closeness to the true dictionary, as compared to NOODL.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] R)they train a NN to generate a model of another NN, we  have a ACNN that learns how to generate its filters.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] MINE-f bar is not visible due to overshooting out of the chart.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] .	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Our work also paves way for the development and analysis of related alternating optimization-based techniques.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Besides, traditional APG method is not friendly for deep learning as extra forward-backward computation is required, also as shown by SSS.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] In contrast, the label flipping attack is less subtle as it does not consider detectability constraints.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] guarantee that when epsilon is smaller than a threshold, no violations can be	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] In robotic tasks, the states of the robot and the object states are normally available [Andrychowicz et al 2018, Plappert et al 2018].	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] (d) There are a total of 28800 parameters in the Boltzmann machine.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] - We set parameters as follows.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] NGE can be applied to evolve humanoids, however, there are two major difficulties in doing that in practice.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Specifically, as you know, all the comparison methods in our paper are generative models which generate graphs, and our experiment is to evaluate how real the generated graphs are.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] A3: Since binary codes and lookup table would be associated with vastly different inference architecture, computation methods, and storage design, it is difficult to analyze detailed comparisons on FleXOR and lookup-table methods.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] In addition to our main theoretical result, which establishes conditions for exact recovery of both factors at a geometric rate, NOODL also has superior empirical performance, leading to a neurally-plausible practical online DL algorithm with strong guarantees; see Section 3 and 4.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] FleXOR, however, provides not only higher model accuracy but also additional storage savings due to the proposed encryption algorithm/architecture using XOR logic.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] We also show several example images of f_i(x) (or f_j(x)) sampled from the ImageNet validation set.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] In the second experiment, we leave the epsilon fixed and simply train the model until the stopping criterion is satisfied.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] If the stopping iteration is much less than 100 times, the epsilon growth rate should be reduced so that the data is added more slowly.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] (In previous versions, the training iterations of fixed mode had been fixed.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] However this could be counteracted by stopping the gradient at the right place in the simplified architecture  which was suggested in the original paper and is now described in the updated paper.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] , the minimal adversarial distortion is lower-bounded by that fixed epsilon.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] These successful algorithms (such as Mairal '09) leverage the progress made on both factors for convergence, however, do not guarantee recovery of the factors.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] (In previous versions, the growth ratio of epsilon for CIFAR-10 was applied to SVHN and CIFAR-100.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] On the other hand, the state-of-the-art provable DL algorithms focus on the progress made on only one of factors (the dictionary), and do not have good performance in practice, since they incur a non-negligible bias; see Section 5 and Appendix E.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] NOODL bridges the gap between these two.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Q1:  It would be nice to see a better case made for spherical convolutions within the experimental section.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] DEMINE approaches give estimations closer to 0.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] perturbation by providing a counter-example (violation).	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] methods. There are some scalable property verification methods that can give a	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] The lossy input affect the performance of rigid shape analysis to some extent.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] We, instead, focus on how the task-specific information is present in popular sentence representations, and how this could be used to assess transfer potential among tasks.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Re: (W5) Multi-task learning	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] When both answers to the two binary questions are false (corresponding to Case III), we cease to source the ground-truth label of x for reasons mentioned by the reviewer, and treat x as a strong counterexample for both f_i and f_j.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] In the case of CIFAR-100, the epsilon is increased by 10 times in log-scale every 27 iterations.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] The three references and the follow-up work that you cite give different methods for obtaining a certificate-of-guarantee that a datapoint is robust in a fixed epsilon l_\infty ball, with varying levels of scalability/generality/ease-of-implementation.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] We added discussions on the same computational savings and additional storage savings of FleXOR in Section 4 and 5.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Thus, in these experiments, K-matrices can be used as a drop-in replacement for linear layers without significant tuning effort.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] compare the sampling based method with these lower and upper bounds.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] These methods can	1.0
"Experimental study not strong enough [SEP] rebuttal_answer [SEP] 4. ""The experiments of this paper lack comparisons to certified verification"	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] However, since the epsilon growth rate is different for each dataset, as the reviewer mentioned, we have performed the cross-validation for SVHN and CIFAR-100 and modified our results.) As a result, the epsilon is gradually increased in log-scale by 10 times every 33 iterations in CIFAR-10 and SVHN.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] When using spherical projection method to represent 3D shape, there will be information loss if the object is non-convex.	1.0
"Experimental study not strong enough [SEP] rebuttal_answer [SEP] example, what is log(I) for epsilon larger than upper bound?"""	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] While the validation accuracy is evaluated using the cross-validation, we set the number of training iteration to be 100 so that the model is trained enough until it saturates.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Further, we experimented with the same threshold in table 4 of the new version and the results have shown that out of class unlabeled data are added even with an extremely small threshold such as 0.99999 (epsilon = 10^-5).	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] lower bound on the input perturbation (see [1][2][3])	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Our synthetic experiments on Gaussians rho=0.0 in Figure 1 do exactly this.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] The authors should	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] The version with only 128 hidden units was not able to reduce the relative entropy to the values of the larger, stacked machine.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] This significant memory gain thus indicates that our approach may still have advantages when used as a method for approximately doing more classical verification, even though this was not our aim.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] > Our goal is somewhat orthogonal to the multitask learning setting where all the tasks are jointly trained.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Regarding the details on hyper-parameters:	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] If we need exact zero, we have to use heuristic thresholding on the \lambda learned, which has already been demonstrated in SSS [1] that is inferior.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Despite these being two different definitions of robustness, to try and demonstrate some comparisons between the two, we extended experiment 6.4 (already using Wong and Kolter (ICML 2018) [3]) and compared the fraction of samples for which I = P_min to the fraction that could be certified by Wong and Kolter for epsilon in {0.1, 0.2, 0.3}. We found that it wasn’t possible to calculate the certificate of Wong and Kolter for epsilon = 0.2/0.3 for all epochs, or epsilon = 0.1 before a certain epoch, due to its exorbitant memory usage.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] This should avoid placing too much emphasis on the cleaner images in the beginning of the MNIST test set.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Moreover, if more than three of our five human annotators find difficulty in labeling x, it is discarded and replaced.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] The number of training iteration and thresholding epsilon are very important parameters in our algorithm and have a considerable correlation with each other.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] For those datapoints where they can produce such a certificate	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Our method achieves a state-of-the-art classification and retrieval performance on Shrec’11.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Moreover, we investigate the mixing distribution learned in Appendix G.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] A: Yes, “gold standard” method is directly trained based on real target graphs instead of generated ones.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] : We believe the best case is the non-rigid shape classification and retrieval.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] It shows that CDNs are able to quantify the heteroscedastic aleatoric uncertainty.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Given an image x, which is associated with two classifiers f_i and f_j , we pick two binary questions for human annotators: “Does x contain an f_i(x)?” and “Does x contain an f_j (x)?”.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Please see the updated paper for full details.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] For each question, we follow  the original ImageNet instructions and include the definition of f_i(x) (or f_j(x))  with a link to a corresponding Wikipedia page.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] At some point, when the fraction of poisoning points increases significantly the decision boundary starts to change in a different (and possibly more abrupt way), so that the false negatives also start to increase.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] In the first experiment, the iteration number remains fixed and the growth rate of epsilon is adjusted so that the validation accuracy saturates near the settled iteration number.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] In particular, we did not modify any hyperparameters (such as number of epochs, optimizer, or learning rate) for the ShuffleNet and DynamicConv experiments.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] But in fact that's exactly because MINE gives false detection but our estimators provably don't.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] If the stopping iteration significantly exceeds 100 iterations, the epsilon growth rate should be increased so that the data is added more easily.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Epsilon is increased in log-scale and begins at a very small value (10^(−5)) where no data is added.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] The growth rate of epsilon is determined according to when the validation accuracy saturates.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] In Figure 6 (right) this happens when the fraction of poisoning points is larger than 25%.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] For the TIMIT speech experiment, we tune only the “preprocessing layer” learning rate.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Other details are the same as those of the first experiment.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] We allow 5 iterations as a deviation from 100 iterations and the growth rate of epsilon is left unchanged in this interval.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] However, we kept the OOD experiments on MNIST and notMNIST in the paper as well, since we consider it as very interesting that the CDN, while being designed for modelling aleatoric uncertainty, is very competitive on this task.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] 2. To evolve realistic humanoid structure (e.g. hands, symmetrical limbs), one would need to have more realistic environments that better reflect tasks and complexity in the real world.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Our experiments show that due to the geometric convergence to the true factors, NOODL outperforms competing state-of-the-art provable online DL techniques both in terms of overall computational time, and convergence performance.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] In “gold standard” method, it directly uses the real graphs to train the classifier (still based on KCNN), so it is expected to get the best performance.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] On the other hand, adversarial attacks give an upper bound of input	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] found.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] One way to evaluate this is by “indirect evaluation”, where we use the graphs generated by different comparison methods as training data to train a classifier based on KCNN (see reference (Nikolentzos, et al.,2017) in the paper), and then compare which model generates “more-real graphs” by testing their corresponding trained classifier on test set which consists of real graphs.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Results show that MINE-f and MINE-f-ES estimates very much non-zero MI when there should have been 0 MI.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] A4: The non-smooth regularization introduced by l1 regularization makes traditional stochastic SGD failed to yield sparse results.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] The use of a fixed embedding space $L$ and a separate space $L^\prime$ was useful as it naturally prevents the collapse of embeddings.	1.0
Experimental study not strong enough [SEP] rebuttal_answer [SEP] (in the intro)	1.0
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] We already comprehensively compare with the prior non-demonstration state-of-the-art, which use a comparable amount of prior knowledge, in each game.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] We do not add a qualitative ablation study for the P2P network, since still-images (as opposed to videos) do not convey the temporal improvement in this case.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] Renting the appropriate equipment (e.g., via Google Cloud) to run a single seed to completion costs ~$1,500.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] As mentioned in the comment, this approach may eventually lead to finding optimal or near-optimal algorithms for a problem (not an instance of a problem) for which no algorithm is known -- but this is outside the scope of this work future work.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] Therefore, we do not consider them as our baseline models.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] However, we only present the most important results in this paper due to the space limit.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] Since we already compare with the prior state-of-the-art approaches, and other approaches perform significantly worse than the prior state-of-the-art approaches, we do not compare with the many other deep RL approaches.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] As the proposed architecture already performs well to solve the ordinal embedding problem, we found it unnecessary to try deeper networks.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] Although GAN-based models show promising imputation results, they usually fail to model data distribution properly.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] 3. Due to the complexity of the network compared to e.g. LeNet and the higher adversarial resistance the optimisation procedure to find adversarial images takes a long time, making it hard to evaluate 10000 images for all training stages and different attacks.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] To run 20 seeds (10 for our approach, 10 for the prior state-of-the-art) would cost 20 x $1,500 = $30,000 or roughly the median US annual salary.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-request [SEP] We note that running 10 seeds would approximately cost $30,000 per additional game in compute.	1.0
"Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Regarding ""false detection"" experiments."	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Section 3 too much redundancy	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] For example, it would be great if the author could demonstrate that without this loss term the distribution of the predicted classes is wrong in the experiments from Section 4.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] A: We actually ran this experiment, where we monitored the KL divergence between the marginal distribution of model predictions and the true marginal distribution of labeled data over the course of training (with and without distribution matching).	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Q2: Can it be applied to more complex morphologies?	1.0
"Experimental study not strong enough [SEP] rebuttal_structuring [SEP] R1: ""missing in the paper is the comparison to two other class of RL methods: count-based exploration... In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details."""	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.”	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Response:	1.0
"Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Q1 ""all the experiments except the last row of Table 2 concern adaptation between two domains. Given the paper title, the reviewer would have expected more experiments in a multiple domain context."""	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] * Non-matched experiment to practice environment:	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] In contrast to PlaNet, the difference is that this work learns an actor-critic model in place of online planning with the cross entropy method.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] maybe?	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] - Q: Illustrative experiments:	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] -- “This work only considers problems for which the optimal input distribution is known, but is motivated by the fact that it could be applied to problems for which the optimal distribution is unknown and thus being able to discover new algorithms. It is hard to support this motivation when no experiments are done in its favor.”	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] 4. Comment:	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Q4: “The sparse regularization of \lambda induces great difficulties in optimization”	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] > Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] -- Comment on scale / speed for large instances of combinatorial optimization:	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] (4) Baselines:	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] (1) Reconstruction from prior during training:	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] 3.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Q3: The authors invite five volunteer graduate students to annotate the selected example.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Q3: The evaluation section lacks experiments that evaluate the computational savings.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Thus the experiment comparison is not really fair.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] 2. Discover new/better worst-case algorithms for problems with the aid of ML, when neither a good algorithms or input distribution is known.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] > 1.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Missing experiments to validate nature of bounds.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Humanoid etc.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] However, for many categories, it’s nor easy for normal people to distinguish.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] 1. One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Response to other minor points:	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Reviewer 1 claims that we do not sufficiently compare with enough other methods, and specifically asks for comparisons with Feudal Networks (FuN) and Roderick et al., 2017.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] - More experiments on the number of layers	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] The long-term agenda / research program is indeed two-fold:	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Interestingly, increased parallelization also significantly helped the exploration strategies as shown in Figure 3(a).	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] 1. Face identity as a proxy for face image diversity	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] This is the case in which the optimal distribution of inputs is also known.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] So the experiments in this paper is also not convincing.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] 1. Investigate whether known optimal worst-case algorithms can be reproduced without any domain knowledge (i.e., “Can ML learn Algorithms”).	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Q: Four, could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behavior, and label accordingly? That said I am not 100% sure of this problem setting.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Responding to R1's additional feedback:	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] 4. Response:	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] It might be beneficial to include comparison to this approach in the experimental section.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] However, it seems the experiments do not seem to support this.	1.0
Experimental study not strong enough [SEP] rebuttal_structuring [SEP] Please refer to our general response.	1.0
Experimental study not strong enough [SEP] rebuttal_by-cr [SEP] We will attempt to make the writing more concise.	1.0
Experimental study not strong enough [SEP] rebuttal_by-cr [SEP] This may take a while for the ImageNet experiments, but we promise to do so in the final version.	1.0
Experimental study not strong enough [SEP] rebuttal_by-cr [SEP] We plan to add SGD as a reference algorithm (as suggested by another reviewer).	1.0
Experimental study not strong enough [SEP] rebuttal_by-cr [SEP] We agree with the reviewer that it would be good to check if ResNets, in general, can also be trained in such a manner (e.g. could global pooling destroy the signal?), so we are running an experiment on a ResNet-18 and will report results in the upcoming days.	1.0
Experimental study not strong enough [SEP] rebuttal_by-cr [SEP] 3)For the experiment: we will spend some time to train GANs with more iteration and modify it.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] The purpose of our work is not to achieve state-of-the-art performance simply by incorporating the latest network architectures and optimisers.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] First, they learn a Q function rather than just a V function.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Our results are not cherry-picked as R1 suggests: following many recent deep RL works, e.g., Ostrovski et al., 2017, Tang et al., 2017, we run 4 seeds on each task, and obtain statistically significant results.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] In this sense, our goal (and the validation experiments on Cell) are focused on MDL.	1.0
"Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] This is realized by utilizing the ""pseudo"" sampling described before (and in the paper)."	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] At a high level, previous approaches can be grouped into those using Reinforce gradients with V baselines (A3C, PPO, ACER) and those using deterministic or reparameterization gradients of learned Q functions (DDPG, SAC, MVE, STEVE).	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Thus our experimental set up	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] The problems we study (ski-rental and Adwords) fall into the first category of problems.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Instead, we provide a novel general framework for automating generalisation, and show that when used with standard classification networks across all baselines, our method performs the best.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Second, the actor only maximizes the Q value predicted for the current time step rather than maximizing multi-step value estimates.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] It is also important to note that VSAE is not a model designed only for imputation, but a generic framework to learn from partially-observed data for both imputation and generation.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] While MVE and STEVE learn dynamics models (from proprioceptive inputs), the dynamics are not directly used to update the policy.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Compared to them, our model handles relatively “larger graph” (6-10 times larger than most existing methods).	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] In particular, we were not able to find any official public implementation of the pseudo-count methods.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] We evaluated the model under two training settings: (I) optimize the final ELBO without conditional log-likelihood for unobserved modalities x_u; and (II) optimize the final ELBO with  conditional log-likelihood of unobserved modalities.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] We have also added to our discussion, making clear that our method represents a significant advantage over competing methods when detailed atomic information is available.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] All baselines considered in the paper are designed to have comparable number of parameters (same or larger than our model) to make the comparison fair.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] In the domain of graph generation, currently, the proposed graph generative models can typically only deal with graphs with dozens of nodes or less (except GraphRNN which can scale to 300).	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] We experimented with a third party implementation trying to see if it could play Breakout without extrinsic rewards, but did not achieve sufficient success and found it to be too slow for scaling it up to a large-scale study.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] is rigorous and justified.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] The competing models do make use of validation set C_p^{val} from the complex data to select amongst the most important hyperparameters of their model -- which is equivalent to what we did in our initial formulation, and favors the competing methods compared to if we use C_r^{val} for hyperparameter search instead.	1.0
"Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] ""Pseudo"" sampling for unobserved modalities during training provides a way to facilitate model training process."	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] In comparison, Dreamer uses reparameterization gradients of V functions by backpropagating the value estimates through the latent dynamics.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Even our *worst seed* outperforms or is competitive with the prior state-of-the-art *best seed*.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] A1 A main difference between domain adaptation and MDL is the fact that the former aims to minimize the target error, while the latter aims to minimize the average error.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Dreamer is a novel algorithm that belongs to the family of actor critic methods.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] By using the complete data, the setting II describes the complete ELBO corresponding to the partially observed multimodal data (in consideration).	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Instead, they only serve for computing multi-step Q targets for learning the Q critic.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] This allows us to be able to run more and larger experiments on many environments.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] => We chose to focus on dynamics-based approaches in this paper because we found them more straightforward to efficiently parallelize than the published pseudo-count methods.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Response: As we discuss above, we believe our experimental setup and analysis is sufficient to demonstrate that our atomic representation transfers much better across atomic tasks.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] We actively argue that the minimal adversarial distortion is not a reliable measure of neural network robustness in many scenarios, as it is dictated by the position of a single violation, and conveys nothing about the amount of violations present.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] The results are comparable but the added term in setting II shows benefits on some datasets.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] A: (1) We did not mention that we handle “large graph”, but instead we only mention that we handle “larger” graph.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Thus, no gradients are backpropagated through the dynamics model for learning the actor or critic.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Based on the above, we cannot concur with the judgement “the experiments in this paper is (are) also not convincing”.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Note that the algorithms in the two cases are very different in structure.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Indeed we do not provide a perfect explanation in this submission, however the factors we have considered and the well-designed numerical investigations could be helpful for future studies on this topic.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Actor-critic algorithms that use analytic gradients of Q critics differ from Dreamer in two ways.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] While setting I is solely based on the observed modalities, the setting II incorporates the unobserved modalities along with the observed ones.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] #2 is a long-term goal, and not tackled in this paper, but we believe #1 (tackled in this paper) is itself of strong interest (and difficult) -- would ML be able to discover the same “pen-and-paper” algorithms that computer scientists invented?	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Further, please note that even though the optimal distribution of input is known in these two problems, we do not use it at all in training.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Specifically, while Reinforce estimators typically learn V functions, these are only used to reduce the variance of the gradient estimate rather than directly maximizing them with respect to the actor.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] We follow this approach even in case #1 when the optimal input distribution is known exactly because we have the ultimate goal #2 in mind, that is, we want to design a framework that can eventually also work without knowledge of optimal input distribution (but that goal is outside the scope of this paper).	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Competitors rely on amino acid-level features that fail to capture specific atomic positions but can be better when the structural is less detailed or accurate.	1.0
Experimental study not strong enough [SEP] rebuttal_reject-criticism [SEP] Indeed, this is the main point of this paper -- the previous work of Kong et al. used these distributions to train the algorithm network (and hence that technique still needed the prior theoretical “pen-and-paper” work), while this work starts with ZERO knowledge.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] [Experiments Section] We now compare our method against Beta-VAE, DIP-VAE, and InfoGAN, both qualitatively and quantitatively.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] Ad. 2) (stable training) We have run repeated experiments with different initializations for all the generative models, as the reviewer has suggested.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] We added an illustrative example in the introduction to give an intuitive understanding of invariance, stability and their relationship.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] On the other hand, for all the arguments in Section 2.2, we have added the citation to support them.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] We managed to counteract the noisiness of predicted embedding by training on noisy embeddings which trains the network to be robust to random changes and improves the prediction of multi-step rewrites significantly.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] We have updated the baseline details in the Appendix B.3.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] We add more analysis with the state of the art in Section 4.2, especially about the case that other methods outperforms our method.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] - We add the experimental details in the Appendix.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] Please refer to the revised version for numerical evaluations in Section 6.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] (b) We have trained a machine with the same connectivity as the stacked machine directly on the 8x8 patches.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] For example, we select different model architectures (layers and building blocks), weight updating schemes of different parts (when and how to update Encoder, Decoder and Classifier) and settings of some important hyper-parameters (the setup of “n” epochs and “k” steps, learning rate) to select the empirically optimized one.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] 1. We have included some more attacks on the most robust model (a transfer attack and a Gaussian random noise attack).	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] We have added appropriate graphs to the paper.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] On the one hand, we have already conducted exhaustive micro-benchmark experiments to determine the current design of RAN.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] A quantitative comparison can be found in Table 1, as well as a qualitative comparison in Fig. 14.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] 3. We have added the values for chosen $\gamma$ in the updated version (see caption of Figure 1).	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] Fig. 3 and other evaluations have been updated for the new test set.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] As can be seen, the results justify each component used.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] We have now evaluated the adversarial resistance throughout the article for 1000 images randomly selected from the 10000 MNIST test images.	1.0
"Experimental study not strong enough [SEP] rebuttal_done [SEP] With respect to initialization of hyperparameters, as explicitly mentioned in the ""experimental setup"" section, ""Most of the TD3 and DDPG hyper-parameters were reused from Fujimoto et al. (2018)."" The justification for this choice is to facilitate comparison with previously published work."	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] As suggested, we have added further analysis of failure cases, and describe strategies for negative mining from these examples.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] We have included a new section 6.3 in the appendix of our revised draft that visualizes the behavior of our attention mechanism, as well as how it evolves over the course of training.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] We have revised the toy experiments to include a heteroscedastic regression task (see Section 6.1.).	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] Pose2Frame -- A qualitative ablation study can be found in Fig. 16.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] On reviewer's recommendation, we compare the performance of NOODL with one of the most popular alternating minimization-based online DL algorithm used in practice -- Mairal `09 -- in Fig. 2 and Table 4 (Appendix E).	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] Pose2Pose -- An ablation study for the P2P network can be found in Table 2, with quantitative results for each contribution.	1.0
Experimental study not strong enough [SEP] rebuttal_done [SEP] We added the results of this experiment to the appendix of the latest revision.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] Furthermore, use of this subset for performance evaluation is justified as as C_p^{test} corresponds to latest released structures in C_p, leading to a more accurate assessment of how such methods would perform on unreleased structures (as they do no sequence identity pruning).	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] However, the results show that our proposed method does not make the network be overfitted to test data as the Reviewer doubted because the difference between the accuracy for validation set and test set are consistent with the values from the previous works.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] We also note that our experiments compare with state-of-the-art approaches, which also use prior knowledge comparable to our usage of RAM state information.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] The prior state-of-the-art approach we compare against, SmartHash, outperforms these approaches by 1.75x and 4x respectively, at the number of frames they report (200M and 50M respectively).	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] Reviewer 1 further asks for evaluation on more games.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] Then, we observed the training and validation accuracy at each training epoch, and measured the test accuracy once after training.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] Pruning [1]                         11.4                         12.2	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] The testing data C_p^{test} is that which has been used in the prior works we compare to (Fout et al. 2017; Sanchez-Garcia et al. 2018).	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] Therefore, we believe that our proposed compression method does not suffer from the concerned overfitting problem regardless of the types of neural networks or dataset.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] Our approach further outperforms SmartHash by over 2x.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] In particular, we follow Aytar et al., 2018, and evaluate on 3 of the hardest exploration games from the Arcade Learning Environment.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] We emphasize that the last line of Table 3 corresponds to the most difficult setup, where the network has been trained with a strong data-augmentation, and we only use the intermediate layers of the network (which amounts to less than 62% of the parameters for e.g. Resnet101), this is why the performance is significantly impacted.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] Baseline	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] 11.5                         12.2	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] The test accuracy in the above table is about 1 % less than the accuracy which we reported in the originally submitted manuscript because the number of training data was decreased as part of the data set is used as a validation set.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017).	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] While it is true that the hyperparameter validation set was initially fixed, the switch to use C_r^{val} as above resolves this.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] However, we explore the robustness of our method to the exact chosen abstraction in section 7.4 and find that our method achieves state-of-the-art results over a wide range of state abstraction functions, suggesting that alternate state abstraction functions could be used.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] We use the same set of minimally tuned hyperparameters (tuned only on Montezuma’s Revenge) and obtain new state-of-the-art results by over 2x, suggesting that our approach can generalize to new tasks.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] To avoid using the test set in the retraining process as the Reviewer pointed out, we randomly selected 5K validation images among the original 50K training images in CIFAR-10 dataset, and applied our scheme.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] More importantly, we can learn the model and the domain weights simultaneously, unlike many existing works that use two-stage learning (learn the weights then the model).	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] We also see value in reporting these negative results.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] From the Table 2, we can see that our proposed scheme maintains the accuracy of the uncompressed baseline network.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] On the other hand, the CIFAR-10 dataset does not include a separate validation set, so we had to use the test set in the retraining process.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] We experimented with more sophisticated models, and it did not bring any improvement (see shadow models in appendix E, e.g. the performance before the softmax layer is 58.2 for Resnet101 and 60.8 for our method).	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] VWM (Ours)	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] We believe further experimental investigations are needed to better train non-Euclidean graph neural network models.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] 11.4                         12.4	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] Compression scheme   Validation Error (%)    Test Error (%)	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] In particular, FuN and Roderick et al., 2017 both report results on Montezuma’s Revenge.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] We do not evaluate on many of the simpler other games (e.g., Breakout), because they do not require sophisticated exploration and can already be solved with current state-of-the-art methods.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] Note that even the uncompressed baseline network exhibits similar accuracy difference between the validation error and the test error compared with the compressed networks.	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)].	1.0
Need more experimental results [SEP] rebuttal_summary [SEP] In fact, in case of PTB and Wikitext-2 corpus, we already used the provided validation set and measured the test PPW only once after training (Table 2) in the original manuscript.	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] (ii) “usually training is stopped much before convergence, in the hope of finding solutions close to minimum with high probability.”	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] Both the results on the development set and on the test set should be reported for the validity of the experiments.	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] Reviewer 1 claims that we do not sufficiently compare with enough other methods, and specifically asks for comparisons with Feudal Networks (FuN) and Roderick et al., 2017.	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] EXPERIMENTAL SETTINGS & HYPERPARAMETERS:	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] > 2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling.	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] …”	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] > “...often these small variations in results can be compensated with better baselines training	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] EXPERIMENTAL RESULTS:	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] Responding to R1's additional feedback:	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] ---------------------	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] ----------------------------------------------------------------------------------	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] “The experimental results of section 5.2 are somewhat disappointing. Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] > “...the results achieved in the experiments are very small improvements compared to the baseline of RGCN	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] R1 asks for experiments that do not use RAM state information.	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] ------------------------------  ---------------------------	1.0
Need more experimental results [SEP] rebuttal_structuring [SEP] The accuracy results are as shown in the following table.	1.0
Need more experimental results [SEP] rebuttal_done [SEP] (3) Of course! We have moved the test accuracy (which previously was only given in the Appendix and thus hard to find) to the legends of the plots to make it more easily accessible.	1.0
Need more experimental results [SEP] rebuttal_done [SEP] To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN.	1.0
Need more experimental results [SEP] rebuttal_done [SEP] - Following your suggestion, we have added one experiment on a synthetic regression task in Appendix C. Here, we show that our method can learn meaningful models for target domains, and also learn the source domain weights in a way that selects only relevant source domains for training.	1.0
Need more experimental results [SEP] rebuttal_done [SEP] Are added to section 4 and appendix E	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] In regards to why we compare the errors on natural images and those of our adversarial images:	1.0
"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal."""	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] “1.[...] without sufficient justification the assertion in the paper that the capacity of the network is well approximated by the number of parameters does not seem correct.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] After training, the output of the circuit is computed from [A, B, Cin] by clamping [A, B, Cin] and sampling [S, Cout] given [A, B, Cin] using Gibbs sampling.”	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] In my opinion, to support the above claim, shouldn’t the authors provide a similar table as Table 1, directly comparing the certified radii of the natural images and adversarial images?]	1.0
"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] 4. ""Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem."	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] An RBM is trained to implement a complete binary adder circuit by having it model the joint distribution of the adder’s inputs and outputs [A, B, Cin, S, Cout] (A is the first input bit, B is the second input bit, Cin is the input carry bit, S is the output sum bit, and Cout is the output carry bit), where (I assume) the distribution over [A, B, Cin] is uniform, and where S and Cout follow deterministically from [A, B, Cin].	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] So, two responses are given below.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] Thus our conclusions about the model do not only hold for one image.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] I’d like the authors to validate my understanding:	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] 3.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] We agree that here we present only results for one image, but we did carry out simulations for many images, and those plots are qualitatively the same for all the images considered.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] [R1: From Figure 9, we see the certificate radii of the natural have at least two peaks. Though on average the certificate radii of the adversarial attacks is higher than that of the natural images, it is smaller than the right peak. Could the authors elaborate more of the results?]	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] Also, the claim in Fig. 1 that the transition from ‘’high capacity’’ to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand (*)”	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_structuring [SEP] Please refer to our general response.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_done [SEP] Response #1: In the revision, we had added a new experiment to zoom in on two categories for clearer utility visualization.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_done [SEP] We redo the visualization in Figure 6 to make the gains provided by SN clearer.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_done [SEP] We have made an effort in the revision to make sure that this is more clear.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_done [SEP] We see that using SN can improve the test performance by over 12% for some FGM, PGM, and WRM cases.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_done [SEP] We now present the validation accuracy instead in Appendix F.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_done [SEP] Please see the (updated) last paragraph of Section 5, which explains this comparison in detail.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] q^D}	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] It is true that that the transformation that removes the desired information must be known before hand which is the main assumption in the paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We agree that this may appear confusing.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] 2) In non-convex-concave settings, HGD will converge to all types of stationary points, as the reviewer points out.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] \mathbb{E}_{I \sim	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] Thank you for these suggestions.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] You are correct.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] The process learns parameters that tend to give high probabilities to the most likely parent vectors regarding infections from the episode.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] (6) Of course, you are right! Previously we showed the test accuracy in Appendix F.  To make it more directly accessible, we have now added the test accuracy achieved by the different models into the legends of the plots, showing that CDN achieves similar predictive power as the baselines.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] This is why we provided multiple eval metrics including edit distances and next-N accuracy.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] 2) For the experiment, we will train our experiments longer and modify our network.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We acknowledge that our presentation focused more than necessary on ideal scenarios that use learning progress LP(z) while the practical version used a (maybe disappointingly) simplistic choice of proxy f(z).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] (4) We agree with the reviewer that in many cases there is a gap between solving the discrete problem and the fractional problem.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] Particularly since it gives the feeling that parents of infections are not involved anymore in the computation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] > We agree that our approach to estimate transfer potential reaps true benefits only when n is large.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] (2) Thank you for this valuable suggestion!	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] It is true that our analysis is quite general considering MLPs and not specifically CNNs and indeed we find it very likely that there are stronger results possible for CNNs than the ones we presented.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We appreciate your acknowledgment of our contributions and pointing out that the properties may only need to be satisfied at some critical points during training deep neural nets.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] Finally, it may also happen that the RL part does not bring benefit just because the current critic is wrong and provides an inadequate gradient, in a non-deceptive gradient case.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] A: You are totally right, it is missing.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] For our use case, we found the greedy forward selection computationally fast enough (of the order of a few minutes), and we observed a significant portion of accuracy captured in a very few dimensions.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] >> We agree it is trivial (and indeed the proof is a one-liner).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] This is an interesting question.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] Besides, ERL even better resists than our approach to the same issue: if the actor generated by DDPG does not perform better than the evolutionary population due to a deceptive gradient issue, then this actor is just ignored, and the evolutionary part behaves as usual, without any loss in performance.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] That said, we agree it is worth investigating the performance of LSTM on this problem further.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] This deceptive gradient issue certainly explains why CEM is the best approach on Swimmer.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We agree that, ideally, a comparison with SOTA architectures would be desirable.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] A1: Thank you for pointing this out.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] you are right, even if the focus of the paper is not on getting the best possible score on language modelling, different settings would make this point not only more convincing, but clearer.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We agree the IR-based metrics have limitations.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] However, admittedly, in this very specific context, CEM-RL is behaving as a CEM with only half a population, thus it is less efficient than the standard CEM.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We acknowledge that our presentation focused maybe more than necessary on ideal scenarios that use learning progress LP(z) while the practical version used a (maybe disappointingly) simplistic choice of proxy f(z).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] A: You are totally right, eq.10 simplifies to $\log p(D) \geq	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] A3: Yes, we agree that our original definition of alt-az convolution is not well defined on both north and south poles.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] > We agree that LARS/LASSO could act as potential ways to attain reduced dimensions.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] (6) Thank you for this feedback.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] In that case, applying that gradient to CEM actors as we do in CEM-RL is counter-productive.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] Specifically, we agree with the 1) and 2) of your analysis.	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] 1) We agree that we did not provide a clear definition of ""task""."	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] In general it is an established approach to solve the fractional problem and use additional techniques such as rounding to fill the gap.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] But we agree this was not a good choice, since this simplification appears obvious to the reader.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] But there are actually, since there remains $P(I_i|D_{<i},I_{<i})$ terms in the expectation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We agree that our algorithms are relatively simple.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] It is true, that one can describe the method as a (non-trivial) combination of beta-vae + GAN.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We agree vanilla REINFORCE can exhibit high variance.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] Thanks for the suggestion.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] If the reviewers feel strongly about this, we can move it to appendix, however we feel it helps to provide a complete picture.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] \left[ \sum\limits_{i=1}^{|D|-1} \log p(D_i |D_{<i},I_{<i})  + \sum\limits_{v \not\in U^D} \log p(v \not\in U^D| D_{\leq |D|-1}, I)\right]$. We were aware of this but for simplicity and the conciseness of presentation we chose to not give this final derivation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] It is something we should have done on our own.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] Response #6: We agree that it is important to justify how the reconstruction error works as a measure of privacy in this paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_concede-criticism [SEP] We acknowledge that f departs from LP in a number of ways.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Regarding the performance of the standard image captioning system, we train a caption model (Show, Attend, and Tell (Xu et al., 2015b)) with fine-tuned encoder (ResNet101) on the COCO dataset to encode the images.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] The result on EN-RO is 33.58.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Here, the function f is the sigmoid over a linear combination of numerical properties of X.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] 1. We study the irregular pixel logo ‘ICLR’ for three image datasets.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Kim et al. (2018) which is concurrent to our work shows similar performance.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] We also note that our experiments compare with state-of-the-art approaches, which also use prior knowledge comparable to our usage of RAM state information.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] While BIPSPI (Sanchez-Garcia et al. 2018) does achieve the best combined performance at 0.942, they also use additional sequence correlation features (note their structure-only performance is comparable to that of Fout et al).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] For DBA there are f distributed attackers and n-f non-Byzantine workers.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] We believe it’s worthwhile to explore the distributed version of other new attack algorithms, e.g. A Little Is Enough (Baruch et al 2019) that manipulates its update to mitigate Krum and Bulyan defenses.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] - We use Bulyan	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] prob(precision=b) ~	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] If reviewer#1 has better suggestions, we are happy to try.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] The total number of poisoned pixel amounts are kept the same.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] We understand this might not be the best “cost-aware” sampling policy.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] In our model, BERT is more than a source of contextual word embeddings as we fine-tune all of its ~110M parameters during training.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] .	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] A3: It’s also useful for irregular shape triggers.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] 2. In [III,IV] the authors have succesfully used a mean field approximation beyond the trivial first order to train RBMs.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] When adding in the sequence features used by Fout et al. via a simple linear model combining our final hidden layer and the additional sequence features, we are able to achieve a superior performance of 0.921 (0.914 +/- 0.009) versus their performance of 0.896 (0.894 +/- 0.004).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] From the variance plots it can be seen that some seeds of DNC-M and DNC-MD converge significantly faster than plain DNC.	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] The amount of possible patterns is simply too large to be enumerated and as such the random projections would serve as unique ""fingerprints"" for unique ""dependency patterns"" that would be used as inputs."	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Other parameters are the same as described in the paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] defends(X,Z) :- primeMinister(Z,Y), militaryBranch(Y,X), f(Y)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] However, since Cramer-Wold metric is defined by characteristic kernel, it can be applied in the large field of kernel-based methods in machine learning (where its particular advantage lies in the fact that it can be efficiently computed for the mixture of radial Gaussians).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] The Multi-Krum parameter m is set to m=n-f.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] disease_has_risk_factors(X,Z) :- f(X), symptom_of_disease(X,Y), disease_has_risk_factors(Y,Z)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] The function f is the sigmoid over a linear combination of numerical properties of Y.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] For both DBA and centralized attack we use the aggregation rule that can tolerate f Byzantine workers among n workers	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] We believe the reason can be explained by the fact that Loan and MNIST are simpler tasks and benign clients quickly agree on the correct gradient direction, so malicious updates are more difficult to succeed.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] This rule with a comparison operator states that a person X prefers neighbours with the maximal order that is less than X's.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] After further optimization, we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] However, we explore the robustness of our method to the exact chosen abstraction in section 7.4 and find that our method achieves state-of-the-art results over a wide range of state abstraction functions, suggesting that alternate state abstraction functions could be used.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Architectures sampled from this distribution are extremely small but with much worse accuracy.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] In [II] a first order approximation is used.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] DBA is also more effective and this conclusion is consistent in different colors of glasses.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] While preparing for the rebuttal, we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] The proposed method outperformed both baseline method and [1] in all simulation results.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] We believe that the adversarial robustness part of our paper has become much stronger than before, thanks to your suggestion.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Specifically, we use ‘ICLR’ as the global trigger pattern and decompose it into ‘I’, ‘C’, ‘L’, ‘R’ for local triggers.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] [1] learn a probabilistic model of one training curve using a handcrafted basis of nonlinear functions of shapes similar to the training curves being modelled.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] As a result, the BLEU score is (33.55), which is comparable to the current lookup table (33.78) based on Multi30K, and outperforms the Trans. (base) (32.66).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] The performance of this policy is much worse since for a conv operator with precision-0 (in our notation, bit-0 denotes we skip the layer), the sampling probability is 33x higher than full-precision convolution, 2x higher than 1-bit, 3x higher than 2-bit, and so on.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] - For other datasets, both attacks fail.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] - To meet the assumption that 2f + 2 < n, we set  (n=10, f=3) for loan and (n=12, f=4) for image datasets.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] 2. We also use the physical trigger glasses (Chen et al.,2017) on Tiny-imagenet and decomposed the pattern into four parts.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Our method does not make any assumptions about the shape of the modelled curves and is able to jointly model many training curves - in our experiments, training and validation loss and accuracy.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] For Tiny-imagenet we decrease the poison ratio to 5/64 for both attacks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] [2] learn a deterministic model of a learning curve, while our method also models stochasticity, hence providing diverse experience for training a reinforcement learning agent.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] This paper is not cited in Table 1 as they report macro-averaged F1 scores, while most other papers (including the current state-of-the-art [2]) report micro-averaged F1 scores, as we did.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] The aim of our study wasn’t to compare contextual vs. distributed embeddings but on how to successfully integrate BERT into a state-of-the-art joint NER and RE architecture.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] - For LOAN and MNIST, both attacks don’t behave well.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] This shows that additional modules help.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] 3. The approach of the authors in [III,IV] and also our approach is based on free energy.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] We generated random 512-by-512 matrices with pruning rate ranging from 70 % to 95 % and simulated the number of parameters fed to PEs in 10000 cycles.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] This shows that masking improves performance on this task especially when combined with improved de-allocation, while sharpness enhancements negatively affect performance in this case.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] - For CIFAR and Tiny-imagenet, we find that DBA is more effective.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] However, as stated in Section 1, off-policy methods benefit from stronger flexibility for experience sampler.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] To meet the assumption that 4f + 3 <= n, we set  (n=15, f=3) for loan and (n=20, f=4) for image datasets.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Yes. Image captioning dataset is absolutely available for creating the lookup table.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] prefer(X,Y) :- isNeignborTo(X,Y), hasBalance(Y,Z1), borrowed(Y,Z2), f(Y)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] We could observe that proposed parallel weight decoding based on the second Viterbi decompressor allowed 10 % to 40 % more parameters to be fed to PEs than the previous design [1].	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] The centralized attack for four datasets totally fails under Multi-Krum and Bulyan while DBA can still succeed in some cases.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] - For CIFAR, DBA is more effective.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] We doubt it can be efficiently applied in this direction.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] The tests based on Cramer-Wold metric were, in general, in the middle of compared tests (Mardia, Henze-Zirkler and Royston tests).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] (Blanchard et al 2017)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] This rule states that prime ministers of countries with certain numerical properties (described by the function f), are supported by military branches of the given country.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] (For information, the complete simulations for this paper take approximately 12 hours --which are easily distributed on a cluster as we multiplied the number of independent learning runs using different classes of parameters, cross-validations and types of sparse coding algorithms - in total approx 500 experiments.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] In summary, Multi-Krum and Bulyan have stricter assumptions on the proportion of attackers than RFA and FoolsGold.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] The rule states that symptoms with certain properties (described by the function f) typically provoke risk factors inherited from diseases which have these symptoms.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] 1/(1 + b)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] based on the Byzantine–resilient aggregation rule Krum	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] However, we note that our distributed and centralized backdoor attacks are not optimized for Byzantine setting.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] This rule states that neighbours with the largest difference between the balance and the borrowed amount are preferred.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] If you were to somehow control for this drop in model capacity, say by adding in an LSTM network, the ablated model would closely match this paper [1], whom we outperform by ~3% overall on the CoNLL04 corpus.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] In our approach we use the approximation up to fourth order in the coupling J.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Also in contrast to [1] and [2], our method allows the hyperparameters to change over the course of training and models the influence of those changes on the training metrics.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] In addition, while RFA and FoolsGold still assign potential outliers with extreme low weights, Krum (Multi-Krum, Krum-based Bulyan) directly removes them, making it impossible to inject backdoors if the malicious updates are obviously far from the benign updates.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Finally, it is well known that contextual embeddings outperform distributed embeddings on a wide range of NLP tasks, including NER [3].	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] prefer(X,Y) :- isNeighbourTo(X,Y), hasOrder(X,Z1), hasOrder(Y,Z2), Z1>Z2, max{Z2:hasOrder(Y,Z2)}	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] An interesting idea would be to actually use other information and encode it as random projections (e.g. syntactic dependency patterns).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] Simply replacing BERT with distributed embeddings and a character-CNN or LSTM wouldn’t allow us to determine the effect of contextualized embeddings because we would simultaneously be removing the majority of our model’s trainable parameters.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] This makes the gradient alignment and policy space constraint not as important as in the on-policy methods.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] More precisely, here f selects among all X those entities, for which the difference between the balance and the borrowed amount is maximal.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] For centralized attack there is 1 attacker and n-1 non-Byzantine workers.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] The systematic expansion of the free energy in the coupling constant forms the basis of our approach.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_summary [SEP] however, the results were not satisfactory.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] As to the shortcomings of our techniques and why we pick the fractional problem, note that the GAN framework needs the computation of the discriminator network (i.e. the algorithm agent in our context) to be differentiable in order to update the generator network (i.e. the adversary in our context) during training.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] - The reason behind using V-GMM is that V-GMM is much faster than KDE in inference and has a better generalization ability compared to GMM.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Meta-SGD         x2	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We see this methodology, as illustrated by our experiments for one model, as the central part of our contribution.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Thus, the time dedicated to quantization is relatively short, especially compared with the fine-tuning and even more with the initial network training.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We tried recurrent posteriors and learned priors with our models, and the results were similar.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The Gaussian kernel itself is not well suited,	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We consider self-modulation as an architectural change in the line of changes such as residual connections or gating: simple, yet widely applicable and robust.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] So, if we consider some target error $\epsilon_{target}$, we can solve eq. 5 for m or n given the other or for both, attaining the m,n contour for $\hat{\epsilon}(m,n) = \epsilon_{target}$.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Lemma 2.4, Point 1: The gradient in your example is indeed perpendicular to w which can be seen as follows.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The time of quantization is around 1 to 2 hours, the rest being dedicated to finetuning.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However, we did use in the paper the first two images we manually selected from Google Image Search (while we did select images with some texture, they have not been otherwise been optimized for good performance in our evaluation)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] - Interpretation of self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In the present paper there are two tasks: classification into primary labels, and classification into secondary labels.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 51.93+-0.67	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Such a discretization has a number of benefits, including multi-modality, which cannot be achieved using a parameterized Gaussian policy.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] While it might be interesting to try to capture interactions between all pairs of words, that is not justified by our model and we didn't explore it.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We note that our principal contribution in this paper is the RAN framework and the training algorithm, which can accommodate different choices of privacy attacker and privacy quantification.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Our objective was to add a method alternative to the WAE method, but simpler in use (e.g. less parameters to be found).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Our formulation is very general, and it could potentially also be applied to other modalities like images for tasks like image classification and captioning.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] .	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In our opinion sliced approach works well for neural networks, as the neural networks see/process data by applying similar one dimensional projections.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] To allow the model to access ancestry nodes during decoding, one way is to concatenate the parent node latent representation with the input of each step for decoding children, and then feed the concatenated vector to LSTM (e.g., Dong & Lapata ACL 2016).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] They make the training of the controller much easier compared to other RL tasks with higher dimensional features or larger output space.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] guarantee that when epsilon is smaller than a threshold, no violations can be	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] although the best hyper-parameters obtained for Arti1 remained near optimal for the other ones.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We use V-GMM as a proof of concept for the idea “Curiosity-Driven Experience Prioritization via Density Estimation”.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] A3: Since binary codes and lookup table would be associated with vastly different inference architecture, computation methods, and storage design, it is difficult to analyze detailed comparisons on FleXOR and lookup-table methods.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We observe the controller performance on all 4 tasks are insensitive to initialization.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Please refer to our general comment above on why our unguided case performs better now.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Moreover, we also also advanced the state-of-the-art results based on practical GANs (SN-GAN).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However this could be counteracted by stopping the gradient at the right place in the simplified architecture  which was suggested in the original paper and is now described in the updated paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The number of parameters of MAML(chx2) is four times of that of MAML, while Meta-dropout is only doubled.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We have found that this leads to worse performance, likely because of the domain shift.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Fundamentally, exploration requires some degree of randomness, and we were already able to achieve state-of-the-art results without overcomplicating the discoverer.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Presumably, it could be possible to observe the behavior in our paper based on a pairing-based mechanism which works approximately, independent of clustering, as predicted by Weber's Law.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We propose some modifications to HGD to allow it to work in non-convex settings in Appendix A, which essentially amount to explicitly determining the local curvature of the problem and running a modified algorithm, such as Hamiltonian Gradient Ascent, near undesirable critical points.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However, if we set f(N+1) to 0, it imaginary part becomes zero again.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We wish to provide some of our analysis of the limitation of BA-Net, and hope our method could provide complementary perspectives to rethink the problem and mitigate the non-optimal issue in terms of performance with more ML component.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Overall, self-modulation appears to yield the most consistent improvement for the deeper ResNet architecture, than the shallower, more poorly performing, SNDC architecture.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We added discussions on the same computational savings and additional storage savings of FleXOR in Section 4 and 5.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Thus the deterministic meta-dropout still “learns to perturb”, although not random, and is actually a core component of meta-dropout (See Table 3 in the revision).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] To our knowledge, these implementation strategies are novel in this context and were key in the development of our method to be able to iterate rapidly.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The bound is applicable if the number of parameters, k^2 is smaller than a logarithmic term times the number of output parameters, i.e., it allows the number of parameters to scale almost linearly in the output dimension.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This yields better per-step convergence, but longer overall training (wallclock) time for the controller to converge.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The bottleneck for this is to calculate the pseudo-inverse of the activations x. However, we fix x when iterating our EM algorithm, therefore we can factor the computation of the pseudo inverse of x before alternating between the E and the M steps (see file src/quantization/solver.py and in particular the docstring).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The kernel used in the derivation is not a Gaussian kernel but has a closed form formula for a product of two Gaussians (see last equation in the current paper), itself not being Gaussian.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This is the regime in which the deep decoder operates throughout the paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] On the other hand, we didn’t observe it harms on NMT task noticeably.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] A good initialization (e.g. in NMT, equally assigning probabilities to each loss at the start of the training) indeed leads to faster learning, but most experiments with random initializations manage to converge to a good optima,	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We used a simple 1D CNN where convolution happens over the time dimension, not the spatial dimensions.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In terms of number of iterations, our method does not have a restriction, since our iteration happens outside the neural network and acts as an incremental improvement.	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] example, what is log(I) for epsilon larger than upper bound?"""	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Interaction between arbitrary word pairs: our model introduces the tensor in order to capture syntactic relationships between pairs of words, such as adjective-noun and verb-object pairs.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However, there are two reasons why this might still be beneficial: one is that the penalty coefficient is now effectively dynamic and can change during training, ensuring higher chances of finding a good solution.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 1. Scientific Contribution: Most recent work on disentangling generative modelling tries to obtain an independent/factorised posterior over the latent generative factors without directly addressing the problem of d-separation, which theoretically prohibits factorisation of the posterior in models such as beta-VAE, conditional GAN or stack GAN.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] lower bound on the input perturbation (see [1][2][3])	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] thanks to \epsilon-greedy sampling used in training.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However, while it may improve accuracy, we do not believe that it adds value in reducing the amount of hand-engineering needed, as the amount of hand-engineering needed is very minimal already.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Back to comment #2: for regression and classification, we have experimented with larger S and found the improvement marginal.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Here we don’t mean that our method can fix the optimality problem in any way.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In this case, the imaginary part is not zero.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] SO(3)  is a group which can be parametrized by a 3-sphere .	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 48.19+-0.64	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Importantly, our purpose in this task is to show that, **all other things being equal**, a neuromodulated plastic LSTM can outperform a standard LSTM in realistic settings.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] … f(N), f(N+1)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Despite these being two different definitions of robustness, to try and demonstrate some comparisons between the two, we extended experiment 6.4 (already using Wong and Kolter (ICML 2018) [3]) and compared the fraction of samples for which I = P_min to the fraction that could be certified by Wong and Kolter for epsilon in {0.1, 0.2, 0.3}. We found that it wasn’t possible to calculate the certificate of Wong and Kolter for epsilon = 0.2/0.3 for all epochs, or epsilon = 0.1 before a certain epoch, due to its exorbitant memory usage.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The prior work by Hsu et al. showed that the oracle trained by deep learning has high accuracy (see Section 5.3 in their paper): for Internet traffic data, the AUC score is 0.9, and for search query data, the AUC score is 0.8.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 65.55+-0.56	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 67.42+-0.52	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] To just give one example, LSTM cell state can increment by a fixed amount at every time step and can count the number of tokens reliably (i.e the string length) [1][4].	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Incorporating these matrices into the framework and filtering rules that have the respective relations in the head should allow us to extract the target rules.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The first paragraph of section 3.2 describes this FiLM model and, given the focus on methodology, we considered the description (plus reference to the paper) sufficient here.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Performing batched update with a larger S might help reduce correlations; However, a large S, as a major drawback, requires performing ST (S>>1) steps of task model training, in order to perform one step of controller update.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We truncated the vocabulary by keeping approximately 100k words with the highest frequency and used the same validation and test sets as (Yang et al. 2017).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Meta-SGD also doubles the number of parameters in the base MAML model, but is significantly outperformed by Meta-dropout.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Another two factors could represent long-rang and short-range correlations.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] -The E-step is performed on the GPU (see file src/quantization/distance.py, lines 61-75) with automatic chunking.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Some of these findings can be attributed to the additive property of the cell state of the LSTMs c_t = f_t c_{t-1} + h_t {c’}_{t}, which is free from matrix rotations.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] First, the use of HRL enables temporal correlation in action exploration that helps reduce the non-stationarity challenge.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] >> Comment #4	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We’ve also revised Appendix A.1 to cover details of how PPO is incorporated.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Re: (W7) Alternatives to CFS / Computational concerns	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] w’ * \nabla L(w) = w’ * (2/w’*w)(Aw - L(w)*w) =  (2/w’*w)(w’Aw - L(w)*(w’*w)) =  (2/w’*w)(w’Aw - w’Aw) = 0.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] A1: Thanks for pointing this out and sorry for the confusion!	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] While the CFS is not invariant to rotations, it is a surprising, and empirically noteworthy, finding that all 4 different ways of producing representations consistently encode a dozen tasks very succinctly.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We test DNC (differentiable neural computers) and RMC (relational memory core) models, which arguably are more specialized for doing mathematics, since they have a slot-based memory that may be appropriate for storing intermediate results.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However, as we have elaborated in the text below Eq.2, to reduce the variance and stabilize the training, we have made the following adaptations referring to previous works [1,2]:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] A: The suggested methods [1-3] are self-supervised methods using less information than the baselines we have included (for example, the AE+classifier baseline is trained on the same synthetic data with the same access to digit labels).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In our experiments, the SN-regularized network still performs better in terms of test accuracy.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 98.54+-0.07	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In this case, the imaginary part in time domain is zero because of symmetry.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Linear classifiers with IP addresses represented as individual bits are unlikely to work well because their expressive power is limited.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Almost all main results are averaged over multiple runs as explicitly indicated in the main text and the table or figure captions (e.g. see captions of Table.1 and Fig.2).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Inferring structural properties of the graph to be used in the prior (e.g. using GraphRNN), as we understand the reviewer suggests, certainly sounds potentially interesting.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However, as the reviewer points out as well, the HGD analysis is also useful because it implies similar convergence results for CO, which is a practical algorithm.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] > It is easy to adopt our approach to study the information encoded in the encoders for other problems involving structured prediction (say POS Tagging).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The blurriness in a VAE can indeed be attributable to a weak inference model.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We apply plus-one smoothing to handle such words.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Other density estimation methods can also be applied.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We are aware of this key difference and we apply the sigmoid function to scale the output of the discriminator to the [0,1] range for the non-saturating loss.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We chose quantization schemes using binary codes in the experimental results because 1) binary codes are being widely studied and 2) we can focus on the practical issues on binary codes.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Now, suppose the output size is 2N+2: f(-N), … , f(0),	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The attack points are therefore not necessarily close to the decision boundary, and thus, the changes produced in the algorithm are more unpredictable and affect the errors for the two classes.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] BERTScore computed with Multilingual-BERT is better than most existing metrics except on few low-resource languages.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 94.96+-0.1698.36+-0.08	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We did not mean to imply that the classification of a specific class is a task on its own.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In contrast, our absolute fitness naturally has this effect when paired with a non-stationary bandit.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Similarly, this $k$-step structured exploration enables learning.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Only T differs across tasks, but we always update \phi whenever a reward is generated.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The fact that CNE could be combined with such inferred structural properties increases its potential impact though, and this remark of the reviewer further underscores the need for methods such as CNE that can take such structural information into account.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] [A] Most of these are described in Section 2 (in particular, discussion on regularization and penalties is in Section 2.2).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] [Unguided Case and Disentanglement] Please refer to our general comment above on why our unguided case performs better now.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However, since the ancestry has a variable-number of nodes (as decoding proceeds)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 3. [1] successfully use PPO with an LSTM policy on a challenging, partially-observable environment.	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] When talking in more general terms about ""deep learning models"", we refer to the proposed methodology for ""investigating deep learning models"", and don't want to claim that we actually evaluate a representative number of models."	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Concerning the closed-form, Cramer-Wold kernel is the only known to the authors, which is given by the sliced approach and has a closed form for radial gaussians.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However, MISC has the advantage that it not only enables the agent to learn to reach but also learn to push and pick & place.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Fig.4 shows an example of odd and even output size of F-pooling.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] [Information of Guidance] In Figure 3, we visualize which part of an image was visible to a siamese network.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We believe the application of NADPEx to off-policy exploration is straightforward.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Of course, even an ideal metric LP(z) would remain a local quantity, and pursuing it would not guarantee the maximal final performance -- but it is valuable if local optima are not the prime concern.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] i) We visualize the latent vectors obtained from demonstrations with probing and without probing.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Also the success of neural networks based on the classical activation functions, as compared to RBF networks, supports this.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This is because meta-dropout consists of two parts: meta-learned deterministic multiplicative perturbation and random noise.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Baseline models are trained on the same training set as our model following the methods proposed in their original paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] But at the same time we also have questions on how much additional insight it brings, as it's not an apples to apples comparison, so neither tight or loose estimations diminish the value of both types of approaches.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In SVHN, 1,000 images are used as the labeled data and 45,000 balanced unlabeled images are used.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Although in principle a strong inference model could produce sharper images, an alternative approach is to use better losses, which is the approach we chose in this work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Novelty: Our method aims to solve the fundamental issue of d-separation in disentangled representation learning.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Indeed, GPipe [2] incurs less memory footprint than our pipelining scheme and PipeDream [1] because it only saves the activations at the boundary of each model partition and re-computes the activations of the model during the backward pass.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We mainly account the success of this simple training strategy to the simplicity of the model, the relatively low dimensionality of our input features, and the simplified action space (though all  three suffice to obtain a good controller in the current settings).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] For instance, at the very least, we would like our classifier to express a DNF hypothesis of the form:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The inception score on CIFAR-10 is improved from 8.22 to 8.45.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This is because we optimized our EM implementation in at least two ways as detailed below.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] First, it does not contain learner-subjective information, but this is partly mitigated through the joint use of with prioritised replay that over-samples high error experience.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We can’t directly measure how the imaginary part affects the performance unless we use complex-valued CNNs.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We compare with pearson's correlation because it's another widely used technique that can perform both correlation analysis as well as significance test.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Empirically, we can achieve better results compared to Reg-GAN as illustrated in Table 1 (top).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] because it has an exponential rate of decay, and loses much information on the outliers (see also Bińkowski et al.,  https://arxiv.org/pdf/1801.01401.pdf, section 2.1).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The simulation will be released with the work for others to use and build on multi-agent learning methods.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] For the other three datasets, self-modulation helps in this setting though.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We hypothesize the tradeoff is insignificant on NMT, as in our multi-task setting, slightly over-optimizing one task objective usually does not have irreversible negative impact on the MT model (as long as the other objectives are optimized appropriately later on).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] For these $k$ timesteps, no noise is added to the low-level policy outputs.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] -Truncation introduces a train-test disparity in G’s inputs--at sampling time, G is given a distribution it has effectively never seen in training.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] >> Comment #12	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The authors should	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This turned out to be unnecessary, as per the above argument.)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Combining them both results in the suggested loss.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We will also clarify that the little phrase “After initial experimentation, we opted for the simple proxy…” implies quite extensive experimentation with other plausible proxies that looked promising in individual environments but were not consistently effective across the suite of Atari games.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In our case, this method results in a significant increase in network parameters for the Q function, which leads to poor learning performance, as can be seen in Figures 2 and 3.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] A5: These four operations were used by ENAS and commonly included in the search space of most NAS papers.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] It was never the intention of the authors to sneak in that VAE cannot do it.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We found that the CDN objective leads to superior results, especially in the adversarial examples experiment.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We believe that outperforming standard LSTMs (again, all else being equal) on their “workhorse” task domain (language processing) is worthy of notice, especially given the ease of implementation of our method which requires only adding a few lines of codes (<10) to a standard LSTM implementation and can then be used as a drop-in replacement to standard LSTM.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Yes. We compared with ResNet101 and ResNet152 on EN-RO.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The reviewer is completely right that we are removing one hyperparameter by introducing another.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Describing all aspects of these techniques would require substantially more space and hence we refer to the original work for precise formulation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Models   #param.Omni-1shotOmni-5shotmimg-1shotmimg-5shot	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We view the simplicity of the discoverer as advantageous.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We know it is challenging to learn Q functions, which implies that the centralized methods that use Q functions will not scale well.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Better architectures, e.g. transformers over time + graph networks over space could improve performance, but not our focus and we leave that to future work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We did not originally discuss weight decay, dropout, and batch normalization as none of these methods were motivated by the theory we introduced in section 3.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] - Clip the final reward to a given range	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Because we need to predict the location or shifts of an image object.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Performance plateaus are a nuisance in general, and within the simple space of modulations we consider, there is no magic bullet to escape them.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Self-modulation doesn’t help in the SNDC/Spectral Norm setting on the Bedroom data, where the SNDC architecture appears to perform very poorly compared to ResNet.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The SimPLe algorithm runs PPO on an MDP approximated by a powerful model that handles stochasticity well, which is also a valid approach.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Thus, we think that it is extremely likely that many other images would work just as well.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The Alt-az rotation, according to our definition, is not a group.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Although our representation resembles positional encoding on the surface, it has not been clear in the time-series community if/how/why positional encoding can be used to replace hand-crafted functions of time, and there has been no empirical evidence to show its merit.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Instead of using a decoder that takes in all the dimensions of the encoded input token, one could iteratively select dimensions that provide the highest gains in decoding the right target sequence (say POS tags).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] DEMINE-sig maximizes mean([m1,m2,m3])-v.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However, SPAMS is a great inspiration for our framework.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Figure 3 shows that the five architectures sampled at epoch-89 are much better than the five architectures at epoch-0, which are essentially drawn from random sampling.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] - We concatenate the goals and estimate the trajectory density instead of state density because HER needs to sample a future state in the trajectory as a virtual goal for training.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] It's probably impossible to ultimately rule out a pairing-based strategy via experiments evaluating extrinsic behavior only, but we note that there is evidence for Weber's Law in other approximate systems where pairing-based strategies are no alternative, thus suggesting that similar mechanisms are at work here.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] , to directly access these nodes during decoding, attentional mechanisms would be an efficient way, which is one of our motivations to use Transformer models that are attention-based.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We hesitated a lot on this, our decision was taken to present things as closely as possible to our implementation (by the way there was a mistake in the algorithm 2 resulting from this hesitation - the implementation for our experiments was correct however).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] ICML 2018	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The use of a fixed embedding space $L$ and a separate space $L^\prime$ was useful as it naturally prevents the collapse of embeddings.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Globally, we drive exploration by incrementally growing the safe set (renamed known set in the updated draft).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] As to the online setting, thanks for pointing us to the “short-horizon bias” paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We note that this random exploration is only for locally discovering nearby abstract states.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The boost in accuracy achieved by CNE, using a model that is arguably also a lot simpler than the state-of-the-art network embedding approaches, is thus achieved without any increased need for hand-engineering.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] yes we have test several layers with adaptive kernels. but we focus on report the results on the first layer to highlight the contribution	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] A first impression is that our technique is more general and will probably give looser bounds, but may be applicable to a wider range of problems not only ones that have specifically that type of covariance, just like DEMINE vs Pearson's correlation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 48.30+-0.64	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] But the changes produced in the system may also depend on the characteristics of the dataset and the learning algorithms used.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 2.The method proposed in our paper stores immediate activations, which is mentioned in Section 3 of the submission.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] pGAN produces poisoning attack points that are close to the decision boundary, “pushing the decision boundary away” from the source class (i.e. the same class as the labels of the poisoning points) towards the samples of the target class.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] As a first step, we provide a careful empirical evaluation of its benefits.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] As we state in our paper, quantizing a ResNet-50 (quantization + finetuning steps) takes about one day on one Volta V100 GPU.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The main goal of our submission is to experimentally show that our pipelined training, using stale weights without weight stashing [1] or micro-batching [2], is simpler and does converge.	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Even if they did show these convincingly, it is not obvious to me that it is valuable."", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate."	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The HRL structure is reducing the dimensionality of the control problem given a low-level designed to perform diverse behaviour wrt to the goal (cite Heess and DIAYN).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We obtained a valid/test perplexity of 44.0/42.5 for the model with PDR and 44.3/43.1 for the model without PDR, showing a gain of 0.6 points in the test perplexity.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] As a result, the SST is still worse than other algorithm [1].	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The extendibility of the Neural LP framework is a very important and relevant question, which we also mentioned explicitly as a possible future work direction.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] [2] show that discretization of the action space improves the average performance, stability and robustness to hyperparameters of reinforcement learning agents on a range of continuous control tasks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We therefore expect that these methods will perform worse than our baselines.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] LSTMs are indeed a strong model for tree prediction on previous tasks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] For example, one latent factor could model middle-range correlations if the transformation remove long-range correlations through shuffling process and short-range correlations get destroyed through a blurring process (e.g. local smoothing transformation).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] While AutoLoss is amenable to different policy optimization algorithms, we empirically find PPO performs better on NMT, but REINFORCE performs better on GANs.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 4. We updated the paper with a justification of our action discretization scheme.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We show in section 6.1 that the dependency of the classification error on the number of layers is also well approximated by eq. 5 (recall $m$ scales linearly with depth).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The proposition is only interesting if k^2 log(n_0)  / n <= 1/20 even without this assumption (due to the right hand side of the lower bound) therefore this assumption is not restrictive.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] , the minimal adversarial distortion is lower-bounded by that fixed epsilon.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Thus, the error of ignoring imaginary part is not larger than ||f(N+1)||.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 96.16+-0.14	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The succesful transfer attack on CapsNet is based on transfer of adversarial images from a different model (LeNet).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The performance of a simple online algorithm would likely depend on the type of classifier used and input feature representation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Thus if it unrolls more iteration steps, the memory cost will increase linearly.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] -> We also ran this experiments for 7*8 GPU days, however the method converged after roughly 3*8 GPU days (meaning that there were no significant differences afterwards).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Our suspicion is that if G is not encouraged to be “smooth” in some sense, then it is likely that G will only properly generate images given points from the untruncated distribution.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] perturbation by providing a counter-example (violation).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The observation that imposing orthogonality constraints improves amenability to truncation is empirical.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 2. We believe F-pooling plays a more important rule in applications where shift-equivalent is serious, such as object detection and object tracking.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Then we transform it back into time domain.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In the rules that we support in our framework all variables are universally quantified.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Q3: alt-az convolution is not well defined on the south pole	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We argue that both pruning and low-rank approximation are orthogonal and complementary approaches to our method, akin to what happens in image compression where the transform stage (e.g., DCT or wavelet) is complementary with quantization. See “Deep neural network compression by in-parallel pruning-quantization”, Tung and Mori for some works investigating this direction.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] - Substitute a moving average B (defined in text) from the reward	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Higher-order and nonlinear covariance tests may make very appealing comparisons and we are looking into it.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Of course, LSTM equipped with Attention would achieve the same benefit.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This poses difficulties if we ask the algorithm agent to make discrete decisions via sampling or rounding since it will not be differentiable.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] But when we reduce one parameter from it, it is not a group anymore mathematically; the composition of two alt-az rotations becomes a general rotation in SO(3).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Suppose the output size of F-pooling is 2N+1.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Second, finding the right measurement for privacy is an open problem in itself.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] These methods can	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 4. ""The experiments of this paper lack comparisons to certified verification"	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We now clarify these reasons in Section “2.3 Density Estimation Methods” of the revised paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] It allows for a theoretically consistent way of obtaining factorisation in the posterior without any information-theoretic penalties.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However, our approach does the next best thing: when performance becomes an uninformative (ie on a plateau), it encourages maximal diversity of behaviour (tending toward uniform probabilities over z), with the hope that some modulation gets lucky -- and then as soon as that happens, very quickly focusing on that modulation to repeat the lucky episode until learning is progressing again.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] As each reward is generated via an independent experiment, the correlations among gradients are unobvious.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The BLEU scores are 33.63 and 33.87.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This is in line with some early work that observed that certain characteristic properties like length [1][2], sentiment [3], presence/absence of brackets [4] are encoded in a single dimension in the space.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We believe this is a feature not a bug: as we showed in Sec. 4.1, our algorithm does not need to be more complex.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We considered the traditional meta-learning for few-shot learning approaches, and combined meta-learning with a popular domain adaptation baseline.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This significant memory gain thus indicates that our approach may still have advantages when used as a method for approximately doing more classical verification, even though this was not our aim.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] A5: Theoretically, Reg-GAN is also a stable training method for GANs but it is computationally less efficient than NF-GAN (ours), as illustrated in Fig. 4.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] It indeed shows that with probing, we are able to find new behaviors that correspond to the new latent vectors.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We want to emphasize that Deterministic meta-dropout is also one of our models, and that its good performance does not hurt our claim on the effectiveness of the multiplicative noise.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We refer to the updated version of our paper for the results (Appendix 3,“LEMONADE with 5 objectives”), but in a nutshell the results are very positive and qualitatively resemble those for two objectives.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We speculate that encouraging G’s filters to have minimum pairwise cosine similarity means that, when exposed to distribution shift, the network’s features are less correlated and less likely to align and amplify an activation path it would otherwise have learned to scale properly.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] There might exist sweet spots for S where one can achieve both good per-step convergence and short training time, but we skip the search of S and simply use S=1 as it performs well.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In the present paper, we chose the “reconstructive error” because it is the most intuitive one to measure the risk of original data disclosure given perturbed data (Encoder output).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] To further elaborate, due to d-separation, models from prior work that have the same underlying plate notation either fail to disentangle the representations (since $p(c,z|x) \neq p(c|x)p(z|x)$ ) ) or do so at the cost of lower generative quality—-because their training relies on having an additive information-theoretic penalty term.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In response to this question, to demonstrate this, wee conducted a new experiment with 5 objectives (performance on Cifar 10, performance on Cifar 100, number of parameters, number of multiply-add operations, inference time) to show that LEMONADE can handle these realistic scenarios natively.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] As the reviewer#2 suggests, we can train the super net until the last epoch, then sample and train architectures from this distribution.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] What is the problem in SVHN (balance problem or dataset or both)?	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] >> Comment #1	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The proof of this statement demonstrates that identifying heavy hitters and placing them in unique bins is an (asymptotically) optimal strategy.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However, the re-computation still incurs pipeline bubbles during training.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] At some point, when the fraction of poisoning points increases significantly the decision boundary starts to change in a different (and possibly more abrupt way), so that the false negatives also start to increase.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This shows a clear distinction between different methods and an important result: when $\ell_2$ normalizing atoms, dictionary learning may converge to a result for which the ratio of activity between the most activated and the least activated is of the order 2.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] It seems deeper ResNet indeed gives better results but the difference is not very significant.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Note that our VAE variant and both SVG variants are able to predict sharp robot arms in the BAIR dataset, but often blur out the small objects being pushed.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Also, note that for CIFAR10-ResNet-110 experiments, the search space contains 7^54 = 4x10^45 possible architectures, 45 sampled architectures are tiny compared with the search space.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We indeed compared our method with generalized I-FGSM/BIM, which is exactly the same as PGD (In [Madry et al.] they also mentioned this in Section 2.1 and they refer it as FGSM^k).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] >> Comments #1, #11	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In contrast, BA_Net’s iteration is part of the LM optimization and it is inside the network.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In Figure 6 (right) this happens when the fraction of poisoning points is larger than 25%.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Re: (W1 & W2) Adversely affected by rotations	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] While learning rules with existential quantifiers in rule heads is a difficult endeavor in general, even for classical relational learners, the Neural LP framework in principle can be extended to support them as follows: For every relation p, we can create a fresh diagonal Boolean matrix $M_{\exists p}$, which has 1 at the position (i,i) iff there exists an entity j, such that p(i,j) is in the KG (similar as for classification operators discussed on p. 5).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Our early experiments with LSTM did not yield good results on this spatial layout problem.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We have indicated in the revision the existence of this bias -- this bias was observed on the GAN task -- overtraining G can increase IS in a short term, but may lead to divergence in a long term as G becomes too strong.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Our model and the baselines were tuned by a grid search process on a validation set for each dataset (whose size is given in the description of the datasets),	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] On the other hand, adversarial attacks give an upper bound of input	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We hypothesize that models which are not amenable end up learning mappings which, when given truncated noise, either attenuate or amplify certain activation pathways, leading to extreme output values (hence the observed saturation artifacts).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 4. In all experiments of our current paper, the imaginary part is already ignored.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] For simplicity, we opted for the former.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This property can be understood to reduce the variance in the policy gradient.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Ignoring this part will destroy the reconstruction optimality, but the effect is small.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We’d like to clarify that S=1 is consistent in the overhead section and Algorithm.1.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Yes, in WGAN, it is preferable to train the critic till optimality.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Another choice is to use temporally correlated latent variables, which would require a stronger prior (e.g. as in Denton & Fergus (2018)).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The paper does achieve this goal, on a number of networks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 96.63+-0.1398.73+-0.06	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 6. While we have not explicitely labelled white box and black box attacks we are using a strong white box attack (gradient based) and the, to our knowledge, strongest black box attack (boundary method).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] (IP address = a1) or (IP address  = a2) or ...	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Then, we can expect an increase of the false positive rate, which is shown in Figure 6 (centre).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Our scheme saves all activations instead of re-computing them to eliminate pipeline bubble, thus achieving better utilization for the accelerators (GPUs).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This would allow us to show similar local convergence guarantees to those proven by other works in the area (see Appendix A).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Another potential mechanism by which the episodic return can be indicative of future learning is because an improved policy tends to be preceded by some higher-return episodes -- in general, there is a lag between best-seen performance and reliably reproducing it.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Since all of quantization techniques in Table 1 and Table 2 follow the form of binary codes with the same q bits, comparisons have been made under the same computational savings (thus, model accuracy is emphasized).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This doesn’t mean that our high-level framework (i.e. training the algorithm and adversary networks simultaneously) is doomed, since we can use other ML techniques (e.g. reinforcement learning) to implement our framework, but in general sampling and rounding will lead to much more work during training, so we pick the GAN structure in this work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] For	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] (Q2) Equivariance property of the Alt-az convolution	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] (3)-(4) To some extent pGAN can control the specific errors produced in the system, as shown both in Figures 5 and 6.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] A : We have experimented with SVHN with data balancing.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Moreover, F-pooling may be better for complex-valued CNNs, such as [1].	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We do not think that our current approach can disentangle continuous features.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Meta-dropout  x2	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Please refer to our overall comments on this question (and also a few more details in reply to Reviewer#1’s similar question).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Regarding fMRI experiments, our focus is on demonstrating neural MI estimation and dependency test on fMRI data.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 65.84+-0.52	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This falls within the map/reduce paradigm.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] -> We think having 4-6 objectives is a realistic dimensionality for NAS applications, and scaling to significantly more objectives (which would indeed be problematic for our method, but also for multi-objective optimization in general) is typically not necessary.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Our particular configuration allows our method to be decentralized, making the individual network for each agent more straightforward.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In contrast, the label flipping attack is less subtle as it does not consider detectability constraints.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We provided pointers to the files in the code anonymously shared on OpenReview.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We first transform a signal into frequency domain and keep 2N+1 components with the lowest frequencies: f(-N), … , f(0), … ,f(N).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Our primary goal was to define a method for training the Gaussian prior generative model using a different closed form formula for the distribution distance.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] FleXOR, however, provides not only higher model accuracy but also additional storage savings due to the proposed encryption algorithm/architecture using XOR logic.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Yes, we experiment only with myopic variants of exploration, but (A) our approach is not limited to this initial set of behaviour modulations, and could be extended to trade off between intrinsic and extrinsic motivation, or between model-free and model-based mechanisms; and (B) the variations we consider may not be ideal, but they are the ones most commonly used in domains like Atari.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Empirically, with random parameter initialization most experiments manage to converge and give fairly good controllers.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Both strategies are documented in the code so that they can benefit to the community.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Instead of having the policy sample an action every step instead, the low-level policy is triggered for $k$ timesteps with a goal proposal.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] -> The population is updated to be all non-dominated points from the current population and the generated children, i.e. the Pareto frontier based on all current models.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] These claims are supported by showing that providing random labels does not lead to any improved performance and by our experience that using hard labels does indeed improve performance.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] methods. There are some scalable property verification methods that can give a	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] [1] Efficient Neural Architecture Search via Parameter Sharing.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] MAML   x1        95.23+-0.1798.38+-0.0749.58+-0.6564.55+-0.52	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Note that we tuned the PDR loss coefficient very coarsely and tuning it further could lead to higher gains.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] - Compared to the dense reward, with the negative L2 distance between the robot and the object, the robot can only learn to reach the object but will not learn to push or pick up the object because when the robot reaches the object, the negative L2 distance is already zero.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] This means that the code chunks the centroids and the weight matrices into blocks, performs the distance computation on those blocks and aggregates the results.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However these models obtained worse performance than the more general architectures, and we are not yet aware of models that are more tailored for doing mathematics that do not simply have their mathematics knowledge built-in and unlearnable; we hope the dataset will spur the development of new models along these lines.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The three references and the follow-up work that you cite give different methods for obtaining a certificate-of-guarantee that a datapoint is robust in a fixed epsilon l_\infty ball, with varying levels of scalability/generality/ease-of-implementation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Moreover, AutoLoss is not restricted to REINFORCE, but open to any off-the-shelf policy optimization method, e.g. for large-scale tasks such as NMT, we introduce PPO to replace REINFORCE, and adjust the reward generation scheme accordingly (see the paragraph “Discussion”).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] >> Comments #2, #3	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Nonetheless, MAML(chx2) does not improve on MAML, demonstrating that the effectiveness of meta-dropout does not simply come from using larger number of parameters.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Indeed, we found in practice that we get similar results for \beta within some orders of magnitude, which requires significantly less tuning compared to a fixed \alpha.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] compare the sampling based method with these lower and upper bounds.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Last, the partial parameter sharing appears to make the learning problem easier.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The main usefulness of our guided approach is to directly capture some of the desired variations in the data.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] - To demonstrate that strong generalization performance of Meta-Dropout is not the effect of using larger number of model parameters, we doubled the number of channels for the base model and report its performances (MAML(x2)).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Specifically,  we use a 2-layer LSTM with hidden dimension 1024 and a word embedding dimension of 1024.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Note, that the conclusions keep unchanged.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 3. The mask loss proposed in the review is similar to our implementation, except that we make a distinction between an inner-mask control and an outer-mask control.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In addition, positional encoding in Transformer also allows us to easily model spatial locations of UI elements.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The correlation is significant, and thus validates the idea.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] -The M-step involves calculating a solution of a least squares problem (see footnote 2 in our paper).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We are definitely interested in any models that learns to do mathematics and symbolic reasoning, which would include more sophisticated models tailored towards doing mathematics (one could imagine models with working memory, etc).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We have also updated Table.1 to show the variance.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] As previously noted, this empirically also results in single cells of the LSTM being interpretable.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] If we think of the policy as a type of VAE that is learning an encoder (high-level) that is trying to learn a good latent goal (z) that will result in the low-level performing the desired sequence of actions.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Please also see our response to the Reviewer #3, comment #4.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Our scheme has less memory footprint than PipeDream because it does not stash weights.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] To evaluate RAN, one has to pick some quantifications.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Q1: “Alt-az” rotation is not a group.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] MAML(x2)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The advantage of this temporal correlation is shown empirically in Figures 2 and 3 where the PPO policies do not improve on learning the tasks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] For those datapoints where they can produce such a certificate	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Our mask regression losses consist of a first loss penalizing the mask from being active outside the densepose mask, and a second loss penalizing the mask from being inactive inside the densepose mask.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] But this description mischaracterizes the fundamental problem that we have identified and proposed a solution for.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Please see the updated paper for full details.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The candidate sentences generated by MT systems may contain words that never appear in the test set.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Also, this was because $P(D_i,I_i|D_{<i},I_{<i})$ is much easier to compute (and more efficient) than $P(D_i |D_{<i},I_{<i})$, given that the computation of $P(I_i| D_{\leq i},I_{<i})$ is in both case required for sampling ($P(D_i,I_i|D_{<i},I_{<i})$ involves a simple product while $P(D_i|D_{<i},I_{<i})$ involves a sum of products).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] - Good point, our reasoning here was mostly influenced by Pietroski et al.'s work and our intuition about the ability of convolutions to learn a local pairing strategy (see addition at the end of section 2.4).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] However, we discount models that already have their mathematics knowledge inbuilt rather than learnt (for example, this includes many of the models that occur in algebraic reasoning tasks, where the model learns to map the input text to an existing equation template, that is then solved by a fixed calculator).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Second, by elevating the hyperparameter one level up, we hope that the learning is indeed less sensitive to its specific setting.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We define the mutual information intrinsic reward as I(S^i_{1}, S^c)+I(S^i_{2}, S^c).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Intuitively, our overall approach that separates heavy hitters from the rest should still be beneficial to such query distribution.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] ii) We also show the correlation between the distance of two consecutive latent vectors m^{t-1} and m^t and, the KL divergence between the two corresponding policies KL(\pi(a|s^{t+1},m^t) || \pi(a|s^{t+1},m^{t-1})), i.e., how different the policy would have been if m^t didn’t change.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We compare our method to MADDPG, a popular centralized method that works by treating the problem as a single agent problem with complete information.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] It is equally principled to use a Transformer policy, since both would operate on the same sequence of observations.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] At the same time VAE requires encoder to be Gaussian non-deterministic, and random decoder, which is not the case in CWAE (as well as in a WAE model, see Tolstikhin https://arxiv.org/pdf/1711.01558.pdf).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Note that the blocks are automatically calculated to be the largest that fit into the GPU, such that the utilization of the GPU is maximized, so as to minimize the compute time.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] x4	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] (In fact, our first attempt at solving the problem was a much more complex algorithm which optimized the allocation of elements to the buckets (i.e., the whole hash function h) to minimize the error.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] The Edit Distance metric was designed by taking into account human factors in interaction tasks based on the key-stroke level GOMS models.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] In addition, we show how changing the corresponding guided knob affects the generated images.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] S controls how many sequences to generate to perform a (batched) policy update (i.e. S is the batch size), and we set S=1 for all tasks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Specifically, our Learned Count-Min algorithm achieves the same asymptotic error as the “Ideal Count-Min”, which is allowed to optimize the whole hash function for the specific given input (Theorem 7.14 and Theorem 8.4 in Table 4.1).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] Second, the fitness is based on absolute returns not differences in returns as suggested by Equation 1; this makes no difference to the relative orderings of z (and the resulting probabilities induced by the bandit), but it has the benefit that the non-stationarity takes a different form: a difference-based metric will appear stationary if the policy performance keeps increasing at a steady rate, but such a policy must be changing significantly to achieve that progress, and therefore the selection mechanism should keep revisiting other modulations.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] found.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] It is worth noting that some recent literature uses a stochastic estimation of the policy gradient with batch size 1 as well, and report strong empirical results [1].	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] We empirically found the two techniques significantly stabilize the controller training.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] 5. While we have not included such transfer experiments in our current work, we do believe that a model trained on enough architectures and tasks will generalize to new ones.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_answer [SEP] See Fig.2 and Fig.3(R) where vertical bars indicate variances.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] With just 12 classes, the dataset is not very suitable for such settings.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] In the practical framework, the algorithm player uses a neural network, and the adversary network tries its best to come up with a bad (but not necessarily worst) input each round.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] If one looks closely at Figure 2 (b), there are still blue and black histogram bars (denoting CIFAR-10 train and test instances) covering the entirety of SVHN’s support (red bars).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] For the other dataset suggested (VisDA), for synthetic-real adaptation, it is difficult to match the training paradigm of meta-learning.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] This is an interesting idea, but we are not sure it is applicable.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] Thus we don’t have all the clean theoretical guarantees anymore, but the intuition should still largely hold (as our empirical result suggests).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] As mentioned in the comment, this approach may eventually lead to finding optimal or near-optimal algorithms for a problem (not an instance of a problem) for which no algorithm is known -- but this is outside the scope of this work future work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] As our paper is primarily about the power and transferrability of our structural features for atomic tasks, we believe a detailed investigation of non-structural features is mostly outside of the scope of this work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] We could definitely investigate further why this mild ensembling yields a small performance increase, but we see this as tangential to the overarching points of the paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] This could be an interesting setting, but we don’t think this will work very well, as the tasks are themselves completely different. We would not expect a character recognition model to transfer to a object recognition task, as the visual features are very different.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] Typically, we desire several classes for meta-train, and several classes for meta-test, so that a variety of (e.g.) 5-way tasks can be crawn.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] To us this seems as a rather challenging setup, not just for the algorithms we’ve tested in this paper, but for any clique finding algorithm.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] (5) We know from theory that if the algorithm player runs a no-regret dynamic (e.g. MWU) and the adversary player responds with the worst input for the algorithm in each round, then the algorithm player converges to the optimal algorithm, and the uniform distribution over the adversary player’s responses gives the adversarial distribution.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-request [SEP] However, we cannot really follow this approach as the space of algorithms is infinite and we cannot run a MWU on this space, and in general it is also hard or impossible to find the absolute worst input in each round.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] We are grateful for your suggestions on the domain adaptation baselines, and fully agree that it is reasonable.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] 1.Thank you for the insightful suggestion.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] For the first and second comments, we appreciate the detailed example you proposed.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] If not, we are glad to address further.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] Thanks for pointing us towards this.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] 1. About image captioning.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] Please see the corresponding section in the revision.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] We hope this remove your concern.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] - Thank you for the helpful suggestion.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_social [SEP] Thank you for mentioning the existing learning curve modeling methods.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 1. The Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] > Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Q: method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Furthermore PTB is not a ""challenging"" LM benchmark."""	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 1. “how deep should a model be for a classification or regression task? “	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal."""	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Concerning the point "" It is not even clear that the final compression of the baselines would not be better."	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Response:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Re: thoughts on how this could be applied outside the context of sentence representations and classification	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 2. Improving adversarial robustness experiment.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 1. Our suggestion to mitigate the catastrophic forgetting looks a naive combination of well-known concepts. Thus, it is more system engineering than science.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Q5: “Why have you chosen the 4 operations at the bottom of page 4?”	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] - There are a few points here:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] - Numerical1:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] > And as mentioned above, it doesn’t seem likely that *any* image would work for this method.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Remark 3.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] It is conceivable that there could be more than one information of interests that get destroyed in a transformation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Comparison with SOTA models for counting and relationship detection	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] -- Comment on scale / speed for large instances of combinatorial optimization:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Q3: The evaluation section lacks experiments that evaluate the computational savings.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] .	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] - Q: Applicable to CNNs:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Additional question 2:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] > Also, the compared methods don’t really use the validation set from the complex data for training at all.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] We also responded to this suggestion in the public comment. For your convenience, we have copied our response here:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 2. Bulyan	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 5 and 6.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] > I disagree with the claim of practicality in the introduction (page 2, top). While training on one image does reduce the burden of number of images, the computational burden remains the same.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] “Lemma 3 is too trivial.”	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] “Domain Adaptation Baselines”,	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Concern 2: Experiments	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] In other terms, how this proxy help to tradeoff between exploration and exploitation ?	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 3. A deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] > All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] “- In the case of the search space II, how many GPU days does the proposed method require?	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] R: ""In the modeling section, authors use p(I|D) as q^D(.) in the Eq. 12, ...  Beyond this simplification, I am not clear if that is actually intended by the authors."""	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] The reviewer mentions it may be possible to construct counter-examples where the gradient updates will prevent convergence.	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 2b) -  ""The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method."""	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] [Response for 1]	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] “It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectives-1 dimensional surface with the population of parents. How could scaling be handled?”	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Regarding your comments:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Reviewer #2 suggests comparing with a “cost-aware” random sampling policy.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 3. Comparison of different feature extractors.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Specifically:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] R1 says that the randomized exploration used by the discoverer is underwhelming.	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] R2: ""this 'finishes' [Pathak et al., ICML17] to its logical conclusion for game-based environments and should spur interesting conversations and further research. In terms of actual technical contributions, I believe much less significant."""	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Additional question 1:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] R1 asks for experiments that do not use RAM state information.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Q: Relationship to optimizers with adaptive learning rates, and comparison between Adam and Adam-APO.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] === Regarding to the empirical results/experiments ===	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Q1 - ""... how general this approach would be? ...if rules contain quantifiers, how would this be extended?"""	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] > Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] *All or at least some of these decisions would need to be relaxed to make a convincing paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] [Q] The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them, give mathematical formulations, making it hard to the non-expert reader to understand what are these techniques and why are they introduced.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] (e.g. [1])	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] -- “proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.”	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Regarding the suggested ablation,	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Q5: Empirical results:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] - I would like to see some more interpretation on why this method works.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] >>> it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Q: Comparison with population-based training (PBT)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] [Algorithm design]	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] - DBPedia:	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 4. ""Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem."	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] R: ""The authors explain how they trained their own model but there is no mention on how they trained benchmark models"""	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] “Dramatic Domain Shift, Omniglot to Fashion-MNIST”	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 1.  “It seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?”	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] How big is the generalization gap for the tested models when adaptive kernel is used?	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] *	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Can this method learn more factor than just two?	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] It is thus unclear whether the advantage can be maintained after applying these standard regularisers."""	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] - FB15K:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 1. Multi-Krum	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] In particular, we will present the following examples of the learned rules from the considered (real-world and synthetic) datasets:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] - It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Re: Utility of the methods is a bit unclear	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 5. ""The baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm)."	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] * models that get scores in the ~80 ppl range for Penn Treebank are important.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 2) Hyperparameter selection	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] The reviewer also points out, that the evidence lower bound ELBO, when used with a notiGaussian prior results in case of VAE in a generally analytic formula.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] - Numerical2:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Q1. My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] It might be more fruitful if these linear combinations were learned or sub-senses of words	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] 5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Q3: If the decomposition is also useful for trigger patterns that are not necessarily regular shapes?	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] How this proxy incentives the agent to explore poorly-understood regions?	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] The reviewer noted that “besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Domain Adaptation Baselines + Other datasets	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] Responding to R1's additional feedback:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] What if the desired factors are not clearly disjoint and collectively exhaustive?	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] “About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.”	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_structuring [SEP] ""(1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective."""	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We will attempt to make the writing more concise.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] >	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We can clarify this further in the revision.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] In any case, we will discuss the extendability of the framework in the paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] The updated paper will change the emphasis, and clarify that a closer, more faithful, learning progress proxy remains future work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] Note, that regarding the methodology there were some misunderstandings (which we try to avoid for future readers in the revised version).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We will also clarify that the little phrase “After initial experimentation, we opted for the simple proxy…” implies quite extensive experimentation with other plausible proxies that looked promising in individual environments but were not consistently effective across the suite of Atari games.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] 3) For the experiments, we will do some modification and improve our network.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We will clarify the sentence in section 2.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We will keep updating the paper and conducting more thorough experiments.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We agree however that a clearer introduction of the terminology would be clearly helpful and we plan to add this to the final submission.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] Will try to make it more clear.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] Thanks for carefully reading our manuscript and noticing this typo which we will correct.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We will update the manuscript with these additional results and discussion and post it shortly.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] Comment 2:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We are now running additional experiments with a deeper encoder and with more filters.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] According to the Reviewer's comment we will extend Section 5 on experimental results by showing more detailed analysis.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_by-cr [SEP] We will release all code and trained models for reproducibility.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] We say that a function is symmetric if we swap the positions of variables, the function value does not change.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Again, CNE outperforms all other methods in accuracy by a wide margin, and is substantially faster as well.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Therefore, we did not include a comparison with Sparsely-Gated MoE in our article and only compare with full softmax.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Another contribution of our paper is the discretization algorithm.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] See also next point.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] This result, combined with the acceleration effect, justifies the empirical success of the replica exchange Langevin diffusion in practice.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Thus, GBDT can learn the stable and robust feature combinations.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Computational complexity is discussed in detail in the manuscript though, and is certainly not higher than competing methods (in part thanks to the low mathematical complexity of the model).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] It reflects the benefits of replica exchange.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] 2) This comment is not entirely correct and we would like to apologies for any confusion in the paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Also, we would argue that the KL-divergence, rather then introducing noise, allows us to avoid collapsing classes which we would claim are due to dying neurons (again, there is not loss/mechanism drawing the auxiliary labels to be the same as the primary ones).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] This is the reason why we have selected and used them for our experiments.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Please check the degraded images in Table 3.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] You are correct in assuming that other discrete building blocks could be more fruitful, but, we chose language modelling as a setting, not a task (see general response) as such, the building block chosen was the word. We could have chosen sub-words, or characters but the goal here is not the get the best possible language model but to understand a property of the mechanism.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] To find top-k predictions, we only search a few subsets.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Although the property doesn’t hold if one performs multiple alt-az rotations to the input spherical image, it is still valuable because we assume the different SO(3) orientation of an input 3D shape is from a composite of an azimuthal rotation and an alt-az rotation, the azimuthal rotation is treated by data augmentation and the single alt-az rotation is treated by the network equivariance and invariance.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] We think it is interesting and useful.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] In fact, a better way to justify this is to note that I^u provides an upper bound for the mutual information subject to the constraint of known variance of marginal distributions of X^(j)_{t-1}, and the upper bound is reached with the diagonal Gaussian distribution, as is proved in Appendix C in the revision.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] This leads to better generalization ability for test tasks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] The optimal CR bound and the adversarial distribution are the same for both cases, and the optimal algorithms basically have the same structure.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] In our approach, each agent needs to embed these observations (along with their actions) before sharing with other agents.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] It takes a dozens of minutes on a 100 nodes cluster.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Our paper is the first one to analyze the discretization theoretically.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] In this case, the derivative of chi^2 divergence is strictly boosted, and hence, the convergence is accelerated strictly.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] This means that for a single alt-az rotation of input spherical image; the output of a convolution layer will rotate in the same way.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] However, this is the first work to present a double-gradient method for auxiliary task generation, and we believe that it is important to present the success of this initial method now given how simple and general it is, and then fine-tune other aspects in future work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] The perplexity becomes a way to observer the effect of a mechanism and not the goal itself.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Furthermore, as we also explained in Reviewer #3, the hyper-parameters for defining a hierarchy is not critical, and we can choose an arbitrary hierarchy whilst still achieving better performance than baselines.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] In most experiments, the total time cost of GBDT part in TabNN is about several minutes, while the NN part often needs several hours for training.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] BadGAN has already theoretically proved that complement data are helpful for semi-supervised learning.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] >>> As mentioned in the main response, the proposed TPN is not a mere combination of CNN representation learning and label propagation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Your thinking of ‘semantically probable’ exchange of information is interesting.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] DS-8       | 0.257 | 0.448 | 0.530 | 2.84x |	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] The theoretical insight in this paper comes from the recent advancements in using a double gradient, such as in MAML [1], or understanding what makes a good auxiliary data sampler [2].	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Next, we wonder how would the reviewer correct the confounds mentioned with regard to the clique experiment.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] In this paper, we demonstrate	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] The use of an outer gradient for auxiliary learning is our key novelty, and has not been used in any works before.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] We note that it is possible to compress each agent’s actions/observations before they are sent to a central critic.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Note that none of the tested methods were given enough samples even to recover the graph itself, as most graphs have more than 100 nodes, and we’ve allowed only for 100 |V| samples in each execution.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] compared to discrete AdWords in terms of difficulty	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Consider a case with high-dimensional image observations.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Notably, Cakewalk approaches the performance of the best clique finding algorithms that directly search a graph, and which are tailored to this specific task.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] The competing models do make use of validation set C_p^{val} from the complex data to select amongst the most important hyperparameters of their model -- which is equivalent to what we did in our initial formulation, and favors the competing methods compared to if we use C_r^{val} for hyperparameter search instead.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] In this paper, we establish the linear convergence rate for the discretization error, which is highly trivial since the process has state-dependent jumps.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] - The datasets we used are as large as the datasets used in other related work in the area.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Actually, the update of the generator depends on the improvement of the classifier for the *principal* labels on the *meta-training* data, i.e. the improvement in generalisation to unseen data.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] To the best of our knowledge Freebase and DBPedia are the only standard KGs with numerical values [Garcia-Duran et al., 2018] used for the evaluation in state-of-the-art works.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] ).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Instead, the objective is designed in a manner that provides information even for partial solutions, thus allowing the tested algorithms to gradually improve the objective.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] This trait makes our approach (and other centralized-critic/decentralized-policy approaches) useful in situations where one can train in a simulation where communication is less taxing, but deploy in the real world, where communication may be more challenging.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] All the algorithms are evaluated on the DIMACS clique dataset which was published as part of the second DIMACS challenge which specifically focused on combinatorial optimization.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] This is a novel contribution since prior works have relied on learned features as a crucial requirement for good performance [Pathak et. al. ICML17].	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] This is fundamentally different from Sparse-gated MoE. We divide the output space into multiple overlapped subsets.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Our (github-shared) code runs in a few dozens of seconds per learning on a standard laptop - but the goal is mainly to be able to test all parameters.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] This being said, Figures 1 and 3 now show the clear qualitative advantage of using homeostasis in unsupervised learning.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] We actively argue that the minimal adversarial distortion is not a reliable measure of neural network robustness in many scenarios, as it is dictated by the position of a single violation, and conveys nothing about the amount of violations present.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] We would also like to re-emphasize the fact that our final trained policies are decentralized and do not require any information exchange between agents.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Indeed, since the release of our paper, there has been some follow-ups on using random features for exploration in achieving state of the art results on hard exploration games when combined with extrinsic reward (in the interest of preserving anonymity, we don't include the references here).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] that,  using our unseen data, the proofs in badGAN still can be satisfied but in a more concise way.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] We clarify that we use the RAM state information for the state abstraction function, which is a fundamental component of our work, so it is not possible to run experiments without this RAM information.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Preserving the inner product means that the distribution of the features is not biased, if we keep adding words to the dictionary, the performance would degrade gracefully with the amount of compression.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Yes, we obtain faster convergence, but as an epiphenomenon of the better efficiency of our adaptive homeostatic algorithm.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] In practice, it is impossible to simulate the continuous process directly, and discretization is necessary.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] We believe this investigation would allow random features to be seen as an easily reproducible and strong baseline for future investigations of feature learning in exploration.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] It was designed to increase the model expressiveness.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] In this respect, this dataset is an important benchmark for clique algorithms very much like CIFAR10 and CIFAR100 are for image classification methods.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] First, the goal is not faster computation on a CPU.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] While in full softmax or MoE, the complexity is linear with output dimension.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] To the best of our knowledge, this phenomenon has never been observed by previous literature, including Dupuis’s paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] 1. We aimed to provide a broad variety of example applications (playing tennis, walking, fencing, dancing), while mainly focusing on the most complicated (tennis) application, for a thorough analysis of our method.	1.0
"Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] In Table 1 and Table 2, the proposed TPN achieved much higher accuracy than the mere combination model (referred to as ""Label Propagation"")."	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] => In the light of the comments on originality and significance, we would like to highlight our finding that random features perform quite well and at times as well as learned features across many environments.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] This allows obtaining a posterior $p(c|y)p(z|x,y)p(y|x)$, which in fact guarantees the disentanglement of the factors c from z while preserving the generative strength of the model.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] 2) the learning of GBDT is just based on statistical information over full dataset.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Originally, to justify the I^u=1/2 \sum_l log(1+Var(X^(j)_{t-1,l})/ \eta_{j,l}^2) term used in our experiments for estimating mutual information, we used diagonal Gaussian assumption for X_{t-1}^(j) in the experiment.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Thus, we assume computational complexity is meant.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] With regards to mathematical complexity, we believe the model is actually rather simple (see also other reviews).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Our algorithm 1 minimizes the empirical learnable noise risk (Eq. 4), which does not assume that X_{t-1}^{(j)} follows a diagonal gaussian distribution.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Since the objective is the only source of information for such algorithms, an all-or-none kind of objective would not be very useful.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Response: As we discuss above, we believe our experimental setup and analysis is sufficient to demonstrate that our atomic representation transfers much better across atomic tasks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] If $\theta=0 or \theta=pi, and “\phi \neq 0$, this rotation belongs to the azimuthal rotation in SO(2) group.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Furthermore, we believe (although acknowledge that this is subjective) that curiosity-driven questions about how the information is encoded are interesting: while they might not be useful in a way that is easily measurable by quantifiable metrics, they provide insights that can help guide future work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] The inner gradient is based on the standard auxiliary learning loss as proposed in other works, whereas the outer gradient uses this inner gradient to actually learn the auxiliary tasks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] we agree with the advice but not with the justification.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] As expected, the Sparsely-Gated MoE does not achieve speedup in terms of softmax inference.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Competitors rely on amino acid-level features that fail to capture specific atomic positions but can be better when the structural is less detailed or accurate.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Specifically, we demonstrate that a strict positive term caused by the replica exchange is added, if the density ratio between current distribution and limiting distribution is not symmetric.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Our motivation is mainly to understand biological vision and hope this would percolate to ML.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] The purpose of our work is not to achieve state-of-the-art performance simply by incorporating the latest network architectures and optimisers.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Our method, on the other hand, decouples the problem of learning disentangled latent representations and high fidelity generation into two separate problems by introducing a hierarchical structure (sub-graph c-y) that is trained separate from the rest of the model.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] While in our work, we adaptively construct the graph structure for each episode (training task) with a learnable graph construction module (Figure 4, Appendix A).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] These images are damaged so badly that TV cannot recover any meaningful thing.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] This notion of compressing embeddings prior to sharing across agents does not fit as naturally into the competing methods.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] In such a regime, we can attribute any performance gains to the algorithms themselves, and not to any prior knowledge that is reflected by the structure of some complex sampling distribution.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Our setup naturally allows for this.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] This now certainly allow to understand *why* convergence speed is a good indicator ---not for an advantage on the running speed on a classical CPU--- but rather in showing that this allows a more efficient dictionary learning overall.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] And since it is slower than the standard softmax by definition, we chose not to compare with it in the paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Instead, we provide a novel general framework for automating generalisation, and show that when used with standard classification networks across all baselines, our method performs the best.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] The original label propagation constructs a fixed graph (Eq (1)) to explore the correlation between examples.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Over the years, this dataset has become a standard benchmark for clique finding algorithms, and results on it are regularly published.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] The main research question we try to address is whether algorithms that only rely on function evaluations can recover locally optimal solutions.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] First, we’d like to emphasize that the clique problem studied in the paper is far from being a toy problem.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] We have not used SPAMS in this work as we could use the similar methods which are used in the sklearn library.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Moreover, (not in this case but) the architectures used to achieve better scores on given datasets are so over-parametrized that it's hardly reasonable to assume that the improvement justifies the cost of accommodating huge models overfitted to a particular dataset (and sometimes to a particular dataset configuration)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] In a situation where information exchange between agents is expensive, even during training, we can select a sufficiently small embedding space such that performance and efficiency are balanced.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] One may arguably say that the optimal algorithm for the fractional problem has richer structure as in the fractional problem the action space is much larger as we can fractionally assign each ad to many advertisers.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] As to AdWords, although the discrete problem naturally corresponds to the real world scenario, we do not consider fractional AdWords below the bar	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] As a handcrafted prior, TV performs much worse than our data-driven baseline method in these tasks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Dupuis’s work includes similar results but in a different form.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] We have also added to our discussion, making clear that our method represents a significant advantage over competing methods when detailed atomic information is available.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Our algorithm addresses speed up in softmax inference.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Our work is for softmax inference speedup	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Our intention wasn’t to say that we can save compute time, but data collection effort (which is also a practical issue in some applications).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Therefore, compared to badGAN that requires extra PixelCNN, DSGAN saves more computational memory and is time-efficienct.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] To the best of our knowledge, no one has discussed the discretization of replica exchange Langevin diffusion before.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] It cannot achieve speedup because each expert still contains full softmax space as we mentioned in the background section (page 2 line 21st) and method section (page 2 last 4th line).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] A2: We think you underestimate the difficulty of those restoration problems.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Moreover, the quotient SO(3)/SO(2) is isomorphic to $S^2$ and to avoid the ill-definition on the two poles (the two degenerate points), we will add a constraint to the alt-az rotation, i.e. $\phi=0, if \theta=0 or \theta=\pi$. This is because, when the altitude rotation is zero or PI, the azimuth rotation is meaningless in a alt-az rotation and is therefore fixed as zero.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Thus, the optimal auxiliary labels are not the ground-truth labels for the principal classes, since this would make both terms in the minimisation for $\theta_1$ (the second equation in 3.2) identical and not allow any leveraging of the meta-training data.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Perhaps a non-orthonormal basis would also work if the network compensates for the different distortions in the inner products.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] while Sparse-Gated MoE (MoE) was not designed to do so.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] In terms of the sampling distribution, as our focus is on the update step, we decided to use the simplest possible sampling distribution we can think of.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] Providing a controlled experiment is always challenging, though the elements mentioned by the reviewer were specifically selected as to reduce various confounds.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] This result is often overlooked in dictionary learning and is a first novel result of the paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] The impact of our approach might appear to be rather modest, since these KGs still have only a limited amount of numerical information.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] We also show that the derivative of chi^2 divergence is boosted.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_reject-criticism [SEP] We explain why in the general response: our goal is not to get good language models, but to use language modelling as a setting to test a property of a mechanism that is proposed.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] Fortunately, we were able to find an existing solution that satisfies the requirements: CycleGAN. Any other domain translators that satisfy the conditions can be used or newly studied.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] By introducing class conditional priors induced by the mutual information maximization, DiVA yields class-wise discriminative one mode Gaussians for latent variable z.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] Naturally, DiVA can conduct both class prediction and class conditional sample generation with one integrated model.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] This is why our comparisons focus on similar methods, that also use multiple sources at the same time.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] 2. We are sorry maybe we didn’t explain it very well in the paper, but this is a misunderstanding.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] We think we can still have the equivariance property but only for single alt-az rotation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] With respect to your concern over scalability, the need to input the actions and observations of all agents in the value function (i.e. centralized value function) limits scalability only during training time, and it is a necessary measure to reduce the non-stationarity of multi-agent environments, as discussed in previous work [1].	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] They are generally more competitive than single-source methods.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] However, our main focus is *multi-source* to single-target adaptation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] To narrowing the distribution gap, we needed a solution that satisfies two conditions (also described in section 5):	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] Compared to the rest of the model, these projections add very little overhead compared to the rest of the model.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] This leads us to a more theoretical formulation for classification-regularized VAE.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] Thus, we defined the two domains: real domain and sample domain.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] With the solution, we could make a breakthrough for GR-based methods.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] - Yes, there are many methods that conduct single-source to single-target adaption in the literature.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] This is important work to be sure, but we view it as predominantly orthogonal to ours, for which we define robustness differently, as the “volume” of adversarial examples rather than the distance to a single adversarial example.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] Since they do not consider the conflict between the unit Gaussian prior and discriminative loss for the latent variable z, their models generate ambiguous samples that negatively affect the performance of incremental learning, which is discussed in section 4.1 in our paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] The second open question was that why GR-based algorithms suffer from serious catastrophic forgetting in natural image datasets, even though generated samples are not completely noisy.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] Being a new problem setting, designing appropriate baselines can be challenging.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] 1. We should translate only the style (a global pattern of a specific domain) as keeping outline patterns of given images.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] The simulation tasks contain robotic humanoid characters that need to learn how to navigate given egocentric vision.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] 2. We should consider an unpaired domain translation between real and generated images because the generated images are sampled randomly.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] However, this is not uncommon in scenarios like machine translation, where there are hundreds of potential language pairs that could be used as candidate tasks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] No other simulation is available that combines these challenges.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] We assumed that this is due to the vulnerability of neural networks [3] triggered by different distributions of pixel values between real and generated images.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] Notice the definition of alt-az convolution do not use any composite rotation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] To the best of our knowledge, this is the first successful approach for a GR-based algorithm to start to resist the catastrophic forgetting problem with a natural image dataset.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] As we explained at the common response, we started our research from clear open questions.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] Our goal was not to make the model model expressive.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] We agree that, while the differences are statistically significant, they are minor.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] The primary goal of the projections is to project all embeddings into the model dimension d so that we can have variable sized embeddings.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] Use on downstream tasks: we believe that capturing syntactic relationships using a tensor can be useful for some downstream tasks, since our results in the paper suggest that it captures additional information above and beyond the standard additive composition.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] We were using that word technically, but do not want to give the wrong impression.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_mitigate-criticism [SEP] Our first open question was that why other GR-based algorithms [1, 2] assume unit Gaussian priors even though they integrate classification loss into their VAE formulation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] In a future work, there could be an auxiliary task method that can create continuous latent variables.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] We can definitively replace GBDT with other methods, such as feature correlations, as long as they can achieve better performance then GBDT.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] As off-policy methods have the potential to be much more data-efficient, we will compare in the future how NADPEx performs comparing with auto-correlated noise in [4] and separate sampler in [5].	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] 6. Maybe a follow-up to consider for the coffee test is to adapt from using a coffee-brewing machine to making it from scratch :)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] Doing without them is an interesting future direction though!	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] We will keep trying to investigate such massive architectures in the future.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] In the new revision, we will use the term alt-az rotation in “quotient SO(3)/SO(2)”  instead of alt-az rotation group.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] Checking cases where the learner incorrectly classifies the image in the second time step is sound and will be inspected in future work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] Yet analysing how well such approach performs in practice is still an open problem, which we leave for future work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] Since this is the first paper on this topic, we chose to focus on introducing the problem and providing Transformer-based approaches as a baseline for future work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] Substantiating this claim in the general case will likely require a large-scale study, which we plan to perform in the future.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] - Eval metrics	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] For example, we could measure the privacy by the hidden failure, i.e., the ratio between the background patterns that were discovered based on RAN’s Encoder output, and the sensitive patterns founded from the raw data, in an object recognition task.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] We would definitely explore this further, and add a detailed analysis on computational efficiency of different methods to reduce dimensions.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] In the future work, we would like to explore how to find the optimal hierarchy in an automatic manner, or provide an alternative solution on building a general type of auxiliary tasks (such as regression).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] Similar to residual connections, gating, dropout, and many other recent advances, more fundamental understanding will happen asynchronously and should not gate its adoption and usefulness for the community.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] We hope that this paper create interesting open problems for future research.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_future [SEP] Third, in the future, we will evaluate RAN using other quantifications of privacy as well in a defined application.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] To show that we can easily include these features, we have included in our appendix some results including non-structural features.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Therefore, we updated Figure 7 in original manuscript to Figure 6c in the updated manuscript according to the new data.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] The recursion formula for G_t^{(i)} is shown in equation (9).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We conducted additional simulations to evaluate the runtime benefit of the proposed method compared to that of the method in [1].	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have updated the paper for this.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] This is now clearer on our quantitative and visual results in the “Experiments” section.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] b) We included more baselines (e.g. Mixup, VIB, and information dropout), and show that our meta-dropout largely and consistently outperforms all of them.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] - We added a few sentences to the end of section 2.4 on our speculative intuition regarding what strategy a model may prefer, and we do indeed think that a pairing-based strategy is plausible for convolution-based networks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] As you suggest, we use MS COCO Image captioning dataset to learn a lookup table and apply it to the EN-RO translation task to do the quick evaluation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Evidence and comparison to other methods on disentanglement is provided in  Table 9 in Appendix G, where we visualize the correlations between our embedding dimensions.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] The results are in Appendix A.3 of our revised version.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] [More Than Two Attributes] We now use 32 dimensions for the CelebA dataset and 10 dimensions for the 2D shapes dataset.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] In contrast, APO only requires a simple grid search over lambda, and all other hyperparameters can be kept at their default settings.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] These include updating the draft to include (i) a detailed analysis of edits performed on SNLI, (ii) results on various datasets using an ELMo based classifier; (iii) concerning your question about larger Bi-LSTMs, we had tried a large Bi-LSTM but it overfit badly.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] - Indeed, our intent in the statement of Theorem 3.2 was to describe the scaling of the solution with respect to those two quantities, but it can be misinterpreted. We have clarified it in the new version of the paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Comparison to Arora, Liang, Ma ICLR 2017: we appreciate the suggestion to include a comparison with the SIF embedding method of Arora et. al., as this method is also obtained from a variant of the original RAND-WALK paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] 3. We have added the values for chosen $\gamma$ in the updated version (see caption of Figure 1).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We now have added related work about video compositional methods in section 2.3 in the second version of the paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have now added additional domain adaptation baselines, designed in the setting suggested by Reviewer 2.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] The assumptions used for the simulation and analysis data have been updated in Figure 6c of the revised manuscript.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] a) We replaced the previous FGSM attack with stronger PGD attack (200 iter.), with $L_1$, $L_2$, and $L_\infty$ norm constraints.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Theorem 2.5: Sorry G_t should be G_t^{(i)}. We will correct this typo in the next revision of this paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Accordingly, based on your suggestions, and suggestions from other reviewers, we have tried to expand the baselines substantially (specifically, we include three state of the art Domain Adaptation methods as baselines – RevGrad [1], ADDA [2] and CyCADA [3]), and our proposed methods outperform them.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] In the revision of the paper, we gave the final derivation you suggested, since it is better for comprehension (It also allowed us to discuss more precisely on what is optimized by considering the given gradient update), and we discuss about its implementation in the appendix as an explanation for the algorithm 2 (that only slightly changed to correct the aforementioned mistake).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] For t = 0, G_t^{(i)} are all initialized to some value.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] To address your question about Adam, we added experiments for tuning the global learning rate of Adam with APO in appendix Section G, Figure 14, where Adam-APO achieves better performance than Adam with a fixed global learning rate, and achieves comparable performance as Adam with a manual schedule.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We used PBT and APO to tune the learning rate of RMSprop while training a ResNet34 model on CIFAR-10.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Nevertheless, we agree that our findings have mostly a theoretical value, so we have adjusted the wording to reflect that.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] However, due to the reviewers’ concern in the updated draft we compare spectrally-normalized networks to networks with the same architecture except with weight decay, dropout, or batch norm in Appendix A.2.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] However, we also trained our model using verb-object pairs, and we have updated section 5 as well as the appendix to include these additional results.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have updated the paper with these recommendations in Section 7.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] The results are included in the revised manuscript.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] To be a bit more specific, obviously a blank image would not work, and textureless images would probably not work well either.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] R)we added an experiment to test the generalization	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] In the paper, we include many details on the environment rewards and design as we consider these simulation tasks part of the contribution of the work.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Taking your comments and comments from other reviewers into account, we have made improvements to the experimental section.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We also updated our “Probabilistic Interpretation” section with analysis on how the contrastive loss helps us to learn a disentangled representation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] 1.We have added more details about sampling strategy to section 3.1 in the new version, with mathematical definition and dimensionality explicitly described.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We added a footnote on this to section 2.4.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Ad. 3) (CW usefulness) We have verified how the Cramer-Wold metric works as a Gaussian goodness of fit,	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] During the rebuttal period, we conducted additional experiments on adversarial robustness as you suggested:	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Following your suggestion, we evaluated the Byzantine settings Multi-Krum (Blanchard et al 2017) and Bulyan (El Mhamdi et al 2018 ICML).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] In the revision, we have added the following explanation and justification on privacy quantification in Section 1, Section 2, Section 4 and Section 5.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Thank you for this suggestion, we have now clarified this connection in Section 3.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We also report mean and variance of losses for different seeds.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] -> Added a discussion on CNNs in the new “Scope” Section in the revision	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] See details in Table 1 (bottom) in the revision and our post about common concerns.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] It is true that we did not quite prove that, so we have reworded the text to tone down this claim.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We added an explanation of differences of our method with those works.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have revised the statement for accuracy -- we observe in our experiments, for DCGANs with the vanilla GAN objective (JSD), more generator training than discriminator training generally performs better (but this may not be an effective hint for other GAN objectives as they behave very differently).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] For example, in section 4.2, our method correctly identifies important causal arrows, while the four other comparison methods either have more false positives and false negatives, or completely fail to discover causal arrows.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] You are right! We have revised the baseline experiments with Bayesian models so that they either use \lambda = 1 or the settings that the original authors recommended, i.e. we only tune \tau in KFLA and set \tau = 0.01 in noisy-KFAC as these are the settings suggested in their respective publications.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] For PBT, we used a population of size 4, and chose to exploit/explore after each epoch of training.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Therefore, in the new revision, we will add the constraints to the definition of alt-az rotation and make it one-to-one corresponds to the set points on $S^2$. See A1.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Nevertheless, we performed the suggested ablation by swapping BERT for GloVe embeddings (300 dimensional) and found that NER performance dropped from 89.46% to 40.33% and RE performance fell from 66.83% to 14.44% on the test set of the ConLL04 corpus (note that we had to increase the learning rate by 10X to get the model to converge).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Practitioners can choose to optimize an upper bound of the learnable noise risk for better efficiency (as is also used in the experiments in this paper), or use differentiable estimate of mutual information for better accuracy, as has also been pointed out in the paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We changed Section 3.1 to explain that the posterior dependence on pairs of adjacent frames is to have temporally local latent variables that capture the ambiguity for only that transition, a sensible choice when using i.i.d. Gaussian priors.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] As reported in their paper, the SIF embeddings yield a strong baseline for sentence embedding tasks, and we find the same to be true in the phrase similarity task for adjective-noun phrases (not so for verb-object phrases).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We computed idf scores on the monolingual English corpus released by WMT18 and experimented with BERTScore computed with the Roberta-large model.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We decide to just call it PGD in the revision to avoid confusion.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] In the revision, we have also added a more detailed comparison with other methods in sections 4.2 and 4.3, showing the strength of our method.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] * Shown that APO is competitive with manual schedules both in terms of test accuracy and training loss with ResNet34.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] For instance, in the updated version of the paper, we show that the learned policy employs similar learning rate and weight decay rate adjustment schemes across very different tasks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] However, we find that we can improve upon the SIF performance by addition of the tensor component from our model. (We note that we have just used the tensors trained in our original model; it is possible that combining the model in SIF and syntactic RAND-WALK more carefully could give even better results.)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] d) We further show that the learned perturbation from our Meta-dropout also generalize across different types of adversarial attacks with $L_1$, $L_2$, and $L_\infty$ attacks.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We tried multiple exploration strategies, and found that it was critical to set the probability of resampling a learning rate from an underlying distribution to be 0; otherwise, the learning rates could jump from small to large values, and yield unstable training.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have updated Table 2 and the discussion in section 5 to include these additional results.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We added this explanation in the new version of the paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] In Section 3.4, we clarified what frames the discriminator takes, and in Section 4.3 we added a description of the deterministic version of our model.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have updated the draft to include this detail.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Furthermore, we present an experimental investigation of these different objectives (Sec. 6.4).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We found that  APO substantially outperformed PBT, achieving a lower final training loss and equal test accuracy in much less wall-clock time; this shows the advantage of gradient-based methods for tuning learning rates, such as APO, compared to evolutionary methods based on random perturbations such as PBT.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] All the above points have now been made much clear in the new version of the paper, in particular we added an appendix dedicated to the swimmer benchmark.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We now present the validation accuracy instead in Appendix F.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have included  a comparison with method [3] (see general comment to all reviewers).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have updated the introduction to rephrase and clarify the lower bound claims.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Following your suggestion, we further studied idf scoring.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have added a comparison between APO and PBT in appendix Section H, Figure 15.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We tried a simple baseline that at each layer, we sample a conv operator with b-bit precision with probability	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] To demonstrate CNE's superior scalability, we included another network with around 200.000 nodes and around 1.000.000 edges (http://snap.stanford.edu/data/loc-Gowalla.html), run on a basic single CPU laptop.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We clarified this in Algorithm 1.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We did follow reviewer recommendations and performed experiments with LSTMs and QRNN (slightly faster) along with WikiText (which is larger but not intractable), unfortunately we couldn't accommodate all the analysis and changes in time.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have included these results in Appendix A.6 of the revised version.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] On evaluating general-purpose models only, we may have phrased this badly in the paper, and have updated it.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] For population-based training, one must carefully select many hyperparameters, including the size of the population, the perturbation strategy (e.g., randomly perturb the learning rate by multiplying it by 1.2 or 0.8), the exploration interval (e.g., the number of training iterations to run before exploiting other members of the population).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] For large-scale tasks, we use memory replay to alleviate correlations in online settings (please see Algorithm 2 in Appendix A.1 in our revised version).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] c) We added more detailed descriptions of the adversarial meta-learning baseline and in-depth analysis on the results.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] For every model with an embedding space (i.e., all except CTIC), we set its dimension to $d=50$ (larger dimensions induce a more difficult convergence of the learning without significant gain in accuracy).	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Therefore, the assumption of diagonal Gaussian assumption is dropped for the experiments in the revision.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Following the suggestions, we added additional results for the associative recall task for many network variants.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have added a new section (Sec. 4) to discuss the differences between the objective used for CDN, when performing variational inference for BNNs, and in the variational information bottleneck (VIB) framework.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have thus modified the text to make it clear that we mean “statistically significant” only.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] The added/modified text are highlighted in the blue color.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We redo the visualization in Figure 6 to make the gains provided by SN clearer.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have included a more detailed analysis with new visualizations in the updated paper.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We added the tutorial from Hansen (2016) as the reference for the common choices for setting \lambda_i in Equation 1, 2.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] The generalization to different types of attacks is an important problem in adversarial learning, and most existing models fail to achieve this goal.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have extended the contribution part (in the introduction) and added Sections A and B to the Appendix, to make things clearer.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have added the detailed PPO-based training algorithm in Appendix A.1.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have added the labels in the text and Table 1 to make this more clear.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We have implemented this attack and added it to our evaluation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] As suggested, we have added further analysis of failure cases, and describe strategies for negative mining from these examples.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Response: We have removed the SASNet ensemble from the paper, as it was based on C_p^{val} and confuses the point we are making about minimally relying on C_p for training and validation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] Just for additional reference, we tested Sparsely-Gated MoE with different experts in PTB dataset; we compared the results to DS-Softmax.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We added the more experimental data of runtime analysis to address the Reviewer's main concern.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We see that using SN can improve the test performance by over 12% for some FGM, PGM, and WRM cases.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_done [SEP] We also removed the adjective “challenging” as regards PTB.	1.0
Need more interesting problem definition [SEP] rebuttal_concede-criticism [SEP] Reviewer #1 is absolutely right -- we don’t know yet how to scale this to more difficult combinatorial problems.	1.0
Need more interesting problem definition [SEP] rebuttal_concede-criticism [SEP] We acknowledge that our presentation focused more than necessary on ideal scenarios that use learning progress LP(z) while the practical version used a (maybe disappointingly) simplistic choice of proxy f(z).	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] Auxiliary tasks are an important component of RL and exploration methods, however, in this work we chose to focus on the most generic setting with minimal assumptions about the environment: providing raw observations in response to actions.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] Both are valuable in different circumstances.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] It is a staple introductory problem because it is elegant and illuminates the essential difficulties in designing online algorithms: there is a nearly-trivial factor-2 competitive algorithm (rent until you’ve spent $B, then buy, so even if the ski season ends the next day, you’ve not spent more than twice the least possible amount), but the 1-1/e competitive ratio algorithm is quite creative and subtle, and serves as an introduction to the richness of the field of online algorithms.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] Our paper simply shows ONE example for this.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] In particular, it generalizes bipartite matching, historically one of the most significant combinatorial optimization problems (led to the development of the classic Hungarian method, see https://en.wikipedia.org/wiki/Hungarian_algorithm).	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches).	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] In environments with privileged access we expect auxiliary tasks to benefit both curiosity-driven and extrinsic-reward-driven RL methods.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] In some sense, this poses us the ideal challenge: can ML approaches discover creative and subtle “solutions” (in our case, an algorithm)?	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] In fact, the Karlin et al. (1986) paper also introduced the notion of competitive analysis of online algorithms, and  is probably the most-cited paper in this field.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] It is easy to describe in that it has a single hidden parameter (the length of the ski season) and a single revealed parameter (the cost of buying).	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] The ski-rental problem is often the first problem studied when teaching online algorithms, but it is certainly far from a “toy problem” when we wish to learn an algorithm from scratch.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] In contrast to these works, we don't assume privileged access to the maze environment in the form of depth estimation or loop closure supervision.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] Learning representations from “big data” (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4).	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] Sure, this is not the scale of 80 million tiny images; but one wouldn’t ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] The AdWords problem considered is actually a difficult combinatorial problem, and is an archetypal online combinatorial optimization problem that captures the class of problems solvable by one of the most powerful techniques in this area -- primal-dual algorithms, which have led to the state-of-the-art approximation algorithms for numerous hard online (and offline) optimization problems.	1.0
Need more interesting problem definition [SEP] rebuttal_reject-criticism [SEP] Representation learning, the topic of this conference, has many facets.	1.0
Lack of analysis [SEP] rebuttal_concede-criticism [SEP] Thank you for suggesting additional experiments to better understand the behavior of the scratchpad component.	1.0
Lack of analysis [SEP] rebuttal_concede-criticism [SEP] As you mentioned, our solution requires significant expenditure (both financial and human capital)	1.0
Lack of analysis [SEP] rebuttal_concede-criticism [SEP] On the one hand, as pointed out, as soon as the preimage is no longer only a point itself it is no longer applicable.	1.0
Lack of analysis [SEP] rebuttal_concede-criticism [SEP] 1. You are right.	1.0
Lack of analysis [SEP] rebuttal_concede-criticism [SEP] compared to simply labeling data	1.0
Lack of analysis [SEP] rebuttal_concede-criticism [SEP] As correctly observed, the application of our algorithm to classify the preimage of one data point of one ReLU layer does not easily translate to more than one layer.	1.0
Lack of analysis [SEP] rebuttal_concede-criticism [SEP] .	1.0
Lack of analysis [SEP] rebuttal_concede-criticism [SEP] We appreciate your acknowledgment of our contributions and pointing out that the properties may only need to be satisfied at some critical points during training deep neural nets.	1.0
Lack of analysis [SEP] rebuttal_concede-criticism [SEP] But we agree with you that intuitively, more diversity among agents leads to greater improvements.	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] 5. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.”	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] Please kindly refer to Appendix A for more detailed results.	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] 1. Residual modules	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] Following the reviewer’s suggestion, we have added a figure that shows the dynamics of neuromodulation in the cue-response task (Figure 3, in the Appendix).	1.0
"Lack of analysis [SEP] rebuttal_structuring [SEP] Re: ""- no qualitative analysis on how modulation is actually use by the systems."	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] This figure shows that while neuromodulation clearly reacts to reward, this reaction is complex and varies both within each episode and between runs.	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] ** Study on diversity of agents **	1.0
"Lack of analysis [SEP] rebuttal_structuring [SEP] E.g., when is modulation strong and when is it not used """	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] - Q: Algorithm applied layer-by-layer:	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] [The machine learned Oracle is assumed to be flawless at identifying the Heavy Hitters]	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] Q2: Linear discriminator and extending the analysis to realistic settings:	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] R: - FSM vs. Classification performance	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] 3. The authors should provide ablation study and analysis of their CTAugment.	1.0
"Lack of analysis [SEP] rebuttal_structuring [SEP] And the analysis of the ""dynamic range"" of the algorithim is missing."	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] Here are our responses to your concerns in “Cons” and “Minor comments”.	1.0
Lack of analysis [SEP] rebuttal_structuring [SEP] === Regarding to the empirical results/experiments ===	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] -- It is true that evaluating the FSM is not necessarily the same as the classification results, which is precisely the reason why we show both in our results.	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] In summary, our results hold even if the learned oracle makes prediction errors with probability O(1/ln(n)).	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] The analysis in the paper already takes into account errors in the machine learning oracle.	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] *See sec 5.1, the 3rd paragraph in P6 for analysis, and Table.1 for comparisons to handcrafted schedules*: we observe AutoLoss optimizes L1 whenever needed during the optimization.	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] - NMT	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] 4) We were a bit confused by what was meant by “practice” here.	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] - d-ary regression and MLP classification	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] - GANs	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] By contrast, linear combination objectives optimize both at each step while handcrafted schedules (e.g. S1-S3) optimize L1 strictly following the given schedule, ignoring the optimization status.	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] It can determine when to optimize G or D by being aware of the current optimization status (e.g. how G and D are balanced) using its parametric controller.	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] *See sec 5.1, the 3rd paragraph in P7 and Fig.3(M)*: we have explicitly visualized in Fig.3(M) the softmax output of a learned controller and explain in text: “...the controller meta-learns to up-weight the target NMT objective at later phase…resemble the “fine-tuning the target task” strategy...”.	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] We have provided substantial analysis and visualizations on what AutoLoss has learned in our *initial submission*. Below, we summarize them for your reference:	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] We believe AutoLoss manages to detect the potential risk of overfitting using designed features, and combat it by optimizing L1 only when necessary.	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] Actually, this is not the case.	1.0
Lack of analysis [SEP] rebuttal_reject-criticism [SEP] Per our observation, AutoLoss gives more flexible schedules than manually designed ones.	1.0
Lack of analysis [SEP] rebuttal_future [SEP] We agree that broader analysis beyond global-local disentanglement is desirable and we hope to perform more experiments in a follow up work.	1.0
Lack of analysis [SEP] rebuttal_future [SEP] It is definitely a good idea to analyse the correlation between changes in classification accuracy and in FSM values, thank you. We will rigorously investigate this in future work.	1.0
Lack of analysis [SEP] rebuttal_future [SEP] In future work we propose to use our method to generate a large dataset and evaluate its performance.	1.0
Lack of analysis [SEP] rebuttal_done [SEP] We updated the draft to include a longer treatment in the appendix.	1.0
Lack of analysis [SEP] rebuttal_done [SEP] We added a new section (6.2) and figure (Fig. 5) for these experiments.	1.0
Lack of analysis [SEP] rebuttal_done [SEP] We updated the discussion in Sec. 3.2 in the revision to make this clearer.	1.0
Lack of analysis [SEP] rebuttal_done [SEP] As suggested, we have added further analysis of failure cases.	1.0
Lack of analysis [SEP] rebuttal_done [SEP] Following your suggestion, we have therefore added experiments with both VGG and DenseNet, each trained with both SGD and Adam, on CIFAR100.	1.0
Lack of analysis [SEP] rebuttal_done [SEP] Fig. 2 (right) provides a diagram of the unregularized WGAN.	1.0
Lack of analysis [SEP] rebuttal_done [SEP] -> For more on this we refer to the newly added Section “Scope” in the revision.	1.0
Lack of analysis [SEP] rebuttal_done [SEP] In this part, we analyzed the dynamics of WGAN in the function space following [*1], i.e., we directly modeled $D(t, x)$ and $G(t, z)$ for all $x$ and $z$. It avoids the nonlinearity issue caused by the neural network, and both G and D are linear dynamics, at least locally around the equilibrium, as discussed in Sec. 3.2 and Appendix D in the revision.	1.0
Lack of analysis [SEP] rebuttal_done [SEP] 2. Following your suggestions, we add a study on the diversity of agents (presented in Appendix A of the updated paper).	1.0
Lack of analysis [SEP] rebuttal_done [SEP] A: As also discussed with reviewer 2, for space reasons we provided only a short description of CTAugment, and how it differs from AutoAugment.	1.0
Lack of analysis [SEP] rebuttal_done [SEP] R) New data was added to the paper exercising multiple topologies, in a wider range of applications.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_answer [SEP] For example, no matter how large the network is, if it is trained to recognize black and white digits, it will still struggle to recognize colored digits.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_answer [SEP] Indeed, we should remind that our adaptive homeostasis allows to be implemented by modifying the norm of each atom of the dictionary (as was done in the original work by Olshausen).	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_answer [SEP] However, we kept the OOD experiments on MNIST and notMNIST in the paper as well, since we consider it as very interesting that the CDN, while being designed for modelling aleatoric uncertainty, is very competitive on this task.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_answer [SEP] Our model and the baselines were tuned by a grid search process on a validation set for each dataset (whose size is given in the description of the datasets),	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_answer [SEP] Second, any benefit of a larger backbone network will likely also enhance the performance of our model.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_answer [SEP] Third, we just wanted to clarify (if there was a misunderstanding), unlike domain adaptation papers, we do not use a pretrained network – we train the full network from scratch (following traditional meta-training settings).	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_answer [SEP] It shows that CDNs are able to quantify the heteroscedastic aleatoric uncertainty.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_answer [SEP] Baseline models are trained on the same training set as our model following the methods proposed in their original paper.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_answer [SEP] although the best hyper-parameters obtained for Arti1 remained near optimal for the other ones.	1.0
Lack of realistic datasets used in experiments (small siz, synthetic) [SEP] rebuttal_answer [SEP] First, when networks are trained in one domain, and evaluated in another, regardless of the backbone network, it is the domain-shift that dominates the performance.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] For directly viewing the error manifolds decoupling the dependency on model and data size, see figure 1 and in appendix C.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Less crowded images could lead to many patches having no gradients (e.g. showing only the sky), leading to a failure of at least RotNet, if not also BiGAN on many samples of the augmented dataset.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Due to the novelty of this line of research, to our knowledge there are no other synset embeddings available than the ones we used, and we included both a 50d dense and a 300d dense version.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Fig 2. Left and Section 3.3 show that any initialization in the top left red region will lead (after a finite number of updates) to a confidence of 0.5 on the corresponding class.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] - Temporal ensembling & Π model [1]: CIFAR-10 (4k), SVHN (500, 1k), CIFAR-100 (10k)	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Minimum data needed for target error (if model size is not a limit):	1.0
Less datasets used [SEP] rebuttal_answer [SEP] $$n_{max}(T) = \left(1/bT\right)^{1/\alpha} m_{lim}^{\beta/\alpha} $$	1.0
Less datasets used [SEP] rebuttal_answer [SEP] The network does not provide correct classification at the end of training even though it does at the beginning.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Moreover, the calibration can target any dense mode for alleviation.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] We considered the traditional meta-learning for few-shot learning approaches, and combined meta-learning with a popular domain adaptation baseline.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] In general, we believe multi-modal data is more general than conventional image-text or video-text pairs.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] We can use them independently according to the feature types of data. And they can be used together for the data with mixed feature types.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] By unifying tabular data also as multi-modal (with each attribute as one modality), we show that VSAE provides us a principled way for imputation, capable of generalizing to more data families.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Therefore, we experimented with the popular setting.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Via careful dataset sub-sampling (as noted by reviewer 3) we show that indeed more data *is* needed to improve performance (reduce error) while holding the class distribution fixed (in expectation), for a given architecture and scaling policy.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] For example, our list contains four different meanings for the object named by the word “baton”, referring to (1) an item in relay races, (2) in twirling, (3) a weapon used by police, and (4) an item used by a musical conductor.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] [2]Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] For the present work, we focused on synset embeddings because they represent a closer match to the meaning of each individual object than word embeddings would and provide a one-to-one match for the meanings.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Specifically, we conducted experiments on two types of data:	1.0
Less datasets used [SEP] rebuttal_answer [SEP] The black-box calibration assumes no read/write to model weights or availability training data, but access to the sampling of random seed.	1.0
"Less datasets used [SEP] rebuttal_answer [SEP] [1] Laine, Samuli, and Timo Aila. ""Temporal ensembling for semi-supervised learning."" arXiv preprint arXiv:1610.02242 (2016)."	1.0
Less datasets used [SEP] rebuttal_answer [SEP] [3] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep	1.0
Less datasets used [SEP] rebuttal_answer [SEP] It has been designed so it is easy to quantify the diversity of generated curves and the fit between the distribution generated by the model and the real one.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] So retraining won't work for dense-mode alleviation.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] The black-box calibration is useful for both model user and API owner.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] The following is the experimented dataset in other papers.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Our proposed black-box calibration has an advantage over retraining with minimum time and energy cost and no touching training data.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] In such a situation, training data may no longer be accessible since it contains private information, e.g. human faces or person images.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Besides, we empirically validate that the dense mode is not caused by imbalanced data or randomness during initialization/optimization.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Retraining consumes much time and energy, especially for complex models trained on a huge dataset.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] In addition, we would have liked to include sparse positive synset embeddings as a reference, however those are currently not available; for that reason, we included NNSE word embeddings instead.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] This task, while simple, showcases the ability of Transformer to model a distribution over curves of similar shape to real training curves with varying speeds of convergence.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Our image choices were thus motivated by striving for simplicity and not further adding a pipeline that would, for example, extract only patches with sufficiently large image gradients.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] $$ n_{min} = \left(\frac{1}{\frac{\epsilon_{target}}{\epsilon_0}\eta-c_\infty}\right)^{1/\alpha} $$	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Black-box calibration enables the API owner to customize the model's sampling process to meet the users' needs.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] In particular, note that there is also a minimal amount of data and model size needed for better-than-random-guess error level, characterized by the location of the pole $\eta$: $n^{-\alpha}+bm^{-\beta}< \eta$	1.0
Less datasets used [SEP] rebuttal_answer [SEP] A : The purpose of our experiments is to show that the SST algorithm is comparable to the conventional SSL algorithms.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Model owner: We suppose that the dense mode happens to be close to a specific training image, thus violating privacy.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] For the ImageNet experiment, we used the code from [4].	1.0
Less datasets used [SEP] rebuttal_answer [SEP] API owner: For enterprise users having access to the face image generation service via cloud API, they are given the ping service for a huge number of times or not even restricted.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] The model owner would like to calibrate the model to alleviate the mode collapse.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] - Mean Teacher [3]: CIFAR-10 (1k, 2k, 4k), SVHN (250, 500, 1k), CIFAR-100 (10k)	1.0
Less datasets used [SEP] rebuttal_answer [SEP] For MNIST and ImageNet experiment, the whole dataset was used.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] Nevertheless, we wish to emphasize that even under Assumption (H2), learning can still fail.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] As stated in Section 2, D&W NNs and many related models can work well with high dimensional sparse features, which are usually in the form of one-hot encoding converted from categorical features.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] We hope this will underline the unique contribution of a behavior-based similarity embedding presented here.	1.0
Less datasets used [SEP] rebuttal_answer [SEP] - VAT [2] : CIFAR-10 (4k), SVHN (1k), CIFAR-100 (no experiment)	1.0
Lack of analysis of proposed method [SEP] rebuttal_answer [SEP] The main goal of our submission is to experimentally show that our pipelined training, using stale weights without weight stashing or micro-batching, is simpler and does converge.	1.0
Lack of analysis of proposed method [SEP] rebuttal_answer [SEP] In fact, unlike previous video compositional methods, even when local events are not well aligned or misclassified, long-term modelling with 4D convolution and video-level aggregation with global average pooling are very likely to correct the partial error.	1.0
Lack of analysis of proposed method [SEP] rebuttal_answer [SEP] The paper does achieve this goal, on a number of networks.	1.0
Lack of analysis of proposed method [SEP] rebuttal_answer [SEP] In the original version of the paper, all experiments are conducted on trimmed video classification datasets.	1.0
Lack of analysis of proposed method [SEP] rebuttal_answer [SEP] Multi-agent dual learning framework can achieve better quantitative results than the baselines.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] The randomized smoothing paper reports certified radii and also accuracy (1-error) under various perturbation bounds.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] In the case of solving factorization problem, we clamp some of the visible units to the integer we are trying to factor, and use gibbs sampling to get statistics for the remaining units conditioned on the output number.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] With suitable settings, the shift consistency of F-pooling is much better.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] We train on the joint density over inputs and outputs, and solving a problem amounts to clamping (conditioning) a subset of the units and sampling the remaining units via Gibbs Sampling.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] (A) In Fig. 1, the difference comes from the definition of node y^(i).	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] However, the CROWN-IBP paper and the improved randomized smoothing paper based on adversarial training of smoothed classifiers (SmoothAdv) only report *error rates* using a fixed distance to the decision boundary.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] We provide an example of the computation of compression ratio in Section 4.1, paragraph “Metrics”.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] Here, our use of “confidence” should not be confused with the confidence of a network (output of the softmax layer).	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] For the purpose of the certified radii, the “confidence” we are interested in is related to the prediction of the network on the Gaussian perturbed images (i.e., a very high “confident” example is an example where all of the perturbed images get the same label).	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] In the original submission, we tried to produce tables that look like the tables in papers that we compare	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] Yes, Lemma 1 shows that our deep GO will reduce to Rep when Rep is applicable.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] This happens because, for CIFAR-10, the smoothed classifier is very “confident” on a subset of the validation images which correspond to that right peak.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] Say we quantize a layer of size 128 × 128 × 3 × 3 with 256 centroids and a block size of 9.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] Finally, we have to store 256 centroids of dimension 9 in fp16, which represents 256 x 9 floats (fp16) = 256 x 9 x 2 = 4,608 bits = 4.5 kB.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] Finally, we deduce the overall compression ratio which is the size of the compressed model divided by the size of the non-compressed model.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] .	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] Then, each block of size 9 is indexed by an integer between 0 and 255: such integer can be stored using 8 bits or 1 byte (as 2^8 = 256).	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] The size of the compressed model is the sum of the sizes of the compressed layers.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] Yes, your understanding is correct.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] The memory footprint of a compressed layer is split between the indexing cost (one index per block indicating the centroid used to encode the block) and the cost of storing the centroids.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] to	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] (B) If you were interested in the difference in Fig. 2 (a)(b), the reasons include (1) the standard Rep cannot be applied to Gamma RVs; (2) both GRep and RSVI are designed to approximately reparametrize Gamma RVs; (3) GO generalizes Rep to non-reparameterizable RVs; or in other words, GO is identical to the exact Rep for Gamma RVs.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] For deterministic deep neural networks, node y^(i) is the activated value after an activation function, where deterministic chain rule can be readily applied; while for deep GO gradient, node y^(i) might be the sample of a non-reparameterizable RV, where deterministic chain rule is not applicable.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] This is in contrast to randomized smoothing, which outputs different radii for different images (a larger radius means a stronger certificate).	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] Let us detail it further here.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] Thus, as we have 128 x 128 blocks, the indexing cost is 128 x 128 x 1 byte = 16,384 bytes = 16 kB.	1.0
Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc) [SEP] rebuttal_answer [SEP] This is done because, unlike the Randomized Smoothing method, the radii are not directly calculated in the CROWN-IBP method and cannot be accessed directly;  CROWN-IBP takes a fixed radius chosen by the user, and either produces or fails to produce a certificate for that radius.	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_answer [SEP] We agree that there are other approaches that may offer different model quality vs. inference speed tradeoffs; we simply highlight that K-matrices are one promising method, especially given their important theoretical properties.	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_answer [SEP] Regarding the IWSLT translation result, the key claim we aim to validate is that the theoretical efficiency of K-matrices translates to practical speedups on real models as well.	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_answer [SEP] We also point out that our DynamicConv model with K-matrices in the decoder attains a comparable BLEU score with the state-of-the-art from two years ago – the Transformer model, which continues to enjoy widespread use today – while having over 60% higher sentence throughput and 30% fewer parameters than this model.	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_answer [SEP] In this paper, we propose the idea of guidance itself and show that it is imposing a desired semantics on the latent space without having labeled data.	1.0
In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. [SEP] rebuttal_answer [SEP] As for other neural network architectures, we chose the one used in the benchmarked methods.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Future improvements can be added by using:  (1) less lossy input in spherical projection methods (e.g. on top of SEF, and EDF, we can also add other statistic information such as the minimum distance of intersection, mean distance of intersections or standard deviation of the intersection and so on to reduce the information losses); (2) extend the spherical parameterization method on to the general 3D shapes.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Currently, the spherical parameterization method only works for genus-0 closed object.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] There is no ‘optimal’ constant lambda for all data and iterations.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Compared to SO(3) spherical convolution method (Cohen et al. 2018), our network is computationally efficient (in terms of network model), but we have to admit that this is at the price of required data augmentation	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] We considered the traditional meta-learning for few-shot learning approaches, and combined meta-learning with a popular domain adaptation baseline.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] The other two spherical convolution methods (Cohen et al 2018 and Esteves et al 2018) use a lat-lon grid and conduct convolution in the Fourier space, which has a common disadvantage: the Fourier transform does not support local spherical filters.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Generalization of spherical parameterization methods to objects with arbitrary topology will be one of the future work.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Again, our method outperform M3SDA in all datasets.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Another disadvantage is the use of lat-lon grid which introduces unevenness of the perception field (due to the high resolution near the poles, and low resolution at the equator).	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Q2: The experiments on SHREC17 show all three spherical methods under-performing other approaches.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] In Figure 6 of the revised version (Appendix B), our method also consistently performs better than different constant lambda values.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] This is because the value of lambda should be adapted to different data and optimization iterations.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] This is why the Levenberg-Marquardt algorithm is the standard choice for conventional bundle adjustment.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] We have further added experiments using even stronger query limit (previously 500000, now 50000) for the additional experiments on ResNet V2 model in the supplemental material. (We did not choose to use smaller epsilon because first, we already used a quite standard choice of epsilon, second, as you said, going for extremely small distortion does not really mean anything in adversarial context.) As you can see, in this even harder setting our proposed algorithm still maintain a performance lead over other baselines.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] It shows that CDNs are able to quantify the heteroscedastic aleatoric uncertainty.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] A2: The experiments on SHREC17 does show all three spherical methods slightly under-perform some other state-of-the art approaches, We believe this is mainly due to the information losses introduced in the spherical projection process which retains only the convex portion geometric information.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] This is because the objective function to be optimized is non-convex, and the vanilla Gauss-Newton method might get stuck at saddle point or local minimum.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] As expected, anisotropic filter perform  better than the isotropic filter proposed in Esteves et al 2018 which limits the model capacity.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] In Table 4 of the revised version (Appendix B), our method outperforms the Gauss-Newton algorithm in the last column.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Cohen et al 2018 and ours obtain similar performances because both of the methods used anisotropic filters, the former achieves rotational invariant using SO(3) rotations for filters, while ours achieves rotational invariant using alt-az rotation of filter and SO(2) rotation augmentation of input shapes.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Other non-spherical method (such as volumetric or multi-view based methods)  can only be generalized into unknown orientations using SO(3) rotation augmentations, their representation of 3D shapes are either too sparse (voxel model) or too redundant (multi-view projections).	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] The 3D models presented on ModelNet and Shrec’17 are of arbitrary genus which prevents us from using spherical parameterization method.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Another advantage is that our network allows local filters and local-to-global multi-level spherical features extraction.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] We ensure that all methods use the same backbone architecture for a fair comparison.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] We refer you to the experiment we have already included in Appendix titled “Detection & Identification results”, where we used the predicted permutations to identify the bounding boxes for similar looking objects across different test images	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Experimentally, we compared the three spherical convolution methods in Table 3 using Shrec’17 perturbed shape retrieval experiment.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Compared to non-spherical method, spherical image is one of the most compact representation for 3D shape analysis, the spherical convolution methods rely on no data augmentation (for Type I and Type II) or reduced data augmentation (for Type III, only SO(2) rotation augmentation is required).	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] However, we kept the OOD experiments on MNIST and notMNIST in the paper as well, since we consider it as very interesting that the CDN, while being designed for modelling aleatoric uncertainty, is very competitive on this task.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] Classifying the permutations provides the extra information about the structure of the problem, e.g. there exist a single order which matters or it can be several different orders or the problem is orderless.	1.0
More variations of experiments needs to be added [SEP] rebuttal_answer [SEP] We simply do not ignore the permutations from Hungarian by allowing the network to learn them.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] There are three main theorems in our paper: Theorem 8.4, Theorem 7.11 and 7.14.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] On the other hand, the proof of Theorem 8.4 uses entirely different techniques.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] - Residual modules are small neural networks (e.g., an MLP for Mvqa, Sec. 3.4, (4)) that a task module may use when other lower level modules are incapable of providing a solution to a given query.	1.0
"Lack of analysis [SEP] rebuttal_answer [SEP] We obtained distinct ""agents"" f_i and g_i through multiple independent runs with different random seeds and different input orders of the training samples."	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] >> Comment #2	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] In particular, it provides a characterization of the hash function optimized for a particular input.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] Therefore, Mvqa would make use of its residual module, which would essentially learn to “pick up” all queries that lower level modules cannot answer.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] We additionally note that for some tasks, such as NLI, creating new datasets already requires annotators to synthesize examples de novo and the fractional increase for soliciting counterfactually-augmented data might not be as onerous as compared to tasks where the default is to rely on annotators only for tags.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] In preliminary work, we have been investigating how to use humans in the loop more effectively.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] > Our goal is somewhat orthogonal to the multitask learning setting where all the tasks are jointly trained.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] We, instead, focus on how the task-specific information is present in popular sentence representations, and how this could be used to assess transfer potential among tasks.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] Re: (W5) Multi-task learning	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] A: See above, where we found that the experiment diverged in the “No weak aug.” ablation (using strong augmentations only).	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] On the other hand it is a first step towards a multilayer analysis and allows a localized layer-by-layer analysis for the first time.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] One approach involves using generative models to propose candidate substitutions and relying on humans only accept or reject the revisions (vs having to write them from scratch).	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] Our proofs of Theorem 7.11 and 7.14 are technically involved, even if the techniques are relatively standard.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] For example, consider the question “is this person going to be happy?” on an image of a person opening a present.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] We would like to note that beyond the gains across all evaluated quantitative metrics (bleu, rouge, meteor), our method shows substantial gains on human evaluations.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] Our experience with crowdsourcing suggests that this feedback would be significantly cheaper to collect (provided that a reasonable fraction of suggestions were appropriate).	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] Lower level modules of Mvqa may not be sufficient to solve the question.	1.0
Lack of analysis [SEP] rebuttal_answer [SEP] There are, of course, many other ways to introduce more diversity, including using different optimization strategies, or training with different subsets as you suggested.	1.0
Less datasets used [SEP] rebuttal_by-cr [SEP] Details about all experiments will be added to the appendix.	1.0
Less datasets used [SEP] rebuttal_by-cr [SEP] We will state this clearer in the paper.	1.0
Need more interesting problem definition [SEP] rebuttal_by-cr [SEP] => We will add a discussion of recent works that deal with navigation tasks in maze environments [Mirowski et. al. ICLR 2017, Jaderberg et. al. ICLR 2017] in the related works section.	1.0
Need more interesting problem definition [SEP] rebuttal_by-cr [SEP] We will also clarify that the little phrase “After initial experimentation, we opted for the simple proxy…” implies quite extensive experimentation with other plausible proxies that looked promising in individual environments but were not consistently effective across the suite of Atari games.	1.0
Need more interesting problem definition [SEP] rebuttal_by-cr [SEP] Comment 2:	1.0
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1). [SEP] rebuttal_by-cr [SEP] These results will be included in the new manuscript.	1.0
generalizability of results is questionable [SEP] rebuttal_by-cr [SEP] We will state this clearer in the paper.	1.0
generalizability of results is questionable [SEP] rebuttal_by-cr [SEP] We shall add these results and explanations to the final version of the paper.	1.0
generalizability of results is questionable [SEP] rebuttal_by-cr [SEP] We will include detailed results and a discussion in the final version of the paper.	1.0
generalizability of results is questionable [SEP] rebuttal_by-cr [SEP] As mentioned above, we will move it to appendices.	1.0
More variations of experiments needs to be added [SEP] rebuttal_by-cr [SEP] We promise to do so in the final version.	1.0
More variations of experiments needs to be added [SEP] rebuttal_by-cr [SEP] We would be happy to include some further experiments in the final version comparing HGD with other algorithms such as extragradient.	1.0
Lack of analysis [SEP] rebuttal_by-cr [SEP] We will revise the text to make it clearer.	1.0
Lack of analysis [SEP] rebuttal_by-cr [SEP] We will keep updating the paper and conducting more thorough experiments.	1.0
Lack of discussion of performance of method on different datasets [SEP] rebuttal_concede-criticism [SEP] As you mentioned, our solution requires significant expenditure (both financial and human capital)	1.0
Lack of discussion of performance of method on different datasets [SEP] rebuttal_concede-criticism [SEP] compared to simply labeling data	1.0
Lack of discussion of performance of method on different datasets [SEP] rebuttal_concede-criticism [SEP] .	1.0
Need more experimental results [SEP] rebuttal_concede-criticism [SEP] We agree that a more comprehensive study could be done in order to asses the viability of our method for ML tasks other than image classification.	1.0
Need more experimental results [SEP] rebuttal_concede-criticism [SEP] We agree that optimizing compression rates should not use the test set before the best compression scheme is selected.	1.0
Need more experimental results [SEP] rebuttal_concede-criticism [SEP] Thanks for pointing this out.	1.0
Need more experimental results [SEP] rebuttal_concede-criticism [SEP] We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true.	1.0
Experimental study not strong enough [SEP] rebuttal_concede-criticism [SEP] (5) Thanks for this valuable comment!	1.0
Experimental study not strong enough [SEP] rebuttal_concede-criticism [SEP] 4. Comparison to other Online DL algorithms — As correctly observed by the reviewer, the overall structure of NOODL is similar to successful online DL algorithms.	1.0
Experimental study not strong enough [SEP] rebuttal_concede-criticism [SEP] As veterans in performing subjective studies, we understand and agree with the reviewer that querying ground truth labels for a 200-class classification problem is difficult. That is exactly why we have carefully designed our subjective experiment.	1.0
Experimental study not strong enough [SEP] rebuttal_concede-criticism [SEP] Response #5: We agree that it is necessary to conduct experiments to compare RAN’s performance concerning privacy and accuracy with/without a different kind of layers so that we can back up the argument mentioned in Section 2.2.	1.0
More variations of experiments needs to be added [SEP] rebuttal_concede-criticism [SEP] (5) Thanks for this valuable comment!	1.0
More variations of experiments needs to be added [SEP] rebuttal_concede-criticism [SEP] Thank you for pointing out these datasets.	1.0
More variations of experiments needs to be added [SEP] rebuttal_concede-criticism [SEP] We acknowledge this problem and agree with you about a possible misinterpretation.	1.0
More variations of experiments needs to be added [SEP] rebuttal_concede-criticism [SEP] It is something we should have done on our own.	1.0
generalizability of results is questionable [SEP] rebuttal_contradict-assertion [SEP] These images are damaged so badly that TV cannot recover any meaningful thing.	1.0
generalizability of results is questionable [SEP] rebuttal_contradict-assertion [SEP] As a handcrafted prior, TV performs much worse than our data-driven baseline method in these tasks.	1.0
generalizability of results is questionable [SEP] rebuttal_contradict-assertion [SEP] A2: We think you underestimate the difficulty of those restoration problems.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_done [SEP] This point has also been emphasized in our paper.	1.0
Experimental study not strong enough [SEP] rebuttal_followup [SEP] We currently thinking about an experiment to better illustrate the intuition of our theory and would appreciate any suggestions.	1.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_followup [SEP] We kindly ask the reviewer to elaborate on the given statement.	1.0
Less datasets used [SEP] rebuttal_future [SEP] In the future, we would like to add sparse positive synset embeddings and test their interpretability relative to our similarity embedding.	1.0
Less datasets used [SEP] rebuttal_future [SEP] Also, using different datasets would help us demonstrate that the effect of the proposed mechanism is data-independent.	1.0
More variations of experiments needs to be added [SEP] rebuttal_future [SEP] We are also considering it's application to a different set of tasks in the future.	1.0
Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] Our work is primarily motivated by the question as to what extent neural networks learn about physical principles or whether they merely follow visual clues and how we can guide the learning process.	1.0
"Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] Remark 4. ""Combining SST with other existing techniques can help."	1.0
"Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] [1] Laine, Samuli, and Timo Aila. ""Temporal ensembling for semi-supervised learning."" arXiv preprint arXiv:1610.02242 (2016)."	1.0
Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] However, the additional cost is expensive.	1.0
Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] In particular, we demonstrate the efficacy of neural stethoscopes in interpreting, promoting and suppressing specific information in the context of the complex interplay between visual clues and physical properties in stability prediction.	1.0
Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] We showcase the stethoscope framework here to that effect and, based on its application, provide some interesting and novel [according to Reviewer 1] insights into representations for visual stability prediction.	1.0
Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] However, our method can reduce the effort of needing to group the data or use labelled data by instead thinking more about prior knowledge (transformation function) of the entire dataset.	1.0
"Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] [2] Miyato, Takeru, et al. ""Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning."" arXiv preprint arXiv:1704.03976 (2017)."	1.0
Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] However, SST can solve the real problem of the existence of out-of-class unlabeled data.	1.0
Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] Being a new problem setting, designing appropriate baselines can be challenging.	1.0
"Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] Further demonstrations are necessary for the proposed SST method."""	1.0
Less datasets used [SEP] rebuttal_mitigate-criticism [SEP] In Table 2 of our paper, SST achieves 34.89% on CIFAR-100, which is higher than TempEns[1](38.65%), 11.82% on CIFAR-10, which is slightly worse than VAT+EntMin[2](10.55%), and perform worse 6.88% on SVHN.	1.0
Need more interesting problem definition [SEP] rebuttal_mitigate-criticism [SEP] Think of our task roughly as learning to play a very hard game on a 9x3 board -- we would, of course, love to learn how to play the same game on arbitrary size boards, but the fact is that the game is mighty hard even at this “board size” (since in each round, one player plays a 0-1 assignment to each cell in the board, and the other player picks a subset of the columns, in fact a weight vector on the columns).	1.0
Need more interesting problem definition [SEP] rebuttal_mitigate-criticism [SEP] Instead of producing an algorithm that works for the 0-1 version of the problem, we produce an algorithm that works for the fractional version of the problem.	1.0
Need more interesting problem definition [SEP] rebuttal_mitigate-criticism [SEP] Our explorations indicated that producing an algorithm for the 0-1 version needs reinforcement learning, and producing an algorithm that works on all 9x3 instances using this approach would still take several days of computation.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_other [SEP] We are a little bit uncertain if we have well understood this request because our task is text to text translation while image captioning is image to text.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] Adam is essentially RMSprop with momentum; APO can be applied to Adam by applying momentum on top of the updates computed by APO.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] Again drawing the analogy of playing Go, the objective is mostly on training an agent that can make competitive moves rather than very fast moves, and there is no known “general-purpose” strategy to accomplish this.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] In case of one variable vector, our proof is to take the derivative of c on both sides of F(w) = F(cw), which is the definition of scale-invariance.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] by chain rule.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] that w’ * \nabla F(w)  = 0.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] While Adam and Adagrad are often described as having “adaptive learning rates,” they still have a global learning rate that is just as critical to tune as for SGD.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] Then the left-hand side becomes 0 and the right-hand side becomes w’ * \nabla F(cw)	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] The point of this work is only to see if ML can find optimal algorithms, and not about doing it faster than the known theoretical algorithms.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] Note that this is not similar to the case of solving an offline combinatorial problem via integer programming or other solvers, since our problems are online, i.e., the instance is not known beforehand, so there is no comparison to such “general-purpose” solvers.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] In our experiments, we consider tuning the learning rate for RMSprop, which also maintains adaptive learning rates for each parameter, and is closely related to Adam/Adagrad.	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] Taking c = 1, we can conclude	1.0
Incomplete details on perfromance of the method [SEP] rebuttal_refute-question [SEP] Thus we don't compare to the running time of offline solvers, but to the worst-case competitive ratio of the optimal online algorithms.	1.0
Experimental study not strong enough [SEP] rebuttal_refute-question [SEP] Again drawing the analogy of playing Go, the objective is mostly on training an agent that can make competitive moves rather than very fast moves, and there is no known “general-purpose” strategy to accomplish this.	1.0
Experimental study not strong enough [SEP] rebuttal_refute-question [SEP] We would like to clarify that we are not using the identity label as a proxy.	1.0
Experimental study not strong enough [SEP] rebuttal_refute-question [SEP] The crux of the proposed model is the selective proposal distribution.	1.0
Experimental study not strong enough [SEP] rebuttal_refute-question [SEP] The point of this work is only to see if ML can find optimal algorithms, and not about doing it faster than the known theoretical algorithms.	1.0
Experimental study not strong enough [SEP] rebuttal_refute-question [SEP] Note that this is not similar to the case of solving an offline combinatorial problem via integer programming or other solvers, since our problems are online, i.e., the instance is not known beforehand, so there is no comparison to such “general-purpose” solvers.	1.0
Experimental study not strong enough [SEP] rebuttal_refute-question [SEP] Instead, we are using the embedding features obtained from the neural network trained on the face recognition task.	1.0
Experimental study not strong enough [SEP] rebuttal_refute-question [SEP] Thus we don't compare to the running time of offline solvers, but to the worst-case competitive ratio of the optimal online algorithms.	1.0
Lack of ablation on different datasets/ sizes [SEP] rebuttal_reject-criticism [SEP] This is the reason why we have selected and used them for our experiments.	1.0
Lack of ablation on different datasets/ sizes [SEP] rebuttal_reject-criticism [SEP] To the best of our knowledge Freebase and DBPedia are the only standard KGs with numerical values [Garcia-Duran et al., 2018] used for the evaluation in state-of-the-art works.	1.0
Lack of ablation on different datasets/ sizes [SEP] rebuttal_reject-criticism [SEP] The impact of our approach might appear to be rather modest, since these KGs still have only a limited amount of numerical information.	1.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_reject-criticism [SEP] The theoretical result is tighter than the one in [Fujita and Maeda, 2018], and it supports general transformations instead of only clipped actions.	1.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_reject-criticism [SEP] Our method, on the other hand, decouples the problem of learning disentangled latent representations and high fidelity generation into two separate problems by introducing a hierarchical structure (sub-graph c-y) that is trained separate from the rest of the model.	1.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_reject-criticism [SEP] We politely point out that many conferences have entire dedicated tracks, and even best paper awards for resources, and that many seminal papers of pivotal importance to the field make precisely this sort of contribution (e.g. ImageNet).	1.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_reject-criticism [SEP] Our MPG framework not only supports the angular transformation but also covers the recently proposed clipped transformation in CAPG [Fujita and Maeda, 2018].	1.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_reject-criticism [SEP] While degrees of novelty and the relevant sorts of novelty are a matter of opinion we respectfully assert our view that new ideas, the new resource that we present, and the scientific insights derived from our experiments, are precisely the sorts of novelty that should be sought by conferences.	1.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_reject-criticism [SEP] This allows obtaining a posterior $p(c|y)p(z|x,y)p(y|x)$, which in fact guarantees the disentanglement of the factors c from z while preserving the generative strength of the model.	1.0
Lack of analysis of proposed method [SEP] rebuttal_social [SEP] Thanks for your suggestions.	1.0
Limited improvement over baselines [SEP] rebuttal_social [SEP] We thank you for pointing out that the bounded variance assumption may also be restrictive and only satisfied on bounded domains.	1.0
Limited improvement over baselines [SEP] rebuttal_social [SEP] If not, we are glad to address further.	1.0
Limited improvement over baselines [SEP] rebuttal_social [SEP] We thank the reviewer for the suggestion.	1.0
Limited improvement over baselines [SEP] rebuttal_social [SEP] Thank you for your suggestion.	1.0
Limited improvement over baselines [SEP] rebuttal_social [SEP] 1. About image captioning.	1.0
generalizability of results is questionable [SEP] rebuttal_social [SEP] Thank you for your suggestion of increasing the discussion of the results.	1.0
Less datasets used [SEP] rebuttal_social [SEP] We are grateful for your suggestions on the domain adaptation baselines, and fully agree that it is reasonable.	1.0
Less datasets used [SEP] rebuttal_social [SEP] If not, we are glad to address further.	1.0
Less datasets used [SEP] rebuttal_social [SEP] (Q1) Thank you very much for the suggestion.	1.0
Less datasets used [SEP] rebuttal_social [SEP] 1. About image captioning.	1.0
Less datasets used [SEP] rebuttal_social [SEP] Please let us know if we missed anything.	1.0
Experimental study not strong enough [SEP] rebuttal_social [SEP] We really appreciate that you brought up this point.	1.0
Experimental study not strong enough [SEP] rebuttal_social [SEP] We would appreciate further suggestions.	1.0
Experimental study not strong enough [SEP] rebuttal_social [SEP] A2: We thank the reviewer for the suggestion.	1.0
More variations of experiments needs to be added [SEP] rebuttal_social [SEP] We are grateful for your suggestions on the domain adaptation baselines, and fully agree that it is reasonable.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_structuring [SEP] Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_structuring [SEP] “The experimental results of section 5.2 are somewhat disappointing. Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.	1.0
Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc) [SEP] rebuttal_structuring [SEP] Response:	1.0
Lack of experiments [SEP] rebuttal_structuring [SEP] Q1: The experiments do not include any strong baseline	1.0
Lack of experiments [SEP] rebuttal_structuring [SEP] Specifically:	1.0
Lack of experiments [SEP] rebuttal_structuring [SEP] 4. Response:	1.0
Lack of experiments [SEP] rebuttal_structuring [SEP] “Simple baseline – combining a subset of a new domain as training set”	1.0
Lack of experiments [SEP] rebuttal_structuring [SEP] 4. Comment:	1.0
Incorrect claims about performance in tables and figures [SEP] rebuttal_structuring [SEP] Main comment 2:	1.0
Incorrect claims about performance in tables and figures [SEP] rebuttal_structuring [SEP] From the plots of learning curves in appendix, the proposed methods doesn’t seem to show a huge boost of performance comparing to the uniform bandit. Could you show aggregated comparison between the proposed method and uniform bandit similarly to what is done in Figure 4 ?	1.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_structuring [SEP] Question 5:	1.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_structuring [SEP] The reviewer observed that “authors need to highlight at least one practical advance introduced by the CW distance” and suggested the following potential options:	1.0
Lacking clarity overall (needs better presentation) [SEP] rebuttal_structuring [SEP] The paper would gain in clarity if its scope was narrowed.”	1.0
Less datasets used [SEP] rebuttal_summary [SEP] VSAE outperforms other baselines on multimodal datasets under partially-observed setting.	1.0
Less datasets used [SEP] rebuttal_summary [SEP] Each dataset contains two or three modalities.	1.0
Less datasets used [SEP] rebuttal_summary [SEP] We did not observe large effects for the shift transformations, although perhaps more hyperparameter tuning may improve these results.	1.0
Less datasets used [SEP] rebuttal_summary [SEP] As a result, the BLEU score is (33.55), which is comparable to the current lookup table (33.78) based on Multi30K, and outperforms the Trans. (base) (32.66).	1.0
Less datasets used [SEP] rebuttal_summary [SEP] Regarding the performance of the standard image captioning system, we train a caption model (Show, Attend, and Tell (Xu et al., 2015b)) with fine-tuned encoder (ResNet101) on the COCO dataset to encode the images.	1.0
Less datasets used [SEP] rebuttal_summary [SEP] The result on EN-RO is 33.58.	1.0
Less datasets used [SEP] rebuttal_summary [SEP] We believe that this addresses both the overfitting concern and the uncertainty estimation concern.	1.0
Less datasets used [SEP] rebuttal_summary [SEP] Moreover, we qualitatively observe some entanglement in the progressive gan transformations -- for example, changing the level of zoom also changes the lighting.	1.0
"Less datasets used [SEP] rebuttal_summary [SEP] (1) low-dimensional tabular data, and (2) high-dimensional data (pixel or text) as ""multimodal"" to better define the overall task of learning from partially-observed data."	1.0
Less datasets used [SEP] rebuttal_summary [SEP] As shown, VSAE consistently outperforms baseline models across the added experiments as well.	1.0
Less datasets used [SEP] rebuttal_summary [SEP] This points to the Stylegan w latent space being more “flexible” and generalizable for transformation, compared to the latent space of progressive GAN.	1.0
Less datasets used [SEP] rebuttal_summary [SEP] One interesting property of the progressive gan interpolations is that they take much longer to train to have a visual effect -- for example for color, we could obtain drastic color changes in Stylegan W latent space using as few as 2k samples, but with progressive gan, we used 60k samples and still did not obtain as strong of an effect.	1.0
Less datasets used [SEP] rebuttal_summary [SEP] Yes. Image captioning dataset is absolutely available for creating the lookup table.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] In our experiments we chose the scheme proposed by Paudice et al. 2018a, as it assumes a stronger model for the defender (as mentioned before), which, in our opinion helps to validate the effectiveness of pGAN to craft successful poisoning attacks even in cases where the defender is in control of a fraction of trusted (clean) data points.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] The detectability constraints included in our model prevents this defence to detect the generated poisoning points.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] Then, we have provided an empirical evaluation of pGAN against 4 different defence mechanisms both in MNIST and FMNIST, showing how our attack bypasses all of these defences.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] In the supplement we included the sensitivity analysis w.r.t. the parameter that controls the fraction of points to be discarded.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] In Figure 8 (left) we can observe that the defence performs worse than the outlier detector and that, when the algorithm is not under attack, the performance slightly decreases, as the algorithm is removing genuine data points that are significant for the training process.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] 2. We compared with various constant lambda values to see how the performance varies along with lambda.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] The PCA-based defence proposed by Rubinstein et al. 2009 (Antidote) is also not capable of mitigating pGAN attack.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] Note that we also fine-tune the network to make sure the features fit different lambda.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] 1. We retrained the whole pipeline with Gauss-Newton, to make sure the features are learned specifically for Gauss-Newton.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] We can observe that the error increases as we increase this threshold.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] First in Figure 2 we show the effect of the attack for different values of alpha tested against the outlier-detection-based defence.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] In summary, the revised paper (see the new version uploaded) now provides a comprehensive comparison of different defence mechanisms and shows the effectiveness of pGAN to bypass all of them.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] As pGAN produces poisoning points that are correlated, the KNN-based algorithm proposed to do the relabelling is not capable of detecting the poisoning points.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] The “Sever” defence (Diakonikolas et al. 2019 ICML) is also not robust against pGAN attack.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] Moreover, some of the genuine points from the target class are incorrectly relabelled, making the problem even worse.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] We can observe that, in this case, the difference in performance is not significant for the different values explored for this threshold.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] Different outlier-detection-based defences have already been proposed in the literature, such as Steinhardt et al. 2017 (“Certified defenses for data poisoning attacks”), Koh et al. 2018 (“Stronger data poisoning attacks break data sanitization defenses”) or Paudice et al. 2018a, to cite some.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] For FMNIST, Sever outperforms the outlier detector when the number of poisoning points is reduced, although the degradation of the algorithm as we increase the fraction of poisoning points is faster compared to the outlier detector and the PCA-based defence.	1.0
More variations of experiments needs to be added [SEP] rebuttal_summary [SEP] Label sanitization (as proposed in Paudice et al. 2018b) completely fails to defend against pGAN attack, as shown in Figure 8 (right).	1.0
Need more experimental results [SEP] rebuttal_reject-request [SEP] We already comprehensively compare with the prior non-demonstration state-of-the-art, which use a comparable amount of prior knowledge, in each game.	1.0
Need more experimental results [SEP] rebuttal_reject-request [SEP] Renting the appropriate equipment (e.g., via Google Cloud) to run a single seed to completion costs ~$1,500.	1.0
Need more experimental results [SEP] rebuttal_reject-request [SEP] Since we already compare with the prior state-of-the-art approaches, and other approaches perform significantly worse than the prior state-of-the-art approaches, we do not compare with the many other deep RL approaches.	1.0
Need more experimental results [SEP] rebuttal_reject-request [SEP] We note that running 10 seeds would approximately cost $30,000 per additional game in compute.	1.0
Need more experimental results [SEP] rebuttal_reject-request [SEP] To run 20 seeds (10 for our approach, 10 for the prior state-of-the-art) would cost 20 x $1,500 = $30,000 or roughly the median US annual salary.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-request [SEP] Therefore, we did not conduct any experiment on data with high-cardinality categorical features.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-request [SEP] However, the train and test sets cannot be mixed as they come from different data distributions (P_r and P_p) and we are trying to show we can transfer with no retraining from P_r to P_p.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-request [SEP] These 2 shortages make GBDT very hard to be used in many real-world scenarios.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-request [SEP] For example, in an online recommender system, we need to update the model frequently to achieve the satisfying real-time performance.	1.0
generalizability of results is questionable [SEP] rebuttal_reject-request [SEP] In this case, GBDT will be very inefficient as it needs to be re-trained from scratch.	1.0
